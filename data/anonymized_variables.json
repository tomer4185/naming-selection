{
  "iterative_dvc": [
    {
      "file_path": "analytics.py",
      "code": "def send(path):\n    \"\"\"\n    Side effect: Removes the report after sending it.\n\n    The report is generated and stored in a temporary file, see:\n    `collect_and_send_report`. Sending happens on another process,\n    thus, the need of removing such file afterwards.\n    \"\"\"\n    import requests\n\n",
      "variables": [
        "path"
      ],
      "anonymized_code": "def send(var_1):\n    \"\"\"\n    Side effect: Removes the report after sending it.\n\n    The report is generated and stored in a temporary file, see:\n    `collect_and_send_report`. Sending happens on another process,\n    thus, the need of removing such file afterwards.\n    \"\"\"\n    import requests\n\n",
      "lines_processed": 10,
      "total_lines": 164
    },
    {
      "file_path": "analytics.py",
      "code": "def _runtime_info():\n    \"\"\"\n    Gather information from the environment where DVC runs to fill a report.\n    \"\"\"\n    from iterative_telemetry import _generate_ci_id, find_or_create_user_id\n\n    from dvc import __version__\n    from dvc.utils import is_binary\n\n    ci_id = _generate_ci_id()\n",
      "variables": [
        "ci_id"
      ],
      "anonymized_code": "def _runtime_info():\n    \"\"\"\n    Gather information from the environment where DVC runs to fill a report.\n    \"\"\"\n    from iterative_telemetry import _generate_ci_id, find_or_create_user_id\n\n    from dvc import __version__\n    from dvc.utils import is_binary\n\n    var_1 = _generate_ci_id()\n",
      "lines_processed": 10,
      "total_lines": 164
    },
    {
      "file_path": "analytics.py",
      "code": "def _system_info():\n    import platform\n    import sys\n\n    import distro\n\n    system = platform.system()\n\n    if system == \"Windows\":\n        version = sys.getwindowsversion()  # type: ignore[attr-defined]\n",
      "variables": [
        "system",
        "version"
      ],
      "anonymized_code": "def _system_info():\n    import platform\n    import sys\n\n    import distro\n\n    var_1 = platform.var_1()\n\n    if var_1 == \"Windows\":\n        var_2 = sys.getwindowsversion()  # type: ignore[attr-defined]\n",
      "lines_processed": 10,
      "total_lines": 164
    },
    {
      "file_path": "experiments.py",
      "code": "def _postprocess(exp_rows):\n    for exp_row in exp_rows:\n        for k, v in exp_row.items():\n            if isinstance(v, Text):\n                v_str = str(v)\n                try:\n                    exp_row[k] = float(v_str)\n                except ValueError:\n                    exp_row[k] = v_str\n\n",
      "variables": [
        "exp_rows",
        "exp_row",
        "k",
        "v",
        "v_str"
      ],
      "anonymized_code": "def _postprocess(var_1):\n    for var_2 in var_1:\n        for var_3, var_4 in var_2.items():\n            if isinstance(var_4, Text):\n                var_5 = str(var_4)\n                try:\n                    var_2[var_3] = float(var_5)\n                except ValueError:\n                    var_2[var_3] = var_5\n\n",
      "lines_processed": 10,
      "total_lines": 120
    },
    {
      "file_path": "_debug.py",
      "code": "def yappi_profile(\n    path: Optional[Union[Callable[[], str], str]] = None,\n    wall_clock: Optional[bool] = True,\n    separate_threads: Optional[bool] = False,\n):\n    try:\n        import yappi\n    except ImportError:\n        print(\"Failed to run profiler, yappi is not installed\")  # noqa: T201\n        yield\n",
      "variables": [
        "path",
        "wall_clock",
        "separate_threads"
      ],
      "anonymized_code": "def yappi_profile(\n    var_1: Optional[Union[Callable[[], str], str]] = None,\n    var_2: Optional[bool] = True,\n    var_3: Optional[bool] = False,\n):\n    try:\n        import yappi\n    except ImportError:\n        print(\"Failed to run profiler, yappi is not installed\")  # noqa: T201\n        yield\n",
      "lines_processed": 10,
      "total_lines": 288
    },
    {
      "file_path": "_debug.py",
      "code": "def _get_path_func(tool: str, ext: str):\n    fmt = f\"{tool}.dvc-{{now:%Y%m%d}}_{{now:%H%M%S}}.{ext}\"\n\n    def func(now: Optional[\"datetime\"] = None) -> str:\n        return fmt.format(now=now or datetime.now())  # noqa: DTZ005\n\n    return func\n",
      "variables": [
        "tool",
        "ext",
        "fmt",
        "now"
      ],
      "anonymized_code": "def _get_path_func(var_1: str, var_2: str):\n    var_3 = f\"{var_1}.dvc-{{var_4:%Y%m%d}}_{{var_4:%H%M%S}}.{var_2}\"\n\n    def func(var_4: Optional[\"datetime\"] = None) -> str:\n        return var_3.format(var_4=var_4 or datetime.var_4())  # noqa: DTZ005\n\n    return func\n",
      "lines_processed": 7,
      "total_lines": 288
    },
    {
      "file_path": "_debug.py",
      "code": "def viztracer_profile(\n    path: Union[Callable[[], str], str],\n    depth: int = -1,\n    log_async: bool = False,\n):\n    try:\n        import viztracer\n    except ImportError:\n        print(\"Failed to run profiler, viztracer is not installed\")  # noqa: T201\n        yield\n",
      "variables": [
        "path",
        "depth",
        "log_async"
      ],
      "anonymized_code": "def viztracer_profile(\n    var_1: Union[Callable[[], str], str],\n    var_2: int = -1,\n    var_3: bool = False,\n):\n    try:\n        import viztracer\n    except ImportError:\n        print(\"Failed to run profiler, viztracer is not installed\")  # noqa: T201\n        yield\n",
      "lines_processed": 10,
      "total_lines": 288
    }
  ],
  "facebookresearch_detectron2": [
    {
      "file_path": "torchvision_imagenet_R_50.py",
      "code": "def build_data_loader(dataset, batch_size, num_workers, training=True):\n    return torch.utils.data.DataLoader(\n        dataset,\n        sampler=(TrainingSampler if training else InferenceSampler)(len(dataset)),\n        batch_size=batch_size,\n        num_workers=num_workers,\n        pin_memory=True,\n    )\n",
      "variables": [
        "dataset",
        "batch_size",
        "num_workers",
        "training"
      ],
      "anonymized_code": "def build_data_loader(var_1, var_2, var_3, var_4=True):\n    return torch.utils.data.DataLoader(\n        var_1,\n        sampler=(TrainingSampler if var_4 else InferenceSampler)(len(var_1)),\n        var_2=var_2,\n        var_3=var_3,\n        pin_memory=True,\n    )\n",
      "lines_processed": 8,
      "total_lines": 149
    }
  ],
  "pytorch_pytorch": [
    {
      "file_path": "smoke_test.py",
      "code": "def cudnn_to_version_str(cudnn_version: int) -> str:\n    patch = int(cudnn_version % 10)\n    minor = int((cudnn_version / 100) % 100)\n    major = int((cudnn_version / 10000) % 10000)\n    return f\"{major}.{minor}.{patch}\"\n",
      "variables": [
        "cudnn_version",
        "patch",
        "minor",
        "major"
      ],
      "anonymized_code": "def cudnn_to_version_str(var_1: int) -> str:\n    var_2 = int(var_1 % 10)\n    var_3 = int((var_1 / 100) % 100)\n    var_4 = int((var_1 / 10000) % 10000)\n    return f\"{var_4}.{var_3}.{var_2}\"\n",
      "lines_processed": 5,
      "total_lines": 484
    },
    {
      "file_path": "smoke_test.py",
      "code": "def test_numpy():\n    try:\n        import numpy as np\n\n        x = np.arange(5)\n        torch.tensor(x)\n    except ImportError:\n        print(\"Numpy check skipped. Numpy is not installed.\")\n",
      "variables": [
        "x"
      ],
      "anonymized_code": "def test_numpy():\n    try:\n        import numpy as np\n\n        var_1 = np.arange(5)\n        torch.tensor(var_1)\n    except ImportError:\n        print(\"Numpy check skipped. Numpy is not installed.\")\n",
      "lines_processed": 8,
      "total_lines": 484
    },
    {
      "file_path": "smoke_test.py",
      "code": "def find_pypi_package_version(package: str) -> Optional[str]:\n    from importlib import metadata\n\n    dists = metadata.distributions()\n    for dist in dists:\n        if dist.metadata[\"Name\"].startswith(package):\n            return dist.version\n    return None\n",
      "variables": [
        "package",
        "dists",
        "dist"
      ],
      "anonymized_code": "def find_pypi_package_version(var_1: str) -> Optional[str]:\n    from importlib import metadata\n\n    var_2 = metadata.distributions()\n    for var_3 in var_2:\n        if var_3.metadata[\"Name\"].startswith(var_1):\n            return var_3.version\n    return None\n",
      "lines_processed": 8,
      "total_lines": 484
    },
    {
      "file_path": "smoke_test.py",
      "code": "def test_linalg(device=\"cpu\") -> None:\n    print(f\"Testing smoke_test_linalg on {device}\")\n    A = torch.randn(5, 3, device=device)\n    U, S, Vh = torch.linalg.svd(A, full_matrices=False)\n    assert (\n        U.shape == A.shape\n        and S.shape == torch.Size([3])\n        and Vh.shape == torch.Size([3, 3])\n    )\n    torch.dist(A, U @ torch.diag(S) @ Vh)\n",
      "variables": [
        "device",
        "A",
        "U",
        "S",
        "Vh"
      ],
      "anonymized_code": "def test_linalg(var_1=\"cpu\") -> None:\n    print(f\"Testing smoke_test_linalg on {var_1}\")\n    var_2 = torch.randn(5, 3, var_1=var_1)\n    var_3, var_4, var_5 = torch.linalg.svd(var_2, full_matrices=False)\n    assert (\n        var_3.shape == var_2.shape\n        and var_4.shape == torch.Size([3])\n        and var_5.shape == torch.Size([3, 3])\n    )\n    torch.dist(var_2, var_3 @ torch.diag(var_4) @ var_5)\n",
      "lines_processed": 10,
      "total_lines": 484
    },
    {
      "file_path": "smoke_test.py",
      "code": "def load_json_from_basedir(filename: str):\n    try:\n        with open(BASE_DIR / filename) as fptr:\n            return json.load(fptr)\n    except FileNotFoundError as exc:\n        raise ImportError(f\"File {filename} not found error: {exc.strerror}\") from exc\n    except json.JSONDecodeError as exc:\n        raise ImportError(f\"Invalid JSON {filename}\") from exc\n",
      "variables": [
        "filename",
        "fptr"
      ],
      "anonymized_code": "def load_json_from_basedir(var_1: str):\n    try:\n        with open(BASE_DIR / var_1) as var_2:\n            return json.load(var_2)\n    except FileNotFoundError as exc:\n        raise ImportError(f\"File {var_1} not found error: {exc.strerror}\") from exc\n    except json.JSONDecodeError as exc:\n        raise ImportError(f\"Invalid JSON {var_1}\") from exc\n",
      "lines_processed": 8,
      "total_lines": 484
    },
    {
      "file_path": "check_binary_symbols.py",
      "code": "def _apply_libtorch_symbols(symbols):\n    return [\n        re.compile(f\"{x}.*{y}\")\n        for (x, y) in itertools.product(LIBTORCH_NAMESPACE_LIST, symbols)\n    ]\n",
      "variables": [
        "symbols",
        "x",
        "y"
      ],
      "anonymized_code": "def _apply_libtorch_symbols(var_1):\n    return [\n        re.compile(f\"{var_2}.*{var_3}\")\n        for (var_2, var_3) in itertools.product(LIBTORCH_NAMESPACE_LIST, var_1)\n    ]\n",
      "lines_processed": 5,
      "total_lines": 113
    },
    {
      "file_path": "check_binary_symbols.py",
      "code": "def main() -> None:\n    if \"install_root\" in os.environ:\n        install_root = Path(os.getenv(\"install_root\"))  # noqa: SIM112\n    else:\n        if os.getenv(\"PACKAGE_TYPE\") == \"libtorch\":\n            install_root = Path(os.getcwd())\n        else:\n            install_root = Path(distutils.sysconfig.get_python_lib()) / \"torch\"\n\n    libtorch_cpu_path = str(install_root / \"lib\" / \"libtorch_cpu.so\")\n",
      "variables": [
        "install_root",
        "libtorch_cpu_path"
      ],
      "anonymized_code": "def main() -> None:\n    if \"var_1\" in os.environ:\n        var_1 = Path(os.getenv(\"var_1\"))  # noqa: SIM112\n    else:\n        if os.getenv(\"PACKAGE_TYPE\") == \"libtorch\":\n            var_1 = Path(os.getcwd())\n        else:\n            var_1 = Path(distutils.sysconfig.get_python_lib()) / \"torch\"\n\n    var_2 = str(var_1 / \"lib\" / \"libtorch_cpu.so\")\n",
      "lines_processed": 10,
      "total_lines": 113
    },
    {
      "file_path": "check_binary_symbols.py",
      "code": "def get_symbols(lib: str) -> list[tuple[str, str, str]]:\n    from subprocess import check_output\n\n    lines = check_output(f'nm \"{lib}\"|c++filt', shell=True)\n    return [x.split(\" \", 2) for x in lines.decode(\"latin1\").split(\"\\n\")[:-1]]\n",
      "variables": [
        "lib",
        "lines",
        "x"
      ],
      "anonymized_code": "def get_symbols(var_1: str) -> list[tuple[str, str, str]]:\n    from subprocess import check_output\n\n    var_2 = check_output(f'nm \"{var_1}\"|c++filt', shell=True)\n    return [var_3.split(\" \", 2) for var_3 in var_2.decode(\"latin1\").split(\"\\n\")[:-1]]\n",
      "lines_processed": 5,
      "total_lines": 113
    },
    {
      "file_path": "check_binary_symbols.py",
      "code": "def grep_symbols(lib: str, patterns: list[Any]) -> list[str]:\n    def _grep_symbols(\n        symbols: list[tuple[str, str, str]], patterns: list[Any]\n    ) -> list[str]:\n        rc = []\n        for _s_addr, _s_type, s_name in symbols:\n            for pattern in patterns:\n                if pattern.match(s_name):\n                    rc.append(s_name)\n                    continue\n",
      "variables": [
        "lib",
        "patterns",
        "symbols",
        "rc",
        "_s_addr",
        "_s_type",
        "s_name",
        "pattern"
      ],
      "anonymized_code": "def grep_symbols(var_1: str, var_2: list[Any]) -> list[str]:\n    def _grep_symbols(\n        var_3: list[tuple[str, str, str]], var_2: list[Any]\n    ) -> list[str]:\n        var_4 = []\n        for var_5, var_6, var_7 in var_3:\n            for var_8 in var_2:\n                if var_8.match(var_7):\n                    var_4.append(var_7)\n                    continue\n",
      "lines_processed": 10,
      "total_lines": 113
    },
    {
      "file_path": "max_autotune.py",
      "code": "def main():\n    # Training settings\n    parser = argparse.ArgumentParser(description=\"PyTorch MNIST Example\")\n    parser.add_argument(\n        \"--batch-size\",\n        type=int,\n        default=64,\n        metavar=\"N\",\n        help=\"input batch size for training (default: 64)\",\n    )\n",
      "variables": [
        "parser"
      ],
      "anonymized_code": "def main():\n    # Training settings\n    var_1 = argparse.ArgumentParser(description=\"PyTorch MNIST Example\")\n    var_1.add_argument(\n        \"--batch-size\",\n        type=int,\n        default=64,\n        metavar=\"N\",\n        help=\"input batch size for training (default: 64)\",\n    )\n",
      "lines_processed": 10,
      "total_lines": 209
    },
    {
      "file_path": "max_autotune.py",
      "code": "def timed(fn):\n    start = torch.cuda.Event(enable_timing=True)\n    end = torch.cuda.Event(enable_timing=True)\n    start.record()\n    result = fn()\n    end.record()\n    torch.cuda.synchronize()\n    return result, start.elapsed_time(end) / 1000\n",
      "variables": [
        "fn",
        "start",
        "end",
        "result"
      ],
      "anonymized_code": "def timed(var_1):\n    var_2 = torch.cuda.Event(enable_timing=True)\n    var_3 = torch.cuda.Event(enable_timing=True)\n    var_2.record()\n    var_4 = var_1()\n    var_3.record()\n    torch.cuda.synchronize()\n    return var_4, var_2.elapsed_time(var_3) / 1000\n",
      "lines_processed": 8,
      "total_lines": 209
    },
    {
      "file_path": "normalize_yaml_fragment.py",
      "code": "def regurgitate(depth, use_pyyaml_formatter=False):\n    data = yaml.safe_load(sys.stdin)\n\n    if use_pyyaml_formatter:\n        output = yaml.dump(data, sort_keys=True)\n        sys.stdout.write(output)\n    else:\n        miniyaml.render(sys.stdout, data, depth)\n",
      "variables": [
        "depth",
        "use_pyyaml_formatter",
        "data",
        "output"
      ],
      "anonymized_code": "def regurgitate(var_1, var_2=False):\n    var_3 = yaml.safe_load(sys.stdin)\n\n    if var_2:\n        var_4 = yaml.dump(var_3, sort_keys=True)\n        sys.stdout.write(var_4)\n    else:\n        miniyaml.render(sys.stdout, var_3, var_1)\n",
      "lines_processed": 8,
      "total_lines": 26
    },
    {
      "file_path": "trigger_azure_pipeline.py",
      "code": "def get_build(_id):\n    get_build_url = (\n        AZURE_PIPELINE_BASE_URL + f\"/_apis/build/builds/{_id}?api-version=6.0\"\n    )\n    get_build_raw = s.get(get_build_url)\n    return get_build_raw.json()\n",
      "variables": [
        "_id",
        "get_build_url",
        "get_build_raw"
      ],
      "anonymized_code": "def get_build(var_1):\n    var_2 = (\n        AZURE_PIPELINE_BASE_URL + f\"/_apis/build/builds/{var_1}?api-version=6.0\"\n    )\n    var_3 = s.get(var_2)\n    return var_3.json()\n",
      "lines_processed": 6,
      "total_lines": 157
    },
    {
      "file_path": "trigger_azure_pipeline.py",
      "code": "def get_build_logs(_id):\n    get_build_logs_url = (\n        AZURE_PIPELINE_BASE_URL + f\"/_apis/build/builds/{_id}/logs?api-version=6.0\"\n    )\n    get_build_logs_raw = s.get(get_build_logs_url)\n    return get_build_logs_raw.json()\n",
      "variables": [
        "_id",
        "get_build_logs_url",
        "get_build_logs_raw"
      ],
      "anonymized_code": "def get_build_logs(var_1):\n    var_2 = (\n        AZURE_PIPELINE_BASE_URL + f\"/_apis/build/builds/{var_1}/logs?api-version=6.0\"\n    )\n    var_3 = s.get(var_2)\n    return var_3.json()\n",
      "lines_processed": 6,
      "total_lines": 157
    },
    {
      "file_path": "trigger_azure_pipeline.py",
      "code": "def get_log_content(url):\n    resp = s.get(url)\n    return resp.text\n",
      "variables": [
        "url",
        "resp"
      ],
      "anonymized_code": "def get_log_content(var_1):\n    var_2 = s.get(var_1)\n    return var_2.text\n",
      "lines_processed": 3,
      "total_lines": 157
    },
    {
      "file_path": "check_gomp.py",
      "code": "def main():\n    omp_max_threads = get_gomp_thread()\n    print(\n        f\"omp_max_threads after loading libgomp.so and libtorch_cpu.so: {omp_max_threads}\"\n    )\n    if omp_max_threads == 1:\n        raise RuntimeError(\n            \"omp_max_threads is 1. Check whether libgomp.so is loaded twice.\"\n        )\n",
      "variables": [
        "omp_max_threads"
      ],
      "anonymized_code": "def main():\n    var_1 = get_gomp_thread()\n    print(\n        f\"var_1 after loading libgomp.so and libtorch_cpu.so: {var_1}\"\n    )\n    if var_1 == 1:\n        raise RuntimeError(\n            \"var_1 is 1. Check whether libgomp.so is loaded twice.\"\n        )\n",
      "lines_processed": 9,
      "total_lines": 74
    },
    {
      "file_path": "build_aarch64_wheel.py",
      "code": "def embed_libgomp(host: RemoteHost, use_conda, wheel_name) -> None:\n    host.run_cmd(\"pip3 install auditwheel\")\n    host.run_cmd(\n        \"conda install -y patchelf\" if use_conda else \"sudo apt-get install -y patchelf\"\n    )\n    from tempfile import NamedTemporaryFile\n\n    with NamedTemporaryFile() as tmp:\n        tmp.write(embed_library_script.encode(\"utf-8\"))\n        tmp.flush()\n",
      "variables": [
        "host",
        "use_conda",
        "wheel_name",
        "tmp"
      ],
      "anonymized_code": "def embed_libgomp(var_1: RemoteHost, var_2, var_3) -> None:\n    var_1.run_cmd(\"pip3 install auditwheel\")\n    var_1.run_cmd(\n        \"conda install -y patchelf\" if var_2 else \"sudo apt-get install -y patchelf\"\n    )\n    from tempfile import NamedTemporaryFile\n\n    with NamedTemporaryFile() as var_4:\n        var_4.write(embed_library_script.encode(\"utf-8\"))\n        var_4.flush()\n",
      "lines_processed": 10,
      "total_lines": 1037
    },
    {
      "file_path": "build_aarch64_wheel.py",
      "code": "def build_OpenBLAS(host: RemoteHost, git_clone_flags: str = \"\") -> None:\n    print(\"Building OpenBLAS\")\n    host.run_cmd(\n        f\"git clone https://github.com/xianyi/OpenBLAS -b v0.3.28 {git_clone_flags}\"\n    )\n    make_flags = \"NUM_THREADS=64 USE_OPENMP=1 NO_SHARED=1 DYNAMIC_ARCH=1 TARGET=ARMV8\"\n    host.run_cmd(\n        f\"pushd OpenBLAS && make {make_flags} -j8 && sudo make {make_flags} install && popd && rm -rf OpenBLAS\"\n    )\n",
      "variables": [
        "host",
        "git_clone_flags",
        "make_flags"
      ],
      "anonymized_code": "def build_OpenBLAS(var_1: RemoteHost, var_2: str = \"\") -> None:\n    print(\"Building OpenBLAS\")\n    var_1.run_cmd(\n        f\"git clone https://github.com/xianyi/OpenBLAS -b v0.3.28 {var_2}\"\n    )\n    var_3 = \"NUM_THREADS=64 USE_OPENMP=1 NO_SHARED=1 DYNAMIC_ARCH=1 TARGET=ARMV8\"\n    var_1.run_cmd(\n        f\"pushd OpenBLAS && make {var_3} -j8 && sudo make {var_3} install && popd && rm -rf OpenBLAS\"\n    )\n",
      "lines_processed": 9,
      "total_lines": 1037
    },
    {
      "file_path": "build_aarch64_wheel.py",
      "code": "def configure_system(\n    host: RemoteHost,\n    *,\n    compiler: str = \"gcc-8\",\n    use_conda: bool = True,\n    python_version: str = \"3.8\",\n) -> None:\n    if use_conda:\n        install_condaforge_python(host, python_version)\n\n",
      "variables": [
        "host",
        "compiler",
        "use_conda",
        "python_version"
      ],
      "anonymized_code": "def configure_system(\n    var_1: RemoteHost,\n    *,\n    var_2: str = \"gcc-8\",\n    var_3: bool = True,\n    var_4: str = \"3.8\",\n) -> None:\n    if var_3:\n        install_condaforge_python(var_1, var_4)\n\n",
      "lines_processed": 10,
      "total_lines": 1037
    },
    {
      "file_path": "aarch64_wheel_ci_build.py",
      "code": "def parse_arguments():\n    \"\"\"\n    Parse inline arguments\n    \"\"\"\n    from argparse import ArgumentParser\n\n    parser = ArgumentParser(\"AARCH64 wheels python CD\")\n    parser.add_argument(\"--debug\", action=\"store_true\")\n    parser.add_argument(\"--build-only\", action=\"store_true\")\n    parser.add_argument(\"--test-only\", type=str)\n",
      "variables": [
        "parser"
      ],
      "anonymized_code": "def parse_arguments():\n    \"\"\"\n    Parse inline arguments\n    \"\"\"\n    from argparse import ArgumentParser\n\n    var_1 = ArgumentParser(\"AARCH64 wheels python CD\")\n    var_1.add_argument(\"--debug\", action=\"store_true\")\n    var_1.add_argument(\"--build-only\", action=\"store_true\")\n    var_1.add_argument(\"--test-only\", type=str)\n",
      "lines_processed": 10,
      "total_lines": 259
    },
    {
      "file_path": "aarch64_wheel_ci_build.py",
      "code": "def list_dir(path: str) -> list[str]:\n    \"\"\"'\n    Helper for getting paths for Python\n    \"\"\"\n    return check_output([\"ls\", \"-1\", path]).decode().split(\"\\n\")\n",
      "variables": [
        "path"
      ],
      "anonymized_code": "def list_dir(var_1: str) -> list[str]:\n    \"\"\"'\n    Helper for getting paths for Python\n    \"\"\"\n    return check_output([\"ls\", \"-1\", var_1]).decode().split(\"\\n\")\n",
      "lines_processed": 5,
      "total_lines": 259
    },
    {
      "file_path": "reuse_old_whl.py",
      "code": "def get_workflow_id(run_id: str) -> Optional[str]:\n    # Get the workflow ID that corresponds to the file for the run ID\n    url = f\"https://api.github.com/repos/pytorch/pytorch/actions/runs/{run_id}\"\n    response = query_github_api(url)\n    if \"workflow_id\" in response:\n        print(f\"Found workflow ID for run ID {run_id}: {response['workflow_id']}\")\n        return cast(str, response[\"workflow_id\"])\n    else:\n        print(\"No workflow ID found.\")\n        return None\n",
      "variables": [
        "run_id",
        "url",
        "response"
      ],
      "anonymized_code": "def get_workflow_id(var_1: str) -> Optional[str]:\n    # Get the workflow ID that corresponds to the file for the run ID\n    var_2 = f\"https://api.github.com/repos/pytorch/pytorch/actions/runs/{var_1}\"\n    var_3 = query_github_api(var_2)\n    if \"workflow_id\" in var_3:\n        print(f\"Found workflow ID for run ID {var_1}: {var_3['workflow_id']}\")\n        return cast(str, var_3[\"workflow_id\"])\n    else:\n        print(\"No workflow ID found.\")\n        return None\n",
      "lines_processed": 10,
      "total_lines": 351
    },
    {
      "file_path": "reuse_old_whl.py",
      "code": "def check_labels_for_pr() -> bool:\n    # Check if the current commit is part of a PR and if it has the\n    # FORCE_REBUILD_LABEL\n    head_sha = get_head_sha()\n    url = f\"https://api.github.com/repos/pytorch/pytorch/commits/{head_sha}/pulls\"\n    response = query_github_api(url)\n\n    print(\n        f\"Found {len(response)} PRs for commit {head_sha}: {[pr['number'] for pr in response]}\"\n    )\n",
      "variables": [
        "head_sha",
        "url",
        "response",
        "pr"
      ],
      "anonymized_code": "def check_labels_for_pr() -> bool:\n    # Check if the current commit is part of a PR and if it has the\n    # FORCE_REBUILD_LABEL\n    var_1 = get_head_sha()\n    var_2 = f\"https://api.github.com/repos/pytorch/pytorch/commits/{var_1}/pulls\"\n    var_3 = query_github_api(var_2)\n\n    print(\n        f\"Found {len(var_3)} PRs for commit {var_1}: {[var_4['number'] for var_4 in var_3]}\"\n    )\n",
      "lines_processed": 10,
      "total_lines": 351
    },
    {
      "file_path": "reuse_old_whl.py",
      "code": "def unzip_artifact_and_replace_files() -> None:\n    # Unzip the artifact and replace files\n    subprocess.check_output(\n        [\"unzip\", \"-o\", \"artifacts.zip\", \"-d\", \"artifacts\"],\n    )\n    os.remove(\"artifacts.zip\")\n\n    head_sha = get_head_sha()\n\n    # Rename wheel into zip\n",
      "variables": [
        "head_sha"
      ],
      "anonymized_code": "def unzip_artifact_and_replace_files() -> None:\n    # Unzip the artifact and replace files\n    subprocess.check_output(\n        [\"unzip\", \"-o\", \"artifacts.zip\", \"-d\", \"artifacts\"],\n    )\n    os.remove(\"artifacts.zip\")\n\n    var_1 = get_head_sha()\n\n    # Rename wheel into zip\n",
      "lines_processed": 10,
      "total_lines": 351
    },
    {
      "file_path": "reuse_old_whl.py",
      "code": "def check_issue_open() -> bool:\n    # Check if issue #153759 is open.  This is the config issue for quickly\n    # forcing everyone to build\n    url = \"https://api.github.com/repos/pytorch/pytorch/issues/153759\"\n    response = query_github_api(url)\n    if response.get(\"state\") == \"open\":\n        print(\"Issue #153759 is open.\")\n        return True\n    else:\n        print(\"Issue #153759 is not open.\")\n",
      "variables": [
        "url",
        "response"
      ],
      "anonymized_code": "def check_issue_open() -> bool:\n    # Check if issue #153759 is open.  This is the config issue for quickly\n    # forcing everyone to build\n    var_1 = \"https://api.github.com/repos/pytorch/pytorch/issues/153759\"\n    var_2 = query_github_api(var_1)\n    if var_2.get(\"state\") == \"open\":\n        print(\"Issue #153759 is open.\")\n        return True\n    else:\n        print(\"Issue #153759 is not open.\")\n",
      "lines_processed": 10,
      "total_lines": 351
    },
    {
      "file_path": "manylinux1-check.py",
      "code": "def have_compatible_glibc(major, minimum_minor):\n    import ctypes\n\n    process_namespace = ctypes.CDLL(None)\n    try:\n        gnu_get_libc_version = process_namespace.gnu_get_libc_version\n    except AttributeError:\n        # Symbol doesn't exist -> therefore, we are not linked to\n        # glibc.\n        return False\n",
      "variables": [
        "major",
        "minimum_minor",
        "process_namespace",
        "gnu_get_libc_version"
      ],
      "anonymized_code": "def have_compatible_glibc(var_1, var_2):\n    import ctypes\n\n    var_3 = ctypes.CDLL(None)\n    try:\n        var_4 = var_3.var_4\n    except AttributeError:\n        # Symbol doesn't exist -> therefore, we are not linked to\n        # glibc.\n        return False\n",
      "lines_processed": 10,
      "total_lines": 60
    }
  ],
  "ivy-llc_ivy": [
    {
      "file_path": "update_db.py",
      "code": "def make_clickable(url, name):\n    return (\n        f'<a href=\"{url}\" rel=\"noopener noreferrer\" '\n        + f'target=\"_blank\"><img src={name}></a>'\n    )\n",
      "variables": [
        "url",
        "name"
      ],
      "anonymized_code": "def make_clickable(var_1, var_2):\n    return (\n        f'<a href=\"{var_1}\" rel=\"noopener noreferrer\" '\n        + f'target=\"_blank\"><img src={var_2}></a>'\n    )\n",
      "lines_processed": 5,
      "total_lines": 56
    },
    {
      "file_path": "update_db.py",
      "code": "def update_test_results():\n    key, workflow, fw_submod, result, run_id = (\n        str(sys.argv[1]),\n        str(sys.argv[2]),\n        str(sys.argv[3]),\n        str(sys.argv[4]),\n        str(sys.argv[5]),\n    )\n    backend = fw_submod.split(\"-\")[0]\n    submodule = fw_submod.split(\"-\")[1]\n",
      "variables": [
        "key",
        "workflow",
        "fw_submod",
        "result",
        "run_id",
        "backend",
        "submodule"
      ],
      "anonymized_code": "def update_test_results():\n    var_1, var_2, var_3, var_4, var_5 = (\n        str(sys.argv[1]),\n        str(sys.argv[2]),\n        str(sys.argv[3]),\n        str(sys.argv[4]),\n        str(sys.argv[5]),\n    )\n    var_6 = var_3.split(\"-\")[0]\n    var_7 = var_3.split(\"-\")[1]\n",
      "lines_processed": 10,
      "total_lines": 56
    },
    {
      "file_path": "gpu_framework_directory.py",
      "code": "def get_latest_package_version(package_name):\n    try:\n        url = f\"https://pypi.org/pypi/{package_name}/json\"\n        response = requests.get(url, timeout=10)\n        response.raise_for_status()\n        package_info = response.json()\n        return package_info[\"info\"][\"version\"]\n    except requests.exceptions.RequestException as e:\n        print(f\"Error: Failed to fetch package information for {package_name}.\")\n        return None\n",
      "variables": [
        "package_name",
        "url",
        "response",
        "package_info"
      ],
      "anonymized_code": "def get_latest_package_version(var_1):\n    try:\n        var_2 = f\"https://pypi.org/pypi/{var_1}/json\"\n        var_3 = requests.get(var_2, timeout=10)\n        var_3.raise_for_status()\n        var_4 = var_3.json()\n        return var_4[\"info\"][\"version\"]\n    except requests.exceptions.RequestException as e:\n        print(f\"Error: Failed to fetch package information for {var_1}.\")\n        return None\n",
      "lines_processed": 10,
      "total_lines": 93
    },
    {
      "file_path": "gpu_framework_directory.py",
      "code": "def directory_generator(req, base=\"/opt/fw/\"):\n    for versions in req:\n        if \"/\" in versions:\n            pkg, ver = versions.split(\"/\")\n            path = base + pkg + \"/\" + ver\n            if not os.path.exists(path):\n                install_pkg(path, pkg + \"==\" + ver)\n        else:\n            install_pkg(base + versions, versions)\n",
      "variables": [
        "req",
        "base",
        "versions",
        "pkg",
        "ver",
        "path"
      ],
      "anonymized_code": "def directory_generator(var_1, var_2=\"/opt/fw/\"):\n    for var_3 in var_1:\n        if \"/\" in var_3:\n            var_4, var_5 = var_3.split(\"/\")\n            var_6 = var_2 + var_4 + \"/\" + var_5\n            if not os.var_6.exists(var_6):\n                install_pkg(var_6, var_4 + \"==\" + var_5)\n        else:\n            install_pkg(var_2 + var_3, var_3)\n",
      "lines_processed": 9,
      "total_lines": 93
    },
    {
      "file_path": "multiversion_framework_directory.py",
      "code": "def directory_generator(req, base=\"/opt/fw/\"):\n    for versions in req:\n        if \"/\" in versions:\n            pkg, ver = versions.split(\"/\")\n            path = base + pkg + \"/\" + ver\n            if not os.path.exists(path):\n                install_pkg(path, pkg + \"==\" + ver)\n        else:\n            install_pkg(base + versions, versions)\n",
      "variables": [
        "req",
        "base",
        "versions",
        "pkg",
        "ver",
        "path"
      ],
      "anonymized_code": "def directory_generator(var_1, var_2=\"/opt/fw/\"):\n    for var_3 in var_1:\n        if \"/\" in var_3:\n            var_4, var_5 = var_3.split(\"/\")\n            var_6 = var_2 + var_4 + \"/\" + var_5\n            if not os.var_6.exists(var_6):\n                install_pkg(var_6, var_4 + \"==\" + var_5)\n        else:\n            install_pkg(var_2 + var_3, var_3)\n",
      "lines_processed": 9,
      "total_lines": 87
    }
  ],
  "pydantic_pydantic": [
    {
      "file_path": "_docs_extraction.py",
      "code": "def _dedent_source_lines(source: list[str]) -> str:\n    # Required for nested class definitions, e.g. in a function block\n    dedent_source = textwrap.dedent(''.join(source))\n    if dedent_source.startswith((' ', '\\t')):\n        # We are in the case where there's a dedented (usually multiline) string\n        # at a lower indentation level than the class itself. We wrap our class\n        # in a function as a workaround.\n        dedent_source = f'def dedent_workaround():\\n{dedent_source}'\n    return dedent_source\n",
      "variables": [
        "source",
        "dedent_source"
      ],
      "anonymized_code": "def _dedent_source_lines(var_1: list[str]) -> str:\n    # Required for nested class definitions, e.g. in a function block\n    var_2 = textwrap.dedent(''.join(var_1))\n    if var_2.startswith((' ', '\\t')):\n        # We are in the case where there's a dedented (usually multiline) string\n        # at a lower indentation level than the class itself. We wrap our class\n        # in a function as a workaround.\n        var_2 = f'def dedent_workaround():\\n{var_2}'\n    return var_2\n",
      "lines_processed": 9,
      "total_lines": 113
    },
    {
      "file_path": "_core_utils.py",
      "code": "def validate_core_schema(schema: CoreSchema) -> CoreSchema:\n    if os.getenv('PYDANTIC_VALIDATE_CORE_SCHEMAS'):\n        return _validate_core_schema(schema)\n    return schema\n",
      "variables": [
        "schema"
      ],
      "anonymized_code": "def validate_core_schema(var_1: CoreSchema) -> CoreSchema:\n    if os.getenv('PYDANTIC_VALIDATE_CORE_SCHEMAS'):\n        return _validate_core_schema(var_1)\n    return var_1\n",
      "lines_processed": 4,
      "total_lines": 182
    },
    {
      "file_path": "_core_utils.py",
      "code": "def get_ref(s: core_schema.CoreSchema) -> None | str:\n    \"\"\"Get the ref from the schema if it has one.\n    This exists just for type checking to work correctly.\n    \"\"\"\n    return s.get('ref', None)\n",
      "variables": [
        "s"
      ],
      "anonymized_code": "def get_ref(var_1: core_schema.CoreSchema) -> None | str:\n    \"\"\"Get the ref from the schema if it has one.\n    This exists just for type checking to work correctly.\n    \"\"\"\n    return var_1.get('ref', None)\n",
      "lines_processed": 5,
      "total_lines": 182
    },
    {
      "file_path": "_core_utils.py",
      "code": "def is_core_schema_field(\n    schema: CoreSchemaOrField,\n) -> TypeGuard[CoreSchemaField]:\n    return schema['type'] in _CORE_SCHEMA_FIELD_TYPES\n",
      "variables": [
        "schema"
      ],
      "anonymized_code": "def is_core_schema_field(\n    var_1: CoreSchemaOrField,\n) -> TypeGuard[CoreSchemaField]:\n    return var_1['type'] in _CORE_SCHEMA_FIELD_TYPES\n",
      "lines_processed": 4,
      "total_lines": 182
    },
    {
      "file_path": "_core_utils.py",
      "code": "def is_function_with_inner_schema(\n    schema: CoreSchemaOrField,\n) -> TypeGuard[FunctionSchemaWithInnerSchema]:\n    return schema['type'] in _FUNCTION_WITH_INNER_SCHEMA_TYPES\n",
      "variables": [
        "schema"
      ],
      "anonymized_code": "def is_function_with_inner_schema(\n    var_1: CoreSchemaOrField,\n) -> TypeGuard[FunctionSchemaWithInnerSchema]:\n    return var_1['type'] in _FUNCTION_WITH_INNER_SCHEMA_TYPES\n",
      "lines_processed": 4,
      "total_lines": 182
    },
    {
      "file_path": "_core_utils.py",
      "code": "def is_list_like_schema_with_items_schema(\n    schema: CoreSchema,\n) -> TypeGuard[core_schema.ListSchema | core_schema.SetSchema | core_schema.FrozenSetSchema]:\n    return schema['type'] in _LIST_LIKE_SCHEMA_WITH_ITEMS_TYPES\n",
      "variables": [
        "schema"
      ],
      "anonymized_code": "def is_list_like_schema_with_items_schema(\n    var_1: CoreSchema,\n) -> TypeGuard[core_schema.ListSchema | core_schema.SetSchema | core_schema.FrozenSetSchema]:\n    return var_1['type'] in _LIST_LIKE_SCHEMA_WITH_ITEMS_TYPES\n",
      "lines_processed": 4,
      "total_lines": 182
    },
    {
      "file_path": "_core_utils.py",
      "code": "def is_core_schema(\n    schema: CoreSchemaOrField,\n) -> TypeGuard[CoreSchema]:\n    return schema['type'] not in _CORE_SCHEMA_FIELD_TYPES\n",
      "variables": [
        "schema"
      ],
      "anonymized_code": "def is_core_schema(\n    var_1: CoreSchemaOrField,\n) -> TypeGuard[CoreSchema]:\n    return var_1['type'] not in _CORE_SCHEMA_FIELD_TYPES\n",
      "lines_processed": 4,
      "total_lines": 182
    },
    {
      "file_path": "_core_utils.py",
      "code": "def get_type_ref(type_: Any, args_override: tuple[type[Any], ...] | None = None) -> str:\n    \"\"\"Produces the ref to be used for this type by pydantic_core's core schemas.\n\n    This `args_override` argument was added for the purpose of creating valid recursive references\n    when creating generic models without needing to create a concrete class.\n    \"\"\"\n    origin = get_origin(type_) or type_\n\n    args = get_args(type_) if is_generic_alias(type_) else (args_override or ())\n    generic_metadata = getattr(type_, '__pydantic_generic_metadata__', None)\n",
      "variables": [
        "type_",
        "args_override",
        "origin",
        "args",
        "generic_metadata"
      ],
      "anonymized_code": "def get_type_ref(var_1: Any, var_2: tuple[type[Any], ...] | None = None) -> str:\n    \"\"\"Produces the ref to be used for this type by pydantic_core's core schemas.\n\n    This `var_2` argument was added for the purpose of creating valid recursive references\n    when creating generic models without needing to create a concrete class.\n    \"\"\"\n    var_3 = get_origin(var_1) or var_1\n\n    var_4 = get_args(var_1) if is_generic_alias(var_1) else (var_2 or ())\n    var_5 = getattr(var_1, '__pydantic_generic_metadata__', None)\n",
      "lines_processed": 10,
      "total_lines": 182
    },
    {
      "file_path": "_decorators.py",
      "code": "def _is_classmethod_from_sig(function: AnyDecoratorCallable) -> bool:\n    sig = signature(unwrap_wrapped_function(function))\n    first = next(iter(sig.parameters.values()), None)\n    if first and first.name == 'cls':\n        return True\n    return False\n",
      "variables": [
        "function",
        "sig",
        "first"
      ],
      "anonymized_code": "def _is_classmethod_from_sig(var_1: AnyDecoratorCallable) -> bool:\n    var_2 = signature(unwrap_wrapped_function(var_1))\n    var_3 = next(iter(var_2.parameters.values()), None)\n    if var_3 and var_3.name == 'cls':\n        return True\n    return False\n",
      "lines_processed": 6,
      "total_lines": 844
    },
    {
      "file_path": "_git.py",
      "code": "def git_revision(dir: Path) -> str:\n    \"\"\"Get the SHA-1 of the HEAD of a git repository.\"\"\"\n    return subprocess.check_output(['git', 'rev-parse', '--short', 'HEAD'], cwd=dir).decode('utf-8').strip()\n",
      "variables": [
        "dir"
      ],
      "anonymized_code": "def git_revision(var_1: Path) -> str:\n    \"\"\"Get the SHA-1 of the HEAD of a git repository.\"\"\"\n    return subprocess.check_output(['git', 'rev-parse', '--short', 'HEAD'], cwd=var_1).decode('utf-8').strip()\n",
      "lines_processed": 3,
      "total_lines": 27
    },
    {
      "file_path": "_git.py",
      "code": "def is_git_repo(dir: Path) -> bool:\n    \"\"\"Is the given directory version-controlled with git?\"\"\"\n    return dir.joinpath('.git').exists()\n",
      "variables": [
        "dir"
      ],
      "anonymized_code": "def is_git_repo(var_1: Path) -> bool:\n    \"\"\"Is the given directory version-controlled with git?\"\"\"\n    return var_1.joinpath('.git').exists()\n",
      "lines_processed": 3,
      "total_lines": 27
    },
    {
      "file_path": "_config.py",
      "code": "def check_deprecated(config_dict: ConfigDict) -> None:\n    \"\"\"Check for deprecated config keys and warn the user.\n\n    Args:\n        config_dict: The input config.\n    \"\"\"\n    deprecated_removed_keys = V2_REMOVED_KEYS & config_dict.keys()\n    deprecated_renamed_keys = V2_RENAMED_KEYS.keys() & config_dict.keys()\n    if deprecated_removed_keys or deprecated_renamed_keys:\n        renamings = {k: V2_RENAMED_KEYS[k] for k in sorted(deprecated_renamed_keys)}\n",
      "variables": [
        "config_dict",
        "deprecated_removed_keys",
        "deprecated_renamed_keys",
        "renamings",
        "k"
      ],
      "anonymized_code": "def check_deprecated(var_1: ConfigDict) -> None:\n    \"\"\"Check for deprecated config keys and warn the user.\n\n    Args:\n        var_1: The input config.\n    \"\"\"\n    var_2 = V2_REMOVED_KEYS & var_1.keys()\n    var_3 = V2_RENAMED_KEYS.keys() & var_1.keys()\n    if var_2 or var_3:\n        var_4 = {var_5: V2_RENAMED_KEYS[var_5] for var_5 in sorted(var_3)}\n",
      "lines_processed": 10,
      "total_lines": 373
    },
    {
      "file_path": "people.py",
      "code": "def get_graphql_issue_edges(*, settings: Settings, after: str | None = None) -> list[IssuesEdge]:\n    \"\"\"Fetch issue edges from GitHub GraphQL API.\n\n    Args:\n        settings: Configuration settings\n        after: Cursor for pagination, if any\n\n    Returns:\n        List of issue edges from the GraphQL response\n    \"\"\"\n",
      "variables": [
        "settings",
        "after"
      ],
      "anonymized_code": "def get_graphql_issue_edges(*, var_1: Settings, var_2: str | None = None) -> list[IssuesEdge]:\n    \"\"\"Fetch issue edges from GitHub GraphQL API.\n\n    Args:\n        var_1: Configuration var_1\n        var_2: Cursor for pagination, if any\n\n    Returns:\n        List of issue edges from the GraphQL response\n    \"\"\"\n",
      "lines_processed": 10,
      "total_lines": 781
    },
    {
      "file_path": "people.py",
      "code": "def get_graphql_pr_edges(*, settings: Settings, after: str | None = None) -> list[PullRequestEdge]:\n    \"\"\"Fetch pull request edges from GitHub GraphQL API.\n\n    Args:\n        settings: Configuration settings\n        after: Cursor for pagination, if any\n\n    Returns:\n        List of pull request edges from the GraphQL response\n    \"\"\"\n",
      "variables": [
        "settings",
        "after"
      ],
      "anonymized_code": "def get_graphql_pr_edges(*, var_1: Settings, var_2: str | None = None) -> list[PullRequestEdge]:\n    \"\"\"Fetch pull request edges from GitHub GraphQL API.\n\n    Args:\n        var_1: Configuration var_1\n        var_2: Cursor for pagination, if any\n\n    Returns:\n        List of pull request edges from the GraphQL response\n    \"\"\"\n",
      "lines_processed": 10,
      "total_lines": 781
    },
    {
      "file_path": "_generics.py",
      "code": "def iter_contained_typevars(v: Any) -> Iterator[TypeVar]:\n    \"\"\"Recursively iterate through all subtypes and type args of `v` and yield any typevars that are found.\n\n    This is inspired as an alternative to directly accessing the `__parameters__` attribute of a GenericAlias,\n    since __parameters__ of (nested) generic BaseModel subclasses won't show up in that list.\n    \"\"\"\n    if isinstance(v, TypeVar):\n        yield v\n    elif is_model_class(v):\n        yield from v.__pydantic_generic_metadata__['parameters']\n",
      "variables": [
        "v"
      ],
      "anonymized_code": "def iter_contained_typevars(var_1: Any) -> Iterator[TypeVar]:\n    \"\"\"Recursively iterate through all subtypes and type args of `var_1` and yield any typevars that are found.\n\n    This is inspired as an alternative to directly accessing the `__parameters__` attribute of a GenericAlias,\n    since __parameters__ of (nested) generic BaseModel subclasses won't show up in that list.\n    \"\"\"\n    if isinstance(var_1, TypeVar):\n        yield var_1\n    elif is_model_class(var_1):\n        yield from var_1.__pydantic_generic_metadata__['parameters']\n",
      "lines_processed": 10,
      "total_lines": 547
    },
    {
      "file_path": "_generics.py",
      "code": "def _early_cache_key(cls: type[BaseModel], typevar_values: Any) -> GenericTypesCacheKey:\n    \"\"\"This is intended for minimal computational overhead during lookups of cached types.\n\n    Note that this is overly simplistic, and it's possible that two different cls/typevar_values\n    inputs would ultimately result in the same type being created in BaseModel.__class_getitem__.\n    To handle this, we have a fallback _late_cache_key that is checked later if the _early_cache_key\n    lookup fails, and should result in a cache hit _precisely_ when the inputs to __class_getitem__\n    would result in the same type.\n    \"\"\"\n    return cls, typevar_values, _union_orderings_key(typevar_values)\n",
      "variables": [
        "cls",
        "typevar_values"
      ],
      "anonymized_code": "def _early_cache_key(var_1: type[BaseModel], var_2: Any) -> GenericTypesCacheKey:\n    \"\"\"This is intended for minimal computational overhead during lookups of cached types.\n\n    Note that this is overly simplistic, and it's possible that two different var_1/var_2\n    inputs would ultimately result in the same type being created in BaseModel.__class_getitem__.\n    To handle this, we have a fallback _late_cache_key that is checked later if the _early_cache_key\n    lookup fails, and should result in a cache hit _precisely_ when the inputs to __class_getitem__\n    would result in the same type.\n    \"\"\"\n    return var_1, var_2, _union_orderings_key(var_2)\n",
      "lines_processed": 10,
      "total_lines": 547
    },
    {
      "file_path": "_generics.py",
      "code": "def get_args(v: Any) -> Any:\n    pydantic_generic_metadata: PydanticGenericMetadata | None = getattr(v, '__pydantic_generic_metadata__', None)\n    if pydantic_generic_metadata:\n        return pydantic_generic_metadata.get('args')\n    return typing_extensions.get_args(v)\n",
      "variables": [
        "v",
        "pydantic_generic_metadata"
      ],
      "anonymized_code": "def get_args(var_1: Any) -> Any:\n    var_2: PydanticGenericMetadata | None = getattr(var_1, '__pydantic_generic_metadata__', None)\n    if var_2:\n        return var_2.get('args')\n    return typing_extensions.get_args(var_1)\n",
      "lines_processed": 5,
      "total_lines": 547
    },
    {
      "file_path": "_generics.py",
      "code": "def recursively_defined_type_refs() -> set[str]:\n    visited = _generic_recursion_cache.get()\n    if not visited:\n        return set()  # not in a generic recursion, so there are no types\n\n    return visited.copy()  # don't allow modifications\n",
      "variables": [
        "visited"
      ],
      "anonymized_code": "def recursively_defined_type_refs() -> set[str]:\n    var_1 = _generic_recursion_cache.get()\n    if not var_1:\n        return set()  # not in a generic recursion, so there are no types\n\n    return var_1.copy()  # don't allow modifications\n",
      "lines_processed": 6,
      "total_lines": 547
    },
    {
      "file_path": "main.py",
      "code": "def _generate_table_row(col_values: list[str]) -> str:\n    return f'| {\" | \".join(col_values)} |\\n'\n",
      "variables": [
        "col_values"
      ],
      "anonymized_code": "def _generate_table_row(var_1: list[str]) -> str:\n    return f'| {\" | \".join(var_1)} |\\n'\n",
      "lines_processed": 2,
      "total_lines": 461
    },
    {
      "file_path": "main.py",
      "code": "def on_page_markdown(markdown: str, page: Page, config: Config, files: Files) -> str:\n    \"\"\"\n    Called on each file after it is read and before it is converted to HTML.\n    \"\"\"\n    markdown = upgrade_python(markdown)\n    markdown = insert_json_output(markdown)\n    if md := render_index(markdown, page):\n        return md\n    if md := render_why(markdown, page):\n        return md\n",
      "variables": [
        "markdown",
        "page",
        "config",
        "files",
        "md"
      ],
      "anonymized_code": "def on_page_markdown(var_1: str, var_2: Page, var_3: Config, var_4: Files) -> str:\n    \"\"\"\n    Called on each file after it is read and before it is converted to HTML.\n    \"\"\"\n    var_1 = upgrade_python(var_1)\n    var_1 = insert_json_output(var_1)\n    if var_5 := render_index(var_1, var_2):\n        return var_5\n    if var_5 := render_why(var_1, var_2):\n        return var_5\n",
      "lines_processed": 10,
      "total_lines": 461
    },
    {
      "file_path": "main.py",
      "code": "def insert_json_output(markdown: str) -> str:\n    \"\"\"\n    Find `output=\"json\"` code fence tags and replace with a separate JSON section\n    \"\"\"\n\n    def replace_json(m: re.Match[str]) -> str:\n        start, attrs, code = m.groups()\n\n        def replace_last_print(m2: re.Match[str]) -> str:\n            ind, json_text = m2.groups()\n",
      "variables": [
        "markdown",
        "m",
        "start",
        "attrs",
        "code",
        "m2",
        "ind",
        "json_text"
      ],
      "anonymized_code": "def insert_json_output(var_1: str) -> str:\n    \"\"\"\n    Find `output=\"json\"` var_5 fence tags and replace with a separate JSON section\n    \"\"\"\n\n    def replace_json(var_2: re.Match[str]) -> str:\n        var_3, var_4, var_5 = var_2.groups()\n\n        def replace_last_print(var_6: re.Match[str]) -> str:\n            var_7, var_8 = var_6.groups()\n",
      "lines_processed": 10,
      "total_lines": 461
    },
    {
      "file_path": "main.py",
      "code": "def add_changelog() -> None:\n    history = (PROJECT_ROOT / 'HISTORY.md').read_text(encoding='utf-8')\n    history = re.sub(r'(\\s)@([\\w\\-]+)', r'\\1[@\\2](https://github.com/\\2)', history, flags=re.I)\n    history = re.sub(r'\\[GitHub release]\\(', r'[:simple-github: GitHub release](', history)\n    history = re.sub('@@', '@', history)\n    new_file = DOCS_DIR / 'changelog.md'\n\n    # avoid writing file unless the content has changed to avoid infinite build loop\n    if not new_file.is_file() or new_file.read_text(encoding='utf-8') != history:\n        new_file.write_text(history, encoding='utf-8')\n",
      "variables": [
        "history",
        "new_file"
      ],
      "anonymized_code": "def add_changelog() -> None:\n    var_1 = (PROJECT_ROOT / 'HISTORY.md').read_text(encoding='utf-8')\n    var_1 = re.sub(r'(\\s)@([\\w\\-]+)', r'\\1[@\\2](https://github.com/\\2)', var_1, flags=re.I)\n    var_1 = re.sub(r'\\[GitHub release]\\(', r'[:simple-github: GitHub release](', var_1)\n    var_1 = re.sub('@@', '@', var_1)\n    var_2 = DOCS_DIR / 'changelog.md'\n\n    # avoid writing file unless the content has changed to avoid infinite build loop\n    if not var_2.is_file() or var_2.read_text(encoding='utf-8') != var_1:\n        var_2.write_text(var_1, encoding='utf-8')\n",
      "lines_processed": 10,
      "total_lines": 461
    },
    {
      "file_path": "main.py",
      "code": "def get_orgs_data() -> list[dict[str, str]]:\n    with (THIS_DIR / 'orgs.toml').open('rb') as f:\n        orgs_data = tomli.load(f)\n    return orgs_data['orgs']\n",
      "variables": [
        "f",
        "orgs_data"
      ],
      "anonymized_code": "def get_orgs_data() -> list[dict[str, str]]:\n    with (THIS_DIR / 'orgs.toml').open('rb') as var_1:\n        var_2 = tomli.load(var_1)\n    return var_2['orgs']\n",
      "lines_processed": 4,
      "total_lines": 461
    },
    {
      "file_path": "main.py",
      "code": "def render_why(markdown: str, page: Page) -> str | None:\n    if page.file.src_uri != 'why.md':\n        return None\n\n    with (THIS_DIR / 'using.toml').open('rb') as f:\n        using = tomli.load(f)['libs']\n\n    libraries = '\\n'.join('* [`{repo}`](https://github.com/{repo}) {stars:,} stars'.format(**lib) for lib in using)\n    markdown = re.sub(r'{{ *libraries *}}', libraries, markdown)\n    default_description = '_(Based on the criteria described above)_'\n",
      "variables": [
        "markdown",
        "page",
        "f",
        "using",
        "libraries",
        "lib",
        "default_description"
      ],
      "anonymized_code": "def render_why(var_1: str, var_2: Page) -> str | None:\n    if var_2.file.src_uri != 'why.md':\n        return None\n\n    with (THIS_DIR / 'var_4.toml').open('rb') as var_3:\n        var_4 = tomli.load(var_3)['libs']\n\n    var_5 = '\\n'.join('* [`{repo}`](https://github.com/{repo}) {stars:,} stars'.format(**var_6) for var_6 in var_4)\n    var_1 = re.sub(r'{{ *var_5 *}}', var_5, var_1)\n    var_7 = '_(Based on the criteria described above)_'\n",
      "lines_processed": 10,
      "total_lines": 461
    },
    {
      "file_path": "_fields.py",
      "code": "def is_valid_privateattr_name(name: str) -> bool:\n    return name.startswith('_') and not name.startswith('__')\n",
      "variables": [
        "name"
      ],
      "anonymized_code": "def is_valid_privateattr_name(var_1: str) -> bool:\n    return var_1.startswith('_') and not var_1.startswith('__')\n",
      "lines_processed": 2,
      "total_lines": 569
    },
    {
      "file_path": "_fields.py",
      "code": "def _apply_alias_generator_to_field_info(\n    alias_generator: Callable[[str], str] | AliasGenerator, field_name: str, field_info: FieldInfo\n):\n    \"\"\"Apply an alias generator to aliases on a `FieldInfo` instance if appropriate.\n\n    Args:\n        alias_generator: A callable that takes a string and returns a string, or an `AliasGenerator` instance.\n        field_name: The name of the field from which to generate the alias.\n        field_info: The `FieldInfo` instance to which the alias generator is (maybe) applied.\n    \"\"\"\n",
      "variables": [
        "alias_generator",
        "field_name",
        "field_info"
      ],
      "anonymized_code": "def _apply_alias_generator_to_field_info(\n    var_1: Callable[[str], str] | AliasGenerator, var_2: str, var_3: FieldInfo\n):\n    \"\"\"Apply an alias generator to aliases on a `FieldInfo` instance if appropriate.\n\n    Args:\n        var_1: A callable that takes a string and returns a string, or an `AliasGenerator` instance.\n        var_2: The name of the field from which to generate the alias.\n        var_3: The `FieldInfo` instance to which the alias generator is (maybe) applied.\n    \"\"\"\n",
      "lines_processed": 10,
      "total_lines": 569
    },
    {
      "file_path": "_fields.py",
      "code": "def update_field_from_config(config_wrapper: ConfigWrapper, field_name: str, field_info: FieldInfo) -> None:\n    \"\"\"Update the `FieldInfo` instance from the configuration set on the model it belongs to.\n\n    This will apply the title and alias generators from the configuration.\n\n    Args:\n        config_wrapper: The configuration from the model.\n        field_name: The field name the `FieldInfo` instance is attached to.\n        field_info: The `FieldInfo` instance to update.\n    \"\"\"\n",
      "variables": [
        "config_wrapper",
        "field_name",
        "field_info"
      ],
      "anonymized_code": "def update_field_from_config(var_1: ConfigWrapper, var_2: str, var_3: FieldInfo) -> None:\n    \"\"\"Update the `FieldInfo` instance from the configuration set on the model it belongs to.\n\n    This will apply the title and alias generators from the configuration.\n\n    Args:\n        var_1: The configuration from the model.\n        var_2: The field name the `FieldInfo` instance is attached to.\n        var_3: The `FieldInfo` instance to update.\n    \"\"\"\n",
      "lines_processed": 10,
      "total_lines": 569
    },
    {
      "file_path": "_fields.py",
      "code": "def _apply_field_title_generator_to_field_info(\n    title_generator: Callable[[str, FieldInfo], str],\n    field_name: str,\n    field_info: FieldInfo,\n):\n    if field_info.title is None:\n        title = title_generator(field_name, field_info)\n        if not isinstance(title, str):\n            raise TypeError(f'field_title_generator {title_generator} must return str, not {title.__class__}')\n\n",
      "variables": [
        "title_generator",
        "field_name",
        "field_info",
        "title"
      ],
      "anonymized_code": "def _apply_field_title_generator_to_field_info(\n    var_1: Callable[[str, FieldInfo], str],\n    var_2: str,\n    var_3: FieldInfo,\n):\n    if var_3.var_4 is None:\n        var_4 = var_1(var_2, var_3)\n        if not isinstance(var_4, str):\n            raise TypeError(f'field_title_generator {var_1} must return str, not {var_4.__class__}')\n\n",
      "lines_processed": 10,
      "total_lines": 569
    },
    {
      "file_path": "_fields.py",
      "code": "def _check_protected_namespaces(\n    protected_namespaces: tuple[str | Pattern[str], ...],\n    ann_name: str,\n    bases: tuple[type[Any], ...],\n    cls_name: str,\n) -> None:\n    BaseModel = import_cached_base_model()\n\n    for protected_namespace in protected_namespaces:\n        ns_violation = False\n",
      "variables": [
        "protected_namespaces",
        "ann_name",
        "bases",
        "cls_name",
        "BaseModel",
        "protected_namespace",
        "ns_violation"
      ],
      "anonymized_code": "def _check_protected_namespaces(\n    var_1: tuple[str | Pattern[str], ...],\n    var_2: str,\n    var_3: tuple[type[Any], ...],\n    var_4: str,\n) -> None:\n    var_5 = import_cached_base_model()\n\n    for var_6 in var_1:\n        var_7 = False\n",
      "lines_processed": 10,
      "total_lines": 569
    },
    {
      "file_path": "_fields.py",
      "code": "def is_valid_field_name(name: str) -> bool:\n    return not name.startswith('_')\n",
      "variables": [
        "name"
      ],
      "anonymized_code": "def is_valid_field_name(var_1: str) -> bool:\n    return not var_1.startswith('_')\n",
      "lines_processed": 2,
      "total_lines": 569
    },
    {
      "file_path": "_fields.py",
      "code": "def pydantic_general_metadata(**metadata: Any) -> BaseMetadata:\n    \"\"\"Create a new `_PydanticGeneralMetadata` class with the given metadata.\n\n    Args:\n        **metadata: The metadata to add.\n\n    Returns:\n        The new `_PydanticGeneralMetadata` class.\n    \"\"\"\n    return _general_metadata_cls()(metadata)  # type: ignore\n",
      "variables": [
        "metadata"
      ],
      "anonymized_code": "def pydantic_general_metadata(**var_1: Any) -> BaseMetadata:\n    \"\"\"Create a new `_PydanticGeneralMetadata` class with the given var_1.\n\n    Args:\n        **var_1: The var_1 to add.\n\n    Returns:\n        The new `_PydanticGeneralMetadata` class.\n    \"\"\"\n    return _general_metadata_cls()(var_1)  # type: ignore\n",
      "lines_processed": 10,
      "total_lines": 569
    },
    {
      "file_path": "_discriminated_union.py",
      "code": "def set_discriminator_in_metadata(schema: CoreSchema, discriminator: Any) -> None:\n    metadata = cast('CoreMetadata', schema.setdefault('metadata', {}))\n    metadata['pydantic_internal_union_discriminator'] = discriminator\n",
      "variables": [
        "schema",
        "discriminator",
        "metadata"
      ],
      "anonymized_code": "def set_discriminator_in_metadata(var_1: CoreSchema, var_2: Any) -> None:\n    var_3 = cast('CoreMetadata', var_1.setdefault('var_3', {}))\n    var_3['pydantic_internal_union_discriminator'] = var_2\n",
      "lines_processed": 3,
      "total_lines": 479
    },
    {
      "file_path": "_core_metadata.py",
      "code": "def update_core_metadata(\n    core_metadata: Any,\n    /,\n    *,\n    pydantic_js_functions: list[GetJsonSchemaFunction] | None = None,\n    pydantic_js_annotation_functions: list[GetJsonSchemaFunction] | None = None,\n    pydantic_js_updates: JsonDict | None = None,\n    pydantic_js_extra: JsonDict | JsonSchemaExtraCallable | None = None,\n) -> None:\n    from ..json_schema import PydanticJsonSchemaWarning\n",
      "variables": [
        "core_metadata",
        "pydantic_js_functions",
        "pydantic_js_annotation_functions",
        "pydantic_js_updates",
        "pydantic_js_extra"
      ],
      "anonymized_code": "def update_core_metadata(\n    var_1: Any,\n    /,\n    *,\n    var_2: list[GetJsonSchemaFunction] | None = None,\n    var_3: list[GetJsonSchemaFunction] | None = None,\n    var_4: JsonDict | None = None,\n    var_5: JsonDict | JsonSchemaExtraCallable | None = None,\n) -> None:\n    from ..json_schema import PydanticJsonSchemaWarning\n",
      "lines_processed": 10,
      "total_lines": 97
    },
    {
      "file_path": "using_update.py",
      "code": "def update_lib(lib, *, retry=0):\n    repo = lib['repo']\n    url = f'https://api.github.com/repos/{repo}'\n    resp = session.get(url)\n    if resp.status_code == 403 and retry < 3:\n        print(f'retrying {repo} {retry}')\n        sleep(5)\n        return update_lib(lib, retry=retry + 1)\n\n    resp.raise_for_status()\n",
      "variables": [
        "lib",
        "retry",
        "repo",
        "url",
        "resp"
      ],
      "anonymized_code": "def update_lib(var_1, *, var_2=0):\n    var_3 = var_1['var_3']\n    var_4 = f'https://api.github.com/repos/{var_3}'\n    var_5 = session.get(var_4)\n    if var_5.status_code == 403 and var_2 < 3:\n        print(f'retrying {var_3} {var_2}')\n        sleep(5)\n        return update_lib(var_1, var_2=var_2 + 1)\n\n    var_5.raise_for_status()\n",
      "lines_processed": 10,
      "total_lines": 39
    },
    {
      "file_path": "_generate_schema.py",
      "code": "def _add_custom_serialization_from_json_encoders(\n    json_encoders: JsonEncoders | None, tp: Any, schema: CoreSchema\n) -> CoreSchema:\n    \"\"\"Iterate over the json_encoders and add the first matching encoder to the schema.\n\n    Args:\n        json_encoders: A dictionary of types and their encoder functions.\n        tp: The type to check for a matching encoder.\n        schema: The schema to add the encoder to.\n    \"\"\"\n",
      "variables": [
        "json_encoders",
        "tp",
        "schema"
      ],
      "anonymized_code": "def _add_custom_serialization_from_json_encoders(\n    var_1: JsonEncoders | None, var_2: Any, var_3: CoreSchema\n) -> CoreSchema:\n    \"\"\"Iterate over the var_1 and add the first matching encoder to the var_3.\n\n    Args:\n        var_1: A dictionary of types and their encoder functions.\n        var_2: The type to check for a matching encoder.\n        var_3: The var_3 to add the encoder to.\n    \"\"\"\n",
      "lines_processed": 10,
      "total_lines": 2743
    },
    {
      "file_path": "algolia.py",
      "code": "def on_page_content(html: str, page: Page, config: Config, files: Files) -> str:\n    if not os.getenv('CI'):\n        return html\n\n    from bs4 import BeautifulSoup\n\n    assert page.title is not None, 'Page title must not be None'\n    title = cast(str, page.title)\n\n    soup = BeautifulSoup(html, 'html.parser')\n",
      "variables": [
        "html",
        "page",
        "config",
        "files",
        "title",
        "soup"
      ],
      "anonymized_code": "def on_page_content(var_1: str, var_2: Page, var_3: Config, var_4: Files) -> str:\n    if not os.getenv('CI'):\n        return var_1\n\n    from bs4 import BeautifulSoup\n\n    assert var_2.var_5 is not None, 'Page var_5 must not be None'\n    var_5 = cast(str, var_2.var_5)\n\n    var_6 = BeautifulSoup(var_1, 'var_1.parser')\n",
      "lines_processed": 10,
      "total_lines": 197
    },
    {
      "file_path": "algolia.py",
      "code": "def on_post_build(config: Config) -> None:\n    if records:\n        algolia_records_path = Path(config['site_dir']) / ALGOLIA_RECORDS_FILE\n        with algolia_records_path.open('wb') as f:\n            f.write(records_ta.dump_json(records))\n",
      "variables": [
        "config",
        "algolia_records_path",
        "f"
      ],
      "anonymized_code": "def on_post_build(var_1: Config) -> None:\n    if records:\n        var_2 = Path(var_1['site_dir']) / ALGOLIA_RECORDS_FILE\n        with var_2.open('wb') as var_3:\n            var_3.write(records_ta.dump_json(records))\n",
      "lines_processed": 5,
      "total_lines": 197
    },
    {
      "file_path": "algolia.py",
      "code": "def get_heading_text(heading: Tag):\n    return heading.get_text().replace('\u00b6', '').strip().replace('\\n', ' ')\n",
      "variables": [
        "heading"
      ],
      "anonymized_code": "def get_heading_text(var_1: Tag):\n    return var_1.get_text().replace('\u00b6', '').strip().replace('\\n', ' ')\n",
      "lines_processed": 2,
      "total_lines": 197
    },
    {
      "file_path": "algolia.py",
      "code": "def algolia_upload() -> None:\n    from algoliasearch.search.client import SearchClientSync\n\n    algolia_write_api_key = os.environ['ALGOLIA_WRITE_API_KEY']\n\n    client = SearchClientSync(ALGOLIA_APP_ID, algolia_write_api_key)\n    filtered_records: list[AlgoliaRecord] = []\n\n    algolia_records_path = Path.cwd() / 'site' / ALGOLIA_RECORDS_FILE\n\n",
      "variables": [
        "algolia_write_api_key",
        "client",
        "filtered_records",
        "algolia_records_path"
      ],
      "anonymized_code": "def algolia_upload() -> None:\n    from algoliasearch.search.var_2 import SearchClientSync\n\n    var_1 = os.environ['ALGOLIA_WRITE_API_KEY']\n\n    var_2 = SearchClientSync(ALGOLIA_APP_ID, var_1)\n    var_3: list[AlgoliaRecord] = []\n\n    var_4 = Path.cwd() / 'site' / ALGOLIA_RECORDS_FILE\n\n",
      "lines_processed": 10,
      "total_lines": 197
    }
  ],
  "langflow-ai_langflow": [
    {
      "file_path": "pypi_nightly_tag.py",
      "code": "def get_latest_published_version(build_type: str, *, is_nightly: bool) -> Version:\n    import requests\n\n    url = \"\"\n    if build_type == \"base\":\n        url = PYPI_LANGFLOW_BASE_NIGHTLY_URL if is_nightly else PYPI_LANGFLOW_BASE_URL\n    elif build_type == \"main\":\n        url = PYPI_LANGFLOW_NIGHTLY_URL if is_nightly else PYPI_LANGFLOW_URL\n    else:\n        msg = f\"Invalid build type: {build_type}\"\n",
      "variables": [
        "build_type",
        "is_nightly",
        "url",
        "msg"
      ],
      "anonymized_code": "def get_latest_published_version(var_1: str, *, var_2: bool) -> Version:\n    import requests\n\n    var_3 = \"\"\n    if var_1 == \"base\":\n        var_3 = PYPI_LANGFLOW_BASE_NIGHTLY_URL if var_2 else PYPI_LANGFLOW_BASE_URL\n    elif var_1 == \"main\":\n        var_3 = PYPI_LANGFLOW_NIGHTLY_URL if var_2 else PYPI_LANGFLOW_URL\n    else:\n        var_4 = f\"Invalid build type: {var_1}\"\n",
      "lines_processed": 10,
      "total_lines": 85
    },
    {
      "file_path": "update_lf_base_dependency.py",
      "code": "def verify_pep440(version):\n    \"\"\"Verify if version is PEP440 compliant.\n\n    https://github.com/pypa/packaging/blob/16.7/packaging/version.py#L191\n    \"\"\"\n    return packaging.version.Version(version)\n",
      "variables": [
        "version"
      ],
      "anonymized_code": "def verify_pep440(var_1):\n    \"\"\"Verify if var_1 is PEP440 compliant.\n\n    https://github.com/pypa/packaging/blob/16.7/packaging/var_1.py#L191\n    \"\"\"\n    return packaging.var_1.Version(var_1)\n",
      "lines_processed": 6,
      "total_lines": 51
    },
    {
      "file_path": "update_lf_base_dependency.py",
      "code": "def main() -> None:\n    if len(sys.argv) != ARGUMENT_NUMBER:\n        msg = \"New version not specified\"\n        raise ValueError(msg)\n    base_version = sys.argv[1]\n\n    # Strip \"v\" prefix from version if present\n    base_version = base_version.removeprefix(\"v\")\n\n    verify_pep440(base_version)\n",
      "variables": [
        "msg",
        "base_version"
      ],
      "anonymized_code": "def main() -> None:\n    if len(sys.argv) != ARGUMENT_NUMBER:\n        var_1 = \"New version not specified\"\n        raise ValueError(var_1)\n    var_2 = sys.argv[1]\n\n    # Strip \"v\" prefix from version if present\n    var_2 = var_2.removeprefix(\"v\")\n\n    verify_pep440(var_2)\n",
      "lines_processed": 10,
      "total_lines": 51
    },
    {
      "file_path": "update_uv_dependency.py",
      "code": "def update_uv_dep(base_version: str) -> None:\n    \"\"\"Update the langflow-base dependency in pyproject.toml.\"\"\"\n    pyproject_path = BASE_DIR / \"pyproject.toml\"\n\n    # Read the pyproject.toml file content\n    content = pyproject_path.read_text(encoding=\"utf-8\")\n\n    # For the main project, update the langflow-base dependency in the UV section\n    pattern = re.compile(r'(dependencies\\s*=\\s*\\[\\s*\\n\\s*)(\"langflow-base==[\\d.]+\")')\n    replacement = rf'\\1\"langflow-base-nightly=={base_version}\"'\n",
      "variables": [
        "base_version",
        "pyproject_path",
        "content",
        "pattern",
        "replacement"
      ],
      "anonymized_code": "def update_uv_dep(var_1: str) -> None:\n    \"\"\"Update the langflow-base dependency in pyproject.toml.\"\"\"\n    var_2 = BASE_DIR / \"pyproject.toml\"\n\n    # Read the pyproject.toml file var_3\n    var_3 = var_2.read_text(encoding=\"utf-8\")\n\n    # For the main project, update the langflow-base dependency in the UV section\n    var_4 = re.compile(r'(dependencies\\s*=\\s*\\[\\s*\\n\\s*)(\"langflow-base==[\\d.]+\")')\n    var_5 = rf'\\1\"langflow-base-nightly=={var_1}\"'\n",
      "lines_processed": 10,
      "total_lines": 44
    },
    {
      "file_path": "update_uv_dependency.py",
      "code": "def main() -> None:\n    if len(sys.argv) != ARGUMENT_NUMBER:\n        msg = \"specify base version\"\n        raise ValueError(msg)\n    base_version = sys.argv[1]\n    base_version = base_version.lstrip(\"v\")\n    update_uv_dep(base_version)\n",
      "variables": [
        "msg",
        "base_version"
      ],
      "anonymized_code": "def main() -> None:\n    if len(sys.argv) != ARGUMENT_NUMBER:\n        var_1 = \"specify base version\"\n        raise ValueError(var_1)\n    var_2 = sys.argv[1]\n    var_2 = var_2.lstrip(\"v\")\n    update_uv_dep(var_2)\n",
      "lines_processed": 7,
      "total_lines": 44
    },
    {
      "file_path": "1b8b740a6fa3_remove_fk_constraint_in_message_.py",
      "code": "def downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    conn = op.get_bind()\n\n    # Recreate tables with constraints\n    # 1. Handle vertex_build table\n    if migration.table_exists(\"vertex_build\", conn):\n        # Create a temporary table with the constraint\n        temp_table_name = \"temp_vertex_build\"\n        pk_name = \"pk_vertex_build\"\n",
      "variables": [
        "conn",
        "temp_table_name",
        "pk_name"
      ],
      "anonymized_code": "def downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    var_1 = op.get_bind()\n\n    # Recreate tables with constraints\n    # 1. Handle vertex_build table\n    if migration.table_exists(\"vertex_build\", var_1):\n        # Create a temporary table with the constraint\n        var_2 = \"temp_vertex_build\"\n        var_3 = \"pk_vertex_build\"\n",
      "lines_processed": 10,
      "total_lines": 337
    },
    {
      "file_path": "1b8b740a6fa3_remove_fk_constraint_in_message_.py",
      "code": "def upgrade() -> None:\n    conn = op.get_bind()\n\n    # For SQLite, we need to recreate the tables without the constraints\n    # This approach preserves all data while removing the constraints\n\n    # 1. Handle vertex_build table\n    if migration.table_exists(\"vertex_build\", conn):\n        # Create a temporary table without the constraint\n        temp_table_name = \"temp_vertex_build\"\n",
      "variables": [
        "conn",
        "temp_table_name"
      ],
      "anonymized_code": "def upgrade() -> None:\n    var_1 = op.get_bind()\n\n    # For SQLite, we need to recreate the tables without the constraints\n    # This approach preserves all data while removing the constraints\n\n    # 1. Handle vertex_build table\n    if migration.table_exists(\"vertex_build\", var_1):\n        # Create a temporary table without the constraint\n        var_2 = \"temp_vertex_build\"\n",
      "lines_processed": 10,
      "total_lines": 337
    },
    {
      "file_path": "1b8b740a6fa3_remove_fk_constraint_in_message_.py",
      "code": "def constraint_exists(constraint_name: str, conn) -> bool:\n    \"\"\"Check if a constraint with the given name already exists in the database.\n\n    Args:\n        constraint_name: The name of the constraint to check\n        conn: SQLAlchemy connection\n\n    Returns:\n        bool: True if the constraint exists, False otherwise\n    \"\"\"\n",
      "variables": [
        "constraint_name",
        "conn"
      ],
      "anonymized_code": "def constraint_exists(var_1: str, var_2) -> bool:\n    \"\"\"Check if a constraint with the given name already exists in the database.\n\n    Args:\n        var_1: The name of the constraint to check\n        var_2: SQLAlchemy connection\n\n    Returns:\n        bool: True if the constraint exists, False otherwise\n    \"\"\"\n",
      "lines_processed": 10,
      "total_lines": 337
    },
    {
      "file_path": "__main__.py",
      "code": "def generate_pip_command(package_names, is_pre_release) -> str:\n    \"\"\"Generate the pip install command based on the packages and whether it's a pre-release.\"\"\"\n    base_command = \"pip install\"\n    if is_pre_release:\n        return f\"{base_command} {' '.join(package_names)} -U --pre\"\n    return f\"{base_command} {' '.join(package_names)} -U\"\n",
      "variables": [
        "package_names",
        "is_pre_release",
        "base_command"
      ],
      "anonymized_code": "def generate_pip_command(var_1, var_2) -> str:\n    \"\"\"Generate the pip install command based on the packages and whether it's a pre-release.\"\"\"\n    var_3 = \"pip install\"\n    if var_2:\n        return f\"{var_3} {' '.join(var_1)} -U --pre\"\n    return f\"{var_3} {' '.join(var_1)} -U\"\n",
      "lines_processed": 6,
      "total_lines": 672
    },
    {
      "file_path": "__main__.py",
      "code": "def handle_sigterm(signum, frame):  # noqa: ARG001\n    \"\"\"Handle SIGTERM signal gracefully.\"\"\"\n    logger.info(\"Received SIGTERM signal. Performing graceful shutdown...\")\n    # Raise SystemExit to trigger graceful shutdown\n    sys.exit(0)\n",
      "variables": [
        "signum",
        "frame"
      ],
      "anonymized_code": "def handle_sigterm(var_1, var_2):  # noqa: ARG001\n    \"\"\"Handle SIGTERM signal gracefully.\"\"\"\n    logger.info(\"Received SIGTERM signal. Performing graceful shutdown...\")\n    # Raise SystemExit to trigger graceful shutdown\n    sys.exit(0)\n",
      "lines_processed": 5,
      "total_lines": 672
    },
    {
      "file_path": "__main__.py",
      "code": "def run_on_mac_or_linux(host, port, log_level, options, app, protocol):\n    webapp_process = Process(target=run_langflow, args=(host, port, log_level, options, app))\n    webapp_process.start()\n    wait_for_server_ready(host, port, protocol)\n\n    print_banner(host, port, protocol)\n    return webapp_process\n",
      "variables": [
        "host",
        "port",
        "log_level",
        "options",
        "app",
        "protocol",
        "webapp_process"
      ],
      "anonymized_code": "def run_on_mac_or_linux(var_1, var_2, var_3, var_4, var_5, var_6):\n    var_7 = Process(target=run_langflow, args=(var_1, var_2, var_3, var_4, var_5))\n    var_7.start()\n    wait_for_server_ready(var_1, var_2, var_6)\n\n    print_banner(var_1, var_2, var_6)\n    return var_7\n",
      "lines_processed": 7,
      "total_lines": 672
    },
    {
      "file_path": "__main__.py",
      "code": "def run_on_windows(host, port, log_level, options, app, protocol) -> None:\n    \"\"\"Run the Langflow server on Windows.\"\"\"\n    print_banner(host, port, protocol)\n    run_langflow(host, port, log_level, options, app)\n",
      "variables": [
        "host",
        "port",
        "log_level",
        "options",
        "app",
        "protocol"
      ],
      "anonymized_code": "def run_on_windows(var_1, var_2, var_3, var_4, var_5, var_6) -> None:\n    \"\"\"Run the Langflow server on Windows.\"\"\"\n    print_banner(var_1, var_2, var_6)\n    run_langflow(var_1, var_2, var_3, var_4, var_5)\n",
      "lines_processed": 4,
      "total_lines": 672
    },
    {
      "file_path": "update_pyproject_name.py",
      "code": "def update_uv_dep(pyproject_path: str, new_project_name: str) -> None:\n    \"\"\"Update the langflow-base dependency in pyproject.toml.\"\"\"\n    filepath = BASE_DIR / pyproject_path\n    content = filepath.read_text(encoding=\"utf-8\")\n\n    if new_project_name == \"langflow-nightly\":\n        pattern = re.compile(r\"langflow = \\{ workspace = true \\}\")\n        replacement = \"langflow-nightly = { workspace = true }\"\n    elif new_project_name == \"langflow-base-nightly\":\n        pattern = re.compile(r\"langflow-base = \\{ workspace = true \\}\")\n",
      "variables": [
        "pyproject_path",
        "new_project_name",
        "filepath",
        "content",
        "pattern",
        "replacement"
      ],
      "anonymized_code": "def update_uv_dep(var_1: str, var_2: str) -> None:\n    \"\"\"Update the langflow-base dependency in pyproject.toml.\"\"\"\n    var_3 = BASE_DIR / var_1\n    var_4 = var_3.read_text(encoding=\"utf-8\")\n\n    if var_2 == \"langflow-nightly\":\n        var_5 = re.compile(r\"langflow = \\{ workspace = true \\}\")\n        var_6 = \"langflow-nightly = { workspace = true }\"\n    elif var_2 == \"langflow-base-nightly\":\n        var_5 = re.compile(r\"langflow-base = \\{ workspace = true \\}\")\n",
      "lines_processed": 10,
      "total_lines": 69
    },
    {
      "file_path": "update_pyproject_name.py",
      "code": "def update_pyproject_name(pyproject_path: str, new_project_name: str) -> None:\n    \"\"\"Update the project name in pyproject.toml.\"\"\"\n    filepath = BASE_DIR / pyproject_path\n    content = filepath.read_text(encoding=\"utf-8\")\n\n    # Regex to match the version line under [tool.poetry]\n    pattern = re.compile(r'(?<=^name = \")[^\"]+(?=\")', re.MULTILINE)\n\n    if not pattern.search(content):\n        msg = f'Project name not found in \"{filepath}\"'\n",
      "variables": [
        "pyproject_path",
        "new_project_name",
        "filepath",
        "content",
        "pattern",
        "msg"
      ],
      "anonymized_code": "def update_pyproject_name(var_1: str, var_2: str) -> None:\n    \"\"\"Update the project name in pyproject.toml.\"\"\"\n    var_3 = BASE_DIR / var_1\n    var_4 = var_3.read_text(encoding=\"utf-8\")\n\n    # Regex to match the version line under [tool.poetry]\n    var_5 = re.compile(r'(?<=^name = \")[^\"]+(?=\")', re.MULTILINE)\n\n    if not var_5.search(var_4):\n        var_6 = f'Project name not found in \"{var_3}\"'\n",
      "lines_processed": 10,
      "total_lines": 69
    },
    {
      "file_path": "update_pyproject_name.py",
      "code": "def main() -> None:\n    if len(sys.argv) != ARGUMENT_NUMBER:\n        msg = \"Must specify project name and build type, e.g. langflow-nightly base\"\n        raise ValueError(msg)\n    new_project_name = sys.argv[1]\n    build_type = sys.argv[2]\n\n    if build_type == \"base\":\n        update_pyproject_name(\"src/backend/base/pyproject.toml\", new_project_name)\n        update_uv_dep(\"pyproject.toml\", new_project_name)\n",
      "variables": [
        "msg",
        "new_project_name",
        "build_type"
      ],
      "anonymized_code": "def main() -> None:\n    if len(sys.argv) != ARGUMENT_NUMBER:\n        var_1 = \"Must specify project name and build type, e.g. langflow-nightly base\"\n        raise ValueError(var_1)\n    var_2 = sys.argv[1]\n    var_3 = sys.argv[2]\n\n    if var_3 == \"base\":\n        update_pyproject_name(\"src/backend/base/pyproject.toml\", var_2)\n        update_uv_dep(\"pyproject.toml\", var_2)\n",
      "lines_processed": 10,
      "total_lines": 69
    },
    {
      "file_path": "update_pyproject_version.py",
      "code": "def update_pyproject_version(pyproject_path: str, new_version: str) -> None:\n    \"\"\"Update the version in pyproject.toml.\"\"\"\n    filepath = BASE_DIR / pyproject_path\n    content = filepath.read_text(encoding=\"utf-8\")\n\n    # Regex to match the version line under [tool.poetry]\n    pattern = re.compile(r'(?<=^version = \")[^\"]+(?=\")', re.MULTILINE)\n\n    if not pattern.search(content):\n        msg = f'Project version not found in \"{filepath}\"'\n",
      "variables": [
        "pyproject_path",
        "new_version",
        "filepath",
        "content",
        "pattern",
        "msg"
      ],
      "anonymized_code": "def update_pyproject_version(var_1: str, var_2: str) -> None:\n    \"\"\"Update the version in pyproject.toml.\"\"\"\n    var_3 = BASE_DIR / var_1\n    var_4 = var_3.read_text(encoding=\"utf-8\")\n\n    # Regex to match the version line under [tool.poetry]\n    var_5 = re.compile(r'(?<=^version = \")[^\"]+(?=\")', re.MULTILINE)\n\n    if not var_5.search(var_4):\n        var_6 = f'Project version not found in \"{var_3}\"'\n",
      "lines_processed": 10,
      "total_lines": 61
    },
    {
      "file_path": "update_pyproject_version.py",
      "code": "def main() -> None:\n    if len(sys.argv) != ARGUMENT_NUMBER:\n        msg = \"New version not specified\"\n        raise ValueError(msg)\n    new_version = sys.argv[1]\n\n    # Strip \"v\" prefix from version if present\n    new_version = new_version.removeprefix(\"v\")\n\n    build_type = sys.argv[2]\n",
      "variables": [
        "msg",
        "new_version",
        "build_type"
      ],
      "anonymized_code": "def main() -> None:\n    if len(sys.argv) != ARGUMENT_NUMBER:\n        var_1 = \"New version not specified\"\n        raise ValueError(var_1)\n    var_2 = sys.argv[1]\n\n    # Strip \"v\" prefix from version if present\n    var_2 = var_2.removeprefix(\"v\")\n\n    var_3 = sys.argv[2]\n",
      "lines_processed": 10,
      "total_lines": 61
    },
    {
      "file_path": "update_pyproject_version.py",
      "code": "def verify_pep440(version):\n    \"\"\"Verify if version is PEP440 compliant.\n\n    https://github.com/pypa/packaging/blob/16.7/packaging/version.py#L191\n    \"\"\"\n    return packaging.version.Version(version)\n",
      "variables": [
        "version"
      ],
      "anonymized_code": "def verify_pep440(var_1):\n    \"\"\"Verify if var_1 is PEP440 compliant.\n\n    https://github.com/pypa/packaging/blob/16.7/packaging/var_1.py#L191\n    \"\"\"\n    return packaging.var_1.Version(var_1)\n",
      "lines_processed": 6,
      "total_lines": 61
    },
    {
      "file_path": "env.py",
      "code": "def _sqlite_do_connect(\n    dbapi_connection,\n    connection_record,  # noqa: ARG001\n):\n    # disable pysqlite's emitting of the BEGIN statement entirely.\n    # also stops it from emitting COMMIT before any DDL.\n    dbapi_connection.isolation_level = None\n",
      "variables": [
        "dbapi_connection",
        "connection_record"
      ],
      "anonymized_code": "def _sqlite_do_connect(\n    var_1,\n    var_2,  # noqa: ARG001\n):\n    # disable pysqlite's emitting of the BEGIN statement entirely.\n    # also stops it from emitting COMMIT before any DDL.\n    var_1.isolation_level = None\n",
      "lines_processed": 7,
      "total_lines": 124
    },
    {
      "file_path": "env.py",
      "code": "def _do_run_migrations(connection):\n    context.configure(\n        connection=connection, target_metadata=target_metadata, render_as_batch=True, prepare_threshold=None\n    )\n\n    with context.begin_transaction():\n        if connection.dialect.name == \"postgresql\":\n            connection.execute(text(\"SET LOCAL lock_timeout = '60s';\"))\n            connection.execute(text(\"SELECT pg_advisory_xact_lock(112233);\"))\n        context.run_migrations()\n",
      "variables": [
        "connection"
      ],
      "anonymized_code": "def _do_run_migrations(var_1):\n    context.configure(\n        var_1=var_1, target_metadata=target_metadata, render_as_batch=True, prepare_threshold=None\n    )\n\n    with context.begin_transaction():\n        if var_1.dialect.name == \"postgresql\":\n            var_1.execute(text(\"SET LOCAL lock_timeout = '60s';\"))\n            var_1.execute(text(\"SELECT pg_advisory_xact_lock(112233);\"))\n        context.run_migrations()\n",
      "lines_processed": 10,
      "total_lines": 124
    },
    {
      "file_path": "env.py",
      "code": "def _sqlite_do_begin(conn):\n    # emit our own BEGIN\n    conn.exec_driver_sql(\"PRAGMA busy_timeout = 60000\")\n    conn.exec_driver_sql(\"BEGIN EXCLUSIVE\")\n",
      "variables": [
        "conn"
      ],
      "anonymized_code": "def _sqlite_do_begin(var_1):\n    # emit our own BEGIN\n    var_1.exec_driver_sql(\"PRAGMA busy_timeout = 60000\")\n    var_1.exec_driver_sql(\"BEGIN EXCLUSIVE\")\n",
      "lines_processed": 4,
      "total_lines": 124
    },
    {
      "file_path": "0d60fcbd4e8e_create_vertex_builds_table.py",
      "code": "def downgrade() -> None:\n    conn = op.get_bind()\n    if migration.table_exists(\"vertex_build\", conn):\n        op.drop_table(\"vertex_build\")\n    pass\n",
      "variables": [
        "conn"
      ],
      "anonymized_code": "def downgrade() -> None:\n    var_1 = op.get_bind()\n    if migration.table_exists(\"vertex_build\", var_1):\n        op.drop_table(\"vertex_build\")\n    pass\n",
      "lines_processed": 5,
      "total_lines": 51
    },
    {
      "file_path": "update_pyproject_combined.py",
      "code": "def main():\n    \"\"\"Universal update script that handles both base and main updates in a single run.\n\n    Usage:\n    update_pyproject_combined.py main <main_tag> <base_tag>\n    \"\"\"\n    arg_count = 4\n    if len(sys.argv) != arg_count:\n        print(\"Usage:\")\n        print(\"  update_pyproject_combined.py main <main_tag> <base_tag>\")\n",
      "variables": [
        "arg_count"
      ],
      "anonymized_code": "def main():\n    \"\"\"Universal update script that handles both base and main updates in a single run.\n\n    Usage:\n    update_pyproject_combined.py main <main_tag> <base_tag>\n    \"\"\"\n    var_1 = 4\n    if len(sys.argv) != var_1:\n        print(\"Usage:\")\n        print(\"  update_pyproject_combined.py main <main_tag> <base_tag>\")\n",
      "lines_processed": 10,
      "total_lines": 52
    }
  ],
  "pandas-dev_pandas": [
    {
      "file_path": "ctors.py",
      "code": "def gen_of_str(arr):\n    return (x for x in arr.astype(str))\n",
      "variables": [
        "arr",
        "x"
      ],
      "anonymized_code": "def gen_of_str(var_1):\n    return (var_2 for var_2 in var_1.astype(str))\n",
      "lines_processed": 2,
      "total_lines": 145
    },
    {
      "file_path": "ctors.py",
      "code": "def arr_dict(arr):\n    return dict(zip(range(len(arr)), arr))\n",
      "variables": [
        "arr"
      ],
      "anonymized_code": "def arr_dict(var_1):\n    return dict(zip(range(len(var_1)), var_1))\n",
      "lines_processed": 2,
      "total_lines": 145
    },
    {
      "file_path": "ctors.py",
      "code": "def list_of_lists(arr):\n    return [[i, -i] for i in arr]\n",
      "variables": [
        "arr",
        "i"
      ],
      "anonymized_code": "def list_of_lists(var_1):\n    return [[var_2, -var_2] for var_2 in var_1]\n",
      "lines_processed": 2,
      "total_lines": 145
    },
    {
      "file_path": "ctors.py",
      "code": "def no_change(arr):\n    return arr\n",
      "variables": [
        "arr"
      ],
      "anonymized_code": "def no_change(var_1):\n    return var_1\n",
      "lines_processed": 2,
      "total_lines": 145
    }
  ],
  "apache_airflow": [
    {
      "file_path": "conf.py",
      "code": "def setup(sphinx):\n    sphinx.connect(\"autoapi-skip-member\", skip_util_classes_extension)\n",
      "variables": [
        "sphinx"
      ],
      "anonymized_code": "def setup(var_1):\n    var_1.connect(\"autoapi-skip-member\", skip_util_classes_extension)\n",
      "lines_processed": 2,
      "total_lines": 375
    },
    {
      "file_path": "airflow_health.py",
      "code": "def get_airflow_health() -> dict[str, Any]:\n    \"\"\"Get the health for Airflow metadatabase, scheduler and triggerer.\"\"\"\n    metadatabase_status = HEALTHY\n    latest_scheduler_heartbeat = None\n    latest_triggerer_heartbeat = None\n    latest_dag_processor_heartbeat = None\n\n    scheduler_status = UNHEALTHY\n    triggerer_status: str | None = UNHEALTHY\n    dag_processor_status: str | None = UNHEALTHY\n",
      "variables": [
        "metadatabase_status",
        "latest_scheduler_heartbeat",
        "latest_triggerer_heartbeat",
        "latest_dag_processor_heartbeat",
        "scheduler_status",
        "triggerer_status",
        "dag_processor_status"
      ],
      "anonymized_code": "def get_airflow_health() -> dict[str, Any]:\n    \"\"\"Get the health for Airflow metadatabase, scheduler and triggerer.\"\"\"\n    var_1 = HEALTHY\n    var_2 = None\n    var_3 = None\n    var_4 = None\n\n    var_5 = UNHEALTHY\n    var_6: str | None = UNHEALTHY\n    var_7: str | None = UNHEALTHY\n",
      "lines_processed": 10,
      "total_lines": 90
    }
  ],
  "numpy_numpy": [
    {
      "file_path": "cmds.py",
      "code": "def _run_asv(cmd):\n    # Always use ccache, if installed\n    PATH = os.environ['PATH']\n    EXTRA_PATH = os.pathsep.join([\n        '/usr/lib/ccache', '/usr/lib/f90cache',\n        '/usr/local/lib/ccache', '/usr/local/lib/f90cache'\n    ])\n    env = os.environ\n    env['PATH'] = f'{EXTRA_PATH}{os.pathsep}{PATH}'\n\n",
      "variables": [
        "cmd",
        "PATH",
        "EXTRA_PATH",
        "env"
      ],
      "anonymized_code": "def _run_asv(var_1):\n    # Always use ccache, if installed\n    var_2 = os.environ['var_2']\n    var_3 = os.pathsep.join([\n        '/usr/lib/ccache', '/usr/lib/f90cache',\n        '/usr/local/lib/ccache', '/usr/local/lib/f90cache'\n    ])\n    var_4 = os.environ\n    var_4['var_2'] = f'{var_3}{os.pathsep}{var_2}'\n\n",
      "lines_processed": 10,
      "total_lines": 626
    },
    {
      "file_path": "cmds.py",
      "code": "def ipython(*, ipython_args, build_dir):\n    \"\"\"\ud83d\udcbb Launch IPython shell with PYTHONPATH set\n\n    OPTIONS are passed through directly to IPython, e.g.:\n\n    spin ipython -i myscript.py\n    \"\"\"\n    env = os.environ\n    env['PYTHONWARNINGS'] = env.get('PYTHONWARNINGS', 'all')\n\n",
      "variables": [
        "ipython_args",
        "build_dir",
        "env"
      ],
      "anonymized_code": "def ipython(*, var_1, var_2):\n    \"\"\"\ud83d\udcbb Launch IPython shell with PYTHONPATH set\n\n    OPTIONS are passed through directly to IPython, e.g.:\n\n    spin ipython -i myscript.py\n    \"\"\"\n    var_3 = os.environ\n    var_3['PYTHONWARNINGS'] = var_3.get('PYTHONWARNINGS', 'all')\n\n",
      "lines_processed": 10,
      "total_lines": 626
    },
    {
      "file_path": "cmds.py",
      "code": "def build(*, parent_callback, with_scipy_openblas, **kwargs):\n    if with_scipy_openblas:\n        _config_openblas(with_scipy_openblas)\n    parent_callback(**kwargs)\n",
      "variables": [
        "parent_callback",
        "with_scipy_openblas",
        "kwargs"
      ],
      "anonymized_code": "def build(*, var_1, var_2, **var_3):\n    if var_2:\n        _config_openblas(var_2)\n    var_1(**var_3)\n",
      "lines_processed": 4,
      "total_lines": 626
    },
    {
      "file_path": "cmds.py",
      "code": "def _get_numpy_tools(filename):\n    filepath = pathlib.Path('tools', filename)\n    spec = importlib.util.spec_from_file_location(filename.stem, filepath)\n    module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(module)\n    return module\n",
      "variables": [
        "filename",
        "filepath",
        "spec",
        "module"
      ],
      "anonymized_code": "def _get_numpy_tools(var_1):\n    var_2 = pathlib.Path('tools', var_1)\n    var_3 = importlib.util.spec_from_file_location(var_1.stem, var_2)\n    var_4 = importlib.util.module_from_spec(var_3)\n    var_3.loader.exec_module(var_4)\n    return var_4\n",
      "lines_processed": 6,
      "total_lines": 626
    },
    {
      "file_path": "cmds.py",
      "code": "def _set_mem_rlimit(max_mem=None):\n    \"\"\"\n    Set address space rlimit\n    \"\"\"\n    import resource\n\n    import psutil\n\n    mem = psutil.virtual_memory()\n\n",
      "variables": [
        "max_mem",
        "mem"
      ],
      "anonymized_code": "def _set_mem_rlimit(var_1=None):\n    \"\"\"\n    Set address space rlimit\n    \"\"\"\n    import resource\n\n    import psutil\n\n    var_2 = psutil.virtual_memory()\n\n",
      "lines_processed": 10,
      "total_lines": 626
    },
    {
      "file_path": "cmds.py",
      "code": "def config_openblas(with_scipy_openblas):\n    \"\"\"\ud83d\udd27 Create .openblas/scipy-openblas.pc file\n\n    Also create _distributor_init_local.py\n\n    Requires a pre-installed scipy-openblas64 or scipy-openblas32\n    \"\"\"\n    _config_openblas(with_scipy_openblas)\n",
      "variables": [
        "with_scipy_openblas"
      ],
      "anonymized_code": "def config_openblas(var_1):\n    \"\"\"\ud83d\udd27 Create .openblas/scipy-openblas.pc file\n\n    Also create _distributor_init_local.py\n\n    Requires a pre-installed scipy-openblas64 or scipy-openblas32\n    \"\"\"\n    _config_openblas(var_1)\n",
      "lines_processed": 8,
      "total_lines": 626
    },
    {
      "file_path": "cmds.py",
      "code": "def mypy(ctx):\n    \"\"\"\ud83e\udd86 Run Mypy tests for NumPy\n    \"\"\"\n    env = os.environ\n    env['NPY_RUN_MYPY_IN_TESTSUITE'] = '1'\n    ctx.params['pytest_args'] = [os.path.join('numpy', 'typing')]\n    ctx.params['markexpr'] = 'full'\n    ctx.forward(test)\n",
      "variables": [
        "ctx",
        "env"
      ],
      "anonymized_code": "def mypy(var_1):\n    \"\"\"\ud83e\udd86 Run Mypy tests for NumPy\n    \"\"\"\n    var_2 = os.environ\n    var_2['NPY_RUN_MYPY_IN_TESTSUITE'] = '1'\n    var_1.params['pytest_args'] = [os.path.join('numpy', 'typing')]\n    var_1.params['markexpr'] = 'full'\n    var_1.forward(test)\n",
      "lines_processed": 8,
      "total_lines": 626
    },
    {
      "file_path": "bench_overrides.py",
      "code": "def _concatenate_dispatcher(arrays, axis=None, out=None):\n    if out is not None:\n        arrays = list(arrays)\n        arrays.append(out)\n    return arrays\n",
      "variables": [
        "arrays",
        "axis",
        "out"
      ],
      "anonymized_code": "def _concatenate_dispatcher(var_1, var_2=None, var_3=None):\n    if var_3 is not None:\n        var_1 = list(var_1)\n        var_1.append(var_3)\n    return var_1\n",
      "lines_processed": 5,
      "total_lines": 67
    },
    {
      "file_path": "bench_overrides.py",
      "code": "def mock_broadcast_to(array, shape, subok=False):\n    pass\n",
      "variables": [
        "array",
        "shape",
        "subok"
      ],
      "anonymized_code": "def mock_broadcast_to(var_1, var_2, var_3=False):\n    pass\n",
      "lines_processed": 2,
      "total_lines": 67
    },
    {
      "file_path": "bench_overrides.py",
      "code": "def _broadcast_to_dispatcher(array, shape, subok=None):\n    return (array,)\n",
      "variables": [
        "array",
        "shape",
        "subok"
      ],
      "anonymized_code": "def _broadcast_to_dispatcher(var_1, var_2, var_3=None):\n    return (var_1,)\n",
      "lines_processed": 2,
      "total_lines": 67
    },
    {
      "file_path": "__init__.py",
      "code": "def dirty_lock(lock_name, lock_on_count=1):\n    # this lock occurred before each round to avoid duplicate printing\n    if not hasattr(os, \"getppid\"):\n        return False\n    ppid = os.getppid()\n    if not ppid or ppid == os.getpid():\n        # not sure if this gonna happen, but ASV run each round in\n        # a separate process so the lock should be based on the parent\n        # process id only\n        return False\n",
      "variables": [
        "lock_name",
        "lock_on_count",
        "ppid"
      ],
      "anonymized_code": "def dirty_lock(var_1, var_2=1):\n    # this lock occurred before each round to avoid duplicate printing\n    if not hasattr(os, \"getppid\"):\n        return False\n    var_3 = os.getppid()\n    if not var_3 or var_3 == os.getpid():\n        # not sure if this gonna happen, but ASV run each round in\n        # a separate process so the lock should be based on the parent\n        # process id only\n        return False\n",
      "lines_processed": 10,
      "total_lines": 55
    },
    {
      "file_path": "__init__.py",
      "code": "def show_cpu_features():\n    from numpy.lib._utils_impl import _opt_info\n    info = _opt_info()\n    info = \"NumPy CPU features: \" + (info or 'nothing enabled')\n    # ASV wrapping stdout & stderr, so we assume having a tty here\n    if 'SHELL' in os.environ and sys.platform != 'win32':\n        # to avoid the red color that imposed by ASV\n        print(f\"\\033[33m{info}\\033[0m\")\n    else:\n        print(info)\n",
      "variables": [
        "info"
      ],
      "anonymized_code": "def show_cpu_features():\n    from numpy.lib._utils_impl import _opt_info\n    var_1 = _opt_info()\n    var_1 = \"NumPy CPU features: \" + (var_1 or 'nothing enabled')\n    # ASV wrapping stdout & stderr, so we assume having a tty here\n    if 'SHELL' in os.environ and sys.platform != 'win32':\n        # to avoid the red color that imposed by ASV\n        print(f\"\\033[33m{var_1}\\033[0m\")\n    else:\n        print(var_1)\n",
      "lines_processed": 10,
      "total_lines": 55
    }
  ],
  "PaddlePaddle_PaddleOCR": [
    {
      "file_path": "make_shrink_map.py",
      "code": "def shrink_polygon_pyclipper(polygon, shrink_ratio):\n    from shapely.geometry import Polygon\n    import pyclipper\n\n    polygon_shape = Polygon(polygon)\n    distance = (\n        polygon_shape.area * (1 - np.power(shrink_ratio, 2)) / polygon_shape.length\n    )\n    subject = [tuple(l) for l in polygon]\n    padding = pyclipper.PyclipperOffset()\n",
      "variables": [
        "polygon",
        "shrink_ratio",
        "polygon_shape",
        "distance",
        "subject",
        "l",
        "padding"
      ],
      "anonymized_code": "def shrink_polygon_pyclipper(var_1, var_2):\n    from shapely.geometry import Polygon\n    import pyclipper\n\n    var_3 = Polygon(var_1)\n    var_4 = (\n        var_3.area * (1 - np.power(var_2, 2)) / var_3.length\n    )\n    var_5 = [tuple(var_6) for var_6 in var_1]\n    var_7 = pyclipper.PyclipperOffset()\n",
      "lines_processed": 10,
      "total_lines": 129
    },
    {
      "file_path": "make_shrink_map.py",
      "code": "def shrink_polygon_py(polygon, shrink_ratio):\n    \"\"\"\n    \u5bf9\u6846\u8fdb\u884c\u7f29\u653e\uff0c\u8fd4\u56de\u53bb\u7684\u6bd4\u4f8b\u4e3a1/shrink_ratio \u5373\u53ef\n    \"\"\"\n    cx = polygon[:, 0].mean()\n    cy = polygon[:, 1].mean()\n    polygon[:, 0] = cx + (polygon[:, 0] - cx) * shrink_ratio\n    polygon[:, 1] = cy + (polygon[:, 1] - cy) * shrink_ratio\n    return polygon\n",
      "variables": [
        "polygon",
        "shrink_ratio",
        "cx",
        "cy"
      ],
      "anonymized_code": "def shrink_polygon_py(var_1, var_2):\n    \"\"\"\n    \u5bf9\u6846\u8fdb\u884c\u7f29\u653e\uff0c\u8fd4\u56de\u53bb\u7684\u6bd4\u4f8b\u4e3a1/var_2 \u5373\u53ef\n    \"\"\"\n    var_3 = var_1[:, 0].mean()\n    var_4 = var_1[:, 1].mean()\n    var_1[:, 0] = var_3 + (var_1[:, 0] - var_3) * var_2\n    var_1[:, 1] = var_4 + (var_1[:, 1] - var_4) * var_2\n    return var_1\n",
      "lines_processed": 9,
      "total_lines": 129
    },
    {
      "file_path": "resnet.py",
      "code": "def load_models(model, model_name):\n    import torch.utils.model_zoo as model_zoo\n\n    torch_patams = model_zoo.load_url(model_urls[model_name])\n    load_torch_params(model, torch_patams)\n",
      "variables": [
        "model",
        "model_name",
        "torch_patams"
      ],
      "anonymized_code": "def load_models(var_1, var_2):\n    import torch.utils.model_zoo as model_zoo\n\n    var_3 = model_zoo.load_url(model_urls[var_2])\n    load_torch_params(var_1, var_3)\n",
      "lines_processed": 5,
      "total_lines": 366
    },
    {
      "file_path": "resnet.py",
      "code": "def deformable_resnet18(pretrained=True, **kwargs):\n    \"\"\"Constructs a ResNet-18 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(BasicBlock, [2, 2, 2, 2], dcn=dict(deformable_groups=1), **kwargs)\n    if pretrained:\n        assert (\n            kwargs.get(\"in_channels\", 3) == 3\n        ), \"in_channels must be 3 when pretrained is True\"\n",
      "variables": [
        "pretrained",
        "kwargs",
        "model"
      ],
      "anonymized_code": "def deformable_resnet18(var_1=True, **var_2):\n    \"\"\"Constructs a ResNet-18 var_3.\n    Args:\n        var_1 (bool): If True, returns a var_3 pre-trained on ImageNet\n    \"\"\"\n    var_3 = ResNet(BasicBlock, [2, 2, 2, 2], dcn=dict(deformable_groups=1), **var_2)\n    if var_1:\n        assert (\n            var_2.get(\"in_channels\", 3) == 3\n        ), \"in_channels must be 3 when var_1 is True\"\n",
      "lines_processed": 10,
      "total_lines": 366
    },
    {
      "file_path": "resnet.py",
      "code": "def resnet18(pretrained=True, **kwargs):\n    \"\"\"Constructs a ResNet-18 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        assert (\n            kwargs.get(\"in_channels\", 3) == 3\n        ), \"in_channels must be 3 when pretrained is True\"\n",
      "variables": [
        "pretrained",
        "kwargs",
        "model"
      ],
      "anonymized_code": "def resnet18(var_1=True, **var_2):\n    \"\"\"Constructs a ResNet-18 var_3.\n    Args:\n        var_1 (bool): If True, returns a var_3 pre-trained on ImageNet\n    \"\"\"\n    var_3 = ResNet(BasicBlock, [2, 2, 2, 2], **var_2)\n    if var_1:\n        assert (\n            var_2.get(\"in_channels\", 3) == 3\n        ), \"in_channels must be 3 when var_1 is True\"\n",
      "lines_processed": 10,
      "total_lines": 366
    },
    {
      "file_path": "resnet.py",
      "code": "def conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2D(\n        in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias_attr=False\n    )\n",
      "variables": [
        "in_planes",
        "out_planes",
        "stride"
      ],
      "anonymized_code": "def conv3x3(var_1, var_2, var_3=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2D(\n        var_1, var_2, kernel_size=3, var_3=var_3, padding=1, bias_attr=False\n    )\n",
      "lines_processed": 5,
      "total_lines": 366
    },
    {
      "file_path": "resnet.py",
      "code": "def resnet50(pretrained=True, **kwargs):\n    \"\"\"Constructs a ResNet-50 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        assert (\n            kwargs.get(\"in_channels\", 3) == 3\n        ), \"in_channels must be 3 when pretrained is True\"\n",
      "variables": [
        "pretrained",
        "kwargs",
        "model"
      ],
      "anonymized_code": "def resnet50(var_1=True, **var_2):\n    \"\"\"Constructs a ResNet-50 var_3.\n    Args:\n        var_1 (bool): If True, returns a var_3 pre-trained on ImageNet\n    \"\"\"\n    var_3 = ResNet(Bottleneck, [3, 4, 6, 3], **var_2)\n    if var_1:\n        assert (\n            var_2.get(\"in_channels\", 3) == 3\n        ), \"in_channels must be 3 when var_1 is True\"\n",
      "lines_processed": 10,
      "total_lines": 366
    },
    {
      "file_path": "resnet.py",
      "code": "def resnet152(pretrained=True, **kwargs):\n    \"\"\"Constructs a ResNet-152 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        assert (\n            kwargs.get(\"in_channels\", 3) == 3\n        ), \"in_channels must be 3 when pretrained is True\"\n",
      "variables": [
        "pretrained",
        "kwargs",
        "model"
      ],
      "anonymized_code": "def resnet152(var_1=True, **var_2):\n    \"\"\"Constructs a ResNet-152 var_3.\n    Args:\n        var_1 (bool): If True, returns a var_3 pre-trained on ImageNet\n    \"\"\"\n    var_3 = ResNet(Bottleneck, [3, 8, 36, 3], **var_2)\n    if var_1:\n        assert (\n            var_2.get(\"in_channels\", 3) == 3\n        ), \"in_channels must be 3 when var_1 is True\"\n",
      "lines_processed": 10,
      "total_lines": 366
    },
    {
      "file_path": "__init__.py",
      "code": "def build_loss(config):\n    copy_config = copy.deepcopy(config)\n    loss_type = copy_config.pop(\"type\")\n    assert loss_type in support_loss, f\"all support loss is {support_loss}\"\n    criterion = eval(loss_type)(**copy_config)\n    return criterion\n",
      "variables": [
        "config",
        "copy_config",
        "loss_type",
        "criterion"
      ],
      "anonymized_code": "def build_loss(var_1):\n    var_2 = copy.deepcopy(var_1)\n    var_3 = var_2.pop(\"type\")\n    assert var_3 in support_loss, f\"all support loss is {support_loss}\"\n    var_4 = eval(var_3)(**var_2)\n    return var_4\n",
      "lines_processed": 6,
      "total_lines": 16
    }
  ],
  "psf_requests": [
    {
      "file_path": "api.py",
      "code": "def options(url, **kwargs):\n    r\"\"\"Sends an OPTIONS request.\n\n    :param url: URL for the new :class:`Request` object.\n    :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    return request(\"options\", url, **kwargs)\n",
      "variables": [
        "url",
        "kwargs"
      ],
      "anonymized_code": "def options(var_1, **var_2):\n    r\"\"\"Sends an OPTIONS request.\n\n    :param var_1: URL for the new :class:`Request` object.\n    :param \\*\\*var_2: Optional arguments that ``request`` takes.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    return request(\"options\", var_1, **var_2)\n",
      "lines_processed": 10,
      "total_lines": 157
    },
    {
      "file_path": "api.py",
      "code": "def delete(url, **kwargs):\n    r\"\"\"Sends a DELETE request.\n\n    :param url: URL for the new :class:`Request` object.\n    :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    return request(\"delete\", url, **kwargs)\n",
      "variables": [
        "url",
        "kwargs"
      ],
      "anonymized_code": "def delete(var_1, **var_2):\n    r\"\"\"Sends a DELETE request.\n\n    :param var_1: URL for the new :class:`Request` object.\n    :param \\*\\*var_2: Optional arguments that ``request`` takes.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    return request(\"delete\", var_1, **var_2)\n",
      "lines_processed": 10,
      "total_lines": 157
    },
    {
      "file_path": "api.py",
      "code": "def get(url, params=None, **kwargs):\n    r\"\"\"Sends a GET request.\n\n    :param url: URL for the new :class:`Request` object.\n    :param params: (optional) Dictionary, list of tuples or bytes to send\n        in the query string for the :class:`Request`.\n    :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n",
      "variables": [
        "url",
        "params",
        "kwargs"
      ],
      "anonymized_code": "def get(var_1, var_2=None, **var_3):\n    r\"\"\"Sends a GET request.\n\n    :param var_1: URL for the new :class:`Request` object.\n    :param var_2: (optional) Dictionary, list of tuples or bytes to send\n        in the query string for the :class:`Request`.\n    :param \\*\\*var_3: Optional arguments that ``request`` takes.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n",
      "lines_processed": 10,
      "total_lines": 157
    },
    {
      "file_path": "api.py",
      "code": "def head(url, **kwargs):\n    r\"\"\"Sends a HEAD request.\n\n    :param url: URL for the new :class:`Request` object.\n    :param \\*\\*kwargs: Optional arguments that ``request`` takes. If\n        `allow_redirects` is not provided, it will be set to `False` (as\n        opposed to the default :meth:`request` behavior).\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n",
      "variables": [
        "url",
        "kwargs"
      ],
      "anonymized_code": "def head(var_1, **var_2):\n    r\"\"\"Sends a HEAD request.\n\n    :param var_1: URL for the new :class:`Request` object.\n    :param \\*\\*var_2: Optional arguments that ``request`` takes. If\n        `allow_redirects` is not provided, it will be set to `False` (as\n        opposed to the default :meth:`request` behavior).\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n",
      "lines_processed": 10,
      "total_lines": 157
    },
    {
      "file_path": "compat.py",
      "code": "def _resolve_char_detection():\n    \"\"\"Find supported character detection libraries.\"\"\"\n    chardet = None\n    for lib in (\"chardet\", \"charset_normalizer\"):\n        if chardet is None:\n            try:\n                chardet = importlib.import_module(lib)\n            except ImportError:\n                pass\n    return chardet\n",
      "variables": [
        "chardet",
        "lib"
      ],
      "anonymized_code": "def _resolve_char_detection():\n    \"\"\"Find supported character detection libraries.\"\"\"\n    var_1 = None\n    for var_2 in (\"var_1\", \"charset_normalizer\"):\n        if var_1 is None:\n            try:\n                var_1 = importlib.import_module(var_2)\n            except ImportError:\n                pass\n    return var_1\n",
      "lines_processed": 10,
      "total_lines": 106
    },
    {
      "file_path": "_internal_utils.py",
      "code": "def to_native_string(string, encoding=\"ascii\"):\n    \"\"\"Given a string object, regardless of type, returns a representation of\n    that string in the native string type, encoding and decoding where\n    necessary. This assumes ASCII unless told otherwise.\n    \"\"\"\n    if isinstance(string, builtin_str):\n        out = string\n    else:\n        out = string.decode(encoding)\n\n",
      "variables": [
        "string",
        "encoding",
        "out"
      ],
      "anonymized_code": "def to_native_string(var_1, var_2=\"ascii\"):\n    \"\"\"Given a var_1 object, regardless of type, returns a representation of\n    that var_1 in the native var_1 type, var_2 and decoding where\n    necessary. This assumes ASCII unless told otherwise.\n    \"\"\"\n    if isinstance(var_1, builtin_str):\n        var_3 = var_1\n    else:\n        var_3 = var_1.decode(var_2)\n\n",
      "lines_processed": 10,
      "total_lines": 50
    },
    {
      "file_path": "adapters.py",
      "code": "def _urllib3_request_context(\n    request: \"PreparedRequest\",\n    verify: \"bool | str | None\",\n    client_cert: \"typing.Tuple[str, str] | str | None\",\n    poolmanager: \"PoolManager\",\n) -> \"(typing.Dict[str, typing.Any], typing.Dict[str, typing.Any])\":\n    host_params = {}\n    pool_kwargs = {}\n    parsed_request_url = urlparse(request.url)\n    scheme = parsed_request_url.scheme.lower()\n",
      "variables": [
        "request",
        "verify",
        "client_cert",
        "poolmanager",
        "host_params",
        "pool_kwargs",
        "parsed_request_url",
        "scheme"
      ],
      "anonymized_code": "def _urllib3_request_context(\n    var_1: \"PreparedRequest\",\n    var_2: \"bool | str | None\",\n    var_3: \"typing.Tuple[str, str] | str | None\",\n    var_4: \"PoolManager\",\n) -> \"(typing.Dict[str, typing.Any], typing.Dict[str, typing.Any])\":\n    var_5 = {}\n    var_6 = {}\n    var_7 = urlparse(var_1.url)\n    var_8 = var_7.var_8.lower()\n",
      "lines_processed": 10,
      "total_lines": 719
    },
    {
      "file_path": "hooks.py",
      "code": "def default_hooks():\n    return {event: [] for event in HOOKS}\n",
      "variables": [
        "event"
      ],
      "anonymized_code": "def default_hooks():\n    return {var_1: [] for var_1 in HOOKS}\n",
      "lines_processed": 2,
      "total_lines": 33
    },
    {
      "file_path": "__init__.py",
      "code": "def check_compatibility(urllib3_version, chardet_version, charset_normalizer_version):\n    urllib3_version = urllib3_version.split(\".\")\n    assert urllib3_version != [\"dev\"]  # Verify urllib3 isn't installed from git.\n\n    # Sometimes, urllib3 only reports its version as 16.1.\n    if len(urllib3_version) == 2:\n        urllib3_version.append(\"0\")\n\n    # Check urllib3 for compatibility.\n    major, minor, patch = urllib3_version  # noqa: F811\n",
      "variables": [
        "urllib3_version",
        "chardet_version",
        "charset_normalizer_version",
        "major",
        "minor",
        "patch"
      ],
      "anonymized_code": "def check_compatibility(var_1, var_2, var_3):\n    var_1 = var_1.split(\".\")\n    assert var_1 != [\"dev\"]  # Verify urllib3 isn't installed from git.\n\n    # Sometimes, urllib3 only reports its version as 16.1.\n    if len(var_1) == 2:\n        var_1.append(\"0\")\n\n    # Check urllib3 for compatibility.\n    var_4, var_5, var_6 = var_1  # noqa: F811\n",
      "lines_processed": 10,
      "total_lines": 184
    },
    {
      "file_path": "status_codes.py",
      "code": "def _init():\n    for code, titles in _codes.items():\n        for title in titles:\n            setattr(codes, title, code)\n            if not title.startswith((\"\\\\\", \"/\")):\n                setattr(codes, title.upper(), code)\n\n    def doc(code):\n        names = \", \".join(f\"``{n}``\" for n in _codes[code])\n        return \"* %d: %s\" % (code, names)\n",
      "variables": [
        "code",
        "titles",
        "title",
        "names",
        "n"
      ],
      "anonymized_code": "def _init():\n    for var_1, var_2 in _codes.items():\n        for var_3 in var_2:\n            setattr(codes, var_3, var_1)\n            if not var_3.startswith((\"\\\\\", \"/\")):\n                setattr(codes, var_3.upper(), var_1)\n\n    def doc(var_1):\n        var_4 = \", \".join(f\"``{var_5}``\" for var_5 in _codes[var_1])\n        return \"* %d: %s\" % (var_1, var_4)\n",
      "lines_processed": 10,
      "total_lines": 128
    }
  ],
  "open-mmlab_mmdetection": [
    {
      "file_path": "benchmark_inference_fps.py",
      "code": "def results2markdown(result_dict):\n    table_data = []\n    is_multiple_results = False\n    for cfg_name, value in result_dict.items():\n        name = cfg_name.replace('configs/', '')\n        fps = value['fps']\n        ms_times_pre_image = value['ms_times_pre_image']\n        if isinstance(fps, list):\n            is_multiple_results = True\n            mean_fps = value['mean_fps']\n",
      "variables": [
        "result_dict",
        "table_data",
        "is_multiple_results",
        "cfg_name",
        "value",
        "name",
        "fps",
        "ms_times_pre_image",
        "mean_fps"
      ],
      "anonymized_code": "def results2markdown(var_1):\n    var_2 = []\n    var_3 = False\n    for var_4, var_5 in var_1.items():\n        var_6 = var_4.replace('configs/', '')\n        var_7 = var_5['var_7']\n        var_8 = var_5['var_8']\n        if isinstance(var_7, list):\n            var_3 = True\n            var_9 = var_5['var_9']\n",
      "lines_processed": 10,
      "total_lines": 171
    },
    {
      "file_path": "benchmark_inference_fps.py",
      "code": "def parse_args():\n    parser = argparse.ArgumentParser(\n        description='MMDet benchmark a model of FPS')\n    parser.add_argument('config', help='test config file path')\n    parser.add_argument('checkpoint_root', help='Checkpoint file root path')\n    parser.add_argument(\n        '--round-num',\n        type=int,\n        default=1,\n        help='round a number to a given precision in decimal digits')\n",
      "variables": [
        "parser"
      ],
      "anonymized_code": "def parse_args():\n    var_1 = argparse.ArgumentParser(\n        description='MMDet benchmark a model of FPS')\n    var_1.add_argument('config', help='test config file path')\n    var_1.add_argument('checkpoint_root', help='Checkpoint file root path')\n    var_1.add_argument(\n        '--round-num',\n        type=int,\n        default=1,\n        help='round a number to a given precision in decimal digits')\n",
      "lines_processed": 10,
      "total_lines": 171
    },
    {
      "file_path": "test_init_backbone.py",
      "code": "def _traversed_config_file():\n    \"\"\"We traversed all potential config files under the `config` file. If you\n    need to print details or debug code, you can use this function.\n\n    If the `backbone.init_cfg` is None (do not use `Pretrained` init way), you\n    need add the folder name in `ignores_folder` (if the config files in this\n    folder all set backbone.init_cfg is None) or add config name in\n    `ignores_file` (if the config file set backbone.init_cfg is None)\n    \"\"\"\n    config_path = _get_config_directory()\n",
      "variables": [
        "config_path"
      ],
      "anonymized_code": "def _traversed_config_file():\n    \"\"\"We traversed all potential config files under the `config` file. If you\n    need to print details or debug code, you can use this function.\n\n    If the `backbone.init_cfg` is None (do not use `Pretrained` init way), you\n    need add the folder name in `ignores_folder` (if the config files in this\n    folder all set backbone.init_cfg is None) or add config name in\n    `ignores_file` (if the config file set backbone.init_cfg is None)\n    \"\"\"\n    var_1 = _get_config_directory()\n",
      "lines_processed": 10,
      "total_lines": 178
    },
    {
      "file_path": "test_init_backbone.py",
      "code": "def _get_config_module(fname):\n    \"\"\"Load a configuration as a python module.\"\"\"\n    config_dpath = _get_config_directory()\n    config_fpath = join(config_dpath, fname)\n    config_mod = Config.fromfile(config_fpath)\n    return config_mod\n",
      "variables": [
        "fname",
        "config_dpath",
        "config_fpath",
        "config_mod"
      ],
      "anonymized_code": "def _get_config_module(var_1):\n    \"\"\"Load a configuration as a python module.\"\"\"\n    var_2 = _get_config_directory()\n    var_3 = join(var_2, var_1)\n    var_4 = Config.fromfile(var_3)\n    return var_4\n",
      "lines_processed": 6,
      "total_lines": 178
    },
    {
      "file_path": "test_init_backbone.py",
      "code": "def test_load_pretrained(config):\n    \"\"\"Check out backbone whether successfully load pretrained model by using\n    `backbone.init_cfg`.\n\n    Details please refer to `_check_backbone`\n    \"\"\"\n    _check_backbone(config, print_cfg=False)\n",
      "variables": [
        "config"
      ],
      "anonymized_code": "def test_load_pretrained(var_1):\n    \"\"\"Check out backbone whether successfully load pretrained model by using\n    `backbone.init_cfg`.\n\n    Details please refer to `_check_backbone`\n    \"\"\"\n    _check_backbone(var_1, print_cfg=False)\n",
      "lines_processed": 7,
      "total_lines": 178
    },
    {
      "file_path": "test_init_backbone.py",
      "code": "def _get_config_directory():\n    \"\"\"Find the predefined detector config directory.\"\"\"\n    try:\n        # Assume we are running in the source mmdetection repo\n        repo_dpath = dirname(dirname(__file__))\n    except NameError:\n        # For IPython development when this __file__ is not defined\n        import mmdet\n        repo_dpath = dirname(dirname(mmdet.__file__))\n    config_dpath = join(repo_dpath, 'configs')\n",
      "variables": [
        "repo_dpath",
        "config_dpath"
      ],
      "anonymized_code": "def _get_config_directory():\n    \"\"\"Find the predefined detector config directory.\"\"\"\n    try:\n        # Assume we are running in the source mmdetection repo\n        var_1 = dirname(dirname(__file__))\n    except NameError:\n        # For IPython development when this __file__ is not defined\n        import mmdet\n        var_1 = dirname(dirname(mmdet.__file__))\n    var_2 = join(var_1, 'configs')\n",
      "lines_processed": 10,
      "total_lines": 178
    },
    {
      "file_path": "test_init_backbone.py",
      "code": "def _get_detector_cfg(fname):\n    \"\"\"Grab configs necessary to create a detector.\n\n    These are deep copied to allow for safe modification of parameters without\n    influencing other tests.\n    \"\"\"\n    config = _get_config_module(fname)\n    model = copy.deepcopy(config.model)\n    return model\n",
      "variables": [
        "fname",
        "config",
        "model"
      ],
      "anonymized_code": "def _get_detector_cfg(var_1):\n    \"\"\"Grab configs necessary to create a detector.\n\n    These are deep copied to allow for safe modification of parameters without\n    influencing other tests.\n    \"\"\"\n    var_2 = _get_config_module(var_1)\n    var_3 = copy.deepcopy(var_2.var_3)\n    return var_3\n",
      "lines_processed": 9,
      "total_lines": 178
    },
    {
      "file_path": "gather_test_benchmark_metric.py",
      "code": "def parse_args():\n    parser = argparse.ArgumentParser(\n        description='Gather benchmarked models metric')\n    parser.add_argument('config', help='test config file path')\n    parser.add_argument(\n        'root',\n        type=str,\n        help='root path of benchmarked models to be gathered')\n    parser.add_argument(\n        '--out', type=str, help='output path of gathered metrics to be stored')\n",
      "variables": [
        "parser"
      ],
      "anonymized_code": "def parse_args():\n    var_1 = argparse.ArgumentParser(\n        description='Gather benchmarked models metric')\n    var_1.add_argument('config', help='test config file path')\n    var_1.add_argument(\n        'root',\n        type=str,\n        help='root path of benchmarked models to be gathered')\n    var_1.add_argument(\n        '--out', type=str, help='output path of gathered metrics to be stored')\n",
      "lines_processed": 10,
      "total_lines": 96
    },
    {
      "file_path": "benchmark_filter.py",
      "code": "def main():\n    args = parse_args()\n\n    benchmark_type = []\n    if args.basic_arch:\n        benchmark_type += basic_arch_root\n    if args.datasets:\n        benchmark_type += datasets_root\n    if args.data_pipeline:\n        benchmark_type += data_pipeline_root\n",
      "variables": [
        "args",
        "benchmark_type"
      ],
      "anonymized_code": "def main():\n    var_1 = parse_args()\n\n    var_2 = []\n    if var_1.basic_arch:\n        var_2 += basic_arch_root\n    if var_1.datasets:\n        var_2 += datasets_root\n    if var_1.data_pipeline:\n        var_2 += data_pipeline_root\n",
      "lines_processed": 10,
      "total_lines": 167
    },
    {
      "file_path": "check_links.py",
      "code": "def main():\n    args = parse_args()\n\n    # setup logger\n    logger = MMLogger.get_instance(name='mmdet', log_file=args.out)\n\n    # setup https_proxy\n    if args.https_proxy:\n        os.environ['https_proxy'] = args.https_proxy\n\n",
      "variables": [
        "args",
        "logger"
      ],
      "anonymized_code": "def main():\n    var_1 = parse_args()\n\n    # setup var_2\n    var_2 = MMLogger.get_instance(name='mmdet', log_file=var_1.out)\n\n    # setup https_proxy\n    if var_1.https_proxy:\n        os.environ['https_proxy'] = var_1.https_proxy\n\n",
      "lines_processed": 10,
      "total_lines": 157
    },
    {
      "file_path": "check_links.py",
      "code": "def check_path(match_tuple: MatchTuple) -> bool:\n    \"\"\"Check if a file in this repository exists.\"\"\"\n    relative_path = match_tuple.link.split('#')[0]\n    full_path = os.path.join(\n        os.path.dirname(str(match_tuple.source)), relative_path)\n    return os.path.exists(full_path)\n",
      "variables": [
        "match_tuple",
        "relative_path",
        "full_path"
      ],
      "anonymized_code": "def check_path(var_1: MatchTuple) -> bool:\n    \"\"\"Check if a file in this repository exists.\"\"\"\n    var_2 = var_1.link.split('#')[0]\n    var_3 = os.path.join(\n        os.path.dirname(str(var_1.source)), var_2)\n    return os.path.exists(var_3)\n",
      "lines_processed": 6,
      "total_lines": 157
    },
    {
      "file_path": "check_links.py",
      "code": "def parse_args():\n    parser = argparse.ArgumentParser(\n        description='Goes through all the inline-links '\n        'in markdown files and reports the breakages')\n    parser.add_argument(\n        '--num-threads',\n        type=int,\n        default=100,\n        help='Number of processes to confirm the link')\n    parser.add_argument('--https-proxy', type=str, help='https proxy')\n",
      "variables": [
        "parser"
      ],
      "anonymized_code": "def parse_args():\n    var_1 = argparse.ArgumentParser(\n        description='Goes through all the inline-links '\n        'in markdown files and reports the breakages')\n    var_1.add_argument(\n        '--num-threads',\n        type=int,\n        default=100,\n        help='Number of processes to confirm the link')\n    var_1.add_argument('--https-proxy', type=str, help='https proxy')\n",
      "lines_processed": 10,
      "total_lines": 157
    },
    {
      "file_path": "gather_models.py",
      "code": "def convert_model_info_to_pwc(model_infos):\n    pwc_files = {}\n    for model in model_infos:\n        cfg_folder_name = osp.split(model['config'])[-2]\n        pwc_model_info = OrderedDict()\n        pwc_model_info['Name'] = osp.split(model['config'])[-1].split('.')[0]\n        pwc_model_info['In Collection'] = 'Please fill in Collection name'\n        pwc_model_info['Config'] = osp.join('configs', model['config'])\n\n        # get metadata\n",
      "variables": [
        "model_infos",
        "pwc_files",
        "model",
        "cfg_folder_name",
        "pwc_model_info"
      ],
      "anonymized_code": "def convert_model_info_to_pwc(var_1):\n    var_2 = {}\n    for var_3 in var_1:\n        var_4 = osp.split(var_3['config'])[-2]\n        var_5 = OrderedDict()\n        var_5['Name'] = osp.split(var_3['config'])[-1].split('.')[0]\n        var_5['In Collection'] = 'Please fill in Collection name'\n        var_5['Config'] = osp.join('configs', var_3['config'])\n\n        # get metadata\n",
      "lines_processed": 10,
      "total_lines": 308
    },
    {
      "file_path": "gather_models.py",
      "code": "def get_final_epoch_or_iter(config):\n    cfg = Config.fromfile('./configs/' + config)\n    if cfg.train_cfg.type == 'EpochBasedTrainLoop':\n        return cfg.train_cfg.max_epochs\n    else:\n        return cfg.train_cfg.max_iters\n",
      "variables": [
        "config",
        "cfg"
      ],
      "anonymized_code": "def get_final_epoch_or_iter(var_1):\n    var_2 = Config.fromfile('./configs/' + var_1)\n    if var_2.train_cfg.type == 'EpochBasedTrainLoop':\n        return var_2.train_cfg.max_epochs\n    else:\n        return var_2.train_cfg.max_iters\n",
      "lines_processed": 6,
      "total_lines": 308
    },
    {
      "file_path": "gather_models.py",
      "code": "def main():\n    args = parse_args()\n    models_root = args.root\n    models_out = args.out\n    mkdir_or_exist(models_out)\n\n    # find all models in the root directory to be gathered\n    raw_configs = list(scandir('./configs', '.py', recursive=True))\n\n    # filter configs that is not trained in the experiments dir\n",
      "variables": [
        "args",
        "models_root",
        "models_out",
        "raw_configs"
      ],
      "anonymized_code": "def main():\n    var_1 = parse_args()\n    var_2 = var_1.root\n    var_3 = var_1.out\n    mkdir_or_exist(var_3)\n\n    # find all models in the root directory to be gathered\n    var_4 = list(scandir('./configs', '.py', recursive=True))\n\n    # filter configs that is not trained in the experiments dir\n",
      "lines_processed": 10,
      "total_lines": 308
    },
    {
      "file_path": "gather_models.py",
      "code": "def get_best_epoch_or_iter(exp_dir):\n    best_epoch_iter_full_path = list(\n        sorted(glob.glob(osp.join(exp_dir, 'best_*.pth'))))[-1]\n    best_epoch_or_iter_model_path = best_epoch_iter_full_path.split('/')[-1]\n    best_epoch_or_iter = best_epoch_or_iter_model_path.\\\n        split('_')[-1].split('.')[0]\n    return best_epoch_or_iter_model_path, int(best_epoch_or_iter)\n",
      "variables": [
        "exp_dir",
        "best_epoch_iter_full_path",
        "best_epoch_or_iter_model_path",
        "best_epoch_or_iter"
      ],
      "anonymized_code": "def get_best_epoch_or_iter(var_1):\n    var_2 = list(\n        sorted(glob.glob(osp.join(var_1, 'best_*.pth'))))[-1]\n    var_3 = var_2.split('/')[-1]\n    var_4 = var_3.\\\n        split('_')[-1].split('.')[0]\n    return var_3, int(var_4)\n",
      "lines_processed": 7,
      "total_lines": 308
    },
    {
      "file_path": "gather_models.py",
      "code": "def find_last_dir(model_dir):\n    dst_times = []\n    for time_stamp in os.scandir(model_dir):\n        if osp.isdir(time_stamp):\n            dst_time = time.mktime(\n                time.strptime(time_stamp.name, '%Y%m%d_%H%M%S'))\n            dst_times.append([dst_time, time_stamp.name])\n    return max(dst_times, key=lambda x: x[0])[1]\n",
      "variables": [
        "model_dir",
        "dst_times",
        "time_stamp",
        "dst_time",
        "x"
      ],
      "anonymized_code": "def find_last_dir(var_1):\n    var_2 = []\n    for var_3 in os.scandir(var_1):\n        if osp.isdir(var_3):\n            var_4 = time.mktime(\n                time.strptime(var_3.name, '%Y%m%d_%H%M%S'))\n            var_2.append([var_4, var_3.name])\n    return max(var_2, key=lambda var_5: var_5[0])[1]\n",
      "lines_processed": 8,
      "total_lines": 308
    },
    {
      "file_path": "gather_models.py",
      "code": "def is_by_epoch(config):\n    cfg = Config.fromfile('./configs/' + config)\n    return cfg.train_cfg.type == 'EpochBasedTrainLoop'\n",
      "variables": [
        "config",
        "cfg"
      ],
      "anonymized_code": "def is_by_epoch(var_1):\n    var_2 = Config.fromfile('./configs/' + var_1)\n    return var_2.train_cfg.type == 'EpochBasedTrainLoop'\n",
      "lines_processed": 3,
      "total_lines": 308
    },
    {
      "file_path": "benchmark_valid_flops.py",
      "code": "def inference(config_file, checkpoint, work_dir, args, exp_name):\n    logger = MMLogger.get_instance(name='MMLogger')\n    logger.warning('if you want test flops, please make sure torch>=1.12')\n    cfg = Config.fromfile(config_file)\n    cfg.work_dir = work_dir\n    cfg.load_from = checkpoint\n    cfg.log_level = 'WARN'\n    cfg.experiment_name = exp_name\n    if args.cfg_options is not None:\n        cfg.merge_from_dict(args.cfg_options)\n",
      "variables": [
        "config_file",
        "checkpoint",
        "work_dir",
        "args",
        "exp_name",
        "logger",
        "cfg"
      ],
      "anonymized_code": "def inference(var_1, var_2, var_3, var_4, var_5):\n    var_6 = MMLogger.get_instance(name='MMLogger')\n    var_6.warning('if you want test flops, please make sure torch>=1.12')\n    var_7 = Config.fromfile(var_1)\n    var_7.var_3 = var_3\n    var_7.load_from = var_2\n    var_7.log_level = 'WARN'\n    var_7.experiment_name = var_5\n    if var_4.cfg_options is not None:\n        var_7.merge_from_dict(var_4.cfg_options)\n",
      "lines_processed": 10,
      "total_lines": 295
    },
    {
      "file_path": "convert_train_benchmark_script.py",
      "code": "def main():\n    args = parse_args()\n    if args.out:\n        out_suffix = args.out.split('.')[-1]\n        assert args.out.endswith('.sh'), \\\n            f'Expected out file path suffix is .sh, but get .{out_suffix}'\n    assert args.out or args.run, \\\n        ('Please specify at least one operation (save/run/ the '\n         'script) with the argument \"--out\" or \"--run\"')\n\n",
      "variables": [
        "args",
        "out_suffix"
      ],
      "anonymized_code": "def main():\n    var_1 = parse_args()\n    if var_1.out:\n        var_2 = var_1.out.split('.')[-1]\n        assert var_1.out.endswith('.sh'), \\\n            f'Expected out file path suffix is .sh, but get .{var_2}'\n    assert var_1.out or var_1.run, \\\n        ('Please specify at least one operation (save/run/ the '\n         'script) with the argument \"--out\" or \"--run\"')\n\n",
      "lines_processed": 10,
      "total_lines": 104
    },
    {
      "file_path": "convert_train_benchmark_script.py",
      "code": "def parse_args():\n    parser = argparse.ArgumentParser(\n        description='Convert benchmark model json to script')\n    parser.add_argument(\n        'txt_path', type=str, help='txt path output by benchmark_filter')\n    parser.add_argument(\n        '--run', action='store_true', help='run script directly')\n    parser.add_argument(\n        '--out', type=str, help='path to save model benchmark script')\n\n",
      "variables": [
        "parser"
      ],
      "anonymized_code": "def parse_args():\n    var_1 = argparse.ArgumentParser(\n        description='Convert benchmark model json to script')\n    var_1.add_argument(\n        'txt_path', type=str, help='txt path output by benchmark_filter')\n    var_1.add_argument(\n        '--run', action='store_true', help='run script directly')\n    var_1.add_argument(\n        '--out', type=str, help='path to save model benchmark script')\n\n",
      "lines_processed": 10,
      "total_lines": 104
    },
    {
      "file_path": "benchmark_test_image.py",
      "code": "def main(args):\n    # register all modules in mmdet into the registries\n    register_all_modules()\n\n    config = Config.fromfile(args.config)\n\n    # init visualizer\n    visualizer_cfg = dict(type='DetLocalVisualizer', name='visualizer')\n    visualizer = VISUALIZERS.build(visualizer_cfg)\n\n",
      "variables": [
        "args",
        "config",
        "visualizer_cfg",
        "visualizer"
      ],
      "anonymized_code": "def main(var_1):\n    # register all modules in mmdet into the registries\n    register_all_modules()\n\n    var_2 = Config.fromfile(var_1.var_2)\n\n    # init var_4\n    var_3 = dict(type='DetLocalVisualizer', name='var_4')\n    var_4 = VISUALIZERS.build(var_3)\n\n",
      "lines_processed": 10,
      "total_lines": 134
    },
    {
      "file_path": "benchmark_test_image.py",
      "code": "def inference_model(config_name, checkpoint, visualizer, args, logger=None):\n    cfg = Config.fromfile(config_name)\n    if args.aug:\n        raise NotImplementedError()\n\n    model = init_detector(\n        cfg, checkpoint, palette=args.palette, device=args.device)\n    visualizer.dataset_meta = model.dataset_meta\n\n    # test a single image\n",
      "variables": [
        "config_name",
        "checkpoint",
        "visualizer",
        "args",
        "logger",
        "cfg",
        "model"
      ],
      "anonymized_code": "def inference_model(var_1, var_2, var_3, var_4, var_5=None):\n    var_6 = Config.fromfile(var_1)\n    if var_4.aug:\n        raise NotImplementedError()\n\n    var_7 = init_detector(\n        var_6, var_2, palette=var_4.palette, device=var_4.device)\n    var_3.dataset_meta = var_7.dataset_meta\n\n    # test a single image\n",
      "lines_processed": 10,
      "total_lines": 134
    },
    {
      "file_path": "convert_test_benchmark_script.py",
      "code": "def parse_args():\n    parser = argparse.ArgumentParser(\n        description='Convert benchmark model list to script')\n    parser.add_argument('config', help='test config file path')\n    parser.add_argument('--port', type=int, default=29666, help='dist port')\n    parser.add_argument(\n        '--run', action='store_true', help='run script directly')\n    parser.add_argument(\n        '--out', type=str, help='path to save model benchmark script')\n\n",
      "variables": [
        "parser"
      ],
      "anonymized_code": "def parse_args():\n    var_1 = argparse.ArgumentParser(\n        description='Convert benchmark model list to script')\n    var_1.add_argument('config', help='test config file path')\n    var_1.add_argument('--port', type=int, default=29666, help='dist port')\n    var_1.add_argument(\n        '--run', action='store_true', help='run script directly')\n    var_1.add_argument(\n        '--out', type=str, help='path to save model benchmark script')\n\n",
      "lines_processed": 10,
      "total_lines": 114
    },
    {
      "file_path": "convert_test_benchmark_script.py",
      "code": "def create_test_bash_info(commands, model_test_dict, port, script_name,\n                          partition):\n    config = model_test_dict['config']\n    job_name = model_test_dict['job_name']\n    checkpoint = model_test_dict['checkpoint']\n    work_dir = model_test_dict['work_dir']\n\n    echo_info = f' \\necho \\'{config}\\' &'\n    commands.append(echo_info)\n    commands.append('\\n')\n",
      "variables": [
        "commands",
        "model_test_dict",
        "port",
        "script_name",
        "partition",
        "config",
        "job_name",
        "checkpoint",
        "work_dir",
        "echo_info"
      ],
      "anonymized_code": "def create_test_bash_info(var_1, var_2, var_3, var_4,\n                          var_5):\n    var_6 = var_2['var_6']\n    var_7 = var_2['var_7']\n    var_8 = var_2['var_8']\n    var_9 = var_2['var_9']\n\n    var_10 = f' \\necho \\'{var_6}\\' &'\n    var_1.append(var_10)\n    var_1.append('\\n')\n",
      "lines_processed": 10,
      "total_lines": 114
    },
    {
      "file_path": "convert_test_benchmark_script.py",
      "code": "def main():\n    args = parse_args()\n    if args.out:\n        out_suffix = args.out.split('.')[-1]\n        assert args.out.endswith('.sh'), \\\n            f'Expected out file path suffix is .sh, but get .{out_suffix}'\n    assert args.out or args.run, \\\n        ('Please specify at least one operation (save/run/ the '\n         'script) with the argument \"--out\" or \"--run\"')\n\n",
      "variables": [
        "args",
        "out_suffix"
      ],
      "anonymized_code": "def main():\n    var_1 = parse_args()\n    if var_1.out:\n        var_2 = var_1.out.split('.')[-1]\n        assert var_1.out.endswith('.sh'), \\\n            f'Expected out file path suffix is .sh, but get .{var_2}'\n    assert var_1.out or var_1.run, \\\n        ('Please specify at least one operation (save/run/ the '\n         'script) with the argument \"--out\" or \"--run\"')\n\n",
      "lines_processed": 10,
      "total_lines": 114
    }
  ],
  "hpcaitech_ColossalAI": [
    {
      "file_path": "init_tokenizer.py",
      "code": "def expand_vocab_tokenizer(\n    source_tokenizer_dir: Union[str, os.PathLike], target_tokenizer_dir: Union[str, os.PathLike], new_tokens: List[str]\n) -> None:\n    \"\"\"Expand tokenizer for continue pre-training.\"\"\"\n    if os.path.exists(target_tokenizer_dir):\n        raise RuntimeError(f\"Find existed directory {target_tokenizer_dir}\")\n\n    source_tokenizer = LlamaTokenizer.from_pretrained(source_tokenizer_dir)\n    logger.info(source_tokenizer)\n    source_sp_processor = source_tokenizer.sp_model\n",
      "variables": [
        "source_tokenizer_dir",
        "target_tokenizer_dir",
        "new_tokens",
        "source_tokenizer",
        "source_sp_processor"
      ],
      "anonymized_code": "def expand_vocab_tokenizer(\n    var_1: Union[str, os.PathLike], var_2: Union[str, os.PathLike], var_3: List[str]\n) -> None:\n    \"\"\"Expand tokenizer for continue pre-training.\"\"\"\n    if os.path.exists(var_2):\n        raise RuntimeError(f\"Find existed directory {var_2}\")\n\n    var_4 = LlamaTokenizer.from_pretrained(var_1)\n    logger.info(var_4)\n    var_5 = var_4.sp_model\n",
      "lines_processed": 10,
      "total_lines": 98
    },
    {
      "file_path": "neftune_patch.py",
      "code": "def deactivate_neftune(model, neftune_hook_handle):\n    \"\"\"\n    Deactivates the neftune method. Make sure to call `_activate_neftune` first.\n    \"\"\"\n    embeddings = unwrap(model).get_input_embeddings()\n\n    neftune_hook_handle.remove()\n    del embeddings.neftune_noise_alpha\n",
      "variables": [
        "model",
        "neftune_hook_handle",
        "embeddings"
      ],
      "anonymized_code": "def deactivate_neftune(var_1, var_2):\n    \"\"\"\n    Deactivates the neftune method. Make sure to call `_activate_neftune` first.\n    \"\"\"\n    var_3 = unwrap(var_1).get_input_embeddings()\n\n    var_2.remove()\n    del var_3.neftune_noise_alpha\n",
      "lines_processed": 8,
      "total_lines": 72
    },
    {
      "file_path": "neftune_patch.py",
      "code": "def activate_neftune(model, neftune_noise_alpha=0.1):\n    r\"\"\"\n    Activates the neftune as presented in this code: https://github.com/neelsjain/NEFTune and paper:\n    https://arxiv.org/abs/2310.05914\n    \"\"\"\n    embeddings = unwrap(model).get_input_embeddings()\n\n    embeddings.neftune_noise_alpha = neftune_noise_alpha\n    hook_handle = embeddings.register_forward_hook(neftune_post_forward_hook)\n    neftune_hook_handle = hook_handle\n",
      "variables": [
        "model",
        "neftune_noise_alpha",
        "embeddings",
        "hook_handle",
        "neftune_hook_handle"
      ],
      "anonymized_code": "def activate_neftune(var_1, var_2=0.1):\n    r\"\"\"\n    Activates the neftune as presented in this code: https://github.com/neelsjain/NEFTune and paper:\n    https://arxiv.org/abs/2310.05914\n    \"\"\"\n    var_3 = unwrap(var_1).get_input_embeddings()\n\n    var_3.var_2 = var_2\n    var_4 = var_3.register_forward_hook(neftune_post_forward_hook)\n    var_5 = var_4\n",
      "lines_processed": 10,
      "total_lines": 72
    },
    {
      "file_path": "neftune_patch.py",
      "code": "def unwrap(model):\n    if hasattr(model, \"module\"):\n        return model.unwrap()\n    else:\n        return model\n",
      "variables": [
        "model"
      ],
      "anonymized_code": "def unwrap(var_1):\n    if hasattr(var_1, \"module\"):\n        return var_1.unwrap()\n    else:\n        return var_1\n",
      "lines_processed": 5,
      "total_lines": 72
    },
    {
      "file_path": "init_model.py",
      "code": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--source_model_and_tokenizer_path\",\n        type=str,\n        required=True,\n        default=None,\n        help=\"Source path of model & tokenizer\",\n    )\n    parser.add_argument(\"--target_tokenizer_path\", type=str, required=True, default=None, help=\"Target tokenizer path\")\n",
      "variables": [
        "parser"
      ],
      "anonymized_code": "def main():\n    var_1 = argparse.ArgumentParser()\n    var_1.add_argument(\n        \"--source_model_and_tokenizer_path\",\n        type=str,\n        required=True,\n        default=None,\n        help=\"Source path of model & tokenizer\",\n    )\n    var_1.add_argument(\"--target_tokenizer_path\", type=str, required=True, default=None, help=\"Target tokenizer path\")\n",
      "lines_processed": 10,
      "total_lines": 110
    },
    {
      "file_path": "loader.py",
      "code": "def load_tokenized_dataset(\n    dataset_paths: Union[PathType, List[PathType]], mode: str = \"train\"\n) -> Optional[DatasetType]:\n    \"\"\"\n    Load pre-tokenized dataset.\n    Each instance of dataset is a dictionary with\n    `{'input_ids': List[int], 'labels': List[int], sequence: str}` format.\n    \"\"\"\n    mode_map = {\"train\": \"train\", \"dev\": \"validation\", \"test\": \"test\"}\n    assert mode in tuple(mode_map), f\"Unsupported mode {mode}, it must be in {tuple(mode_map)}\"\n",
      "variables": [
        "dataset_paths",
        "mode",
        "mode_map"
      ],
      "anonymized_code": "def load_tokenized_dataset(\n    var_1: Union[PathType, List[PathType]], var_2: str = \"train\"\n) -> Optional[DatasetType]:\n    \"\"\"\n    Load pre-tokenized dataset.\n    Each instance of dataset is a dictionary with\n    `{'input_ids': List[int], 'labels': List[int], sequence: str}` format.\n    \"\"\"\n    var_3 = {\"train\": \"train\", \"dev\": \"validation\", \"test\": \"test\"}\n    assert var_2 in tuple(var_3), f\"Unsupported var_2 {var_2}, it must be in {tuple(var_3)}\"\n",
      "lines_processed": 10,
      "total_lines": 175
    },
    {
      "file_path": "generate_release_draft.py",
      "code": "def get_commit_info(commit_hash, headers=None):\n    api = f\"{COMMIT_API}/{commit_hash}\"\n    res = requests.get(url=api, headers=headers)\n    return res.json()\n",
      "variables": [
        "commit_hash",
        "headers",
        "api",
        "res"
      ],
      "anonymized_code": "def get_commit_info(var_1, var_2=None):\n    var_3 = f\"{COMMIT_API}/{var_1}\"\n    var_4 = requests.get(url=var_3, var_2=var_2)\n    return var_4.json()\n",
      "lines_processed": 4,
      "total_lines": 131
    },
    {
      "file_path": "generate_release_draft.py",
      "code": "def get_latest_tag_commit(headers=None):\n    res = requests.get(url=TAGS_API, headers=headers)\n    data = res.json()\n    commit_hash = data[0][\"commit\"][\"sha\"]\n    version = data[0][\"name\"]\n    return commit_hash, version\n",
      "variables": [
        "headers",
        "res",
        "data",
        "commit_hash",
        "version"
      ],
      "anonymized_code": "def get_latest_tag_commit(var_1=None):\n    var_2 = requests.get(url=TAGS_API, var_1=var_1)\n    var_3 = var_2.json()\n    var_4 = var_3[0][\"commit\"][\"sha\"]\n    var_5 = var_3[0][\"name\"]\n    return var_4, var_5\n",
      "lines_processed": 6,
      "total_lines": 131
    },
    {
      "file_path": "generate_release_draft.py",
      "code": "def generate_release_post_markdown(current_version, last_version, release_info):\n    text = []\n\n    # add highlights\n    highlights = \"## What's Changed \\n\\n\"\n    text.append(highlights)\n\n    # add items\n    for k, v in release_info.items():\n        topic = f\"### {k} \\n\"\n",
      "variables": [
        "current_version",
        "last_version",
        "release_info",
        "text",
        "highlights",
        "k",
        "v",
        "topic"
      ],
      "anonymized_code": "def generate_release_post_markdown(var_1, var_2, var_3):\n    var_4 = []\n\n    # add var_5\n    var_5 = \"## What's Changed \\n\\n\"\n    var_4.append(var_5)\n\n    # add items\n    for var_6, var_7 in var_3.items():\n        var_8 = f\"### {var_6} \\n\"\n",
      "lines_processed": 10,
      "total_lines": 131
    },
    {
      "file_path": "generate_release_draft.py",
      "code": "def parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--out\", type=str, help=\"output path for the release draft\", required=True)\n    parser.add_argument(\"--version\", type=str, help=\"current version to release\", required=True)\n    return parser.parse_args()\n",
      "variables": [
        "parser"
      ],
      "anonymized_code": "def parse_args():\n    var_1 = argparse.ArgumentParser()\n    var_1.add_argument(\"--out\", type=str, help=\"output path for the release draft\", required=True)\n    var_1.add_argument(\"--version\", type=str, help=\"current version to release\", required=True)\n    return var_1.parse_args()\n",
      "lines_processed": 5,
      "total_lines": 131
    },
    {
      "file_path": "generate_release_draft.py",
      "code": "def get_all_commit_info(since, headers=None):\n    page = 1\n    results = []\n\n    while True:\n        api = f\"{COMMIT_API}?since={since}&per_page=100&page={page}\"\n        resp = requests.get(url=api, headers=headers)\n        data = resp.json()\n\n        # exit when no more data\n",
      "variables": [
        "since",
        "headers",
        "page",
        "results",
        "api",
        "resp",
        "data"
      ],
      "anonymized_code": "def get_all_commit_info(var_1, var_2=None):\n    var_3 = 1\n    var_4 = []\n\n    while True:\n        var_5 = f\"{COMMIT_API}?var_1={var_1}&per_page=100&var_3={var_3}\"\n        var_6 = requests.get(url=var_5, var_2=var_2)\n        var_7 = var_6.json()\n\n        # exit when no more var_7\n",
      "lines_processed": 10,
      "total_lines": 131
    },
    {
      "file_path": "check_dispatch_inputs.py",
      "code": "def check_inputs(input_list):\n    for path in input_list:\n        real_path = os.path.join(\"examples\", path)\n        if not os.path.exists(real_path):\n            return False\n    return True\n",
      "variables": [
        "input_list",
        "path",
        "real_path"
      ],
      "anonymized_code": "def check_inputs(var_1):\n    for var_2 in var_1:\n        var_3 = os.var_2.join(\"examples\", var_2)\n        if not os.var_2.exists(var_3):\n            return False\n    return True\n",
      "lines_processed": 6,
      "total_lines": 27
    },
    {
      "file_path": "froze.py",
      "code": "def freeze_non_embeds_parameters(model: LlamaForCausalLM) -> None:\n    \"\"\"Freeze all parameters except embeddings.\"\"\"\n    for name, params in model.named_parameters():\n        if \"embed_tokens\" not in name and \"lm_head\" not in name:\n            params.requires_grad = False\n        else:\n            params.requires_grad = True\n",
      "variables": [
        "model",
        "name",
        "params"
      ],
      "anonymized_code": "def freeze_non_embeds_parameters(var_1: LlamaForCausalLM) -> None:\n    \"\"\"Freeze all parameters except embeddings.\"\"\"\n    for var_2, var_3 in var_1.named_parameters():\n        if \"embed_tokens\" not in var_2 and \"lm_head\" not in var_2:\n            var_3.requires_grad = False\n        else:\n            var_3.requires_grad = True\n",
      "lines_processed": 7,
      "total_lines": 18
    },
    {
      "file_path": "froze.py",
      "code": "def unfreeze_parameters(model: LlamaForCausalLM) -> None:\n    for name, params in model.named_parameters():\n        params.requires_grad = False\n",
      "variables": [
        "model",
        "name",
        "params"
      ],
      "anonymized_code": "def unfreeze_parameters(var_1: LlamaForCausalLM) -> None:\n    for var_2, var_3 in var_1.named_parameters():\n        var_3.requires_grad = False\n",
      "lines_processed": 3,
      "total_lines": 18
    },
    {
      "file_path": "generate_leaderboard_and_send_to_lark.py",
      "code": "def generate_lark_tenant_access_token(app_id: str, app_secret: str) -> str:\n    \"\"\"\n    Generate Lark tenant access token.\n\n    Args:\n        app_id (str): Lark app id\n        app_secret (str): Lark app secret\n    \"\"\"\n    url = \"https://open.feishu.cn/open-apis/auth/v3/tenant_access_token/internal\"\n    data = {\"app_id\": app_id, \"app_secret\": app_secret}\n",
      "variables": [
        "app_id",
        "app_secret",
        "url",
        "data"
      ],
      "anonymized_code": "def generate_lark_tenant_access_token(var_1: str, var_2: str) -> str:\n    \"\"\"\n    Generate Lark tenant access token.\n\n    Args:\n        var_1 (str): Lark app id\n        var_2 (str): Lark app secret\n    \"\"\"\n    var_3 = \"https://open.feishu.cn/open-apis/auth/v3/tenant_access_token/internal\"\n    var_4 = {\"var_1\": var_1, \"var_2\": var_2}\n",
      "lines_processed": 10,
      "total_lines": 569
    },
    {
      "file_path": "generate_leaderboard_and_send_to_lark.py",
      "code": "def datetime2str(dt):\n    \"\"\"\n    Convert datetime to string in the format of YYYY-MM-DDTHH:MM:SSZ\n    \"\"\"\n    return dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
      "variables": [
        "dt"
      ],
      "anonymized_code": "def datetime2str(var_1):\n    \"\"\"\n    Convert datetime to string in the format of YYYY-MM-DDTHH:MM:SSZ\n    \"\"\"\n    return var_1.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
      "lines_processed": 5,
      "total_lines": 569
    },
    {
      "file_path": "generate_leaderboard_and_send_to_lark.py",
      "code": "def generate_user_engagement_leaderboard_image(\n    github_token: str, org_name: str, repo_list: List[str], output_path: str\n) -> bool:\n    \"\"\"\n    Generate the user engagement leaderboard image for stats within the last 7 days\n\n    Args:\n        github_token (str): GitHub access token for API calls\n        output_path (str): the path to save the image\n    \"\"\"\n",
      "variables": [
        "github_token",
        "org_name",
        "repo_list",
        "output_path"
      ],
      "anonymized_code": "def generate_user_engagement_leaderboard_image(\n    var_1: str, var_2: str, var_3: List[str], var_4: str\n) -> bool:\n    \"\"\"\n    Generate the user engagement leaderboard image for stats within the last 7 days\n\n    Args:\n        var_1 (str): GitHub access token for API calls\n        var_4 (str): the path to save the image\n    \"\"\"\n",
      "lines_processed": 10,
      "total_lines": 569
    },
    {
      "file_path": "generate_leaderboard_and_send_to_lark.py",
      "code": "def upload_image_to_lark(lark_tenant_token: str, image_path: str) -> str:\n    \"\"\"\n    Upload image to Lark and return the image key\n\n    Args:\n        lark_tenant_token (str): Lark tenant access token\n        image_path (str): the path to the image to be uploaded\n    \"\"\"\n    url = \"https://open.feishu.cn/open-apis/im/v1/images\"\n    form = {\"image_type\": \"message\", \"image\": (open(image_path, \"rb\"))}  # \u9700\u8981\u66ff\u6362\u5177\u4f53\u7684path\n",
      "variables": [
        "lark_tenant_token",
        "image_path",
        "url",
        "form"
      ],
      "anonymized_code": "def upload_image_to_lark(var_1: str, var_2: str) -> str:\n    \"\"\"\n    Upload image to Lark and return the image key\n\n    Args:\n        var_1 (str): Lark tenant access token\n        var_2 (str): the path to the image to be uploaded\n    \"\"\"\n    var_3 = \"https://open.feishu.cn/open-apis/im/v1/images\"\n    var_4 = {\"image_type\": \"message\", \"image\": (open(var_2, \"rb\"))}  # \u9700\u8981\u66ff\u6362\u5177\u4f53\u7684path\n",
      "lines_processed": 10,
      "total_lines": 569
    },
    {
      "file_path": "ckpt_io.py",
      "code": "def load_json(file_path: Union[str, os.PathLike]) -> Dict[str, Any]:\n    \"\"\"\n    Load file in JSON format\n    \"\"\"\n    with open(file=file_path, mode=\"r\", encoding=\"utf-8\") as fp:\n        return json.load(fp)\n",
      "variables": [
        "file_path",
        "fp"
      ],
      "anonymized_code": "def load_json(var_1: Union[str, os.PathLike]) -> Dict[str, Any]:\n    \"\"\"\n    Load file in JSON format\n    \"\"\"\n    with open(file=var_1, mode=\"r\", encoding=\"utf-8\") as var_2:\n        return json.load(var_2)\n",
      "lines_processed": 6,
      "total_lines": 92
    },
    {
      "file_path": "ckpt_io.py",
      "code": "def load_checkpoint(\n    load_dir: Union[str, os.PathLike],\n    booster: Booster,\n    model: torch.nn.Module,\n    optimizer: Optimizer,\n    lr_scheduler: _LRScheduler,\n) -> Tuple[int, int, int]:\n    \"\"\"\n    Load model checkpoint, optimizer, LR scheduler and intermedidate running states.\n    \"\"\"\n",
      "variables": [
        "load_dir",
        "booster",
        "model",
        "optimizer",
        "lr_scheduler"
      ],
      "anonymized_code": "def load_checkpoint(\n    var_1: Union[str, os.PathLike],\n    var_2: Booster,\n    var_3: torch.nn.Module,\n    var_4: Optimizer,\n    var_5: _LRScheduler,\n) -> Tuple[int, int, int]:\n    \"\"\"\n    Load var_3 checkpoint, var_4, LR scheduler and intermedidate running states.\n    \"\"\"\n",
      "lines_processed": 10,
      "total_lines": 92
    },
    {
      "file_path": "ckpt_io.py",
      "code": "def save_json(data: Dict[str, Any], file_path: Union[str, os.PathLike]) -> None:\n    \"\"\"\n    Save as JSON format\n    \"\"\"\n    with open(file=file_path, mode=\"w\", encoding=\"utf-8\") as fp:\n        json.dump(data, fp=fp, ensure_ascii=False, indent=4)\n",
      "variables": [
        "data",
        "file_path",
        "fp"
      ],
      "anonymized_code": "def save_json(var_1: Dict[str, Any], var_2: Union[str, os.PathLike]) -> None:\n    \"\"\"\n    Save as JSON format\n    \"\"\"\n    with open(file=var_2, mode=\"w\", encoding=\"utf-8\") as var_3:\n        json.dump(var_1, var_3=var_3, ensure_ascii=False, indent=4)\n",
      "lines_processed": 6,
      "total_lines": 92
    },
    {
      "file_path": "send_message_to_lark.py",
      "code": "def send_message_to_lark(message, webhook_url):\n    data = {\"msg_type\": \"text\", \"content\": {\"text\": message}}\n    requests.post(webhook_url, json=data)\n",
      "variables": [
        "message",
        "webhook_url",
        "data"
      ],
      "anonymized_code": "def send_message_to_lark(var_1, var_2):\n    var_3 = {\"msg_type\": \"text\", \"content\": {\"text\": var_1}}\n    requests.post(var_2, json=var_3)\n",
      "lines_processed": 3,
      "total_lines": 20
    },
    {
      "file_path": "send_message_to_lark.py",
      "code": "def parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-m\", \"--message\", type=str)\n    parser.add_argument(\"-u\", \"--url\", type=str)\n    return parser.parse_args()\n",
      "variables": [
        "parser"
      ],
      "anonymized_code": "def parse_args():\n    var_1 = argparse.ArgumentParser()\n    var_1.add_argument(\"-m\", \"--message\", type=str)\n    var_1.add_argument(\"-u\", \"--url\", type=str)\n    return var_1.parse_args()\n",
      "lines_processed": 5,
      "total_lines": 20
    },
    {
      "file_path": "check_doc_i18n.py",
      "code": "def compare_dirs(dir1, dir2):\n    # First, we need to check if the two directories exist\n    if not os.path.exists(dir1) or not os.path.exists(dir2):\n        return False\n\n    # Now, we compare the list of items in each directory\n    items1 = os.listdir(dir1)\n    items2 = os.listdir(dir2)\n\n    # If the number of items in each directory is different, the directories are different\n",
      "variables": [
        "dir1",
        "dir2",
        "items1",
        "items2"
      ],
      "anonymized_code": "def compare_dirs(var_1, var_2):\n    # First, we need to check if the two directories exist\n    if not os.path.exists(var_1) or not os.path.exists(var_2):\n        return False\n\n    # Now, we compare the list of items in each directory\n    var_3 = os.listdir(var_1)\n    var_4 = os.listdir(var_2)\n\n    # If the number of items in each directory is different, the directories are different\n",
      "lines_processed": 10,
      "total_lines": 67
    },
    {
      "file_path": "update_setup_for_nightly.py",
      "code": "def write_setup_file(file_lines):\n    with open(\"setup.py\", \"w\") as f:\n        f.writelines(file_lines)\n",
      "variables": [
        "file_lines",
        "f"
      ],
      "anonymized_code": "def write_setup_file(var_1):\n    with open(\"setup.py\", \"w\") as var_2:\n        var_2.writelines(var_1)\n",
      "lines_processed": 3,
      "total_lines": 34
    },
    {
      "file_path": "update_setup_for_nightly.py",
      "code": "def main():\n    file_lines = open_setup_file()\n    file_lines = replace_nightly_package_info(file_lines)\n    write_setup_file(file_lines)\n",
      "variables": [
        "file_lines"
      ],
      "anonymized_code": "def main():\n    var_1 = open_setup_file()\n    var_1 = replace_nightly_package_info(var_1)\n    write_setup_file(var_1)\n",
      "lines_processed": 4,
      "total_lines": 34
    },
    {
      "file_path": "update_setup_for_nightly.py",
      "code": "def open_setup_file():\n    with open(\"setup.py\", \"r\") as f:\n        file_lines = f.readlines()\n    return file_lines\n",
      "variables": [
        "f",
        "file_lines"
      ],
      "anonymized_code": "def open_setup_file():\n    with open(\"setup.py\", \"r\") as var_1:\n        var_2 = var_1.readlines()\n    return var_2\n",
      "lines_processed": 4,
      "total_lines": 34
    },
    {
      "file_path": "update_setup_for_nightly.py",
      "code": "def replace_nightly_package_info(file_lines):\n    version = datetime.today().strftime(\"%Y.%m.%d\")\n    package_name = \"colossalai-nightly\"\n\n    for idx, line in enumerate(file_lines):\n        if \"version = get_version()\" in line:\n            file_lines[idx] = f'version = \"{version}\"\\n'\n        if 'package_name = \"colossalai\"' in line:\n            file_lines[idx] = f'package_name = \"{package_name}\"\\n'\n    return file_lines\n",
      "variables": [
        "file_lines",
        "version",
        "package_name",
        "idx",
        "line"
      ],
      "anonymized_code": "def replace_nightly_package_info(var_1):\n    var_2 = datetime.today().strftime(\"%Y.%m.%d\")\n    var_3 = \"colossalai-nightly\"\n\n    for var_4, var_5 in enumerate(var_1):\n        if \"var_2 = get_version()\" in var_5:\n            var_1[var_4] = f'var_2 = \"{var_2}\"\\n'\n        if 'var_3 = \"colossalai\"' in var_5:\n            var_1[var_4] = f'var_3 = \"{var_3}\"\\n'\n    return var_1\n",
      "lines_processed": 10,
      "total_lines": 34
    },
    {
      "file_path": "check_example_weekly.py",
      "code": "def show_files(path, all_files):\n    # Traverse all the folder/file in current directory\n    file_list = os.listdir(path)\n    # Determine the element is folder or file. If file, pass it into list, if folder, recurse.\n    for file_name in file_list:\n        # Get the abs directory using os.path.join() and store into cur_path.\n        cur_path = os.path.join(path, file_name)\n        # Determine whether folder\n        if os.path.isdir(cur_path):\n            show_files(cur_path, all_files)\n",
      "variables": [
        "path",
        "all_files",
        "file_list",
        "file_name",
        "cur_path"
      ],
      "anonymized_code": "def show_files(var_1, var_2):\n    # Traverse all the folder/file in current directory\n    var_3 = os.listdir(var_1)\n    # Determine the element is folder or file. If file, pass it into list, if folder, recurse.\n    for var_4 in var_3:\n        # Get the abs directory using os.var_1.join() and store into var_5.\n        var_5 = os.var_1.join(var_1, var_4)\n        # Determine whether folder\n        if os.var_1.isdir(var_5):\n            show_files(var_5, var_2)\n",
      "lines_processed": 10,
      "total_lines": 37
    },
    {
      "file_path": "check_example_weekly.py",
      "code": "def join(input_list, sep=None):\n    return (sep or \" \").join(input_list)\n",
      "variables": [
        "input_list",
        "sep"
      ],
      "anonymized_code": "def join(var_1, var_2=None):\n    return (var_2 or \" \").join(var_1)\n",
      "lines_processed": 2,
      "total_lines": 37
    },
    {
      "file_path": "check_example_weekly.py",
      "code": "def main():\n    contents = show_files(\"examples/\", [])\n    all_loc = []\n    for file_loc in contents:\n        split_loc = file_loc.split(\"/\")\n        # must have two sub-folder levels after examples folder, such as examples/images/vit is acceptable, examples/images/README.md is not, examples/requirements.txt is not.\n        if len(split_loc) >= 4:\n            re_loc = \"/\".join(split_loc[1:3])\n            if re_loc not in all_loc:\n                all_loc.append(re_loc)\n",
      "variables": [
        "contents",
        "all_loc",
        "file_loc",
        "split_loc",
        "re_loc"
      ],
      "anonymized_code": "def main():\n    var_1 = show_files(\"examples/\", [])\n    var_2 = []\n    for var_3 in var_1:\n        var_4 = var_3.split(\"/\")\n        # must have two sub-folder levels after examples folder, such as examples/images/vit is acceptable, examples/images/README.md is not, examples/requirements.txt is not.\n        if len(var_4) >= 4:\n            var_5 = \"/\".join(var_4[1:3])\n            if var_5 not in var_2:\n                var_2.append(var_5)\n",
      "lines_processed": 10,
      "total_lines": 37
    }
  ],
  "scikit-learn_scikit-learn": [
    {
      "file_path": "bench_covertype.py",
      "code": "def load_data(dtype=np.float32, order=\"C\", random_state=13):\n    \"\"\"Load the data, then cache and memmap the train/test split\"\"\"\n    ######################################################################\n    # Load dataset\n    print(\"Loading dataset...\")\n    data = fetch_covtype(\n        download_if_missing=True, shuffle=True, random_state=random_state\n    )\n    X = check_array(data[\"data\"], dtype=dtype, order=order)\n    y = (data[\"target\"] != 1).astype(int)\n",
      "variables": [
        "dtype",
        "order",
        "random_state",
        "data",
        "X",
        "y"
      ],
      "anonymized_code": "def load_data(var_1=np.float32, var_2=\"C\", var_3=13):\n    \"\"\"Load the var_4, then cache and memmap the train/test split\"\"\"\n    ######################################################################\n    # Load dataset\n    print(\"Loading dataset...\")\n    var_4 = fetch_covtype(\n        download_if_missing=True, shuffle=True, var_3=var_3\n    )\n    var_5 = check_array(var_4[\"var_4\"], var_1=var_1, var_2=var_2)\n    var_6 = (var_4[\"target\"] != 1).astype(int)\n",
      "lines_processed": 10,
      "total_lines": 234
    },
    {
      "file_path": "datasets.py",
      "code": "def _blobs_dataset(n_samples=500000, n_features=3, n_clusters=100, dtype=np.float32):\n    X, _ = make_blobs(\n        n_samples=n_samples, n_features=n_features, centers=n_clusters, random_state=0\n    )\n    X = X.astype(dtype, copy=False)\n\n    X, X_val = train_test_split(X, test_size=0.1, random_state=0)\n    return X, X_val, None, None\n",
      "variables": [
        "n_samples",
        "n_features",
        "n_clusters",
        "dtype",
        "X",
        "_",
        "X_val"
      ],
      "anonymized_code": "def _blobs_dataset(var_1=500000, var_2=3, var_3=100, var_4=np.float32):\n    var_5, var_6 = make_blobs(\n        var_1=var_1, var_2=var_2, centers=var_3, random_state=0\n    )\n    var_5 = var_5.astype(var_4, copy=False)\n\n    var_5, var_7 = train_test_split(var_5, test_size=0.1, random_state=0)\n    return var_5, var_7, None, None\n",
      "lines_processed": 8,
      "total_lines": 168
    },
    {
      "file_path": "datasets.py",
      "code": "def _olivetti_faces_dataset():\n    dataset = fetch_olivetti_faces(shuffle=True, random_state=42)\n    faces = dataset.data\n    n_samples, n_features = faces.shape\n    faces_centered = faces - faces.mean(axis=0)\n    # local centering\n    faces_centered -= faces_centered.mean(axis=1).reshape(n_samples, -1)\n    X = faces_centered\n\n    X, X_val = train_test_split(X, test_size=0.1, random_state=0)\n",
      "variables": [
        "dataset",
        "faces",
        "n_samples",
        "n_features",
        "faces_centered",
        "X",
        "X_val"
      ],
      "anonymized_code": "def _olivetti_faces_dataset():\n    var_1 = fetch_olivetti_faces(shuffle=True, random_state=42)\n    var_2 = var_1.data\n    var_3, var_4 = var_2.shape\n    var_5 = var_2 - var_2.mean(axis=0)\n    # local centering\n    var_5 -= var_5.mean(axis=1).reshape(var_3, -1)\n    var_6 = var_5\n\n    var_6, var_7 = train_test_split(var_6, test_size=0.1, random_state=0)\n",
      "lines_processed": 10,
      "total_lines": 168
    },
    {
      "file_path": "datasets.py",
      "code": "def _synth_regression_sparse_dataset(\n    n_samples=10000, n_features=10000, density=0.01, dtype=np.float32\n):\n    X = sp.random(\n        m=n_samples, n=n_features, density=density, format=\"csr\", random_state=0\n    )\n    X.data = np.random.RandomState(0).randn(X.getnnz())\n    X = X.astype(dtype, copy=False)\n    coefs = sp.random(m=n_features, n=1, density=0.5, random_state=0)\n    coefs.data = np.random.RandomState(0).randn(coefs.getnnz())\n",
      "variables": [
        "n_samples",
        "n_features",
        "density",
        "dtype",
        "X",
        "coefs"
      ],
      "anonymized_code": "def _synth_regression_sparse_dataset(\n    var_1=10000, var_2=10000, var_3=0.01, var_4=np.float32\n):\n    var_5 = sp.random(\n        m=var_1, n=var_2, var_3=var_3, format=\"csr\", random_state=0\n    )\n    var_5.data = np.random.RandomState(0).randn(var_5.getnnz())\n    var_5 = var_5.astype(var_4, copy=False)\n    var_6 = sp.random(m=var_2, n=1, var_3=0.5, random_state=0)\n    var_6.data = np.random.RandomState(0).randn(var_6.getnnz())\n",
      "lines_processed": 10,
      "total_lines": 168
    },
    {
      "file_path": "datasets.py",
      "code": "def _mnist_dataset(dtype=np.float32):\n    X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n    X = X.astype(dtype, copy=False)\n    X = MaxAbsScaler().fit_transform(X)\n\n    X, X_val, y, y_val = train_test_split(X, y, test_size=0.1, random_state=0)\n    return X, X_val, y, y_val\n",
      "variables": [
        "dtype",
        "X",
        "y",
        "X_val",
        "y_val"
      ],
      "anonymized_code": "def _mnist_dataset(var_1=np.float32):\n    var_2, var_3 = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n    var_2 = var_2.astype(var_1, copy=False)\n    var_2 = MaxAbsScaler().fit_transform(var_2)\n\n    var_2, var_4, var_3, var_5 = train_test_split(var_2, var_3, test_size=0.1, random_state=0)\n    return var_2, var_4, var_3, var_5\n",
      "lines_processed": 7,
      "total_lines": 168
    },
    {
      "file_path": "datasets.py",
      "code": "def _20newsgroups_highdim_dataset(n_samples=None, ngrams=(1, 1), dtype=np.float32):\n    newsgroups = fetch_20newsgroups(random_state=0)\n    vectorizer = TfidfVectorizer(ngram_range=ngrams, dtype=dtype)\n    X = vectorizer.fit_transform(newsgroups.data[:n_samples])\n    y = newsgroups.target[:n_samples]\n\n    X, X_val, y, y_val = train_test_split(X, y, test_size=0.1, random_state=0)\n    return X, X_val, y, y_val\n",
      "variables": [
        "n_samples",
        "ngrams",
        "dtype",
        "newsgroups",
        "vectorizer",
        "X",
        "y",
        "X_val",
        "y_val"
      ],
      "anonymized_code": "def _20newsgroups_highdim_dataset(var_1=None, var_2=(1, 1), var_3=np.float32):\n    var_4 = fetch_20newsgroups(random_state=0)\n    var_5 = TfidfVectorizer(ngram_range=var_2, var_3=var_3)\n    var_6 = var_5.fit_transform(var_4.data[:var_1])\n    var_7 = var_4.target[:var_1]\n\n    var_6, var_8, var_7, var_9 = train_test_split(var_6, var_7, test_size=0.1, random_state=0)\n    return var_6, var_8, var_7, var_9\n",
      "lines_processed": 8,
      "total_lines": 168
    },
    {
      "file_path": "datasets.py",
      "code": "def _synth_regression_dataset(n_samples=100000, n_features=100, dtype=np.float32):\n    X, y = make_regression(\n        n_samples=n_samples,\n        n_features=n_features,\n        n_informative=n_features // 10,\n        noise=50,\n        random_state=0,\n    )\n    X = X.astype(dtype, copy=False)\n    X = StandardScaler().fit_transform(X)\n",
      "variables": [
        "n_samples",
        "n_features",
        "dtype",
        "X",
        "y"
      ],
      "anonymized_code": "def _synth_regression_dataset(var_1=100000, var_2=100, var_3=np.float32):\n    var_4, var_5 = make_regression(\n        var_1=var_1,\n        var_2=var_2,\n        n_informative=var_2 // 10,\n        noise=50,\n        random_state=0,\n    )\n    var_4 = var_4.astype(var_3, copy=False)\n    var_4 = StandardScaler().fit_transform(var_4)\n",
      "lines_processed": 10,
      "total_lines": 168
    },
    {
      "file_path": "datasets.py",
      "code": "def _digits_dataset(n_samples=None, dtype=np.float32):\n    X, y = load_digits(return_X_y=True)\n    X = X.astype(dtype, copy=False)\n    X = MaxAbsScaler().fit_transform(X)\n    X = X[:n_samples]\n    y = y[:n_samples]\n\n    X, X_val, y, y_val = train_test_split(X, y, test_size=0.1, random_state=0)\n    return X, X_val, y, y_val\n",
      "variables": [
        "n_samples",
        "dtype",
        "X",
        "y",
        "X_val",
        "y_val"
      ],
      "anonymized_code": "def _digits_dataset(var_1=None, var_2=np.float32):\n    var_3, var_4 = load_digits(return_X_y=True)\n    var_3 = var_3.astype(var_2, copy=False)\n    var_3 = MaxAbsScaler().fit_transform(var_3)\n    var_3 = var_3[:var_1]\n    var_4 = var_4[:var_1]\n\n    var_3, var_5, var_4, var_6 = train_test_split(var_3, var_4, test_size=0.1, random_state=0)\n    return var_3, var_5, var_4, var_6\n",
      "lines_processed": 9,
      "total_lines": 168
    },
    {
      "file_path": "datasets.py",
      "code": "def _20newsgroups_lowdim_dataset(n_components=100, ngrams=(1, 1), dtype=np.float32):\n    newsgroups = fetch_20newsgroups()\n    vectorizer = TfidfVectorizer(ngram_range=ngrams)\n    X = vectorizer.fit_transform(newsgroups.data)\n    X = X.astype(dtype, copy=False)\n    svd = TruncatedSVD(n_components=n_components)\n    X = svd.fit_transform(X)\n    y = newsgroups.target\n\n    X, X_val, y, y_val = train_test_split(X, y, test_size=0.1, random_state=0)\n",
      "variables": [
        "n_components",
        "ngrams",
        "dtype",
        "newsgroups",
        "vectorizer",
        "X",
        "svd",
        "y",
        "X_val",
        "y_val"
      ],
      "anonymized_code": "def _20newsgroups_lowdim_dataset(var_1=100, var_2=(1, 1), var_3=np.float32):\n    var_4 = fetch_20newsgroups()\n    var_5 = TfidfVectorizer(ngram_range=var_2)\n    var_6 = var_5.fit_transform(var_4.data)\n    var_6 = var_6.astype(var_3, copy=False)\n    var_7 = TruncatedSVD(var_1=var_1)\n    var_6 = var_7.fit_transform(var_6)\n    var_8 = var_4.target\n\n    var_6, var_9, var_8, var_10 = train_test_split(var_6, var_8, test_size=0.1, random_state=0)\n",
      "lines_processed": 10,
      "total_lines": 168
    },
    {
      "file_path": "utils.py",
      "code": "def explained_variance_ratio(Xt, X):\n    return np.var(Xt, axis=0).sum() / np.var(X, axis=0).sum()\n",
      "variables": [
        "Xt",
        "X"
      ],
      "anonymized_code": "def explained_variance_ratio(var_1, var_2):\n    return np.var(var_1, axis=0).sum() / np.var(var_2, axis=0).sum()\n",
      "lines_processed": 2,
      "total_lines": 47
    },
    {
      "file_path": "utils.py",
      "code": "def neg_mean_inertia(X, labels, centers):\n    return -(np.asarray(X - centers[labels]) ** 2).sum(axis=1).mean()\n",
      "variables": [
        "X",
        "labels",
        "centers"
      ],
      "anonymized_code": "def neg_mean_inertia(var_1, var_2, var_3):\n    return -(np.asarray(var_1 - var_3[var_2]) ** 2).sum(axis=1).mean()\n",
      "lines_processed": 2,
      "total_lines": 47
    },
    {
      "file_path": "utils.py",
      "code": "def neg_mean_data_error(X, U, V):\n    return -np.sqrt(((X - U.dot(V)) ** 2).mean())\n",
      "variables": [
        "X",
        "U",
        "V"
      ],
      "anonymized_code": "def neg_mean_data_error(var_1, var_2, var_3):\n    return -np.sqrt(((var_1 - var_2.dot(var_3)) ** 2).mean())\n",
      "lines_processed": 2,
      "total_lines": 47
    },
    {
      "file_path": "utils.py",
      "code": "def make_gen_classif_scorers(caller):\n    caller.train_scorer = balanced_accuracy_score\n    caller.test_scorer = balanced_accuracy_score\n",
      "variables": [
        "caller"
      ],
      "anonymized_code": "def make_gen_classif_scorers(var_1):\n    var_1.train_scorer = balanced_accuracy_score\n    var_1.test_scorer = balanced_accuracy_score\n",
      "lines_processed": 3,
      "total_lines": 47
    },
    {
      "file_path": "utils.py",
      "code": "def make_gen_reg_scorers(caller):\n    caller.test_scorer = r2_score\n    caller.train_scorer = r2_score\n",
      "variables": [
        "caller"
      ],
      "anonymized_code": "def make_gen_reg_scorers(var_1):\n    var_1.test_scorer = r2_score\n    var_1.train_scorer = r2_score\n",
      "lines_processed": 3,
      "total_lines": 47
    },
    {
      "file_path": "utils.py",
      "code": "def make_pca_scorers(caller):\n    caller.train_scorer = lambda _, __: caller.estimator.explained_variance_ratio_.sum()\n    caller.test_scorer = lambda _, __: (\n        explained_variance_ratio(caller.estimator.transform(caller.X_val), caller.X_val)\n    )\n",
      "variables": [
        "caller",
        "_",
        "__"
      ],
      "anonymized_code": "def make_pca_scorers(var_1):\n    var_1.train_scorer = lambda var_2, var_3: var_1.estimator.explained_variance_ratio_.sum()\n    var_1.test_scorer = lambda var_2, var_3: (\n        explained_variance_ratio(var_1.estimator.transform(var_1.X_val), var_1.X_val)\n    )\n",
      "lines_processed": 5,
      "total_lines": 47
    },
    {
      "file_path": "common.py",
      "code": "def clear_tmp():\n    \"\"\"Clean the tmp directory\"\"\"\n    path = Path(__file__).resolve().parent / \"cache\" / \"tmp\"\n    for child in path.iterdir():\n        child.unlink()\n",
      "variables": [
        "path",
        "child"
      ],
      "anonymized_code": "def clear_tmp():\n    \"\"\"Clean the tmp directory\"\"\"\n    var_1 = Path(__file__).resolve().parent / \"cache\" / \"tmp\"\n    for var_2 in var_1.iterdir():\n        var_2.unlink()\n",
      "lines_processed": 5,
      "total_lines": 256
    },
    {
      "file_path": "bench_glmnet.py",
      "code": "def rmse(a, b):\n    return np.sqrt(np.mean((a - b) ** 2))\n",
      "variables": [
        "a",
        "b"
      ],
      "anonymized_code": "def rmse(var_1, var_2):\n    return np.sqrt(np.mean((var_1 - var_2) ** 2))\n",
      "lines_processed": 2,
      "total_lines": 140
    },
    {
      "file_path": "bench_glmnet.py",
      "code": "def bench(factory, X, Y, X_test, Y_test, ref_coef):\n    gc.collect()\n\n    # start time\n    tstart = time()\n    clf = factory(alpha=alpha).fit(X, Y)\n    delta = time() - tstart\n    # stop time\n\n    print(\"duration: %0.3fs\" % delta)\n",
      "variables": [
        "factory",
        "X",
        "Y",
        "X_test",
        "Y_test",
        "ref_coef",
        "tstart",
        "clf",
        "delta"
      ],
      "anonymized_code": "def bench(var_1, var_2, var_3, var_4, var_5, var_6):\n    gc.collect()\n\n    # start time\n    var_7 = time()\n    var_8 = var_1(alpha=alpha).fit(var_2, var_3)\n    var_9 = time() - var_7\n    # stop time\n\n    print(\"duration: %0.3fs\" % var_9)\n",
      "lines_processed": 10,
      "total_lines": 140
    }
  ],
  "matplotlib_matplotlib": [
    {
      "file_path": "redirect_from.py",
      "code": "def setup(app):\n    app.add_directive(\"redirect-from\", RedirectFrom)\n    app.add_domain(RedirectFromDomain)\n    app.connect(\"builder-inited\", _clear_redirects)\n    app.connect(\"build-finished\", _generate_redirects)\n\n    metadata = {'parallel_read_safe': True}\n    return metadata\n",
      "variables": [
        "app",
        "metadata"
      ],
      "anonymized_code": "def setup(var_1):\n    var_1.add_directive(\"redirect-from\", RedirectFrom)\n    var_1.add_domain(RedirectFromDomain)\n    var_1.connect(\"builder-inited\", _clear_redirects)\n    var_1.connect(\"build-finished\", _generate_redirects)\n\n    var_2 = {'parallel_read_safe': True}\n    return var_2\n",
      "lines_processed": 8,
      "total_lines": 127
    },
    {
      "file_path": "redirect_from.py",
      "code": "def _clear_redirects(app):\n    domain = app.env.get_domain('redirect_from')\n    if domain.redirects:\n        logger.info('clearing cached redirects')\n        domain.redirects.clear()\n",
      "variables": [
        "app",
        "domain"
      ],
      "anonymized_code": "def _clear_redirects(var_1):\n    var_2 = var_1.env.get_domain('redirect_from')\n    if var_2.redirects:\n        logger.info('clearing cached redirects')\n        var_2.redirects.clear()\n",
      "lines_processed": 5,
      "total_lines": 127
    },
    {
      "file_path": "vendor_schemas.py",
      "code": "def print_progress(block_count, block_size, total_size):\n    size = block_count * block_size\n    if total_size != -1:\n        size = min(size, total_size)\n        width = 50\n        percent = size / total_size * 100\n        filled = int(percent // (100 // width))\n        percent_str = '\\N{Full Block}' * filled + '\\N{Light Shade}' * (width - filled)\n    print(f'{percent_str} {size:6d} / {total_size:6d}', end='\\r')\n",
      "variables": [
        "block_count",
        "block_size",
        "total_size",
        "size",
        "width",
        "percent",
        "filled",
        "percent_str"
      ],
      "anonymized_code": "def print_progress(var_1, var_2, var_3):\n    var_4 = var_1 * var_2\n    if var_3 != -1:\n        var_4 = min(var_4, var_3)\n        var_5 = 50\n        var_6 = var_4 / var_3 * 100\n        var_7 = int(var_6 // (100 // var_5))\n        var_8 = '\\N{Full Block}' * var_7 + '\\N{Light Shade}' * (var_5 - var_7)\n    print(f'{var_8} {var_4:6d} / {var_3:6d}', end='\\r')\n",
      "lines_processed": 9,
      "total_lines": 50
    },
    {
      "file_path": "animate_decay.py",
      "code": "def run(data):\n    # update the data\n    t, y = data\n    xdata.append(t)\n    ydata.append(y)\n    xmin, xmax = ax.get_xlim()\n\n    if t >= xmax:\n        ax.set_xlim(xmin, 2*xmax)\n        ax.figure.canvas.draw()\n",
      "variables": [
        "data",
        "t",
        "y",
        "xmin",
        "xmax"
      ],
      "anonymized_code": "def run(var_1):\n    # update the var_1\n    var_2, var_3 = var_1\n    xdata.append(var_2)\n    ydata.append(var_3)\n    var_4, var_5 = ax.get_xlim()\n\n    if var_2 >= var_5:\n        ax.set_xlim(var_4, 2*var_5)\n        ax.figure.canvas.draw()\n",
      "lines_processed": 10,
      "total_lines": 59
    },
    {
      "file_path": "animate_decay.py",
      "code": "def data_gen():\n    for cnt in itertools.count():\n        t = cnt / 10\n        yield t, np.sin(2*np.pi*t) * np.exp(-t/10.)\n",
      "variables": [
        "cnt",
        "t"
      ],
      "anonymized_code": "def data_gen():\n    for var_1 in itertools.count():\n        var_2 = var_1 / 10\n        yield var_2, np.sin(2*np.pi*var_2) * np.exp(-var_2/10.)\n",
      "lines_processed": 4,
      "total_lines": 59
    },
    {
      "file_path": "skip_deprecated.py",
      "code": "def skip_deprecated(app, what, name, obj, skip, options):\n    if skip:\n        return skip\n    skipped = {\"matplotlib.colors\": [\"ColorConverter\", \"hex2color\", \"rgb2hex\"]}\n    skip_list = skipped.get(getattr(obj, \"__module__\", None))\n    if skip_list is not None:\n        return getattr(obj, \"__name__\", None) in skip_list\n",
      "variables": [
        "app",
        "what",
        "name",
        "obj",
        "skip",
        "options",
        "skipped",
        "skip_list"
      ],
      "anonymized_code": "def skip_deprecated(var_1, var_2, var_3, var_4, var_5, var_6):\n    if var_5:\n        return var_5\n    var_7 = {\"matplotlib.colors\": [\"ColorConverter\", \"hex2color\", \"rgb2hex\"]}\n    var_8 = var_7.get(getattr(var_4, \"__module__\", None))\n    if var_8 is not None:\n        return getattr(var_4, \"__name__\", None) in var_8\n",
      "lines_processed": 7,
      "total_lines": 17
    },
    {
      "file_path": "skip_deprecated.py",
      "code": "def setup(app):\n    app.connect('autodoc-skip-member', skip_deprecated)\n\n    metadata = {'parallel_read_safe': True, 'parallel_write_safe': True}\n    return metadata\n",
      "variables": [
        "app",
        "metadata"
      ],
      "anonymized_code": "def setup(var_1):\n    var_1.connect('autodoc-skip-member', skip_deprecated)\n\n    var_2 = {'parallel_read_safe': True, 'parallel_write_safe': True}\n    return var_2\n",
      "lines_processed": 5,
      "total_lines": 17
    },
    {
      "file_path": "mock_gui_toolkits.py",
      "code": "def setup(app):\n    sys.modules.update(\n        cairocffi=MyCairoCffi(),\n    )\n    return {'parallel_read_safe': True, 'parallel_write_safe': True}\n",
      "variables": [
        "app"
      ],
      "anonymized_code": "def setup(var_1):\n    sys.modules.update(\n        cairocffi=MyCairoCffi(),\n    )\n    return {'parallel_read_safe': True, 'parallel_write_safe': True}\n",
      "lines_processed": 5,
      "total_lines": 13
    },
    {
      "file_path": "generate_credits.py",
      "code": "def check_duplicates():\n    text = subprocess.check_output(['git', 'shortlog', '--summary', '--email'])\n    lines = text.decode('utf8').split('\\n')\n    contributors = [line.split('\\t', 1)[1].strip() for line in lines if line]\n    emails = [re.match('.*<(.*)>', line).group(1) for line in contributors]\n    email_counter = Counter(emails)\n\n    if email_counter.most_common(1)[0][1] > 1:\n        print('DUPLICATE CHECK: The following email addresses are used with '\n              'more than one name.\\nConsider adding them to .mailmap.\\n')\n",
      "variables": [
        "text",
        "lines",
        "contributors",
        "line",
        "emails",
        "email_counter"
      ],
      "anonymized_code": "def check_duplicates():\n    var_1 = subprocess.check_output(['git', 'shortlog', '--summary', '--email'])\n    var_2 = var_1.decode('utf8').split('\\n')\n    var_3 = [var_4.split('\\t', 1)[1].strip() for var_4 in var_2 if var_4]\n    var_5 = [re.match('.*<(.*)>', var_4).group(1) for var_4 in var_3]\n    var_6 = Counter(var_5)\n\n    if var_6.most_common(1)[0][1] > 1:\n        print('DUPLICATE CHECK: The following email addresses are used with '\n              'more than one name.\\nConsider adding them to .mailmap.\\n')\n",
      "lines_processed": 10,
      "total_lines": 89
    },
    {
      "file_path": "generate_credits.py",
      "code": "def generate_credits():\n    text = subprocess.check_output(['git', 'shortlog', '--summary'])\n    lines = text.decode('utf8').split('\\n')\n    contributors = [line.split('\\t', 1)[1].strip() for line in lines if line]\n    contributors.sort(key=locale.strxfrm)\n    with open('credits.rst', 'w') as f:\n        f.write(TEMPLATE.format(contributors=',\\n'.join(contributors)))\n",
      "variables": [
        "text",
        "lines",
        "contributors",
        "line",
        "f"
      ],
      "anonymized_code": "def generate_credits():\n    var_1 = subprocess.check_output(['git', 'shortlog', '--summary'])\n    var_2 = var_1.decode('utf8').split('\\n')\n    var_3 = [var_4.split('\\t', 1)[1].strip() for var_4 in var_2 if var_4]\n    var_3.sort(key=locale.strxfrm)\n    with open('credits.rst', 'w') as var_5:\n        var_5.write(TEMPLATE.format(var_3=',\\n'.join(var_3)))\n",
      "lines_processed": 7,
      "total_lines": 89
    },
    {
      "file_path": "github.py",
      "code": "def make_link_node(rawtext, app, type, slug, options):\n    \"\"\"\n    Create a link to a github resource.\n\n    :param rawtext: Text being replaced with link node.\n    :param app: Sphinx application context\n    :param type: Link type (issues, changeset, etc.)\n    :param slug: ID of the thing to link to\n    :param options: Options dictionary passed to role func.\n    \"\"\"\n",
      "variables": [
        "rawtext",
        "app",
        "type",
        "slug",
        "options"
      ],
      "anonymized_code": "def make_link_node(var_1, var_2, var_3, var_4, var_5):\n    \"\"\"\n    Create a link to a github resource.\n\n    :param var_1: Text being replaced with link node.\n    :param var_2: Sphinx application context\n    :param var_3: Link var_3 (issues, changeset, etc.)\n    :param var_4: ID of the thing to link to\n    :param var_5: Options dictionary passed to role func.\n    \"\"\"\n",
      "lines_processed": 10,
      "total_lines": 167
    },
    {
      "file_path": "github.py",
      "code": "def setup(app):\n    \"\"\"\n    Install the plugin.\n\n    :param app: Sphinx application context.\n    \"\"\"\n    app.add_role('ghissue', ghissue_role)\n    app.add_role('ghpull', ghissue_role)\n    app.add_role('ghuser', ghuser_role)\n    app.add_role('ghcommit', ghcommit_role)\n",
      "variables": [
        "app"
      ],
      "anonymized_code": "def setup(var_1):\n    \"\"\"\n    Install the plugin.\n\n    :param var_1: Sphinx application context.\n    \"\"\"\n    var_1.add_role('ghissue', ghissue_role)\n    var_1.add_role('ghpull', ghissue_role)\n    var_1.add_role('ghuser', ghuser_role)\n    var_1.add_role('ghcommit', ghcommit_role)\n",
      "lines_processed": 10,
      "total_lines": 167
    },
    {
      "file_path": "math_symbol_table.py",
      "code": "def setup(app):\n    app.add_directive(\"math_symbol_table\", MathSymbolTableDirective)\n\n    metadata = {'parallel_read_safe': True, 'parallel_write_safe': True}\n    return metadata\n",
      "variables": [
        "app",
        "metadata"
      ],
      "anonymized_code": "def setup(var_1):\n    var_1.add_directive(\"math_symbol_table\", MathSymbolTableDirective)\n\n    var_2 = {'parallel_read_safe': True, 'parallel_write_safe': True}\n    return var_2\n",
      "lines_processed": 5,
      "total_lines": 152
    },
    {
      "file_path": "math_symbol_table.py",
      "code": "def run(state_machine):\n\n    def render_symbol(sym, ignore_variant=False):\n        if ignore_variant and sym not in (r\"\\varnothing\", r\"\\varlrtriangle\"):\n            sym = sym.replace(r\"\\var\", \"\\\\\")\n        if sym.startswith(\"\\\\\"):\n            sym = sym.lstrip(\"\\\\\")\n            if sym not in (_mathtext.Parser._overunder_functions |\n                           _mathtext.Parser._function_names):\n                sym = chr(_mathtext_data.tex2uni[sym])\n",
      "variables": [
        "state_machine",
        "sym",
        "ignore_variant"
      ],
      "anonymized_code": "def run(var_1):\n\n    def render_symbol(var_2, var_3=False):\n        if var_3 and var_2 not in (r\"\\varnothing\", r\"\\varlrtriangle\"):\n            var_2 = var_2.replace(r\"\\var\", \"\\\\\")\n        if var_2.startswith(\"\\\\\"):\n            var_2 = var_2.lstrip(\"\\\\\")\n            if var_2 not in (_mathtext.Parser._overunder_functions |\n                           _mathtext.Parser._function_names):\n                var_2 = chr(_mathtext_data.tex2uni[var_2])\n",
      "lines_processed": 10,
      "total_lines": 152
    },
    {
      "file_path": "axes_margins.py",
      "code": "def arrow(p1, p2, **props):\n    ax.annotate(\"\", p1, p2,\n                arrowprops=dict(arrowstyle=\"<->\", shrinkA=0, shrinkB=0, **props))\n",
      "variables": [
        "p1",
        "p2",
        "props"
      ],
      "anonymized_code": "def arrow(var_1, var_2, **var_3):\n    ax.annotate(\"\", var_1, var_2,\n                arrowprops=dict(arrowstyle=\"<->\", shrinkA=0, shrinkB=0, **var_3))\n",
      "lines_processed": 3,
      "total_lines": 42
    },
    {
      "file_path": "missing_references.py",
      "code": "def _write_missing_references_json(records, json_path):\n    \"\"\"\n    Convert ignored references to a format which we can write as JSON\n\n    Convert from ``{(domain_type, target): locations}`` to\n    ``{domain_type: {target: locations}}`` since JSON can't serialize tuples.\n    \"\"\"\n    # Sorting records and keys avoids needlessly big diffs when\n    # missing_references.json is regenerated.\n    transformed_records = defaultdict(dict)\n",
      "variables": [
        "records",
        "json_path",
        "transformed_records"
      ],
      "anonymized_code": "def _write_missing_references_json(var_1, var_2):\n    \"\"\"\n    Convert ignored references to a format which we can write as JSON\n\n    Convert from ``{(domain_type, target): locations}`` to\n    ``{domain_type: {target: locations}}`` since JSON can't serialize tuples.\n    \"\"\"\n    # Sorting var_1 and keys avoids needlessly big diffs when\n    # missing_references.json is regenerated.\n    var_3 = defaultdict(dict)\n",
      "lines_processed": 10,
      "total_lines": 232
    },
    {
      "file_path": "missing_references.py",
      "code": "def _read_missing_references_json(json_path):\n    \"\"\"\n    Convert from the JSON file to the form used internally by this\n    extension.\n\n    The JSON file is stored as ``{domain_type: {target: [locations,]}}``\n    since JSON can't store dictionary keys which are tuples. We convert\n    this back to ``{(domain_type, target):[locations]}`` for internal use.\n\n    \"\"\"\n",
      "variables": [
        "json_path"
      ],
      "anonymized_code": "def _read_missing_references_json(var_1):\n    \"\"\"\n    Convert from the JSON file to the form used internally by this\n    extension.\n\n    The JSON file is stored as ``{domain_type: {target: [locations,]}}``\n    since JSON can't store dictionary keys which are tuples. We convert\n    this back to ``{(domain_type, target):[locations]}`` for internal use.\n\n    \"\"\"\n",
      "lines_processed": 10,
      "total_lines": 232
    },
    {
      "file_path": "missing_references.py",
      "code": "def save_missing_references(app, exc):\n    \"\"\"\n    Write a new JSON file containing missing references.\n    \"\"\"\n    json_path = Path(app.confdir) / app.config.missing_references_filename\n    references_warnings = app.env.missing_references_events\n    _write_missing_references_json(references_warnings, json_path)\n",
      "variables": [
        "app",
        "exc",
        "json_path",
        "references_warnings"
      ],
      "anonymized_code": "def save_missing_references(var_1, var_2):\n    \"\"\"\n    Write a new JSON file containing missing references.\n    \"\"\"\n    var_3 = Path(var_1.confdir) / var_1.config.missing_references_filename\n    var_4 = var_1.env.missing_references_events\n    _write_missing_references_json(var_4, var_3)\n",
      "lines_processed": 7,
      "total_lines": 232
    },
    {
      "file_path": "missing_references.py",
      "code": "def setup(app):\n    app.add_config_value(\"missing_references_enabled\", True, \"env\")\n    app.add_config_value(\"missing_references_write_json\", False, \"env\")\n    app.add_config_value(\"missing_references_warn_unused_ignores\", True, \"env\")\n    app.add_config_value(\"missing_references_filename\",\n                         \"missing-references.json\", \"env\")\n\n    app.connect(\"builder-inited\", prepare_missing_references_setup)\n\n    return {'parallel_read_safe': True}\n",
      "variables": [
        "app"
      ],
      "anonymized_code": "def setup(var_1):\n    var_1.add_config_value(\"missing_references_enabled\", True, \"env\")\n    var_1.add_config_value(\"missing_references_write_json\", False, \"env\")\n    var_1.add_config_value(\"missing_references_warn_unused_ignores\", True, \"env\")\n    var_1.add_config_value(\"missing_references_filename\",\n                         \"missing-references.json\", \"env\")\n\n    var_1.connect(\"builder-inited\", prepare_missing_references_setup)\n\n    return {'parallel_read_safe': True}\n",
      "lines_processed": 10,
      "total_lines": 232
    },
    {
      "file_path": "missing_references.py",
      "code": "def handle_missing_reference(app, domain, node):\n    \"\"\"\n    Handle the warn-missing-reference Sphinx event.\n\n    This function will:\n\n    #. record missing references for saving/comparing with ignored list.\n    #. prevent Sphinx from raising a warning on ignored references.\n    \"\"\"\n    refdomain = node[\"refdomain\"]\n",
      "variables": [
        "app",
        "domain",
        "node",
        "refdomain"
      ],
      "anonymized_code": "def handle_missing_reference(var_1, var_2, var_3):\n    \"\"\"\n    Handle the warn-missing-reference Sphinx event.\n\n    This function will:\n\n    #. record missing references for saving/comparing with ignored list.\n    #. prevent Sphinx from raising a warning on ignored references.\n    \"\"\"\n    var_4 = var_3[\"var_4\"]\n",
      "lines_processed": 10,
      "total_lines": 232
    },
    {
      "file_path": "missing_references.py",
      "code": "def _truncate_location(location):\n    \"\"\"\n    Cuts off anything after the first colon in location strings.\n\n    This allows for easy comparison even when line numbers change\n    (as they do regularly).\n    \"\"\"\n    return location.split(\":\", 1)[0]\n",
      "variables": [
        "location"
      ],
      "anonymized_code": "def _truncate_location(var_1):\n    \"\"\"\n    Cuts off anything after the first colon in var_1 strings.\n\n    This allows for easy comparison even when line numbers change\n    (as they do regularly).\n    \"\"\"\n    return var_1.split(\":\", 1)[0]\n",
      "lines_processed": 8,
      "total_lines": 232
    },
    {
      "file_path": "conf.py",
      "code": "def _check_dependencies():\n    names = {\n        **{ext: ext.split(\".\")[0] for ext in extensions},\n        # Explicitly list deps that are not extensions, or whose PyPI package\n        # name does not match the (toplevel) module name.\n        \"colorspacious\": 'colorspacious',\n        \"mpl_sphinx_theme\": 'mpl_sphinx_theme',\n        \"sphinxcontrib.inkscapeconverter\": 'sphinxcontrib-svg2pdfconverter',\n    }\n    missing = []\n",
      "variables": [
        "names",
        "ext",
        "missing"
      ],
      "anonymized_code": "def _check_dependencies():\n    var_1 = {\n        **{var_2: var_2.split(\".\")[0] for var_2 in extensions},\n        # Explicitly list deps that are not extensions, or whose PyPI package\n        # name does not match the (toplevel) module name.\n        \"colorspacious\": 'colorspacious',\n        \"mpl_sphinx_theme\": 'mpl_sphinx_theme',\n        \"sphinxcontrib.inkscapeconverter\": 'sphinxcontrib-svg2pdfconverter',\n    }\n    var_3 = []\n",
      "lines_processed": 10,
      "total_lines": 928
    },
    {
      "file_path": "conf.py",
      "code": "def setup(app):\n    if any(st in version for st in ('post', 'dev', 'alpha', 'beta')):\n        bld_type = 'dev'\n    else:\n        bld_type = 'rel'\n    app.add_config_value('skip_sub_dirs', 0, '')\n    app.add_config_value('releaselevel', bld_type, 'env')\n    app.connect('autodoc-process-bases', autodoc_process_bases)\n    if sphinx.version_info[:2] < (7, 1):\n        app.connect('html-page-context', add_html_cache_busting, priority=1000)\n",
      "variables": [
        "app",
        "st",
        "bld_type"
      ],
      "anonymized_code": "def setup(var_1):\n    if any(var_2 in version for var_2 in ('post', 'dev', 'alpha', 'beta')):\n        var_3 = 'dev'\n    else:\n        var_3 = 'rel'\n    var_1.add_config_value('skip_sub_dirs', 0, '')\n    var_1.add_config_value('releaselevel', var_3, 'env')\n    var_1.connect('autodoc-process-bases', autodoc_process_bases)\n    if sphinx.version_info[:2] < (7, 1):\n        var_1.connect('html-page-context', add_html_cache_busting, priority=1000)\n",
      "lines_processed": 10,
      "total_lines": 928
    },
    {
      "file_path": "conf.py",
      "code": "def tutorials_download_error(record):\n    if re.match(\"download file not readable: .*tutorials_(python|jupyter).zip\",\n                record.msg):\n        return False\n",
      "variables": [
        "record"
      ],
      "anonymized_code": "def tutorials_download_error(var_1):\n    if re.match(\"download file not readable: .*tutorials_(python|jupyter).zip\",\n                var_1.msg):\n        return False\n",
      "lines_processed": 4,
      "total_lines": 928
    },
    {
      "file_path": "util.py",
      "code": "def matplotlib_reduced_latex_scraper(block, block_vars, gallery_conf,\n                                     **kwargs):\n    \"\"\"\n    Reduce srcset when creating a PDF.\n\n    Because sphinx-gallery runs *very* early, we cannot modify this even in the\n    earliest builder-inited signal. Thus we do it at scraping time.\n    \"\"\"\n    from sphinx_gallery.scrapers import matplotlib_scraper\n\n",
      "variables": [
        "block",
        "block_vars",
        "gallery_conf",
        "kwargs"
      ],
      "anonymized_code": "def matplotlib_reduced_latex_scraper(var_1, var_2, var_3,\n                                     **var_4):\n    \"\"\"\n    Reduce srcset when creating a PDF.\n\n    Because sphinx-gallery runs *very* early, we cannot modify this even in the\n    earliest builder-inited signal. Thus we do it at scraping time.\n    \"\"\"\n    from sphinx_gallery.scrapers import matplotlib_scraper\n\n",
      "lines_processed": 10,
      "total_lines": 21
    },
    {
      "file_path": "util.py",
      "code": "def clear_basic_units(gallery_conf, fname):\n    return sys.modules.pop('basic_units', None)\n",
      "variables": [
        "gallery_conf",
        "fname"
      ],
      "anonymized_code": "def clear_basic_units(var_1, var_2):\n    return sys.modules.pop('basic_units', None)\n",
      "lines_processed": 2,
      "total_lines": 21
    }
  ],
  "django_django": [
    {
      "file_path": "global_settings.py",
      "code": "def gettext_noop(s):\n    return s\n",
      "variables": [
        "s"
      ],
      "anonymized_code": "def gettext_noop(var_1):\n    return var_1\n",
      "lines_processed": 2,
      "total_lines": 669
    }
  ],
  "psf_black": [
    {
      "file_path": "generate_schema.py",
      "code": "def generate_schema_from_click(\n    cmd: click.Command,\n) -> dict[str, Any]:\n    result: dict[str, dict[str, Any]] = {}\n    for param in cmd.params:\n        if not isinstance(param, click.Option) or param.is_eager:\n            continue\n\n        assert param.name\n        name = param.name.replace(\"_\", \"-\")\n",
      "variables": [
        "cmd",
        "result",
        "param",
        "name"
      ],
      "anonymized_code": "def generate_schema_from_click(\n    var_1: click.Command,\n) -> dict[str, Any]:\n    var_2: dict[str, dict[str, Any]] = {}\n    for var_3 in var_1.params:\n        if not isinstance(var_3, click.Option) or var_3.is_eager:\n            continue\n\n        assert var_3.var_4\n        var_4 = var_3.var_4.replace(\"_\", \"-\")\n",
      "lines_processed": 10,
      "total_lines": 74
    },
    {
      "file_path": "diff_shades_gha_helper.py",
      "code": "def comment_details(run_id: str) -> None:\n    data = http_get(f\"https://api.github.com/repos/{REPO}/actions/runs/{run_id}\")\n    if data[\"event\"] != \"pull_request\" or data[\"conclusion\"] == \"cancelled\":\n        set_output(\"needs-comment\", \"false\")\n        return\n\n    set_output(\"needs-comment\", \"true\")\n    jobs = http_get(data[\"jobs_url\"])[\"jobs\"]\n    job = next(j for j in jobs if j[\"name\"] == \"analysis / preview-changes\")\n    diff_step = next(s for s in job[\"steps\"] if s[\"name\"] == DIFF_STEP_NAME)\n",
      "variables": [
        "run_id",
        "data",
        "jobs",
        "job",
        "j",
        "diff_step",
        "s"
      ],
      "anonymized_code": "def comment_details(var_1: str) -> None:\n    var_2 = http_get(f\"https://api.github.com/repos/{REPO}/actions/runs/{var_1}\")\n    if var_2[\"event\"] != \"pull_request\" or var_2[\"conclusion\"] == \"cancelled\":\n        set_output(\"needs-comment\", \"false\")\n        return\n\n    set_output(\"needs-comment\", \"true\")\n    var_3 = http_get(var_2[\"jobs_url\"])[\"var_3\"]\n    var_4 = next(var_5 for var_5 in var_3 if var_5[\"name\"] == \"analysis / preview-changes\")\n    var_6 = next(var_7 for var_7 in var_4[\"steps\"] if var_7[\"name\"] == DIFF_STEP_NAME)\n",
      "lines_processed": 10,
      "total_lines": 231
    },
    {
      "file_path": "diff_shades_gha_helper.py",
      "code": "def get_pypi_version() -> Version:\n    data = http_get(\"https://pypi.org/pypi/black/json\")\n    versions = [Version(v) for v in data[\"releases\"]]\n    sorted_versions = sorted(versions, reverse=True)\n    return sorted_versions[0]\n",
      "variables": [
        "data",
        "versions",
        "v",
        "sorted_versions"
      ],
      "anonymized_code": "def get_pypi_version() -> Version:\n    var_1 = http_get(\"https://pypi.org/pypi/black/json\")\n    var_2 = [Version(var_3) for var_3 in var_1[\"releases\"]]\n    var_4 = sorted(var_2, reverse=True)\n    return var_4[0]\n",
      "lines_processed": 5,
      "total_lines": 231
    },
    {
      "file_path": "diff_shades_gha_helper.py",
      "code": "def get_pr_revision(pr: int) -> str:\n    data = http_get(f\"https://api.github.com/repos/{REPO}/pulls/{pr}\")\n    assert isinstance(data[\"head\"][\"sha\"], str)\n    return data[\"head\"][\"sha\"]\n",
      "variables": [
        "pr",
        "data"
      ],
      "anonymized_code": "def get_pr_revision(var_1: int) -> str:\n    var_2 = http_get(f\"https://api.github.com/repos/{REPO}/pulls/{var_1}\")\n    assert isinstance(var_2[\"head\"][\"sha\"], str)\n    return var_2[\"head\"][\"sha\"]\n",
      "lines_processed": 4,
      "total_lines": 231
    },
    {
      "file_path": "diff_shades_gha_helper.py",
      "code": "def config(event: Literal[\"push\", \"pull_request\"]) -> None:\n    import diff_shades  # type: ignore[import-not-found]\n\n    if event == \"push\":\n        jobs = [{\"mode\": \"preview-changes\", \"force-flag\": \"--force-preview-style\"}]\n        # Push on main, let's use PyPI Black as the baseline.\n        baseline_name = str(get_pypi_version())\n        baseline_cmd = f\"git checkout {baseline_name}\"\n        target_rev = os.getenv(\"GITHUB_SHA\")\n        assert target_rev is not None\n",
      "variables": [
        "event",
        "jobs",
        "baseline_name",
        "baseline_cmd",
        "target_rev"
      ],
      "anonymized_code": "def config(var_1: Literal[\"push\", \"pull_request\"]) -> None:\n    import diff_shades  # type: ignore[import-not-found]\n\n    if var_1 == \"push\":\n        var_2 = [{\"mode\": \"preview-changes\", \"force-flag\": \"--force-preview-style\"}]\n        # Push on main, let's use PyPI Black as the baseline.\n        var_3 = str(get_pypi_version())\n        var_4 = f\"git checkout {var_3}\"\n        var_5 = os.getenv(\"GITHUB_SHA\")\n        assert var_5 is not None\n",
      "lines_processed": 10,
      "total_lines": 231
    },
    {
      "file_path": "diff_shades_gha_helper.py",
      "code": "def http_get(url: str, *, is_json: bool = True, **kwargs: Any) -> Any:\n    headers = kwargs.get(\"headers\") or {}\n    headers[\"User-Agent\"] = USER_AGENT\n    if \"github\" in url:\n        if GH_API_TOKEN:\n            headers[\"Authorization\"] = f\"token {GH_API_TOKEN}\"\n        headers[\"Accept\"] = \"application/vnd.github.v3+json\"\n    kwargs[\"headers\"] = headers\n\n    r = http.request(\"GET\", url, **kwargs)\n",
      "variables": [
        "url",
        "is_json",
        "kwargs",
        "headers",
        "r"
      ],
      "anonymized_code": "def http_get(var_1: str, *, var_2: bool = True, **var_3: Any) -> Any:\n    var_4 = var_3.get(\"var_4\") or {}\n    var_4[\"User-Agent\"] = USER_AGENT\n    if \"github\" in var_1:\n        if GH_API_TOKEN:\n            var_4[\"Authorization\"] = f\"token {GH_API_TOKEN}\"\n        var_4[\"Accept\"] = \"application/vnd.github.v3+json\"\n    var_3[\"var_4\"] = var_4\n\n    var_5 = http.request(\"GET\", var_1, **var_3)\n",
      "lines_processed": 10,
      "total_lines": 231
    },
    {
      "file_path": "diff_shades_gha_helper.py",
      "code": "def set_output(name: str, value: str) -> None:\n    if len(value) < 200:\n        print(f\"[INFO]: setting '{name}' to '{value}'\")\n    else:\n        print(f\"[INFO]: setting '{name}' to [{len(value)} chars]\")\n\n    if \"GITHUB_OUTPUT\" in os.environ:\n        if \"\\n\" in value:\n            # https://docs.github.com/en/actions/using-workflows/workflow-commands-for-github-actions#multiline-strings\n            delimiter = b64encode(os.urandom(16)).decode()\n",
      "variables": [
        "name",
        "value",
        "delimiter"
      ],
      "anonymized_code": "def set_output(var_1: str, var_2: str) -> None:\n    if len(var_2) < 200:\n        print(f\"[INFO]: setting '{var_1}' to '{var_2}'\")\n    else:\n        print(f\"[INFO]: setting '{var_1}' to [{len(var_2)} chars]\")\n\n    if \"GITHUB_OUTPUT\" in os.environ:\n        if \"\\n\" in var_2:\n            # https://docs.github.com/en/actions/using-workflows/workflow-commands-for-github-actions#multiline-strings\n            var_3 = b64encode(os.urandom(16)).decode()\n",
      "lines_processed": 10,
      "total_lines": 231
    },
    {
      "file_path": "diff_shades_gha_helper.py",
      "code": "def get_main_revision() -> str:\n    data = http_get(\n        f\"https://api.github.com/repos/{REPO}/commits\",\n        fields={\"per_page\": \"1\", \"sha\": \"main\"},\n    )\n    assert isinstance(data[0][\"sha\"], str)\n    return data[0][\"sha\"]\n",
      "variables": [
        "data"
      ],
      "anonymized_code": "def get_main_revision() -> str:\n    var_1 = http_get(\n        f\"https://api.github.com/repos/{REPO}/commits\",\n        fields={\"per_page\": \"1\", \"sha\": \"main\"},\n    )\n    assert isinstance(var_1[0][\"sha\"], str)\n    return var_1[0][\"sha\"]\n",
      "lines_processed": 7,
      "total_lines": 231
    },
    {
      "file_path": "diff_shades_gha_helper.py",
      "code": "def comment_body(\n    baseline: Path, target: Path, baseline_sha: str, target_sha: str, pr_num: int\n) -> None:\n    # fmt: off\n    cmd = [\n        sys.executable, \"-m\", \"diff_shades\", \"--no-color\",\n        \"compare\", str(baseline), str(target), \"--quiet\", \"--check\"\n    ]\n    # fmt: on\n    proc = subprocess.run(cmd, stdout=subprocess.PIPE, encoding=\"utf-8\")\n",
      "variables": [
        "baseline",
        "target",
        "baseline_sha",
        "target_sha",
        "pr_num",
        "cmd",
        "proc"
      ],
      "anonymized_code": "def comment_body(\n    var_1: Path, var_2: Path, var_3: str, var_4: str, var_5: int\n) -> None:\n    # fmt: off\n    var_6 = [\n        sys.executable, \"-m\", \"diff_shades\", \"--no-color\",\n        \"compare\", str(var_1), str(var_2), \"--quiet\", \"--check\"\n    ]\n    # fmt: on\n    var_7 = subprocess.run(var_6, stdout=subprocess.PIPE, encoding=\"utf-8\")\n",
      "lines_processed": 10,
      "total_lines": 231
    },
    {
      "file_path": "make_width_table.py",
      "code": "def main() -> None:\n    table_path = join(dirname(__file__), \"..\", \"src\", \"black\", \"_width_table.py\")\n    with open(table_path, \"w\") as f:\n        f.write(f\"\"\"# Generated by {basename(__file__)}\n# wcwidth {wcwidth.__version__}\n# Unicode {wcwidth.list_versions()[-1]}\nfrom typing import Final\n\nWIDTH_TABLE: Final[list[tuple[int, int, int]]] = [\n\"\"\")\n",
      "variables": [
        "table_path",
        "f"
      ],
      "anonymized_code": "def main() -> None:\n    var_1 = join(dirname(__file__), \"..\", \"src\", \"black\", \"_width_table.py\")\n    with open(var_1, \"w\") as var_2:\n        var_2.write(var_2\"\"\"# Generated by {basename(__file__)}\n# wcwidth {wcwidth.__version__}\n# Unicode {wcwidth.list_versions()[-1]}\nfrom typing import Final\n\nWIDTH_TABLE: Final[list[tuple[int, int, int]]] = [\n\"\"\")\n",
      "lines_processed": 10,
      "total_lines": 66
    },
    {
      "file_path": "fuzz.py",
      "code": "def test_idempotent_any_syntatically_valid_python(\n    src_contents: str, mode: black.FileMode\n) -> None:\n    # Before starting, let's confirm that the input string is valid Python:\n    compile(src_contents, \"<string>\", \"exec\")  # else the bug is in hypothesmith\n\n    # Then format the code...\n    dst_contents = black.format_str(src_contents, mode=mode)\n\n    # And check that we got equivalent and stable output.\n",
      "variables": [
        "src_contents",
        "mode",
        "dst_contents"
      ],
      "anonymized_code": "def test_idempotent_any_syntatically_valid_python(\n    var_1: str, var_2: black.FileMode\n) -> None:\n    # Before starting, let's confirm that the input string is valid Python:\n    compile(var_1, \"<string>\", \"exec\")  # else the bug is in hypothesmith\n\n    # Then format the code...\n    var_3 = black.format_str(var_1, var_2=var_2)\n\n    # And check that we got equivalent and stable output.\n",
      "lines_processed": 10,
      "total_lines": 73
    },
    {
      "file_path": "migrate-black.py",
      "code": "def blackify(base_branch: str, black_command: str, logger: logging.Logger) -> int:\n    current_branch = git(\"branch\", \"--show-current\")\n\n    if not current_branch or base_branch == current_branch:\n        logger.error(\"You need to check out a feature branch to work on\")\n        return 1\n\n    if not os.path.exists(\".git\"):\n        logger.error(\"Run me in the root of your repo\")\n        return 1\n",
      "variables": [
        "base_branch",
        "black_command",
        "logger",
        "current_branch"
      ],
      "anonymized_code": "def blackify(var_1: str, var_2: str, var_3: logging.Logger) -> int:\n    var_4 = git(\"branch\", \"--show-current\")\n\n    if not var_4 or var_1 == var_4:\n        var_3.error(\"You need to check out a feature branch to work on\")\n        return 1\n\n    if not os.path.exists(\".git\"):\n        var_3.error(\"Run me in the root of your repo\")\n        return 1\n",
      "lines_processed": 10,
      "total_lines": 96
    },
    {
      "file_path": "migrate-black.py",
      "code": "def git(*args: str) -> str:\n    return check_output([\"git\", *args]).decode(\"utf8\").strip()\n",
      "variables": [
        "args"
      ],
      "anonymized_code": "def git(*var_1: str) -> str:\n    return check_output([\"git\", *var_1]).decode(\"utf8\").strip()\n",
      "lines_processed": 2,
      "total_lines": 96
    },
    {
      "file_path": "conf.py",
      "code": "def make_pypi_svg(version: str) -> None:\n    template: Path = CURRENT_DIR / \"_static\" / \"pypi_template.svg\"\n    target: Path = CURRENT_DIR / \"_static\" / \"pypi.svg\"\n    with open(str(template), encoding=\"utf8\") as f:\n        svg: str = string.Template(f.read()).substitute(version=version)\n    with open(str(target), \"w\", encoding=\"utf8\") as f:\n        f.write(svg)\n",
      "variables": [
        "version",
        "template",
        "target",
        "f",
        "svg"
      ],
      "anonymized_code": "def make_pypi_svg(var_1: str) -> None:\n    var_2: Path = CURRENT_DIR / \"_static\" / \"pypi_template.var_5\"\n    var_3: Path = CURRENT_DIR / \"_static\" / \"pypi.var_5\"\n    with open(str(var_2), encoding=\"utf8\") as var_4:\n        var_5: str = string.Template(var_4.read()).substitute(var_1=var_1)\n    with open(str(var_3), \"w\", encoding=\"utf8\") as var_4:\n        var_4.write(var_5)\n",
      "lines_processed": 7,
      "total_lines": 241
    },
    {
      "file_path": "conf.py",
      "code": "def handle_include_read(\n    app: Sphinx,\n    relative_path: Path,\n    parent_docname: str,\n    content: list[str],\n) -> None:\n    \"\"\"Handler for the include-read sphinx event.\"\"\"\n    if parent_docname == \"change_log\":\n        content[0] = replace_pr_numbers_with_links(content[0])\n",
      "variables": [
        "app",
        "relative_path",
        "parent_docname",
        "content"
      ],
      "anonymized_code": "def handle_include_read(\n    var_1: Sphinx,\n    var_2: Path,\n    var_3: str,\n    var_4: list[str],\n) -> None:\n    \"\"\"Handler for the include-read sphinx event.\"\"\"\n    if var_3 == \"change_log\":\n        var_4[0] = replace_pr_numbers_with_links(var_4[0])\n",
      "lines_processed": 9,
      "total_lines": 241
    },
    {
      "file_path": "conf.py",
      "code": "def replace_pr_numbers_with_links(content: str) -> str:\n    \"\"\"Replaces all PR numbers with the corresponding GitHub link.\"\"\"\n    return re.sub(r\"#(\\d+)\", r\"[#\\1](https://github.com/psf/black/pull/\\1)\", content)\n",
      "variables": [
        "content"
      ],
      "anonymized_code": "def replace_pr_numbers_with_links(var_1: str) -> str:\n    \"\"\"Replaces all PR numbers with the corresponding GitHub link.\"\"\"\n    return re.sub(r\"#(\\d+)\", r\"[#\\1](https://github.com/psf/black/pull/\\1)\", var_1)\n",
      "lines_processed": 3,
      "total_lines": 241
    },
    {
      "file_path": "conf.py",
      "code": "def setup(app: Sphinx) -> None:\n    \"\"\"Sets up a minimal sphinx extension.\"\"\"\n    app.connect(\"include-read\", handle_include_read)\n",
      "variables": [
        "app"
      ],
      "anonymized_code": "def setup(var_1: Sphinx) -> None:\n    \"\"\"Sets up a minimal sphinx extension.\"\"\"\n    var_1.connect(\"include-read\", handle_include_read)\n",
      "lines_processed": 3,
      "total_lines": 241
    },
    {
      "file_path": "gallery.py",
      "code": "def get_top_packages() -> list[str]:\n    with urlopen(PYPI_TOP_PACKAGES) as page:\n        result = json.load(page)\n\n    return [package[\"project\"] for package in result[\"rows\"]]\n",
      "variables": [
        "page",
        "result",
        "package"
      ],
      "anonymized_code": "def get_top_packages() -> list[str]:\n    with urlopen(PYPI_TOP_PACKAGES) as var_1:\n        var_2 = json.load(var_1)\n\n    return [var_3[\"project\"] for var_3 in var_2[\"rows\"]]\n",
      "lines_processed": 5,
      "total_lines": 295
    },
    {
      "file_path": "gallery.py",
      "code": "def black_runner(version: str, black_repo: Path) -> Path:\n    directory = tempfile.TemporaryDirectory()\n    venv.create(directory.name, with_pip=True)\n\n    python = Path(directory.name) / \"bin\" / \"python\"\n    subprocess.run([python, \"-m\", \"pip\", \"install\", \"-e\", black_repo])\n\n    atexit.register(directory.cleanup)\n    return python\n",
      "variables": [
        "version",
        "black_repo",
        "directory",
        "python"
      ],
      "anonymized_code": "def black_runner(var_1: str, var_2: Path) -> Path:\n    var_3 = tempfile.TemporaryDirectory()\n    venv.create(var_3.name, with_pip=True)\n\n    var_4 = Path(var_3.name) / \"bin\" / \"var_4\"\n    subprocess.run([var_4, \"-m\", \"pip\", \"install\", \"-e\", var_2])\n\n    atexit.register(var_3.cleanup)\n    return var_4\n",
      "lines_processed": 9,
      "total_lines": 295
    },
    {
      "file_path": "gallery.py",
      "code": "def download_and_extract_top_packages(\n    directory: Path,\n    workers: int = 8,\n    limit: slice = DEFAULT_SLICE,\n) -> Generator[Path, None, None]:\n    with ThreadPoolExecutor(max_workers=workers) as executor:\n        bound_downloader = partial(get_package, version=None, directory=directory)\n        for package in executor.map(bound_downloader, get_top_packages()[limit]):\n            if package is not None:\n                yield package\n",
      "variables": [
        "directory",
        "workers",
        "limit",
        "executor",
        "bound_downloader",
        "package"
      ],
      "anonymized_code": "def download_and_extract_top_packages(\n    var_1: Path,\n    var_2: int = 8,\n    var_3: slice = DEFAULT_SLICE,\n) -> Generator[Path, None, None]:\n    with ThreadPoolExecutor(max_workers=var_2) as var_4:\n        var_5 = partial(get_package, version=None, var_1=var_1)\n        for var_6 in var_4.map(var_5, get_top_packages()[var_3]):\n            if var_6 is not None:\n                yield var_6\n",
      "lines_processed": 10,
      "total_lines": 295
    },
    {
      "file_path": "gallery.py",
      "code": "def get_first_archive_member(archive: ArchiveKind) -> str:\n    if isinstance(archive, tarfile.TarFile):\n        return archive.getnames()[0]\n    elif isinstance(archive, zipfile.ZipFile):\n        return archive.namelist()[0]\n",
      "variables": [
        "archive"
      ],
      "anonymized_code": "def get_first_archive_member(var_1: ArchiveKind) -> str:\n    if isinstance(var_1, tarfile.TarFile):\n        return var_1.getnames()[0]\n    elif isinstance(var_1, zipfile.ZipFile):\n        return var_1.namelist()[0]\n",
      "lines_processed": 5,
      "total_lines": 295
    },
    {
      "file_path": "gallery.py",
      "code": "def git_add_and_commit(msg: str, repo: Path) -> None:\n    subprocess.run([\"git\", \"add\", \".\"], cwd=repo)\n    subprocess.run([\"git\", \"commit\", \"-m\", msg, \"--allow-empty\"], cwd=repo)\n",
      "variables": [
        "msg",
        "repo"
      ],
      "anonymized_code": "def git_add_and_commit(var_1: str, var_2: Path) -> None:\n    subprocess.run([\"git\", \"add\", \".\"], cwd=var_2)\n    subprocess.run([\"git\", \"commit\", \"-m\", var_1, \"--allow-empty\"], cwd=var_2)\n",
      "lines_processed": 3,
      "total_lines": 295
    },
    {
      "file_path": "gallery.py",
      "code": "def git_switch_branch(\n    branch: str, repo: Path, new: bool = False, from_branch: Optional[str] = None\n) -> None:\n    args = [\"git\", \"checkout\"]\n    if new:\n        args.append(\"-b\")\n    args.append(branch)\n    if from_branch:\n        args.append(from_branch)\n    subprocess.run(args, cwd=repo)\n",
      "variables": [
        "branch",
        "repo",
        "new",
        "from_branch",
        "args"
      ],
      "anonymized_code": "def git_switch_branch(\n    var_1: str, var_2: Path, var_3: bool = False, var_4: Optional[str] = None\n) -> None:\n    var_5 = [\"git\", \"checkout\"]\n    if var_3:\n        var_5.append(\"-b\")\n    var_5.append(var_1)\n    if var_4:\n        var_5.append(var_4)\n    subprocess.run(var_5, cwd=var_2)\n",
      "lines_processed": 10,
      "total_lines": 295
    },
    {
      "file_path": "gallery.py",
      "code": "def format_repo_with_version(\n    repo: Path,\n    from_branch: Optional[str],\n    black_repo: Path,\n    black_version: BlackVersion,\n    input_directory: Path,\n) -> str:\n    current_branch = f\"black-{black_version.version}\"\n    git_switch_branch(black_version.version, repo=black_repo)\n    git_switch_branch(current_branch, repo=repo, new=True, from_branch=from_branch)\n",
      "variables": [
        "repo",
        "from_branch",
        "black_repo",
        "black_version",
        "input_directory",
        "current_branch"
      ],
      "anonymized_code": "def format_repo_with_version(\n    var_1: Path,\n    var_2: Optional[str],\n    var_3: Path,\n    var_4: BlackVersion,\n    var_5: Path,\n) -> str:\n    var_6 = f\"black-{var_4.version}\"\n    git_switch_branch(var_4.version, var_1=var_3)\n    git_switch_branch(var_6, var_1=var_1, new=True, var_2=var_2)\n",
      "lines_processed": 10,
      "total_lines": 295
    },
    {
      "file_path": "gallery.py",
      "code": "def git_create_repository(repo: Path) -> None:\n    subprocess.run([\"git\", \"init\"], cwd=repo)\n    git_add_and_commit(msg=\"Initial commit\", repo=repo)\n",
      "variables": [
        "repo"
      ],
      "anonymized_code": "def git_create_repository(var_1: Path) -> None:\n    subprocess.run([\"git\", \"init\"], cwd=var_1)\n    git_add_and_commit(msg=\"Initial commit\", var_1=var_1)\n",
      "lines_processed": 3,
      "total_lines": 295
    },
    {
      "file_path": "gallery.py",
      "code": "def get_package(\n    package: str, version: Optional[str], directory: Path\n) -> Optional[Path]:\n    try:\n        return download_and_extract(package, version, directory)\n    except Exception:\n        print(f\"Caught an exception while downloading {package}.\")\n        traceback.print_exc()\n        return None\n",
      "variables": [
        "package",
        "version",
        "directory"
      ],
      "anonymized_code": "def get_package(\n    var_1: str, var_2: Optional[str], var_3: Path\n) -> Optional[Path]:\n    try:\n        return download_and_extract(var_1, var_2, var_3)\n    except Exception:\n        print(f\"Caught an exception while downloading {var_1}.\")\n        traceback.print_exc()\n        return None\n",
      "lines_processed": 9,
      "total_lines": 295
    },
    {
      "file_path": "release.py",
      "code": "def _handle_debug(debug: bool) -> None:\n    \"\"\"Turn on debugging if asked otherwise INFO default\"\"\"\n    log_level = logging.DEBUG if debug else logging.INFO\n    logging.basicConfig(\n        format=\"[%(asctime)s] %(levelname)s: %(message)s (%(filename)s:%(lineno)d)\",\n        level=log_level,\n    )\n",
      "variables": [
        "debug",
        "log_level"
      ],
      "anonymized_code": "def _handle_debug(var_1: bool) -> None:\n    \"\"\"Turn on debugging if asked otherwise INFO default\"\"\"\n    var_2 = logging.DEBUG if var_1 else logging.INFO\n    logging.basicConfig(\n        format=\"[%(asctime)s] %(levelname)s: %(message)s (%(filename)s:%(lineno)d)\",\n        level=var_2,\n    )\n",
      "lines_processed": 7,
      "total_lines": 244
    },
    {
      "file_path": "release.py",
      "code": "def tuple_calver(calver: str) -> tuple[int, ...]:  # mypy can't notice maxsplit below\n    \"\"\"Convert a calver string into a tuple of ints for sorting\"\"\"\n    try:\n        return tuple(map(int, calver.split(\".\", maxsplit=2)))\n    except ValueError:\n        return (0, 0, 0)\n",
      "variables": [
        "calver"
      ],
      "anonymized_code": "def tuple_calver(var_1: str) -> tuple[int, ...]:  # mypy can't notice maxsplit below\n    \"\"\"Convert a var_1 string into a tuple of ints for sorting\"\"\"\n    try:\n        return tuple(map(int, var_1.split(\".\", maxsplit=2)))\n    except ValueError:\n        return (0, 0, 0)\n",
      "lines_processed": 6,
      "total_lines": 244
    },
    {
      "file_path": "release.py",
      "code": "def get_git_tags(versions_only: bool = True) -> list[str]:\n    \"\"\"Pull out all tags or calvers only\"\"\"\n    cp = run([\"git\", \"tag\"], capture_output=True, check=True, encoding=\"utf8\")\n    if not cp.stdout:\n        LOG.error(f\"Returned no git tags stdout: {cp.stderr}\")\n        raise NoGitTagsError\n    git_tags = cp.stdout.splitlines()\n    if versions_only:\n        return [t for t in git_tags if t[0].isdigit()]\n    return git_tags\n",
      "variables": [
        "versions_only",
        "cp",
        "git_tags",
        "t"
      ],
      "anonymized_code": "def get_git_tags(var_1: bool = True) -> list[str]:\n    \"\"\"Pull out all tags or calvers only\"\"\"\n    var_2 = run([\"git\", \"tag\"], capture_output=True, check=True, encoding=\"utf8\")\n    if not var_2.stdout:\n        LOG.error(f\"Returned no git tags stdout: {var_2.stderr}\")\n        raise NoGitTagsError\n    var_3 = var_2.stdout.splitlines()\n    if var_1:\n        return [var_4 for var_4 in var_3 if var_4[0].isdigit()]\n    return var_3\n",
      "lines_processed": 10,
      "total_lines": 244
    },
    {
      "file_path": "release.py",
      "code": "def main() -> int:\n    args = parse_args()\n\n    # Need parent.parent cause script is in scripts/ directory\n    sf = SourceFiles(Path(__file__).parent.parent)\n\n    if args.add_changes_template:\n        return sf.add_template_to_changes()\n\n    LOG.info(f\"Current version detected to be {sf.current_version}\")\n",
      "variables": [
        "args",
        "sf"
      ],
      "anonymized_code": "def main() -> int:\n    var_1 = parse_args()\n\n    # Need parent.parent cause script is in scripts/ directory\n    var_2 = SourceFiles(Path(__file__).parent.parent)\n\n    if var_1.add_changes_template:\n        return var_2.add_template_to_changes()\n\n    LOG.info(f\"Current version detected to be {var_2.current_version}\")\n",
      "lines_processed": 10,
      "total_lines": 244
    }
  ],
  "python-poetry_poetry": [
    {
      "file_path": "config.py",
      "code": "def int_normalizer(val: str) -> int:\n    return int(val)\n",
      "variables": [
        "val"
      ],
      "anonymized_code": "def int_normalizer(var_1: str) -> int:\n    return int(var_1)\n",
      "lines_processed": 2,
      "total_lines": 432
    },
    {
      "file_path": "config.py",
      "code": "def boolean_validator(val: str) -> bool:\n    return val in {\"true\", \"false\", \"1\", \"0\"}\n",
      "variables": [
        "val"
      ],
      "anonymized_code": "def boolean_validator(var_1: str) -> bool:\n    return var_1 in {\"true\", \"false\", \"1\", \"0\"}\n",
      "lines_processed": 2,
      "total_lines": 432
    },
    {
      "file_path": "application.py",
      "code": "def main() -> int:\n    exit_code: int = Application().run()\n    return exit_code\n",
      "variables": [
        "exit_code"
      ],
      "anonymized_code": "def main() -> int:\n    var_1: int = Application().run()\n    return var_1\n",
      "lines_processed": 3,
      "total_lines": 646
    }
  ],
  "wandb_wandb": [
    {
      "file_path": "use-model-error.py",
      "code": "def main():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        with wandb.init() as run:\n            wandb.log({\"metric\": 5})\n            artifact = wandb.Artifact(\"test-use-model-error\", \"test-type\")\n            with open(tmpdir + \"/boom.txt\", \"w\") as f:\n                f.write(\"testing\")\n\n            local_path = f\"{tmpdir}/boom.txt\"\n            artifact.add_file(local_path, \"boom/test-name\")\n",
      "variables": [
        "tmpdir",
        "run",
        "artifact",
        "f",
        "local_path"
      ],
      "anonymized_code": "def main():\n    with tempfile.TemporaryDirectory() as var_1:\n        with wandb.init() as var_2:\n            wandb.log({\"metric\": 5})\n            var_3 = wandb.Artifact(\"test-use-model-error\", \"test-type\")\n            with open(var_1 + \"/boom.txt\", \"w\") as var_4:\n                var_4.write(\"testing\")\n\n            var_5 = var_4\"{var_1}/boom.txt\"\n            var_3.add_file(var_5, \"boom/test-name\")\n",
      "lines_processed": 10,
      "total_lines": 24
    },
    {
      "file_path": "log-model.py",
      "code": "def main():\n    my_model = Net()\n\n    _ = log_model(my_model, \"my-model\", aliases=[\"boom\"], scope_project=True)\n\n    wandb.finish()\n",
      "variables": [
        "my_model",
        "_"
      ],
      "anonymized_code": "def main():\n    var_1 = Net()\n\n    var_2 = log_model(var_1, \"my-model\", aliases=[\"boom\"], scope_project=True)\n\n    wandb.finish()\n",
      "lines_processed": 6,
      "total_lines": 44
    },
    {
      "file_path": "log-artifact.py",
      "code": "def main():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        with wandb.init() as run:\n            wandb.log({\"metric\": 5})\n            artifact = wandb.Artifact(\"test-log-artifact\", \"test-type\")\n            with open(tmpdir + \"/boom.txt\", \"w\") as f:\n                f.write(\"testing\")\n\n            local_path = f\"{tmpdir}/boom.txt\"\n            artifact.add_file(local_path, \"test-name\")\n",
      "variables": [
        "tmpdir",
        "run",
        "artifact",
        "f",
        "local_path"
      ],
      "anonymized_code": "def main():\n    with tempfile.TemporaryDirectory() as var_1:\n        with wandb.init() as var_2:\n            wandb.log({\"metric\": 5})\n            var_3 = wandb.Artifact(\"test-log-var_3\", \"test-type\")\n            with open(var_1 + \"/boom.txt\", \"w\") as var_4:\n                var_4.write(\"testing\")\n\n            var_5 = var_4\"{var_1}/boom.txt\"\n            var_3.add_file(var_5, \"test-name\")\n",
      "lines_processed": 10,
      "total_lines": 20
    },
    {
      "file_path": "hatch_build.py",
      "code": "def _get_env_bool(name: str, default: bool) -> bool:\n    \"\"\"Returns the value of a boolean environment variable.\"\"\"\n    value = os.getenv(name)\n\n    if value is None:\n        return default\n    elif value.lower() in (\"1\", \"true\"):\n        return True\n    elif value.lower() in (\"0\", \"false\"):\n        return False\n",
      "variables": [
        "name",
        "default",
        "value"
      ],
      "anonymized_code": "def _get_env_bool(var_1: str, var_2: bool) -> bool:\n    \"\"\"Returns the var_3 of a boolean environment variable.\"\"\"\n    var_3 = os.getenv(var_1)\n\n    if var_3 is None:\n        return var_2\n    elif var_3.lower() in (\"1\", \"true\"):\n        return True\n    elif var_3.lower() in (\"0\", \"false\"):\n        return False\n",
      "lines_processed": 10,
      "total_lines": 245
    },
    {
      "file_path": "hatch_build.py",
      "code": "def _to_goarch(arch: str) -> str:\n    \"\"\"Returns a valid GOARCH value or the empty string.\"\"\"\n    return {\n        # amd64 synonyms\n        \"amd64\": \"amd64\",\n        \"x86_64\": \"amd64\",\n        # arm64 synonyms\n        \"arm64\": \"arm64\",\n        \"aarch64\": \"arm64\",\n    }.get(arch, \"\")\n",
      "variables": [
        "arch"
      ],
      "anonymized_code": "def _to_goarch(var_1: str) -> str:\n    \"\"\"Returns a valid GOARCH value or the empty string.\"\"\"\n    return {\n        # amd64 synonyms\n        \"amd64\": \"amd64\",\n        \"x86_64\": \"amd64\",\n        # arm64 synonyms\n        \"arm64\": \"arm64\",\n        \"aarch64\": \"arm64\",\n    }.get(var_1, \"\")\n",
      "lines_processed": 10,
      "total_lines": 245
    },
    {
      "file_path": "use-model.py",
      "code": "def main():\n    run = wandb.init()\n\n    my_model = Net()\n\n    sm = _SavedModel.init(my_model)\n    art = wandb.Artifact(\"my-model\", \"model\")\n    art.add(sm, \"index\")\n\n    art = run.log_artifact(art)\n",
      "variables": [
        "run",
        "my_model",
        "sm",
        "art"
      ],
      "anonymized_code": "def main():\n    var_1 = wandb.init()\n\n    var_2 = Net()\n\n    var_3 = _SavedModel.init(var_2)\n    var_4 = wandb.Artifact(\"my-model\", \"model\")\n    var_4.add(var_3, \"index\")\n\n    var_4 = var_1.log_artifact(var_4)\n",
      "lines_processed": 10,
      "total_lines": 58
    },
    {
      "file_path": "link-artifact.py",
      "code": "def main():\n    with wandb.init() as run:\n        wandb.log({\"metric\": 5})\n        try:\n            artifact = run.use_artifact(\"test-link-artifact:latest\", \"model\")\n        except CommError:\n            artifact = wandb.Artifact(\"test-link-artifact\", \"model\")\n            with tempfile.TemporaryDirectory() as tmpdir:\n                with open(tmpdir + \"/boom.txt\", \"w\") as f:\n                    f.write(\"testing\")\n",
      "variables": [
        "run",
        "artifact",
        "tmpdir",
        "f"
      ],
      "anonymized_code": "def main():\n    with wandb.init() as var_1:\n        wandb.log({\"metric\": 5})\n        try:\n            var_2 = var_1.use_artifact(\"test-link-var_2:latest\", \"model\")\n        except CommError:\n            var_2 = wandb.Artifact(\"test-link-var_2\", \"model\")\n            with tempfile.TemporaryDirectory() as var_3:\n                with open(var_3 + \"/boom.txt\", \"w\") as var_4:\n                    var_4.write(\"testing\")\n",
      "lines_processed": 10,
      "total_lines": 30
    },
    {
      "file_path": "public_collections.py",
      "code": "def main():\n    run = wandb.init(entity=\"mock_server_entity\", project=\"test\")\n    art = wandb.Artifact(\"test_artifact\", type=\"model\")\n    art.add_file(\"public_collection.py\")\n    run.link_artifact(art, \"mock_server_entity/test/test_port\")\n    run.finish()\n\n    collections = wandb.Api().artifact_type(\"model\", \"test\").collections()\n    assert len(collections) == 2\n",
      "variables": [
        "run",
        "art",
        "collections"
      ],
      "anonymized_code": "def main():\n    var_1 = wandb.init(entity=\"mock_server_entity\", project=\"test\")\n    var_2 = wandb.Artifact(\"test_artifact\", type=\"model\")\n    var_2.add_file(\"public_collection.py\")\n    var_1.link_artifact(var_2, \"mock_server_entity/test/test_port\")\n    var_1.finish()\n\n    var_3 = wandb.Api().artifact_type(\"model\", \"test\").var_3()\n    assert len(var_3) == 2\n",
      "lines_processed": 9,
      "total_lines": 16
    },
    {
      "file_path": "use-model-outside-run-error.py",
      "code": "def main():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        with wandb.init() as run:\n            wandb.log({\"metric\": 5})\n            artifact = wandb.Artifact(\"test-use-model-artifact\", \"test-type\")\n            with open(tmpdir + \"/boom.txt\", \"w\") as f:\n                f.write(\"testing\")\n\n            local_path = f\"{tmpdir}/boom.txt\"\n            artifact.add_file(local_path, \"index/test-name\")\n",
      "variables": [
        "tmpdir",
        "run",
        "artifact",
        "f",
        "local_path"
      ],
      "anonymized_code": "def main():\n    with tempfile.TemporaryDirectory() as var_1:\n        with wandb.init() as var_2:\n            wandb.log({\"metric\": 5})\n            var_3 = wandb.Artifact(\"test-use-model-var_3\", \"test-type\")\n            with open(var_1 + \"/boom.txt\", \"w\") as var_4:\n                var_4.write(\"testing\")\n\n            var_5 = var_4\"{var_1}/boom.txt\"\n            var_3.add_file(var_5, \"index/test-name\")\n",
      "lines_processed": 10,
      "total_lines": 24
    },
    {
      "file_path": "link-model.py",
      "code": "def main():\n    my_model = Net()\n\n    wandb.init()\n\n    best_model = log_model(my_model, \"my-model\", aliases=[\"boom\"], scope_project=True)\n\n    link_model(best_model, \"project/test_portfolio\")\n\n    wandb.finish()\n",
      "variables": [
        "my_model",
        "best_model"
      ],
      "anonymized_code": "def main():\n    var_1 = Net()\n\n    wandb.init()\n\n    var_2 = log_model(var_1, \"my-model\", aliases=[\"boom\"], scope_project=True)\n\n    link_model(var_2, \"project/test_portfolio\")\n\n    wandb.finish()\n",
      "lines_processed": 10,
      "total_lines": 48
    },
    {
      "file_path": "log-image-artifact-path.py",
      "code": "def main():\n    # Base Case\n    with wandb.init() as run:\n        run.log({\"image\": make_image()})\n\n    # With Logged Target\n    with wandb.init() as run:\n        art = wandb.Artifact(\"examples\", \"images\")\n        image = make_image()\n        art.add(image, \"image\")\n",
      "variables": [
        "run",
        "art",
        "image"
      ],
      "anonymized_code": "def main():\n    # Base Case\n    with wandb.init() as var_1:\n        var_1.log({\"var_3\": make_image()})\n\n    # With Logged Target\n    with wandb.init() as var_1:\n        var_2 = wandb.Artifact(\"examples\", \"images\")\n        var_3 = make_image()\n        var_2.add(var_3, \"var_3\")\n",
      "lines_processed": 10,
      "total_lines": 25
    },
    {
      "file_path": "link-model-outside-run.py",
      "code": "def main():\n    my_model = Net()\n    sm = _SavedModel.init(my_model)\n    art = wandb.Artifact(\"my-model\", \"model\")\n    art.add(sm, \"index\")\n\n    link_model(sm, \"project/test_portfolio\")\n",
      "variables": [
        "my_model",
        "sm",
        "art"
      ],
      "anonymized_code": "def main():\n    var_1 = Net()\n    var_2 = _SavedModel.init(var_1)\n    var_3 = wandb.Artifact(\"my-model\", \"model\")\n    var_3.add(var_2, \"index\")\n\n    link_model(var_2, \"project/test_portfolio\")\n",
      "lines_processed": 7,
      "total_lines": 46
    },
    {
      "file_path": "log-unicode-artifact.py",
      "code": "def main():\n    with wandb.init() as run:\n        artifact = wandb.Artifact(\"my_artifact\", type=\"unicode_artifact\")\n        with artifact.new_file(\"euler.txt\", mode=\"w\", encoding=\"utf-8\") as f:\n            f.write(\"e^(i\u03c0)+1=0\")\n\n        run.log_artifact(artifact)\n",
      "variables": [
        "run",
        "artifact",
        "f"
      ],
      "anonymized_code": "def main():\n    with wandb.init() as var_1:\n        var_2 = wandb.Artifact(\"my_artifact\", type=\"unicode_artifact\")\n        with var_2.new_file(\"euler.txt\", mode=\"w\", encoding=\"utf-8\") as var_3:\n            var_3.write(\"e^(i\u03c0)+1=0\")\n\n        var_1.log_artifact(var_2)\n",
      "lines_processed": 7,
      "total_lines": 14
    },
    {
      "file_path": "use-and-link-model.py",
      "code": "def main():\n    run = wandb.init()\n\n    my_model = Net()\n\n    sm = _SavedModel.init(my_model)\n    art = wandb.Artifact(\"my-model\", \"model\")\n    art.add(sm, \"index\")\n\n    art = run.log_artifact(art)\n",
      "variables": [
        "run",
        "my_model",
        "sm",
        "art"
      ],
      "anonymized_code": "def main():\n    var_1 = wandb.init()\n\n    var_2 = Net()\n\n    var_3 = _SavedModel.init(var_2)\n    var_4 = wandb.Artifact(\"my-model\", \"model\")\n    var_4.add(var_3, \"index\")\n\n    var_4 = var_1.log_artifact(var_4)\n",
      "lines_processed": 10,
      "total_lines": 59
    }
  ],
  "celery_celery": [
    {
      "file_path": "backends.py",
      "code": "def by_url(backend=None, loader=None):\n    \"\"\"Get backend class by URL.\"\"\"\n    url = None\n    if backend and '://' in backend:\n        url = backend\n        scheme, _, _ = url.partition('://')\n        if '+' in scheme:\n            backend, url = url.split('+', 1)\n        else:\n            backend = scheme\n",
      "variables": [
        "backend",
        "loader",
        "url",
        "scheme",
        "_"
      ],
      "anonymized_code": "def by_url(var_1=None, var_2=None):\n    \"\"\"Get var_1 class by URL.\"\"\"\n    var_3 = None\n    if var_1 and '://' in var_1:\n        var_3 = var_1\n        var_4, var_5, var_5 = var_3.partition('://')\n        if '+' in var_4:\n            var_1, var_3 = var_3.split('+', 1)\n        else:\n            var_1 = var_4\n",
      "lines_processed": 10,
      "total_lines": 69
    },
    {
      "file_path": "trace.py",
      "code": "def trace_task(task, uuid, args, kwargs, request=None, **opts):\n    \"\"\"Trace task execution.\"\"\"\n    request = {} if not request else request\n    try:\n        if task.__trace__ is None:\n            task.__trace__ = build_tracer(task.name, task, **opts)\n        return task.__trace__(uuid, args, kwargs, request)\n    except Exception as exc:\n        _signal_internal_error(task, uuid, args, kwargs, request, exc)\n        return trace_ok_t(report_internal_error(task, exc), TraceInfo(FAILURE, exc), 0.0, None)\n",
      "variables": [
        "task",
        "uuid",
        "args",
        "kwargs",
        "request",
        "opts"
      ],
      "anonymized_code": "def trace_task(var_1, var_2, var_3, var_4, var_5=None, **var_6):\n    \"\"\"Trace var_1 execution.\"\"\"\n    var_5 = {} if not var_5 else var_5\n    try:\n        if var_1.__trace__ is None:\n            var_1.__trace__ = build_tracer(var_1.name, var_1, **var_6)\n        return var_1.__trace__(var_2, var_3, var_4, var_5)\n    except Exception as exc:\n        _signal_internal_error(var_1, var_2, var_3, var_4, var_5, exc)\n        return trace_ok_t(report_internal_error(var_1, exc), TraceInfo(FAILURE, exc), 0.0, None)\n",
      "lines_processed": 10,
      "total_lines": 739
    },
    {
      "file_path": "trace.py",
      "code": "def get_log_policy(task, einfo, exc):\n    if isinstance(exc, Reject):\n        return log_policy_reject\n    elif isinstance(exc, Ignore):\n        return log_policy_ignore\n    elif einfo.internal:\n        return log_policy_internal\n    else:\n        if task.throws and isinstance(exc, task.throws):\n            return log_policy_expected\n",
      "variables": [
        "task",
        "einfo",
        "exc"
      ],
      "anonymized_code": "def get_log_policy(var_1, var_2, var_3):\n    if isinstance(var_3, Reject):\n        return log_policy_reject\n    elif isinstance(var_3, Ignore):\n        return log_policy_ignore\n    elif var_2.internal:\n        return log_policy_internal\n    else:\n        if var_1.throws and isinstance(var_3, var_1.throws):\n            return log_policy_expected\n",
      "lines_processed": 10,
      "total_lines": 739
    },
    {
      "file_path": "trace.py",
      "code": "def setup_worker_optimizations(app, hostname=None):\n    \"\"\"Setup worker related optimizations.\"\"\"\n    hostname = hostname or gethostname()\n\n    # make sure custom Task.__call__ methods that calls super\n    # won't mess up the request/task stack.\n    _install_stack_protection()\n\n    # all new threads start without a current app, so if an app is not\n    # passed on to the thread it will fall back to the \"default app\",\n",
      "variables": [
        "app",
        "hostname"
      ],
      "anonymized_code": "def setup_worker_optimizations(var_1, var_2=None):\n    \"\"\"Setup worker related optimizations.\"\"\"\n    var_2 = var_2 or gethostname()\n\n    # make sure custom Task.__call__ methods that calls super\n    # won't mess up the request/task stack.\n    _install_stack_protection()\n\n    # all new threads start without a current var_1, so if an var_1 is not\n    # passed on to the thread it will fall back to the \"default var_1\",\n",
      "lines_processed": 10,
      "total_lines": 739
    },
    {
      "file_path": "trace.py",
      "code": "def fast_trace_task(task, uuid, request, body, content_type,\n                    content_encoding, loads=loads_message, _loc=None,\n                    hostname=None, **_):\n    _loc = _localized if not _loc else _loc\n    embed = None\n    tasks, accept, hostname = _loc\n    if content_type:\n        args, kwargs, embed = loads(\n            body, content_type, content_encoding, accept=accept,\n        )\n",
      "variables": [
        "task",
        "uuid",
        "request",
        "body",
        "content_type",
        "content_encoding",
        "loads",
        "_loc",
        "hostname",
        "_",
        "embed",
        "tasks",
        "accept",
        "args",
        "kwargs"
      ],
      "anonymized_code": "def fast_trace_task(var_1, var_2, var_3, var_4, var_5,\n                    var_6, var_7=loads_message, var_8=None,\n                    var_9=None, **var_10):\n    var_8 = _localized if not var_8 else var_8\n    var_11 = None\n    var_12, var_13, var_9 = var_8\n    if var_5:\n        var_14, var_15, var_11 = var_7(\n            var_4, var_5, var_6, var_13=var_13,\n        )\n",
      "lines_processed": 10,
      "total_lines": 739
    },
    {
      "file_path": "amqp.py",
      "code": "def utf8dict(d, encoding='utf-8'):\n    return {k.decode(encoding) if isinstance(k, bytes) else k: v\n            for k, v in d.items()}\n",
      "variables": [
        "d",
        "encoding",
        "k",
        "v"
      ],
      "anonymized_code": "def utf8dict(var_1, var_2='utf-8'):\n    return {var_3.decode(var_2) if isinstance(var_3, bytes) else var_3: var_4\n            for var_3, var_4 in var_1.items()}\n",
      "lines_processed": 3,
      "total_lines": 621
    },
    {
      "file_path": "annotations.py",
      "code": "def resolve_all(anno, task):\n    \"\"\"Resolve all pending annotations.\"\"\"\n    return (x for x in (_first_match(anno, task), _first_match_any(anno)) if x)\n",
      "variables": [
        "anno",
        "task",
        "x"
      ],
      "anonymized_code": "def resolve_all(var_1, var_2):\n    \"\"\"Resolve all pending annotations.\"\"\"\n    return (var_3 for var_3 in (_first_match(var_1, var_2), _first_match_any(var_1)) if var_3)\n",
      "lines_processed": 3,
      "total_lines": 52
    },
    {
      "file_path": "task.py",
      "code": "def _reprtask(task, fmt=None, flags=None):\n    flags = list(flags) if flags is not None else []\n    flags.append('v2 compatible') if task.__v2_compat__ else None\n    if not fmt:\n        fmt = R_BOUND_TASK if task._app else R_UNBOUND_TASK\n    return fmt.format(\n        task, flags=_strflags(flags),\n        app=appstr(task._app) if task._app else None,\n    )\n",
      "variables": [
        "task",
        "fmt",
        "flags"
      ],
      "anonymized_code": "def _reprtask(var_1, var_2=None, var_3=None):\n    var_3 = list(var_3) if var_3 is not None else []\n    var_3.append('v2 compatible') if var_1.__v2_compat__ else None\n    if not var_2:\n        var_2 = R_BOUND_TASK if var_1._app else R_UNBOUND_TASK\n    return var_2.format(\n        var_1, var_3=_strflags(var_3),\n        app=appstr(var_1._app) if var_1._app else None,\n    )\n",
      "lines_processed": 9,
      "total_lines": 1161
    },
    {
      "file_path": "registry.py",
      "code": "def _unpickle_task_v2(name, module=None):\n    if module:\n        import_module(module)\n    return get_current_app().tasks[name]\n",
      "variables": [
        "name",
        "module"
      ],
      "anonymized_code": "def _unpickle_task_v2(var_1, var_2=None):\n    if var_2:\n        import_module(var_2)\n    return get_current_app().tasks[var_1]\n",
      "lines_processed": 4,
      "total_lines": 68
    },
    {
      "file_path": "registry.py",
      "code": "def _unpickle_task(name):\n    return get_current_app().tasks[name]\n",
      "variables": [
        "name"
      ],
      "anonymized_code": "def _unpickle_task(var_1):\n    return get_current_app().tasks[var_1]\n",
      "lines_processed": 2,
      "total_lines": 68
    },
    {
      "file_path": "builtins.py",
      "code": "def add_chunk_task(app):\n    from celery.canvas import chunks as _chunks\n\n    @app.task(name='celery.chunks', shared=False, lazy=False)\n    def chunks(task, it, n):\n        return _chunks.apply_chunks(task, it, n)\n    return chunks\n",
      "variables": [
        "app",
        "task",
        "it",
        "n"
      ],
      "anonymized_code": "def add_chunk_task(var_1):\n    from celery.canvas import chunks as _chunks\n\n    @var_1.var_2(name='celery.chunks', shared=False, lazy=False)\n    def chunks(var_2, var_3, var_4):\n        return _chunks.apply_chunks(var_2, var_3, var_4)\n    return chunks\n",
      "lines_processed": 7,
      "total_lines": 187
    },
    {
      "file_path": "builtins.py",
      "code": "def add_unlock_chord_task(app):\n    \"\"\"Task used by result backends without native chord support.\n\n    Will joins chord by creating a task chain polling the header\n    for completion.\n    \"\"\"\n    from celery.canvas import maybe_signature\n    from celery.exceptions import ChordError\n    from celery.result import allow_join_result, result_from_tuple\n\n",
      "variables": [
        "app"
      ],
      "anonymized_code": "def add_unlock_chord_task(var_1):\n    \"\"\"Task used by result backends without native chord support.\n\n    Will joins chord by creating a task chain polling the header\n    for completion.\n    \"\"\"\n    from celery.canvas import maybe_signature\n    from celery.exceptions import ChordError\n    from celery.result import allow_join_result, result_from_tuple\n\n",
      "lines_processed": 10,
      "total_lines": 187
    },
    {
      "file_path": "builtins.py",
      "code": "def add_starmap_task(app):\n    from celery.canvas import signature\n\n    @app.task(name='celery.starmap', shared=False, lazy=False)\n    def xstarmap(task, it):\n        task = signature(task, app=app).type\n        return [task(*item) for item in it]\n    return xstarmap\n",
      "variables": [
        "app",
        "task",
        "it",
        "item"
      ],
      "anonymized_code": "def add_starmap_task(var_1):\n    from celery.canvas import signature\n\n    @var_1.var_2(name='celery.starmap', shared=False, lazy=False)\n    def xstarmap(var_2, var_3):\n        var_2 = signature(var_2, var_1=var_1).type\n        return [var_2(*var_4) for var_4 in var_3]\n    return xstarmap\n",
      "lines_processed": 8,
      "total_lines": 187
    },
    {
      "file_path": "builtins.py",
      "code": "def add_map_task(app):\n    from celery.canvas import signature\n\n    @app.task(name='celery.map', shared=False, lazy=False)\n    def xmap(task, it):\n        task = signature(task, app=app).type\n        return [task(item) for item in it]\n    return xmap\n",
      "variables": [
        "app",
        "task",
        "it",
        "item"
      ],
      "anonymized_code": "def add_map_task(var_1):\n    from celery.canvas import signature\n\n    @var_1.var_2(name='celery.map', shared=False, lazy=False)\n    def xmap(var_2, var_3):\n        var_2 = signature(var_2, var_1=var_1).type\n        return [var_2(var_4) for var_4 in var_3]\n    return xmap\n",
      "lines_processed": 8,
      "total_lines": 187
    },
    {
      "file_path": "builtins.py",
      "code": "def add_backend_cleanup_task(app):\n    \"\"\"Task used to clean up expired results.\n\n    If the configured backend requires periodic cleanup this task is also\n    automatically configured to run every day at 4am (requires\n    :program:`celery beat` to be running).\n    \"\"\"\n    @app.task(name='celery.backend_cleanup', shared=False, lazy=False)\n    def backend_cleanup():\n        app.backend.cleanup()\n",
      "variables": [
        "app"
      ],
      "anonymized_code": "def add_backend_cleanup_task(var_1):\n    \"\"\"Task used to clean up expired results.\n\n    If the configured backend requires periodic cleanup this task is also\n    automatically configured to run every day at 4am (requires\n    :program:`celery beat` to be running).\n    \"\"\"\n    @var_1.task(name='celery.backend_cleanup', shared=False, lazy=False)\n    def backend_cleanup():\n        var_1.backend.cleanup()\n",
      "lines_processed": 10,
      "total_lines": 187
    },
    {
      "file_path": "defaults.py",
      "code": "def flatten(d, root='', keyfilter=_flatten_keys):\n    \"\"\"Flatten settings.\"\"\"\n    stack = deque([(root, d)])\n    while stack:\n        ns, options = stack.popleft()\n        for key, opt in options.items():\n            if isinstance(opt, dict):\n                stack.append((ns + key + '_', opt))\n            else:\n                yield from keyfilter(ns, key, opt)\n",
      "variables": [
        "d",
        "root",
        "keyfilter",
        "stack",
        "ns",
        "options",
        "key",
        "opt"
      ],
      "anonymized_code": "def flatten(var_1, var_2='', var_3=_flatten_keys):\n    \"\"\"Flatten settings.\"\"\"\n    var_4 = deque([(var_2, var_1)])\n    while var_4:\n        var_5, var_6 = var_4.popleft()\n        for var_7, var_8 in var_6.items():\n            if isinstance(var_8, dict):\n                var_4.append((var_5 + var_7 + '_', var_8))\n            else:\n                yield from var_3(var_5, var_7, var_8)\n",
      "lines_processed": 10,
      "total_lines": 427
    },
    {
      "file_path": "defaults.py",
      "code": "def find_deprecated_settings(source):  # pragma: no cover\n    from celery.utils import deprecated\n    for name, opt in flatten(NAMESPACES):\n        if (opt.deprecate_by or opt.remove_by) and getattr(source, name, None):\n            deprecated.warn(description=f'The {name!r} setting',\n                            deprecation=opt.deprecate_by,\n                            removal=opt.remove_by,\n                            alternative=f'Use the {opt.alt} instead')\n    return source\n",
      "variables": [
        "source",
        "name",
        "opt"
      ],
      "anonymized_code": "def find_deprecated_settings(var_1):  # pragma: no cover\n    from celery.utils import deprecated\n    for var_2, var_3 in flatten(NAMESPACES):\n        if (var_3.deprecate_by or var_3.remove_by) and getattr(var_1, var_2, None):\n            deprecated.warn(description=f'The {var_2!r} setting',\n                            deprecation=var_3.deprecate_by,\n                            removal=var_3.remove_by,\n                            alternative=f'Use the {var_3.alt} instead')\n    return var_1\n",
      "lines_processed": 9,
      "total_lines": 427
    },
    {
      "file_path": "defaults.py",
      "code": "def Namespace(__old__=None, **options):\n    if __old__ is not None:\n        for key, opt in options.items():\n            if not opt.old:\n                opt.old = {o.format(key) for o in __old__}\n    return options\n",
      "variables": [
        "__old__",
        "options",
        "key",
        "opt",
        "o"
      ],
      "anonymized_code": "def Namespace(var_1=None, **var_2):\n    if var_1 is not None:\n        for var_3, var_4 in var_2.items():\n            if not var_4.old:\n                var_4.old = {var_5.format(var_3) for var_5 in var_1}\n    return var_2\n",
      "lines_processed": 6,
      "total_lines": 427
    },
    {
      "file_path": "defaults.py",
      "code": "def _to_compat(ns, key, opt):\n    if opt.old:\n        return [\n            (oldkey.format(key).upper(), ns + key, opt)\n            for oldkey in opt.old\n        ]\n    return [((ns + key).upper(), ns + key, opt)]\n",
      "variables": [
        "ns",
        "key",
        "opt",
        "oldkey"
      ],
      "anonymized_code": "def _to_compat(var_1, var_2, var_3):\n    if var_3.old:\n        return [\n            (var_4.format(var_2).upper(), var_1 + var_2, var_3)\n            for var_4 in var_3.old\n        ]\n    return [((var_1 + var_2).upper(), var_1 + var_2, var_3)]\n",
      "lines_processed": 7,
      "total_lines": 427
    },
    {
      "file_path": "defaults.py",
      "code": "def old_ns(ns):\n    return {f'{ns}_{{0}}'}\n",
      "variables": [
        "ns"
      ],
      "anonymized_code": "def old_ns(var_1):\n    return {f'{var_1}_{{0}}'}\n",
      "lines_processed": 2,
      "total_lines": 427
    },
    {
      "file_path": "defaults.py",
      "code": "def _flatten_keys(ns, key, opt):\n    return [(ns + key, opt)]\n",
      "variables": [
        "ns",
        "key",
        "opt"
      ],
      "anonymized_code": "def _flatten_keys(var_1, var_2, var_3):\n    return [(var_1 + var_2, var_3)]\n",
      "lines_processed": 2,
      "total_lines": 427
    },
    {
      "file_path": "utils.py",
      "code": "def _unpickle_app(cls, pickler, *args):\n    \"\"\"Rebuild app for versions 2.5+.\"\"\"\n    return pickler()(cls, *args)\n",
      "variables": [
        "cls",
        "pickler",
        "args"
      ],
      "anonymized_code": "def _unpickle_app(var_1, var_2, *var_3):\n    \"\"\"Rebuild app for versions 2.5+.\"\"\"\n    return var_2()(var_1, *var_3)\n",
      "lines_processed": 3,
      "total_lines": 415
    },
    {
      "file_path": "routes.py",
      "code": "def expand_router_string(router):\n    router = symbol_by_name(router)\n    if hasattr(router, 'route_for_task'):\n        # need to instantiate pre 4.0 router classes\n        router = router()\n    return router\n",
      "variables": [
        "router"
      ],
      "anonymized_code": "def expand_router_string(var_1):\n    var_1 = symbol_by_name(var_1)\n    if hasattr(var_1, 'route_for_task'):\n        # need to instantiate pre 4.0 var_1 classes\n        var_1 = var_1()\n    return var_1\n",
      "lines_processed": 6,
      "total_lines": 136
    },
    {
      "file_path": "routes.py",
      "code": "def prepare(routes):\n    \"\"\"Expand the :setting:`task_routes` setting.\"\"\"\n\n    def expand_route(route):\n        if isinstance(route, (Mapping, list, tuple)):\n            return MapRoute(route)\n        if isinstance(route, str):\n            return mlazy(expand_router_string, route)\n        return route\n\n",
      "variables": [
        "routes",
        "route"
      ],
      "anonymized_code": "def prepare(var_1):\n    \"\"\"Expand the :setting:`task_routes` setting.\"\"\"\n\n    def expand_route(var_2):\n        if isinstance(var_2, (Mapping, list, tuple)):\n            return MapRoute(var_2)\n        if isinstance(var_2, str):\n            return mlazy(expand_router_string, var_2)\n        return var_2\n\n",
      "lines_processed": 10,
      "total_lines": 136
    },
    {
      "file_path": "_state.py",
      "code": "def _app_or_default(app=None):\n    if app is None:\n        return get_current_app()\n    return app\n",
      "variables": [
        "app"
      ],
      "anonymized_code": "def _app_or_default(var_1=None):\n    if var_1 is None:\n        return get_current_app()\n    return var_1\n",
      "lines_processed": 4,
      "total_lines": 197
    },
    {
      "file_path": "_state.py",
      "code": "def _app_or_default_trace(app=None):  # pragma: no cover\n    from traceback import print_stack\n    try:\n        from billiard.process import current_process\n    except ImportError:\n        current_process = None\n    if app is None:\n        if getattr(_tls, 'current_app', None):\n            print('-- RETURNING TO CURRENT APP --')  # +\n            print_stack()\n",
      "variables": [
        "app",
        "current_process"
      ],
      "anonymized_code": "def _app_or_default_trace(var_1=None):  # pragma: no cover\n    from traceback import print_stack\n    try:\n        from billiard.process import var_2\n    except ImportError:\n        var_2 = None\n    if var_1 is None:\n        if getattr(_tls, 'current_app', None):\n            print('-- RETURNING TO CURRENT APP --')  # +\n            print_stack()\n",
      "lines_processed": 10,
      "total_lines": 197
    },
    {
      "file_path": "_state.py",
      "code": "def enable_trace():\n    \"\"\"Enable tracing of app instances.\"\"\"\n    global app_or_default\n    app_or_default = _app_or_default_trace\n",
      "variables": [
        "app_or_default"
      ],
      "anonymized_code": "def enable_trace():\n    \"\"\"Enable tracing of app instances.\"\"\"\n    global var_1\n    var_1 = _app_or_default_trace\n",
      "lines_processed": 4,
      "total_lines": 197
    },
    {
      "file_path": "_state.py",
      "code": "def _register_app(app):\n    _apps.add(app)\n",
      "variables": [
        "app"
      ],
      "anonymized_code": "def _register_app(var_1):\n    _apps.add(var_1)\n",
      "lines_processed": 2,
      "total_lines": 197
    },
    {
      "file_path": "_state.py",
      "code": "def _deregister_app(app):\n    _apps.discard(app)\n",
      "variables": [
        "app"
      ],
      "anonymized_code": "def _deregister_app(var_1):\n    _apps.discard(var_1)\n",
      "lines_processed": 2,
      "total_lines": 197
    },
    {
      "file_path": "_state.py",
      "code": "def get_current_worker_task():\n    \"\"\"Currently executing task, that was applied by the worker.\n\n    This is used to differentiate between the actual task\n    executed by the worker and any task that was called within\n    a task (using ``task.__call__`` or ``task.apply``)\n    \"\"\"\n    for task in reversed(_task_stack.stack):\n        if not task.request.called_directly:\n            return task\n",
      "variables": [
        "task"
      ],
      "anonymized_code": "def get_current_worker_task():\n    \"\"\"Currently executing var_1, that was applied by the worker.\n\n    This is used to differentiate between the actual var_1\n    executed by the worker and any var_1 that was called within\n    a var_1 (using ``var_1.__call__`` or ``var_1.apply``)\n    \"\"\"\n    for var_1 in reversed(_task_stack.stack):\n        if not var_1.request.called_directly:\n            return var_1\n",
      "lines_processed": 10,
      "total_lines": 197
    },
    {
      "file_path": "_state.py",
      "code": "def _announce_app_finalized(app):\n    callbacks = set(_on_app_finalizers)\n    for callback in callbacks:\n        callback(app)\n",
      "variables": [
        "app",
        "callbacks",
        "callback"
      ],
      "anonymized_code": "def _announce_app_finalized(var_1):\n    var_2 = set(_on_app_finalizers)\n    for var_3 in var_2:\n        var_3(var_1)\n",
      "lines_processed": 4,
      "total_lines": 197
    },
    {
      "file_path": "_state.py",
      "code": "def disable_trace():\n    \"\"\"Disable tracing of app instances.\"\"\"\n    global app_or_default\n    app_or_default = _app_or_default\n",
      "variables": [
        "app_or_default"
      ],
      "anonymized_code": "def disable_trace():\n    \"\"\"Disable tracing of app instances.\"\"\"\n    global var_1\n    var_1 = _app_or_default\n",
      "lines_processed": 4,
      "total_lines": 197
    },
    {
      "file_path": "autoretry.py",
      "code": "def add_autoretry_behaviour(task, **options):\n    \"\"\"Wrap task's `run` method with auto-retry functionality.\"\"\"\n    autoretry_for = tuple(\n        options.get('autoretry_for',\n                    getattr(task, 'autoretry_for', ()))\n    )\n    dont_autoretry_for = tuple(\n        options.get('dont_autoretry_for',\n                    getattr(task, 'dont_autoretry_for', ()))\n    )\n",
      "variables": [
        "task",
        "options",
        "autoretry_for",
        "dont_autoretry_for"
      ],
      "anonymized_code": "def add_autoretry_behaviour(var_1, **var_2):\n    \"\"\"Wrap var_1's `run` method with auto-retry functionality.\"\"\"\n    var_3 = tuple(\n        var_2.get('var_3',\n                    getattr(var_1, 'var_3', ()))\n    )\n    var_4 = tuple(\n        var_2.get('var_4',\n                    getattr(var_1, 'var_4', ()))\n    )\n",
      "lines_processed": 10,
      "total_lines": 66
    }
  ],
  "pallets_jinja": [
    {
      "file_path": "generate_identifier_pattern.py",
      "code": "def collapse_ranges(data):\n    \"\"\"Given a sorted list of unique characters, generate ranges representing\n    sequential code points.\n\n    Source: https://stackoverflow.com/a/4629241/400617\n    \"\"\"\n    for _, g in itertools.groupby(enumerate(data), lambda x: ord(x[1]) - x[0]):\n        lb = list(g)\n        yield lb[0][1], lb[-1][1]\n",
      "variables": [
        "data",
        "_",
        "g",
        "x",
        "lb"
      ],
      "anonymized_code": "def collapse_ranges(var_1):\n    \"\"\"Given a sorted list of unique characters, generate ranges representing\n    sequential code points.\n\n    Source: https://stackoverflow.com/a/4629241/400617\n    \"\"\"\n    for var_2, var_3 in itertools.groupby(enumerate(var_1), lambda var_4: ord(var_4[1]) - var_4[0]):\n        var_5 = list(var_3)\n        yield var_5[0][1], var_5[-1][1]\n",
      "lines_processed": 9,
      "total_lines": 73
    },
    {
      "file_path": "generate_identifier_pattern.py",
      "code": "def main():\n    \"\"\"Build the regex pattern and write it to ``jinja2/_identifier.py``.\"\"\"\n    pattern = build_pattern(collapse_ranges(get_characters()))\n    filename = os.path.abspath(\n        os.path.join(os.path.dirname(__file__), \"..\", \"src\", \"jinja2\", \"_identifier.py\")\n    )\n\n    with open(filename, \"w\", encoding=\"utf8\") as f:\n        f.write(\"# generated by scripts/generate_identifier_pattern.py\")\n        f.write(f\"# Python {sys.version_info[0]}.{sys.version_info[1]}\\n\")\n",
      "variables": [
        "pattern",
        "filename",
        "f"
      ],
      "anonymized_code": "def main():\n    \"\"\"Build the regex var_1 and write it to ``jinja2/_identifier.py``.\"\"\"\n    var_1 = build_pattern(collapse_ranges(get_characters()))\n    var_2 = os.path.abspath(\n        os.path.join(os.path.dirname(__file__), \"..\", \"src\", \"jinja2\", \"_identifier.py\")\n    )\n\n    with open(var_2, \"w\", encoding=\"utf8\") as var_3:\n        var_3.write(\"# generated by scripts/generate_identifier_pattern.py\")\n        var_3.write(var_3\"# Python {sys.version_info[0]}.{sys.version_info[1]}\\n\")\n",
      "lines_processed": 10,
      "total_lines": 73
    },
    {
      "file_path": "generate_identifier_pattern.py",
      "code": "def build_pattern(ranges):\n    \"\"\"Output the regex pattern for ranges of characters.\n\n    One and two character ranges output the individual characters.\n    \"\"\"\n    out = []\n\n    for a, b in ranges:\n        if a == b:  # single char\n            out.append(a)\n",
      "variables": [
        "ranges",
        "out",
        "a",
        "b"
      ],
      "anonymized_code": "def build_pattern(var_1):\n    \"\"\"Output the regex pattern for var_1 of characters.\n\n    One and two character var_1 output the individual characters.\n    \"\"\"\n    var_2 = []\n\n    for var_3, var_4 in var_1:\n        if var_3 == var_4:  # single char\n            var_2.append(var_3)\n",
      "lines_processed": 10,
      "total_lines": 73
    },
    {
      "file_path": "async_utils.py",
      "code": "def async_variant(normal_func):  # type: ignore\n    def decorator(async_func):  # type: ignore\n        pass_arg = _PassArg.from_obj(normal_func)\n        need_eval_context = pass_arg is None\n\n        if pass_arg is _PassArg.environment:\n\n            def is_async(args: t.Any) -> bool:\n                return t.cast(bool, args[0].is_async)\n\n",
      "variables": [
        "normal_func",
        "async_func",
        "pass_arg",
        "need_eval_context",
        "args"
      ],
      "anonymized_code": "def async_variant(var_1):  # type: ignore\n    def decorator(var_2):  # type: ignore\n        var_3 = _PassArg.from_obj(var_1)\n        var_4 = var_3 is None\n\n        if var_3 is _PassArg.environment:\n\n            def is_async(var_5: t.Any) -> bool:\n                return t.cast(bool, var_5[0].is_async)\n\n",
      "lines_processed": 10,
      "total_lines": 99
    },
    {
      "file_path": "async_utils.py",
      "code": "def auto_aiter(\n    iterable: \"t.AsyncIterable[V] | t.Iterable[V]\",\n) -> \"t.AsyncIterator[V]\":\n    if hasattr(iterable, \"__aiter__\"):\n        return iterable.__aiter__()\n    else:\n        return _IteratorToAsyncIterator(iter(iterable))\n",
      "variables": [
        "iterable"
      ],
      "anonymized_code": "def auto_aiter(\n    var_1: \"t.AsyncIterable[V] | t.Iterable[V]\",\n) -> \"t.AsyncIterator[V]\":\n    if hasattr(var_1, \"__aiter__\"):\n        return var_1.__aiter__()\n    else:\n        return _IteratorToAsyncIterator(iter(var_1))\n",
      "lines_processed": 7,
      "total_lines": 99
    }
  ],
  "facebookresearch_fairseq": [
    {
      "file_path": "locallaunch.py",
      "code": "def main(args):\n    job_type = args.jobtype if args.jobtype else None\n    # parse multiple pipelines.\n    pipelines = [Pipeline(fn) for fn in args.yamls.split(\",\")]\n\n    for pipe_id, pipeline in enumerate(pipelines):\n        if not hasattr(pipeline, \"project_dir\"):\n            for job in pipeline[0]:\n                job(job_type=job_type, dryrun=args.dryrun)\n",
      "variables": [
        "args",
        "job_type",
        "pipelines",
        "fn",
        "pipe_id",
        "pipeline",
        "job"
      ],
      "anonymized_code": "def main(var_1):\n    var_2 = var_1.jobtype if var_1.jobtype else None\n    # parse multiple var_3.\n    var_3 = [Pipeline(var_4) for var_4 in var_1.yamls.split(\",\")]\n\n    for var_5, var_6 in enumerate(var_3):\n        if not hasattr(var_6, \"project_dir\"):\n            for var_7 in var_6[0]:\n                var_7(var_2=var_2, dryrun=var_1.dryrun)\n",
      "lines_processed": 9,
      "total_lines": 148
    },
    {
      "file_path": "fairseqmmmodel.py",
      "code": "def mmarch(args):\n    pass\n",
      "variables": [
        "args"
      ],
      "anonymized_code": "def mmarch(var_1):\n    pass\n",
      "lines_processed": 2,
      "total_lines": 51
    }
  ],
  "pallets_flask": [
    {
      "file_path": "blog.py",
      "code": "def create():\n    \"\"\"Create a new post for the current user.\"\"\"\n    if request.method == \"POST\":\n        title = request.form[\"title\"]\n        body = request.form[\"body\"]\n        error = None\n\n        if not title:\n            error = \"Title is required.\"\n\n",
      "variables": [
        "title",
        "body",
        "error"
      ],
      "anonymized_code": "def create():\n    \"\"\"Create a new post for the current user.\"\"\"\n    if request.method == \"POST\":\n        var_1 = request.form[\"var_1\"]\n        var_2 = request.form[\"var_2\"]\n        var_3 = None\n\n        if not var_1:\n            var_3 = \"Title is required.\"\n\n",
      "lines_processed": 10,
      "total_lines": 125
    },
    {
      "file_path": "blog.py",
      "code": "def delete(id):\n    \"\"\"Delete a post.\n\n    Ensures that the post exists and that the logged in user is the\n    author of the post.\n    \"\"\"\n    get_post(id)\n    db = get_db()\n    db.execute(\"DELETE FROM post WHERE id = ?\", (id,))\n    db.commit()\n",
      "variables": [
        "id",
        "db"
      ],
      "anonymized_code": "def delete(var_1):\n    \"\"\"Delete a post.\n\n    Ensures that the post exists and that the logged in user is the\n    author of the post.\n    \"\"\"\n    get_post(var_1)\n    var_2 = get_db()\n    var_2.execute(\"DELETE FROM post WHERE var_1 = ?\", (var_1,))\n    var_2.commit()\n",
      "lines_processed": 10,
      "total_lines": 125
    },
    {
      "file_path": "blog.py",
      "code": "def index():\n    \"\"\"Show all the posts, most recent first.\"\"\"\n    db = get_db()\n    posts = db.execute(\n        \"SELECT p.id, title, body, created, author_id, username\"\n        \" FROM post p JOIN user u ON p.author_id = u.id\"\n        \" ORDER BY created DESC\"\n    ).fetchall()\n    return render_template(\"blog/index.html\", posts=posts)\n",
      "variables": [
        "db",
        "posts"
      ],
      "anonymized_code": "def index():\n    \"\"\"Show all the var_2, most recent first.\"\"\"\n    var_1 = get_db()\n    var_2 = var_1.execute(\n        \"SELECT p.id, title, body, created, author_id, username\"\n        \" FROM post p JOIN user u ON p.author_id = u.id\"\n        \" ORDER BY created DESC\"\n    ).fetchall()\n    return render_template(\"blog/index.html\", var_2=var_2)\n",
      "lines_processed": 9,
      "total_lines": 125
    },
    {
      "file_path": "test_blog.py",
      "code": "def test_create(client, auth, app):\n    auth.login()\n    assert client.get(\"/create\").status_code == 200\n    client.post(\"/create\", data={\"title\": \"created\", \"body\": \"\"})\n\n    with app.app_context():\n        db = get_db()\n        count = db.execute(\"SELECT COUNT(id) FROM post\").fetchone()[0]\n        assert count == 2\n",
      "variables": [
        "client",
        "auth",
        "app",
        "db",
        "count"
      ],
      "anonymized_code": "def test_create(var_1, var_2, var_3):\n    var_2.login()\n    assert var_1.get(\"/create\").status_code == 200\n    var_1.post(\"/create\", data={\"title\": \"created\", \"body\": \"\"})\n\n    with var_3.app_context():\n        var_4 = get_db()\n        var_5 = var_4.execute(\"SELECT COUNT(id) FROM post\").fetchone()[0]\n        assert var_5 == 2\n",
      "lines_processed": 9,
      "total_lines": 83
    },
    {
      "file_path": "test_blog.py",
      "code": "def test_login_required(client, path):\n    response = client.post(path)\n    assert response.headers[\"Location\"] == \"/auth/login\"\n",
      "variables": [
        "client",
        "path",
        "response"
      ],
      "anonymized_code": "def test_login_required(var_1, var_2):\n    var_3 = var_1.post(var_2)\n    assert var_3.headers[\"Location\"] == \"/auth/login\"\n",
      "lines_processed": 3,
      "total_lines": 83
    },
    {
      "file_path": "test_blog.py",
      "code": "def test_author_required(app, client, auth):\n    # change the post author to another user\n    with app.app_context():\n        db = get_db()\n        db.execute(\"UPDATE post SET author_id = 2 WHERE id = 1\")\n        db.commit()\n\n    auth.login()\n    # current user can't modify other user's post\n    assert client.post(\"/1/update\").status_code == 403\n",
      "variables": [
        "app",
        "client",
        "auth",
        "db"
      ],
      "anonymized_code": "def test_author_required(var_1, var_2, var_3):\n    # change the post author to another user\n    with var_1.app_context():\n        var_4 = get_db()\n        var_4.execute(\"UPDATE post SET author_id = 2 WHERE id = 1\")\n        var_4.commit()\n\n    var_3.login()\n    # current user can't modify other user's post\n    assert var_2.post(\"/1/update\").status_code == 403\n",
      "lines_processed": 10,
      "total_lines": 83
    },
    {
      "file_path": "test_blog.py",
      "code": "def test_delete(client, auth, app):\n    auth.login()\n    response = client.post(\"/1/delete\")\n    assert response.headers[\"Location\"] == \"/\"\n\n    with app.app_context():\n        db = get_db()\n        post = db.execute(\"SELECT * FROM post WHERE id = 1\").fetchone()\n        assert post is None\n",
      "variables": [
        "client",
        "auth",
        "app",
        "response",
        "db",
        "post"
      ],
      "anonymized_code": "def test_delete(var_1, var_2, var_3):\n    var_2.login()\n    var_4 = var_1.var_6(\"/1/delete\")\n    assert var_4.headers[\"Location\"] == \"/\"\n\n    with var_3.app_context():\n        var_5 = get_db()\n        var_6 = var_5.execute(\"SELECT * FROM var_6 WHERE id = 1\").fetchone()\n        assert var_6 is None\n",
      "lines_processed": 9,
      "total_lines": 83
    },
    {
      "file_path": "test_blog.py",
      "code": "def test_index(client, auth):\n    response = client.get(\"/\")\n    assert b\"Log In\" in response.data\n    assert b\"Register\" in response.data\n\n    auth.login()\n    response = client.get(\"/\")\n    assert b\"test title\" in response.data\n    assert b\"by test on 2018-01-01\" in response.data\n    assert b\"test\\nbody\" in response.data\n",
      "variables": [
        "client",
        "auth",
        "response"
      ],
      "anonymized_code": "def test_index(var_1, var_2):\n    var_3 = var_1.get(\"/\")\n    assert b\"Log In\" in var_3.data\n    assert b\"Register\" in var_3.data\n\n    var_2.login()\n    var_3 = var_1.get(\"/\")\n    assert b\"test title\" in var_3.data\n    assert b\"by test on 2018-01-01\" in var_3.data\n    assert b\"test\\nbody\" in var_3.data\n",
      "lines_processed": 10,
      "total_lines": 83
    },
    {
      "file_path": "test_blog.py",
      "code": "def test_update(client, auth, app):\n    auth.login()\n    assert client.get(\"/1/update\").status_code == 200\n    client.post(\"/1/update\", data={\"title\": \"updated\", \"body\": \"\"})\n\n    with app.app_context():\n        db = get_db()\n        post = db.execute(\"SELECT * FROM post WHERE id = 1\").fetchone()\n        assert post[\"title\"] == \"updated\"\n",
      "variables": [
        "client",
        "auth",
        "app",
        "db",
        "post"
      ],
      "anonymized_code": "def test_update(var_1, var_2, var_3):\n    var_2.login()\n    assert var_1.get(\"/1/update\").status_code == 200\n    var_1.var_5(\"/1/update\", data={\"title\": \"updated\", \"body\": \"\"})\n\n    with var_3.app_context():\n        var_4 = get_db()\n        var_5 = var_4.execute(\"SELECT * FROM var_5 WHERE id = 1\").fetchone()\n        assert var_5[\"title\"] == \"updated\"\n",
      "lines_processed": 9,
      "total_lines": 83
    },
    {
      "file_path": "test_blog.py",
      "code": "def test_create_update_validate(client, auth, path):\n    auth.login()\n    response = client.post(path, data={\"title\": \"\", \"body\": \"\"})\n    assert b\"Title is required.\" in response.data\n",
      "variables": [
        "client",
        "auth",
        "path",
        "response"
      ],
      "anonymized_code": "def test_create_update_validate(var_1, var_2, var_3):\n    var_2.login()\n    var_4 = var_1.post(var_3, data={\"title\": \"\", \"body\": \"\"})\n    assert b\"Title is required.\" in var_4.data\n",
      "lines_processed": 4,
      "total_lines": 83
    },
    {
      "file_path": "test_blog.py",
      "code": "def test_exists_required(client, auth, path):\n    auth.login()\n    assert client.post(path).status_code == 404\n",
      "variables": [
        "client",
        "auth",
        "path"
      ],
      "anonymized_code": "def test_exists_required(var_1, var_2, var_3):\n    var_2.login()\n    assert var_1.post(var_3).status_code == 404\n",
      "lines_processed": 3,
      "total_lines": 83
    },
    {
      "file_path": "views.py",
      "code": "def index(js):\n    return render_template(f\"{js}.html\", js=js)\n",
      "variables": [
        "js"
      ],
      "anonymized_code": "def index(var_1):\n    return render_template(f\"{var_1}.html\", var_1=var_1)\n",
      "lines_processed": 2,
      "total_lines": 18
    },
    {
      "file_path": "views.py",
      "code": "def add():\n    a = request.form.get(\"a\", 0, type=float)\n    b = request.form.get(\"b\", 0, type=float)\n    return jsonify(result=a + b)\n",
      "variables": [
        "a",
        "b"
      ],
      "anonymized_code": "def add():\n    var_1 = request.form.get(\"var_1\", 0, type=float)\n    var_2 = request.form.get(\"var_2\", 0, type=float)\n    return jsonify(result=var_1 + var_2)\n",
      "lines_processed": 4,
      "total_lines": 18
    },
    {
      "file_path": "test_js_example.py",
      "code": "def test_index(app, client, path, template_name):\n    def check(sender, template, context):\n        assert template.name == template_name\n\n    with template_rendered.connected_to(check, app):\n        client.get(path)\n",
      "variables": [
        "app",
        "client",
        "path",
        "template_name",
        "sender",
        "template",
        "context"
      ],
      "anonymized_code": "def test_index(var_1, var_2, var_3, var_4):\n    def check(var_5, var_6, var_7):\n        assert var_6.name == var_4\n\n    with template_rendered.connected_to(check, var_1):\n        var_2.get(var_3)\n",
      "lines_processed": 6,
      "total_lines": 27
    },
    {
      "file_path": "test_js_example.py",
      "code": "def test_add(client, a, b, result):\n    response = client.post(\"/add\", data={\"a\": a, \"b\": b})\n    assert response.get_json()[\"result\"] == result\n",
      "variables": [
        "client",
        "a",
        "b",
        "result",
        "response"
      ],
      "anonymized_code": "def test_add(var_1, var_2, var_3, var_4):\n    var_5 = var_1.post(\"/add\", data={\"var_2\": var_2, \"var_3\": var_3})\n    assert var_5.get_json()[\"var_4\"] == var_4\n",
      "lines_processed": 3,
      "total_lines": 27
    },
    {
      "file_path": "test_db.py",
      "code": "def test_init_db_command(runner, monkeypatch):\n    class Recorder:\n        called = False\n\n    def fake_init_db():\n        Recorder.called = True\n\n    monkeypatch.setattr(\"flaskr.db.init_db\", fake_init_db)\n    result = runner.invoke(args=[\"init-db\"])\n    assert \"Initialized\" in result.output\n",
      "variables": [
        "runner",
        "monkeypatch",
        "called",
        "result"
      ],
      "anonymized_code": "def test_init_db_command(var_1, var_2):\n    class Recorder:\n        var_3 = False\n\n    def fake_init_db():\n        Recorder.var_3 = True\n\n    var_2.setattr(\"flaskr.db.init_db\", fake_init_db)\n    var_4 = var_1.invoke(args=[\"init-db\"])\n    assert \"Initialized\" in var_4.output\n",
      "lines_processed": 10,
      "total_lines": 29
    },
    {
      "file_path": "test_db.py",
      "code": "def test_get_close_db(app):\n    with app.app_context():\n        db = get_db()\n        assert db is get_db()\n\n    with pytest.raises(sqlite3.ProgrammingError) as e:\n        db.execute(\"SELECT 1\")\n\n    assert \"closed\" in str(e.value)\n",
      "variables": [
        "app",
        "db",
        "e"
      ],
      "anonymized_code": "def test_get_close_db(var_1):\n    with var_1.app_context():\n        var_2 = get_db()\n        assert var_2 is get_db()\n\n    with pytest.raises(sqlite3.ProgrammingError) as var_3:\n        var_2.execute(\"SELECT 1\")\n\n    assert \"closed\" in str(var_3.value)\n",
      "lines_processed": 9,
      "total_lines": 29
    },
    {
      "file_path": "auth.py",
      "code": "def login_required(view):\n    \"\"\"View decorator that redirects anonymous users to the login page.\"\"\"\n\n    @functools.wraps(view)\n    def wrapped_view(**kwargs):\n        if g.user is None:\n            return redirect(url_for(\"auth.login\"))\n\n        return view(**kwargs)\n\n",
      "variables": [
        "view",
        "kwargs"
      ],
      "anonymized_code": "def login_required(var_1):\n    \"\"\"View decorator that redirects anonymous users to the login page.\"\"\"\n\n    @functools.wraps(var_1)\n    def wrapped_view(**var_2):\n        if g.user is None:\n            return redirect(url_for(\"auth.login\"))\n\n        return var_1(**var_2)\n\n",
      "lines_processed": 10,
      "total_lines": 116
    },
    {
      "file_path": "auth.py",
      "code": "def login():\n    \"\"\"Log in a registered user by adding the user id to the session.\"\"\"\n    if request.method == \"POST\":\n        username = request.form[\"username\"]\n        password = request.form[\"password\"]\n        db = get_db()\n        error = None\n        user = db.execute(\n            \"SELECT * FROM user WHERE username = ?\", (username,)\n        ).fetchone()\n",
      "variables": [
        "username",
        "password",
        "db",
        "error",
        "user"
      ],
      "anonymized_code": "def login():\n    \"\"\"Log in a registered var_5 by adding the var_5 id to the session.\"\"\"\n    if request.method == \"POST\":\n        var_1 = request.form[\"var_1\"]\n        var_2 = request.form[\"var_2\"]\n        var_3 = get_db()\n        var_4 = None\n        var_5 = var_3.execute(\n            \"SELECT * FROM var_5 WHERE var_1 = ?\", (var_1,)\n        ).fetchone()\n",
      "lines_processed": 10,
      "total_lines": 116
    },
    {
      "file_path": "auth.py",
      "code": "def register():\n    \"\"\"Register a new user.\n\n    Validates that the username is not already taken. Hashes the\n    password for security.\n    \"\"\"\n    if request.method == \"POST\":\n        username = request.form[\"username\"]\n        password = request.form[\"password\"]\n        db = get_db()\n",
      "variables": [
        "username",
        "password",
        "db"
      ],
      "anonymized_code": "def register():\n    \"\"\"Register a new user.\n\n    Validates that the var_1 is not already taken. Hashes the\n    var_2 for security.\n    \"\"\"\n    if request.method == \"POST\":\n        var_1 = request.form[\"var_1\"]\n        var_2 = request.form[\"var_2\"]\n        var_3 = get_db()\n",
      "lines_processed": 10,
      "total_lines": 116
    },
    {
      "file_path": "tasks.py",
      "code": "def process(self: Task, total: int) -> object:\n    for i in range(total):\n        self.update_state(state=\"PROGRESS\", meta={\"current\": i + 1, \"total\": total})\n        time.sleep(1)\n\n    return {\"current\": total, \"total\": total}\n",
      "variables": [
        "self",
        "total",
        "i"
      ],
      "anonymized_code": "def process(var_1: Task, var_2: int) -> object:\n    for var_3 in range(var_2):\n        var_1.update_state(state=\"PROGRESS\", meta={\"current\": var_3 + 1, \"var_2\": var_2})\n        time.sleep(1)\n\n    return {\"current\": var_2, \"var_2\": var_2}\n",
      "lines_processed": 6,
      "total_lines": 23
    },
    {
      "file_path": "tasks.py",
      "code": "def add(a: int, b: int) -> int:\n    return a + b\n",
      "variables": [
        "a",
        "b"
      ],
      "anonymized_code": "def add(var_1: int, var_2: int) -> int:\n    return var_1 + var_2\n",
      "lines_processed": 2,
      "total_lines": 23
    },
    {
      "file_path": "test_auth.py",
      "code": "def test_login(client, auth):\n    # test that viewing the page renders without template errors\n    assert client.get(\"/auth/login\").status_code == 200\n\n    # test that successful login redirects to the index page\n    response = auth.login()\n    assert response.headers[\"Location\"] == \"/\"\n\n    # login request set the user_id in the session\n    # check that the user is loaded from the session\n",
      "variables": [
        "client",
        "auth",
        "response"
      ],
      "anonymized_code": "def test_login(var_1, var_2):\n    # test that viewing the page renders without template errors\n    assert var_1.get(\"/var_2/login\").status_code == 200\n\n    # test that successful login redirects to the index page\n    var_3 = var_2.login()\n    assert var_3.headers[\"Location\"] == \"/\"\n\n    # login request set the user_id in the session\n    # check that the user is loaded from the session\n",
      "lines_processed": 10,
      "total_lines": 69
    },
    {
      "file_path": "test_auth.py",
      "code": "def test_register_validate_input(client, username, password, message):\n    response = client.post(\n        \"/auth/register\", data={\"username\": username, \"password\": password}\n    )\n    assert message in response.data\n",
      "variables": [
        "client",
        "username",
        "password",
        "message",
        "response"
      ],
      "anonymized_code": "def test_register_validate_input(var_1, var_2, var_3, var_4):\n    var_5 = var_1.post(\n        \"/auth/register\", data={\"var_2\": var_2, \"var_3\": var_3}\n    )\n    assert var_4 in var_5.data\n",
      "lines_processed": 5,
      "total_lines": 69
    },
    {
      "file_path": "test_auth.py",
      "code": "def test_logout(client, auth):\n    auth.login()\n\n    with client:\n        auth.logout()\n        assert \"user_id\" not in session\n",
      "variables": [
        "client",
        "auth"
      ],
      "anonymized_code": "def test_logout(var_1, var_2):\n    var_2.login()\n\n    with var_1:\n        var_2.logout()\n        assert \"user_id\" not in session\n",
      "lines_processed": 6,
      "total_lines": 69
    },
    {
      "file_path": "test_auth.py",
      "code": "def test_login_validate_input(auth, username, password, message):\n    response = auth.login(username, password)\n    assert message in response.data\n",
      "variables": [
        "auth",
        "username",
        "password",
        "message",
        "response"
      ],
      "anonymized_code": "def test_login_validate_input(var_1, var_2, var_3, var_4):\n    var_5 = var_1.login(var_2, var_3)\n    assert var_4 in var_5.data\n",
      "lines_processed": 3,
      "total_lines": 69
    },
    {
      "file_path": "db.py",
      "code": "def init_db():\n    \"\"\"Clear existing data and create new tables.\"\"\"\n    db = get_db()\n\n    with current_app.open_resource(\"schema.sql\") as f:\n        db.executescript(f.read().decode(\"utf8\"))\n",
      "variables": [
        "db",
        "f"
      ],
      "anonymized_code": "def init_db():\n    \"\"\"Clear existing data and create new tables.\"\"\"\n    var_1 = get_db()\n\n    with current_app.open_resource(\"schema.sql\") as var_2:\n        var_1.executescript(var_2.read().decode(\"utf8\"))\n",
      "lines_processed": 6,
      "total_lines": 56
    },
    {
      "file_path": "db.py",
      "code": "def init_app(app):\n    \"\"\"Register database functions with the Flask app. This is called by\n    the application factory.\n    \"\"\"\n    app.teardown_appcontext(close_db)\n    app.cli.add_command(init_db_command)\n",
      "variables": [
        "app"
      ],
      "anonymized_code": "def init_app(var_1):\n    \"\"\"Register database functions with the Flask var_1. This is called by\n    the application factory.\n    \"\"\"\n    var_1.teardown_appcontext(close_db)\n    var_1.cli.add_command(init_db_command)\n",
      "lines_processed": 6,
      "total_lines": 56
    },
    {
      "file_path": "db.py",
      "code": "def close_db(e=None):\n    \"\"\"If this request connected to the database, close the\n    connection.\n    \"\"\"\n    db = g.pop(\"db\", None)\n\n    if db is not None:\n        db.close()\n",
      "variables": [
        "e",
        "db"
      ],
      "anonymized_code": "def close_db(var_1=None):\n    \"\"\"If this request connected to the database, close the\n    connection.\n    \"\"\"\n    var_2 = g.pop(\"var_2\", None)\n\n    if var_2 is not None:\n        var_2.close()\n",
      "lines_processed": 8,
      "total_lines": 56
    },
    {
      "file_path": "conf.py",
      "code": "def github_link(name, rawtext, text, lineno, inliner, options=None, content=None):\n    app = inliner.document.settings.env.app\n    release = app.config.release\n    base_url = \"https://github.com/pallets/flask/tree/\"\n\n    if text.endswith(\">\"):\n        words, text = text[:-1].rsplit(\"<\", 1)\n        words = words.strip()\n    else:\n        words = None\n",
      "variables": [
        "name",
        "rawtext",
        "text",
        "lineno",
        "inliner",
        "options",
        "content",
        "app",
        "release",
        "base_url",
        "words"
      ],
      "anonymized_code": "def github_link(var_1, var_2, var_3, var_4, var_5, var_6=None, var_7=None):\n    var_8 = var_5.document.settings.env.var_8\n    var_9 = var_8.config.var_9\n    var_10 = \"https://github.com/pallets/flask/tree/\"\n\n    if var_3.endswith(\">\"):\n        var_11, var_3 = var_3[:-1].rsplit(\"<\", 1)\n        var_11 = var_11.strip()\n    else:\n        var_11 = None\n",
      "lines_processed": 10,
      "total_lines": 101
    },
    {
      "file_path": "conf.py",
      "code": "def setup(app):\n    app.add_role(\"gh\", github_link)\n",
      "variables": [
        "app"
      ],
      "anonymized_code": "def setup(var_1):\n    var_1.add_role(\"gh\", github_link)\n",
      "lines_processed": 2,
      "total_lines": 101
    },
    {
      "file_path": "conftest.py",
      "code": "def app():\n    \"\"\"Create and configure a new app instance for each test.\"\"\"\n    # create a temporary file to isolate the database for each test\n    db_fd, db_path = tempfile.mkstemp()\n    # create the app with common test config\n    app = create_app({\"TESTING\": True, \"DATABASE\": db_path})\n\n    # create the database and load test data\n    with app.app_context():\n        init_db()\n",
      "variables": [
        "db_fd",
        "db_path",
        "app"
      ],
      "anonymized_code": "def var_3():\n    \"\"\"Create and configure a new var_3 instance for each test.\"\"\"\n    # create a temporary file to isolate the database for each test\n    var_1, var_2 = tempfile.mkstemp()\n    # create the var_3 with common test config\n    var_3 = create_app({\"TESTING\": True, \"DATABASE\": var_2})\n\n    # create the database and load test data\n    with var_3.app_context():\n        init_db()\n",
      "lines_processed": 10,
      "total_lines": 62
    },
    {
      "file_path": "conftest.py",
      "code": "def client(app):\n    \"\"\"A test client for the app.\"\"\"\n    return app.test_client()\n",
      "variables": [
        "app"
      ],
      "anonymized_code": "def client(var_1):\n    \"\"\"A test client for the var_1.\"\"\"\n    return var_1.test_client()\n",
      "lines_processed": 3,
      "total_lines": 62
    },
    {
      "file_path": "conftest.py",
      "code": "def runner(app):\n    \"\"\"A test runner for the app's Click commands.\"\"\"\n    return app.test_cli_runner()\n",
      "variables": [
        "app"
      ],
      "anonymized_code": "def runner(var_1):\n    \"\"\"A test runner for the var_1's Click commands.\"\"\"\n    return var_1.test_cli_runner()\n",
      "lines_processed": 3,
      "total_lines": 62
    },
    {
      "file_path": "conftest.py",
      "code": "def auth(client):\n    return AuthActions(client)\n",
      "variables": [
        "client"
      ],
      "anonymized_code": "def auth(var_1):\n    return AuthActions(var_1)\n",
      "lines_processed": 2,
      "total_lines": 62
    },
    {
      "file_path": "test_factory.py",
      "code": "def test_hello(client):\n    response = client.get(\"/hello\")\n    assert response.data == b\"Hello, World!\"\n",
      "variables": [
        "client",
        "response"
      ],
      "anonymized_code": "def test_hello(var_1):\n    var_2 = var_1.get(\"/hello\")\n    assert var_2.data == b\"Hello, World!\"\n",
      "lines_processed": 3,
      "total_lines": 12
    }
  ],
  "PrefectHQ_prefect": [
    {
      "file_path": "client_context_lifespan.py",
      "code": "def make_lifespan(startup, shutdown) -> Callable:\n    async def lifespan(app):\n        try:\n            startup()\n            yield\n        finally:\n            shutdown()\n\n    return asynccontextmanager(lifespan)\n",
      "variables": [
        "startup",
        "shutdown",
        "app"
      ],
      "anonymized_code": "def make_lifespan(var_1, var_2) -> Callable:\n    async def lifespan(var_3):\n        try:\n            var_1()\n            yield\n        finally:\n            var_2()\n\n    return asynccontextmanager(lifespan)\n",
      "lines_processed": 9,
      "total_lines": 124
    },
    {
      "file_path": "flows.py",
      "code": "def my_nested_flow(msg):\n    pass\n",
      "variables": [
        "msg"
      ],
      "anonymized_code": "def my_nested_flow(var_1):\n    pass\n",
      "lines_processed": 2,
      "total_lines": 11
    },
    {
      "file_path": "client_flow.py",
      "code": "def smoke_test_task(*args: Any, **kwargs: Any):\n    print(args, kwargs)\n",
      "variables": [
        "args",
        "kwargs"
      ],
      "anonymized_code": "def smoke_test_task(*var_1: Any, **var_2: Any):\n    print(var_1, var_2)\n",
      "lines_processed": 2,
      "total_lines": 35
    },
    {
      "file_path": "bench_cli.py",
      "code": "def bench_prefect_help(benchmark):\n    benchmark(subprocess.check_call, [\"prefect\", \"--help\"])\n",
      "variables": [
        "benchmark"
      ],
      "anonymized_code": "def bench_prefect_help(var_1):\n    var_1(subprocess.check_call, [\"prefect\", \"--help\"])\n",
      "lines_processed": 2,
      "total_lines": 21
    },
    {
      "file_path": "bench_cli.py",
      "code": "def bench_prefect_profile_ls(benchmark):\n    benchmark(subprocess.check_call, [\"prefect\", \"profile\", \"ls\"])\n",
      "variables": [
        "benchmark"
      ],
      "anonymized_code": "def bench_prefect_profile_ls(var_1):\n    var_1(subprocess.check_call, [\"prefect\", \"profile\", \"ls\"])\n",
      "lines_processed": 2,
      "total_lines": 21
    },
    {
      "file_path": "bench_cli.py",
      "code": "def bench_prefect_short_version(benchmark):\n    benchmark(subprocess.check_call, [\"prefect\", \"--version\"])\n",
      "variables": [
        "benchmark"
      ],
      "anonymized_code": "def bench_prefect_short_version(var_1):\n    var_1(subprocess.check_call, [\"prefect\", \"--version\"])\n",
      "lines_processed": 2,
      "total_lines": 21
    },
    {
      "file_path": "bench_cli.py",
      "code": "def bench_prefect_version(benchmark):\n    benchmark(subprocess.check_call, [\"prefect\", \"version\"])\n",
      "variables": [
        "benchmark"
      ],
      "anonymized_code": "def bench_prefect_version(var_1):\n    var_1(subprocess.check_call, [\"prefect\", \"version\"])\n",
      "lines_processed": 2,
      "total_lines": 21
    },
    {
      "file_path": "flow_pauses.py",
      "code": "def resume_after_started():\n    print(\"Waiting for flow run id to be reported...\")\n    flow_run_id = flow_run_id_future.result()\n    time.sleep(5)\n    print(\"Resuming flow run...\")\n    resume_flow_run(flow_run_id)\n",
      "variables": [
        "flow_run_id"
      ],
      "anonymized_code": "def resume_after_started():\n    print(\"Waiting for flow run id to be reported...\")\n    var_1 = flow_run_id_future.result()\n    time.sleep(5)\n    print(\"Resuming flow run...\")\n    resume_flow_run(var_1)\n",
      "lines_processed": 6,
      "total_lines": 34
    },
    {
      "file_path": "bench_import.py",
      "code": "def bench_import_prefect(benchmark: \"BenchmarkFixture\"):\n    def import_prefect():\n        reset_imports()\n\n        import prefect  # noqa\n\n    benchmark(import_prefect)\n",
      "variables": [
        "benchmark"
      ],
      "anonymized_code": "def bench_import_prefect(var_1: \"BenchmarkFixture\"):\n    def import_prefect():\n        reset_imports()\n\n        import prefect  # noqa\n\n    var_1(import_prefect)\n",
      "lines_processed": 7,
      "total_lines": 44
    },
    {
      "file_path": "bench_import.py",
      "code": "def reset_imports():\n    # Remove the module from sys.modules if it's there\n    prefect_modules = [key for key in sys.modules if key.startswith(\"prefect\")]\n    for module in prefect_modules:\n        del sys.modules[module]\n\n    # Clear importlib cache\n    importlib.invalidate_caches()\n\n    # reset the prometheus registry to clear any previously measured metrics\n",
      "variables": [
        "prefect_modules",
        "key",
        "module"
      ],
      "anonymized_code": "def reset_imports():\n    # Remove the var_3 from sys.modules if it's there\n    var_1 = [var_2 for var_2 in sys.modules if var_2.startswith(\"prefect\")]\n    for var_3 in var_1:\n        del sys.modules[var_3]\n\n    # Clear importlib cache\n    importlib.invalidate_caches()\n\n    # reset the prometheus registry to clear any previously measured metrics\n",
      "lines_processed": 10,
      "total_lines": 44
    },
    {
      "file_path": "bench_import.py",
      "code": "def bench_import_prefect_flow(benchmark: \"BenchmarkFixture\"):\n    def import_prefect_flow():\n        reset_imports()\n\n        from prefect import flow  # noqa\n\n    benchmark(import_prefect_flow)\n",
      "variables": [
        "benchmark"
      ],
      "anonymized_code": "def bench_import_prefect_flow(var_1: \"BenchmarkFixture\"):\n    def import_prefect_flow():\n        reset_imports()\n\n        from prefect import flow  # noqa\n\n    var_1(import_prefect_flow)\n",
      "lines_processed": 7,
      "total_lines": 44
    },
    {
      "file_path": "bench_flows.py",
      "code": "def bench_flow_decorator(benchmark: \"BenchmarkFixture\"):\n    benchmark(flow, noop_function)\n",
      "variables": [
        "benchmark"
      ],
      "anonymized_code": "def bench_flow_decorator(var_1: \"BenchmarkFixture\"):\n    var_1(flow, noop_function)\n",
      "lines_processed": 2,
      "total_lines": 122
    },
    {
      "file_path": "bench_flows.py",
      "code": "def bench_async_flow_with_sequential_subflows(\n    benchmark: \"BenchmarkFixture\", num_flows: int\n):\n    test_flow = flow(anoop_function)\n\n    @flow\n    async def benchmark_flow():\n        for _ in range(num_flows):\n            await test_flow()\n\n",
      "variables": [
        "benchmark",
        "num_flows",
        "test_flow",
        "_"
      ],
      "anonymized_code": "def bench_async_flow_with_sequential_subflows(\n    var_1: \"BenchmarkFixture\", var_2: int\n):\n    var_3 = flow(anoop_function)\n\n    @flow\n    async def benchmark_flow():\n        for var_4 in range(var_2):\n            await var_3()\n\n",
      "lines_processed": 10,
      "total_lines": 122
    },
    {
      "file_path": "bench_flows.py",
      "code": "def bench_async_flow_with_concurrent_subflows(\n    benchmark: \"BenchmarkFixture\", num_flows: int\n):\n    test_flow = flow(anoop_function)\n\n    @flow\n    async def benchmark_flow():\n        async with anyio.create_task_group() as tg:\n            for _ in range(num_flows):\n                tg.start_soon(test_flow)\n",
      "variables": [
        "benchmark",
        "num_flows",
        "test_flow",
        "tg",
        "_"
      ],
      "anonymized_code": "def bench_async_flow_with_concurrent_subflows(\n    var_1: \"BenchmarkFixture\", var_2: int\n):\n    var_3 = flow(anoop_function)\n\n    @flow\n    async def benchmark_flow():\n        async with anyio.create_task_group() as var_4:\n            for var_5 in range(var_2):\n                var_4.start_soon(var_3)\n",
      "lines_processed": 10,
      "total_lines": 122
    },
    {
      "file_path": "bench_flows.py",
      "code": "def bench_flow_with_submitted_tasks(benchmark: \"BenchmarkFixture\", num_tasks: int):\n    test_task = task(noop_function)\n\n    @flow\n    def benchmark_flow():\n        for _ in range(num_tasks):\n            test_task.submit()\n\n    benchmark(benchmark_flow)\n",
      "variables": [
        "benchmark",
        "num_tasks",
        "test_task",
        "_"
      ],
      "anonymized_code": "def bench_flow_with_submitted_tasks(var_1: \"BenchmarkFixture\", var_2: int):\n    var_3 = task(noop_function)\n\n    @flow\n    def benchmark_flow():\n        for var_4 in range(var_2):\n            var_3.submit()\n\n    var_1(benchmark_flow)\n",
      "lines_processed": 9,
      "total_lines": 122
    },
    {
      "file_path": "bench_flows.py",
      "code": "def bench_flow_with_subflows(benchmark: \"BenchmarkFixture\", num_flows: int):\n    test_flow = flow(noop_function)\n\n    @flow\n    def benchmark_flow():\n        for _ in range(num_flows):\n            test_flow()\n\n    benchmark(benchmark_flow)\n",
      "variables": [
        "benchmark",
        "num_flows",
        "test_flow",
        "_"
      ],
      "anonymized_code": "def bench_flow_with_subflows(var_1: \"BenchmarkFixture\", var_2: int):\n    var_3 = flow(noop_function)\n\n    @flow\n    def benchmark_flow():\n        for var_4 in range(var_2):\n            var_3()\n\n    var_1(benchmark_flow)\n",
      "lines_processed": 9,
      "total_lines": 122
    },
    {
      "file_path": "bench_flows.py",
      "code": "def bench_flow_call(benchmark: \"BenchmarkFixture\", options):\n    noop_flow = flow(**options)(noop_function)\n    benchmark(noop_flow)\n",
      "variables": [
        "benchmark",
        "options",
        "noop_flow"
      ],
      "anonymized_code": "def bench_flow_call(var_1: \"BenchmarkFixture\", var_2):\n    var_3 = flow(**var_2)(noop_function)\n    var_1(var_3)\n",
      "lines_processed": 3,
      "total_lines": 122
    },
    {
      "file_path": "tasks.py",
      "code": "def my_background_task(name: str): ...\n",
      "variables": [
        "name"
      ],
      "anonymized_code": "def my_background_task(var_1: str): ...\n",
      "lines_processed": 1,
      "total_lines": 5
    },
    {
      "file_path": "utils.py",
      "code": "def post(*args, **kwargs):\n    pass\n",
      "variables": [
        "args",
        "kwargs"
      ],
      "anonymized_code": "def post(*var_1, **var_2):\n    pass\n",
      "lines_processed": 2,
      "total_lines": 6
    },
    {
      "file_path": "utils.py",
      "code": "def put(*args, **kwargs):\n    pass\n",
      "variables": [
        "args",
        "kwargs"
      ],
      "anonymized_code": "def put(*var_1, **var_2):\n    pass\n",
      "lines_processed": 2,
      "total_lines": 6
    },
    {
      "file_path": "bench_tasks.py",
      "code": "def bench_task_call(benchmark: \"BenchmarkFixture\"):\n    noop_task = task(noop_function)\n\n    @flow\n    def benchmark_flow():\n        benchmark(noop_task)\n\n    benchmark_flow()\n",
      "variables": [
        "benchmark",
        "noop_task"
      ],
      "anonymized_code": "def bench_task_call(var_1: \"BenchmarkFixture\"):\n    var_2 = task(noop_function)\n\n    @flow\n    def benchmark_flow():\n        var_1(var_2)\n\n    benchmark_flow()\n",
      "lines_processed": 8,
      "total_lines": 37
    },
    {
      "file_path": "bench_tasks.py",
      "code": "def bench_task_decorator(benchmark: \"BenchmarkFixture\"):\n    benchmark(task, noop_function)\n",
      "variables": [
        "benchmark"
      ],
      "anonymized_code": "def bench_task_decorator(var_1: \"BenchmarkFixture\"):\n    var_1(task, noop_function)\n",
      "lines_processed": 2,
      "total_lines": 37
    },
    {
      "file_path": "bench_tasks.py",
      "code": "def bench_task_submit(benchmark: \"BenchmarkFixture\"):\n    noop_task = task(noop_function)\n\n    # The benchmark occurs within the flow to measure _submission_ time without\n    # measuring any other part of orchestration / collection of results\n\n    @flow\n    def benchmark_flow():\n        benchmark(noop_task.submit)\n\n",
      "variables": [
        "benchmark",
        "noop_task"
      ],
      "anonymized_code": "def bench_task_submit(var_1: \"BenchmarkFixture\"):\n    var_2 = task(noop_function)\n\n    # The var_1 occurs within the flow to measure _submission_ time without\n    # measuring any other part of orchestration / collection of results\n\n    @flow\n    def benchmark_flow():\n        var_1(var_2.submit)\n\n",
      "lines_processed": 10,
      "total_lines": 37
    },
    {
      "file_path": "db.py",
      "code": "def execute(query: str):\n    pass\n",
      "variables": [
        "query"
      ],
      "anonymized_code": "def execute(var_1: str):\n    pass\n",
      "lines_processed": 2,
      "total_lines": 6
    },
    {
      "file_path": "conftest.py",
      "code": "def pytest_collection_modifyitems(items):\n    for item in items:\n        for file, reason in SKIP_FILES.items():\n            full_path = os.path.join(project_root, file)\n            if str(item.fspath) == full_path:\n                item.add_marker(pytest.mark.skip(reason=reason))\n",
      "variables": [
        "items",
        "item",
        "file",
        "reason",
        "full_path"
      ],
      "anonymized_code": "def pytest_collection_modifyitems(var_1):\n    for var_2 in var_1:\n        for var_3, var_4 in SKIP_FILES.var_1():\n            var_5 = os.path.join(project_root, var_3)\n            if str(var_2.fspath) == var_5:\n                var_2.add_marker(pytest.mark.skip(var_4=var_4))\n",
      "lines_processed": 6,
      "total_lines": 106
    },
    {
      "file_path": "conftest.py",
      "code": "def mock_post_200(monkeypatch):\n    mock_response = mock.Mock()\n    mock_response.status_code = 200\n    mock_response.json.return_value = []\n\n    def mock_post(*args, **kwargs):\n        return mock_response\n\n    monkeypatch.setattr(\"requests.post\", mock_post)\n    return mock_response\n",
      "variables": [
        "monkeypatch",
        "mock_response",
        "args",
        "kwargs"
      ],
      "anonymized_code": "def mock_post_200(var_1):\n    var_2 = mock.Mock()\n    var_2.status_code = 200\n    var_2.json.return_value = []\n\n    def mock_post(*var_3, **var_4):\n        return var_2\n\n    var_1.setattr(\"requests.post\", mock_post)\n    return var_2\n",
      "lines_processed": 10,
      "total_lines": 106
    },
    {
      "file_path": "docker_deploy.py",
      "code": "def flow_that_needs_pandas() -> str:\n    \"\"\"A flow that needs pandas.\"\"\"\n    import pandas\n\n    df = pandas.DataFrame({\"a\": [1, 2, 3]})\n\n    assert isinstance(df, pandas.DataFrame)\n\n    return \"we're done\"\n",
      "variables": [
        "df"
      ],
      "anonymized_code": "def flow_that_needs_pandas() -> str:\n    \"\"\"A flow that needs pandas.\"\"\"\n    import pandas\n\n    var_1 = pandas.DataFrame({\"a\": [1, 2, 3]})\n\n    assert isinstance(var_1, pandas.DataFrame)\n\n    return \"we're done\"\n",
      "lines_processed": 9,
      "total_lines": 105
    }
  ],
  "pygame_pygame": [
    {
      "file_path": "setup_win_common.py",
      "code": "def read():\n    \"\"\"Return the contents of the Windows Common Setup as a string\"\"\"\n    with open(PATH) as setup_in:\n        return setup_in.read()\n",
      "variables": [
        "setup_in"
      ],
      "anonymized_code": "def read():\n    \"\"\"Return the contents of the Windows Common Setup as a string\"\"\"\n    with open(PATH) as var_1:\n        return var_1.read()\n",
      "lines_processed": 4,
      "total_lines": 43
    },
    {
      "file_path": "setup_win_common.py",
      "code": "def get_definitions():\n    \"\"\"Return a list of definitions in the Windows Common Setup\n\n    Each macro definition object has a 'name' and 'value' attribute.\n    \"\"\"\n    import re\n\n    deps = []\n    match = re.compile(r'([a-zA-Z0-9_]+) += +(.+)$').match\n\n",
      "variables": [
        "deps",
        "match"
      ],
      "anonymized_code": "def get_definitions():\n    \"\"\"Return a list of definitions in the Windows Common Setup\n\n    Each macro definition object has a 'name' and 'value' attribute.\n    \"\"\"\n    import re\n\n    var_1 = []\n    var_2 = re.compile(r'([a-zA-Z0-9_]+) += +(.+)$').var_2\n\n",
      "lines_processed": 10,
      "total_lines": 43
    },
    {
      "file_path": "download_win_prebuilt.py",
      "code": "def ask(x86=True, x64=True):\n    move_to_dir = \".\"\n    if x64:\n        dest_str = f\"\\\"{move_to_dir}/prebuilt-x64\\\"\"\n    else:\n        dest_str = \"\"\n    if x86:\n        if dest_str:\n            dest_str = f\"{dest_str} and \"\n        dest_str = f\"{dest_str}\\\"{move_to_dir}/prebuilt-x86\\\"\"\n",
      "variables": [
        "x86",
        "x64",
        "move_to_dir",
        "dest_str"
      ],
      "anonymized_code": "def ask(var_1=True, var_2=True):\n    var_3 = \".\"\n    if var_2:\n        var_4 = f\"\\\"{var_3}/prebuilt-var_2\\\"\"\n    else:\n        var_4 = \"\"\n    if var_1:\n        if var_4:\n            var_4 = f\"{var_4} and \"\n        var_4 = f\"{var_4}\\\"{var_3}/prebuilt-var_1\\\"\"\n",
      "lines_processed": 10,
      "total_lines": 279
    },
    {
      "file_path": "download_win_prebuilt.py",
      "code": "def download_sha1_unzip(url, checksum, save_to_directory, unzip=True):\n    \"\"\" This\n    - downloads a url,\n    - sha1 checksum check,\n    - save_to_directory,\n    - then unzips it.\n\n    Does not download again if the file is there.\n    Does not unzip again if the file is there.\n    \"\"\"\n",
      "variables": [
        "url",
        "checksum",
        "save_to_directory",
        "unzip"
      ],
      "anonymized_code": "def download_sha1_unzip(var_1, var_2, var_3, var_4=True):\n    \"\"\" This\n    - downloads a var_1,\n    - sha1 var_2 check,\n    - var_3,\n    - then unzips it.\n\n    Does not download again if the file is there.\n    Does not var_4 again if the file is there.\n    \"\"\"\n",
      "lines_processed": 10,
      "total_lines": 279
    },
    {
      "file_path": "download_win_prebuilt.py",
      "code": "def download_prebuilts(temp_dir, x86=True, x64=True):\n    \"\"\" For downloading prebuilt dependencies.\n    \"\"\"\n    if not os.path.exists(temp_dir):\n        print(f\"Making dir :{temp_dir}:\")\n        os.makedirs(temp_dir)\n    for url, checksum in get_urls(x86=x86, x64=x64):\n        download_sha1_unzip(url, checksum, temp_dir, 1)\n",
      "variables": [
        "temp_dir",
        "x86",
        "x64",
        "url",
        "checksum"
      ],
      "anonymized_code": "def download_prebuilts(var_1, var_2=True, var_3=True):\n    \"\"\" For downloading prebuilt dependencies.\n    \"\"\"\n    if not os.path.exists(var_1):\n        print(f\"Making dir :{var_1}:\")\n        os.makedirs(var_1)\n    for var_4, var_5 in get_urls(var_2=var_2, var_3=var_3):\n        download_sha1_unzip(var_4, var_5, var_1, 1)\n",
      "lines_processed": 8,
      "total_lines": 279
    },
    {
      "file_path": "download_win_prebuilt.py",
      "code": "def cached(x86=True, x64=True):\n    if not os.path.isdir(download_dir):\n        return False\n    for url, check in get_urls(x86=x86, x64=x64):\n        filename = os.path.split(url)[-1]\n        save_to = os.path.join(download_dir, filename)\n        if not os.path.exists(save_to):\n            return False\n    return True\n",
      "variables": [
        "x86",
        "x64",
        "url",
        "check",
        "filename",
        "save_to"
      ],
      "anonymized_code": "def cached(var_1=True, var_2=True):\n    if not os.path.isdir(download_dir):\n        return False\n    for var_3, var_4 in get_urls(var_1=var_1, var_2=var_2):\n        var_5 = os.path.split(var_3)[-1]\n        var_6 = os.path.join(download_dir, var_5)\n        if not os.path.exists(var_6):\n            return False\n    return True\n",
      "lines_processed": 9,
      "total_lines": 279
    },
    {
      "file_path": "download_win_prebuilt.py",
      "code": "def copytree(src, dst, symlinks=False, ignore=None):\n    \"\"\"like shutil.copytree() but ignores existing files\n    https://stackoverflow.com/a/22331852/1239986\n    \"\"\"\n    if not os.path.exists(dst):\n        os.makedirs(dst)\n        shutil.copystat(src, dst)\n    lst = os.listdir(src)\n    if ignore:\n        excl = ignore(src, lst)\n",
      "variables": [
        "src",
        "dst",
        "symlinks",
        "ignore",
        "lst",
        "excl"
      ],
      "anonymized_code": "def copytree(var_1, var_2, var_3=False, var_4=None):\n    \"\"\"like shutil.copytree() but ignores existing files\n    https://stackoverflow.com/a/22331852/1239986\n    \"\"\"\n    if not os.path.exists(var_2):\n        os.makedirs(var_2)\n        shutil.copystat(var_1, var_2)\n    var_5 = os.listdir(var_1)\n    if var_4:\n        var_6 = var_4(var_1, var_5)\n",
      "lines_processed": 10,
      "total_lines": 279
    },
    {
      "file_path": "download_win_prebuilt.py",
      "code": "def place_downloaded_prebuilts(temp_dir, move_to_dir, x86=True, x64=True):\n    \"\"\" puts the downloaded prebuilt files into the right place.\n\n    Leaves the files in temp_dir. copies to move_to_dir\n    \"\"\"\n    prebuilt_x64 = os.path.join(\n        temp_dir,\n        'prebuilt-x64-pygame-2.1.4-20220319',\n        'prebuilt-x64'\n    )\n",
      "variables": [
        "temp_dir",
        "move_to_dir",
        "x86",
        "x64",
        "prebuilt_x64"
      ],
      "anonymized_code": "def place_downloaded_prebuilts(var_1, var_2, var_3=True, var_4=True):\n    \"\"\" puts the downloaded prebuilt files into the right place.\n\n    Leaves the files in var_1. copies to var_2\n    \"\"\"\n    var_5 = os.path.join(\n        var_1,\n        'prebuilt-var_4-pygame-2.1.4-20220319',\n        'prebuilt-var_4'\n    )\n",
      "lines_processed": 10,
      "total_lines": 279
    },
    {
      "file_path": "vstools.py",
      "code": "def lib_from_def(def_file, arch=None):\n    if not arch:\n        arch = get_build_architecture()\n        if arch == 'Intel':\n            arch = 'x86'\n        elif arch == 'Itanium':\n            arch = 'IA64'\n        else:\n            arch = 'x64'\n    lib_file = f'{os.path.splitext(def_file)[0]}.lib'\n",
      "variables": [
        "def_file",
        "arch",
        "lib_file"
      ],
      "anonymized_code": "def lib_from_def(var_1, var_2=None):\n    if not var_2:\n        var_2 = get_build_architecture()\n        if var_2 == 'Intel':\n            var_2 = 'x86'\n        elif var_2 == 'Itanium':\n            var_2 = 'IA64'\n        else:\n            var_2 = 'x64'\n    var_3 = f'{os.path.splitext(var_1)[0]}.lib'\n",
      "lines_processed": 10,
      "total_lines": 81
    },
    {
      "file_path": "vstools.py",
      "code": "def dump_def(dll, def_file=None):\n    if not def_file:\n        def_file = f'{os.path.splitext(dll)[0]}.def'\n    dll_base = os.path.basename(dll)\n    with open(def_file, 'w') as f:\n        f.write(_fmt_header % dll_base)\n        f.write(f'LIBRARY \"{dll_base}\\\"\\n')\n        f.write('EXPORTS\\n')\n        f.writelines(f\"{line}\\n\" for line in find_symbols(dll))\n",
      "variables": [
        "dll",
        "def_file",
        "dll_base",
        "f",
        "line"
      ],
      "anonymized_code": "def dump_def(var_1, var_2=None):\n    if not var_2:\n        var_2 = var_4'{os.path.splitext(var_1)[0]}.def'\n    var_3 = os.path.basename(var_1)\n    with open(var_2, 'w') as var_4:\n        var_4.write(_fmt_header % var_3)\n        var_4.write(var_4'LIBRARY \"{var_3}\\\"\\n')\n        var_4.write('EXPORTS\\n')\n        var_4.writelines(var_4\"{var_5}\\n\" for var_5 in find_symbols(var_1))\n",
      "lines_processed": 9,
      "total_lines": 81
    },
    {
      "file_path": "vstools.py",
      "code": "def find_symbols(dll):\n    dumpbin_path = compiler.find_exe('dumpbin.exe')\n    try:\n        output = subprocess.check_output(\n            [dumpbin_path, '/nologo', '/exports', dll],\n            universal_newlines=True,\n        )\n    except subprocess.CalledProcessError as e:\n        raise DumpbinError(e.output)\n\n",
      "variables": [
        "dll",
        "dumpbin_path",
        "output"
      ],
      "anonymized_code": "def find_symbols(var_1):\n    var_2 = compiler.find_exe('dumpbin.exe')\n    try:\n        var_3 = subprocess.check_output(\n            [var_2, '/nologo', '/exports', var_1],\n            universal_newlines=True,\n        )\n    except subprocess.CalledProcessError as e:\n        raise DumpbinError(e.var_3)\n\n",
      "lines_processed": 10,
      "total_lines": 81
    },
    {
      "file_path": "config_conan.py",
      "code": "def conan_install(force_build=True):\n    \"\"\"\n    \"\"\"\n    build_dir = os.path.join('build', 'conan')\n\n    if not os.path.exists(build_dir):\n        if not os.path.exists('build'):\n            os.mkdir('build')\n        os.mkdir(build_dir)\n\n",
      "variables": [
        "force_build",
        "build_dir"
      ],
      "anonymized_code": "def conan_install(var_1=True):\n    \"\"\"\n    \"\"\"\n    var_2 = os.path.join('build', 'conan')\n\n    if not os.path.exists(var_2):\n        if not os.path.exists('build'):\n            os.mkdir('build')\n        os.mkdir(var_2)\n\n",
      "lines_processed": 10,
      "total_lines": 91
    },
    {
      "file_path": "__main__.py",
      "code": "def has_local_docs():\n    pkg_dir = os.path.dirname(os.path.abspath(__file__))\n    main_page = os.path.join(pkg_dir, \"generated\", \"index.html\")\n    return os.path.exists(main_page)\n",
      "variables": [
        "pkg_dir",
        "main_page"
      ],
      "anonymized_code": "def has_local_docs():\n    var_1 = os.path.dirname(os.path.abspath(__file__))\n    var_2 = os.path.join(var_1, \"generated\", \"index.html\")\n    return os.path.exists(var_2)\n",
      "lines_processed": 4,
      "total_lines": 37
    },
    {
      "file_path": "__main__.py",
      "code": "def _iterpath(path):\n    path, last = os.path.split(path)\n    if last:\n        yield from _iterpath(path)\n        yield last\n",
      "variables": [
        "path",
        "last"
      ],
      "anonymized_code": "def _iterpath(var_1):\n    var_1, var_2 = os.var_1.split(var_1)\n    if var_2:\n        yield from _iterpath(var_1)\n        yield var_2\n",
      "lines_processed": 5,
      "total_lines": 37
    },
    {
      "file_path": "install_mac_deps.py",
      "code": "def rmpath(path: Path, verbose: bool = False):\n    \"\"\"\n    Tries to remove a path of any kind\n    \"\"\"\n    if path.is_symlink():\n        if verbose:\n            print(f\"- Removing existing symlink at '{path}'\")\n\n        path.unlink()\n\n",
      "variables": [
        "path",
        "verbose"
      ],
      "anonymized_code": "def rmpath(var_1: Path, var_2: bool = False):\n    \"\"\"\n    Tries to remove a var_1 of any kind\n    \"\"\"\n    if var_1.is_symlink():\n        if var_2:\n            print(f\"- Removing existing symlink at '{var_1}'\")\n\n        var_1.unlink()\n\n",
      "lines_processed": 10,
      "total_lines": 61
    },
    {
      "file_path": "config_darwin.py",
      "code": "def find_freetype():\n    \"\"\" modern freetype uses pkg-config. However, some older systems don't have that.\n    \"\"\"\n    pkg_config = DependencyProg(\n        'FREETYPE', 'FREETYPE_CONFIG', 'pkg-config freetype2', '2.0',\n        ['freetype2'], '--modversion'\n    )\n    if pkg_config.found:\n        return pkg_config\n\n",
      "variables": [
        "pkg_config"
      ],
      "anonymized_code": "def find_freetype():\n    \"\"\" modern freetype uses pkg-config. However, some older systems don't have that.\n    \"\"\"\n    var_1 = DependencyProg(\n        'FREETYPE', 'FREETYPE_CONFIG', 'pkg-config freetype2', '2.0',\n        ['freetype2'], '--modversion'\n    )\n    if var_1.found:\n        return var_1\n\n",
      "lines_processed": 10,
      "total_lines": 178
    },
    {
      "file_path": "config_win.py",
      "code": "def _add_sdl2_dll_deps(DEPS):\n    # MIXER\n    DEPS.add_dll(r'(libogg-0|ogg)\\.dll$', 'ogg', ['libogg-[1-9].*'])\n    DEPS.add_dll(r'(lib)?modplug[-0-9]*\\.dll$', 'modplug', ['*modplug-[0-9]*'])\n    DEPS.add_dll(r'(lib)?opus[-0-9]*\\.dll$', 'opus', ['*opus-[0-9]*'])\n    DEPS.add_dll(r'(lib)?opusfile[-0-9]*\\.dll$', 'opusfile', ['*opusfile-[0-9]*'])\n    # IMAGE\n    DEPS.add_dll(r'(lib){0,1}tiff[-0-9]*\\.dll$', 'tiff', ['tiff-[0-9]*'], ['jpeg', 'z'])\n    DEPS.add_dll(r'(z|zlib1)\\.dll$', 'z', ['zlib-[1-9].*'])\n    DEPS.add_dll(r'(lib)?webp[-0-9]*\\.dll$', 'webp', ['*webp-[0-9]*'])\n",
      "variables": [
        "DEPS"
      ],
      "anonymized_code": "def _add_sdl2_dll_deps(var_1):\n    # MIXER\n    var_1.add_dll(r'(libogg-0|ogg)\\.dll$', 'ogg', ['libogg-[1-9].*'])\n    var_1.add_dll(r'(lib)?modplug[-0-9]*\\.dll$', 'modplug', ['*modplug-[0-9]*'])\n    var_1.add_dll(r'(lib)?opus[-0-9]*\\.dll$', 'opus', ['*opus-[0-9]*'])\n    var_1.add_dll(r'(lib)?opusfile[-0-9]*\\.dll$', 'opusfile', ['*opusfile-[0-9]*'])\n    # IMAGE\n    var_1.add_dll(r'(lib){0,1}tiff[-0-9]*\\.dll$', 'tiff', ['tiff-[0-9]*'], ['jpeg', 'z'])\n    var_1.add_dll(r'(z|zlib1)\\.dll$', 'z', ['zlib-[1-9].*'])\n    var_1.add_dll(r'(lib)?webp[-0-9]*\\.dll$', 'webp', ['*webp-[0-9]*'])\n",
      "lines_processed": 10,
      "total_lines": 508
    },
    {
      "file_path": "config_win.py",
      "code": "def setup_prebuilt_sdl2(prebuilt_dir):\n    Dependency.huntpaths[:] = [prebuilt_dir]\n    Dependency.lib_hunt.extend([\n        '',\n        'lib',\n        os.path.join('lib', get_machine_type()),\n        # SDL also provides some optional DLLs sometimes, so we also look for\n        # those\n        os.path.join('lib', get_machine_type(), \"optional\"),\n    ])\n",
      "variables": [
        "prebuilt_dir"
      ],
      "anonymized_code": "def setup_prebuilt_sdl2(var_1):\n    Dependency.huntpaths[:] = [var_1]\n    Dependency.lib_hunt.extend([\n        '',\n        'lib',\n        os.path.join('lib', get_machine_type()),\n        # SDL also provides some optional DLLs sometimes, so we also look for\n        # those\n        os.path.join('lib', get_machine_type(), \"optional\"),\n    ])\n",
      "lines_processed": 10,
      "total_lines": 508
    },
    {
      "file_path": "bundle_docs.py",
      "code": "def add_files(bundle, root, alias, file_names):\n    \"\"\"Add files to the bundle.\"\"\"\n    for file_name in file_names:\n        file_alias = os.path.join(alias, file_name)\n        print(f\"  {file_name} --> {file_alias}\")\n        bundle.add(os.path.join(root, file_name), file_alias)\n",
      "variables": [
        "bundle",
        "root",
        "alias",
        "file_names",
        "file_name",
        "file_alias"
      ],
      "anonymized_code": "def add_files(var_1, var_2, var_3, var_4):\n    \"\"\"Add files to the var_1.\"\"\"\n    for var_5 in var_4:\n        var_6 = os.path.join(var_3, var_5)\n        print(f\"  {var_5} --> {var_6}\")\n        var_1.add(os.path.join(var_2, var_5), var_6)\n",
      "lines_processed": 6,
      "total_lines": 68
    },
    {
      "file_path": "msysio.py",
      "code": "def print_(*args, **kwds):\n    \"\"\"Print arguments in an MSYS console friendly way\n\n    Keyword arguments:\n        file, sep, end\n    \"\"\"\n\n    stream = kwds.get('file', sys.stdout)\n    sep = kwds.get('sep', ' ')\n    end = kwds.get('end', '\\n')\n",
      "variables": [
        "args",
        "kwds",
        "stream",
        "sep",
        "end"
      ],
      "anonymized_code": "def print_(*var_1, **var_2):\n    \"\"\"Print arguments in an MSYS console friendly way\n\n    Keyword arguments:\n        file, var_4, var_5\n    \"\"\"\n\n    var_3 = var_2.get('file', sys.stdout)\n    var_4 = var_2.get('var_4', ' ')\n    var_5 = var_2.get('var_5', '\\n')\n",
      "lines_processed": 10,
      "total_lines": 42
    },
    {
      "file_path": "config.py",
      "code": "def print_(*args, **kwds):\n    \"\"\"Similar to the Python 3.0 print function\"\"\"\n    # This not only supports MSYS but is also a head start on the move to\n    # Python 3.0. Also, this function can be overridden for testing.\n    msysio.print_(*args, **kwds)\n",
      "variables": [
        "args",
        "kwds"
      ],
      "anonymized_code": "def print_(*var_1, **var_2):\n    \"\"\"Similar to the Python 3.0 print function\"\"\"\n    # This not only supports MSYS but is also a head start on the move to\n    # Python 3.0. Also, this function can be overridden for testing.\n    msysio.print_(*var_1, **var_2)\n",
      "lines_processed": 5,
      "total_lines": 250
    },
    {
      "file_path": "config.py",
      "code": "def writesetupfile(deps, basepath, additional_lines):\n    \"\"\"create a modified copy of Setup.SDLx.in\"\"\"\n    sdl_setup_filename = os.path.join(BASE_PATH, 'buildconfig',\n                                          'Setup.SDL2.in')\n\n    with open(sdl_setup_filename) as origsetup, \\\n            open(os.path.join(BASE_PATH, 'Setup'), 'w') as newsetup:\n        line = ''\n        while line.find('#--StartConfig') == -1:\n            newsetup.write(line)\n",
      "variables": [
        "deps",
        "basepath",
        "additional_lines",
        "sdl_setup_filename",
        "origsetup",
        "newsetup",
        "line"
      ],
      "anonymized_code": "def writesetupfile(var_1, var_2, var_3):\n    \"\"\"create a modified copy of Setup.SDLx.in\"\"\"\n    var_4 = os.path.join(BASE_PATH, 'buildconfig',\n                                          'Setup.SDL2.in')\n\n    with open(var_4) as var_5, \\\n            open(os.path.join(BASE_PATH, 'Setup'), 'w') as var_6:\n        var_7 = ''\n        while var_7.find('#--StartConfig') == -1:\n            var_6.write(var_7)\n",
      "lines_processed": 10,
      "total_lines": 250
    },
    {
      "file_path": "config.py",
      "code": "def prepdep(dep, basepath):\n    \"\"\"add some vars to a dep\"\"\"\n    if dep.libs:\n        dep.line = dep.name + ' ='\n        for lib in dep.libs:\n            dep.line += ' -l' + lib\n    else:\n        dep.line = dep.name + ' = -I.'\n\n    dep.varname = '$('+dep.name+')'\n",
      "variables": [
        "dep",
        "basepath",
        "lib"
      ],
      "anonymized_code": "def prepdep(var_1, var_2):\n    \"\"\"add some vars to a var_1\"\"\"\n    if var_1.libs:\n        var_1.line = var_1.name + ' ='\n        for var_3 in var_1.libs:\n            var_1.line += ' -l' + var_3\n    else:\n        var_1.line = var_1.name + ' = -I.'\n\n    var_1.varname = '$('+var_1.name+')'\n",
      "lines_processed": 10,
      "total_lines": 250
    },
    {
      "file_path": "gen_stubs.py",
      "code": "def get_all(mod: Any):\n    \"\"\"\n    Get the attributes that are imported from 'mod' when 'from mod import *'\n    First try to use '__all__' if it is defined, else fallback to 'dir'\n    \"\"\"\n    if hasattr(mod, \"__all__\") and isinstance(mod.__all__, list):\n        return sorted({str(i) for i in mod.__all__})\n\n    return [i for i in dir(mod) if not i.startswith(\"_\")]\n",
      "variables": [
        "mod",
        "i"
      ],
      "anonymized_code": "def get_all(var_1: Any):\n    \"\"\"\n    Get the attributes that are imported from 'var_1' when 'from var_1 import *'\n    First try to use '__all__' if it is defined, else fallback to 'dir'\n    \"\"\"\n    if hasattr(var_1, \"__all__\") and isinstance(var_1.__all__, list):\n        return sorted({str(var_2) for var_2 in var_1.__all__})\n\n    return [var_2 for var_2 in dir(var_1) if not var_2.startswith(\"_\")]\n",
      "lines_processed": 9,
      "total_lines": 139
    },
    {
      "file_path": "makeref.py",
      "code": "def run():\n    global rst_dir, rst_source_dir, rst_build_dir, rst_doctree_dir, c_header_dir\n    rst_dir = 'docs'\n    rst_source_dir = os.path.join(rst_dir, 'reST')\n    rst_build_dir = os.path.join('docs', 'generated')\n\n    rst_doctree_dir = os.path.join(rst_build_dir, 'doctrees')\n    c_header_dir = os.path.join('src_c', 'doc')\n    print(\"Generating:\", rst_source_dir, rst_build_dir)\n    runit()\n",
      "variables": [
        "rst_dir",
        "rst_source_dir",
        "rst_build_dir",
        "rst_doctree_dir",
        "c_header_dir"
      ],
      "anonymized_code": "def run():\n    global var_1, var_2, var_3, var_4, var_5\n    var_1 = 'docs'\n    var_2 = os.path.join(var_1, 'reST')\n    var_3 = os.path.join('docs', 'generated')\n\n    var_4 = os.path.join(var_3, 'doctrees')\n    var_5 = os.path.join('src_c', 'doc')\n    print(\"Generating:\", var_2, var_3)\n    runit()\n",
      "lines_processed": 10,
      "total_lines": 62
    },
    {
      "file_path": "config_unix.py",
      "code": "def main(auto_config=False):\n    global origincdirs, origlibdirs\n\n    #these get prefixes with '/usr' and '/usr/local' or the $LOCALBASE\n    origincdirs = ['/include', '/include/SDL2']\n    origlibdirs = ['/lib', '/lib64', '/X11R6/lib']\n\n    # If we are on a debian based system, we also need to handle\n    # /lib/<multiarch-tuple>\n    # We have a few commands to get the correct <multiarch-tuple>, we try those\n",
      "variables": [
        "auto_config",
        "origincdirs",
        "origlibdirs"
      ],
      "anonymized_code": "def main(var_1=False):\n    global var_2, var_3\n\n    #these get prefixes with '/usr' and '/usr/local' or the $LOCALBASE\n    var_2 = ['/include', '/include/SDL2']\n    var_3 = ['/lib', '/lib64', '/X11R6/lib']\n\n    # If we are on a debian based system, we also need to handle\n    # /lib/<multiarch-tuple>\n    # We have a few commands to get the correct <multiarch-tuple>, we try those\n",
      "lines_processed": 10,
      "total_lines": 281
    },
    {
      "file_path": "config_msys2.py",
      "code": "def _add_sdl2_dll_deps(DEPS):\n    # MIXER\n    DEPS.add_dll(r'(libvorbis-0|vorbis)\\.dll$', 'vorbis', ['libvorbis-[1-9].*'],\n                 ['ogg'])\n    DEPS.add_dll(r'(libvorbisfile-3|vorbisfile)\\.dll$', 'vorbisfile',\n                 link_lib='vorbis', libs=['vorbis'])\n    DEPS.add_dll(r'(libogg-0|ogg)\\.dll$', 'ogg', ['libogg-[1-9].*'])\n    DEPS.add_dll(r'(lib)?FLAC[-0-9]*\\.dll$', 'flac', ['*FLAC-[0-9]*'])\n    DEPS.add_dll(r'(lib)?modplug[-0-9]*\\.dll$', 'modplug', ['*modplug-[0-9]*'])\n    DEPS.add_dll(r'(lib)?mpg123[-0-9]*\\.dll$', 'mpg123', ['*mpg123-[0-9]*'])\n",
      "variables": [
        "DEPS"
      ],
      "anonymized_code": "def _add_sdl2_dll_deps(var_1):\n    # MIXER\n    var_1.add_dll(r'(libvorbis-0|vorbis)\\.dll$', 'vorbis', ['libvorbis-[1-9].*'],\n                 ['ogg'])\n    var_1.add_dll(r'(libvorbisfile-3|vorbisfile)\\.dll$', 'vorbisfile',\n                 link_lib='vorbis', libs=['vorbis'])\n    var_1.add_dll(r'(libogg-0|ogg)\\.dll$', 'ogg', ['libogg-[1-9].*'])\n    var_1.add_dll(r'(lib)?FLAC[-0-9]*\\.dll$', 'flac', ['*FLAC-[0-9]*'])\n    var_1.add_dll(r'(lib)?modplug[-0-9]*\\.dll$', 'modplug', ['*modplug-[0-9]*'])\n    var_1.add_dll(r'(lib)?mpg123[-0-9]*\\.dll$', 'mpg123', ['*mpg123-[0-9]*'])\n",
      "lines_processed": 10,
      "total_lines": 501
    },
    {
      "file_path": "config_msys2.py",
      "code": "def as_machine_type(size):\n    \"\"\"Return pointer bit size as a Windows machine type\"\"\"\n    if size == 32:\n        return \"x86\"\n    if size == 64:\n        return \"x64\"\n    raise ValueError(\"Unknown pointer size {}\".format(size))\n",
      "variables": [
        "size"
      ],
      "anonymized_code": "def as_machine_type(var_1):\n    \"\"\"Return pointer bit var_1 as a Windows machine type\"\"\"\n    if var_1 == 32:\n        return \"x86\"\n    if var_1 == 64:\n        return \"x64\"\n    raise ValueError(\"Unknown pointer var_1 {}\".format(var_1))\n",
      "lines_processed": 7,
      "total_lines": 501
    },
    {
      "file_path": "download_msys2_prebuilt.py",
      "code": "def install_prebuilts(arch):\n    \"\"\"For installing prebuilt dependencies.\"\"\"\n    errors = False\n    print(\"Installing pre-built dependencies\")\n    for pkg in get_packages(arch):\n        print(f\"Installing {pkg}\")\n        error = install_pacman_package(pkg)\n        errors = errors or error\n    if errors:\n        raise Exception(\"Some dependencies could not be installed\")\n",
      "variables": [
        "arch",
        "errors",
        "pkg",
        "error"
      ],
      "anonymized_code": "def install_prebuilts(var_1):\n    \"\"\"For installing prebuilt dependencies.\"\"\"\n    var_2 = False\n    print(\"Installing pre-built dependencies\")\n    for var_3 in get_packages(var_1):\n        print(f\"Installing {var_3}\")\n        var_4 = install_pacman_package(var_3)\n        var_2 = var_2 or var_4\n    if var_2:\n        raise Exception(\"Some dependencies could not be installed\")\n",
      "lines_processed": 10,
      "total_lines": 134
    },
    {
      "file_path": "download_msys2_prebuilt.py",
      "code": "def update(arch=None):\n    install_prebuilts(arch if arch else detect_arch())\n",
      "variables": [
        "arch"
      ],
      "anonymized_code": "def update(var_1=None):\n    install_prebuilts(var_1 if var_1 else detect_arch())\n",
      "lines_processed": 2,
      "total_lines": 134
    }
  ]
}