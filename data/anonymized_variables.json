{
  "open-mmlab_mmdetection": [
    {
      "file_path": "gather_models.py",
      "code": "def convert_model_info_to_pwc(model_infos):\n    pwc_files = {}\n    for model in model_infos:\n        cfg_folder_name = osp.split(model['config'])[-2]\n        pwc_model_info = OrderedDict()\n        pwc_model_info['Name'] = osp.split(model['config'])[-1].split('.')[0]\n        pwc_model_info['In Collection'] = 'Please fill in Collection name'\n        pwc_model_info['Config'] = osp.join('configs', model['config'])\n\n        # get metadata\n        meta_data = OrderedDict()\n        if 'epochs' in model:\n            meta_data['Epochs'] = get_real_epoch_or_iter(model['config'])\n        else:\n            meta_data['Iterations'] = get_real_epoch_or_iter(model['config'])\n        pwc_model_info['Metadata'] = meta_data\n\n        # get dataset name\n        dataset_name = get_dataset_name(model['config'])\n",
      "variables": [
        "model_infos",
        "pwc_files",
        "model",
        "cfg_folder_name",
        "pwc_model_info",
        "meta_data",
        "dataset_name"
      ],
      "anonymized_code": "def convert_model_info_to_pwc(var_1):\n    var_2 = {}\n    for var_3 in var_1:\n        var_4 = osp.split(var_3['config'])[-2]\n        var_5 = OrderedDict()\n        var_5['Name'] = osp.split(var_3['config'])[-1].split('.')[0]\n        var_5['In Collection'] = 'Please fill in Collection name'\n        var_5['Config'] = osp.join('configs', var_3['config'])\n\n        # get metadata\n        var_6 = OrderedDict()\n        if 'epochs' in var_3:\n            var_6['Epochs'] = get_real_epoch_or_iter(var_3['config'])\n        else:\n            var_6['Iterations'] = get_real_epoch_or_iter(var_3['config'])\n        var_5['Metadata'] = var_6\n\n        # get dataset name\n        var_7 = get_dataset_name(var_3['config'])\n",
      "lines_processed": 19,
      "total_lines": 308
    },
    {
      "file_path": "gather_models.py",
      "code": "def get_real_epoch_or_iter(config):\n    cfg = Config.fromfile('./configs/' + config)\n    if cfg.train_cfg.type == 'EpochBasedTrainLoop':\n        epoch = cfg.train_cfg.max_epochs\n        return epoch\n    else:\n        return cfg.train_cfg.max_iters\n",
      "variables": [
        "config",
        "cfg",
        "epoch"
      ],
      "anonymized_code": "def get_real_epoch_or_iter(var_1):\n    var_2 = Config.fromfile('./configs/' + var_1)\n    if var_2.train_cfg.type == 'EpochBasedTrainLoop':\n        var_3 = var_2.train_cfg.max_epochs\n        return var_3\n    else:\n        return var_2.train_cfg.max_iters\n",
      "lines_processed": 7,
      "total_lines": 308
    },
    {
      "file_path": "gather_models.py",
      "code": "def get_best_epoch_or_iter(exp_dir):\n    best_epoch_iter_full_path = list(\n        sorted(glob.glob(osp.join(exp_dir, 'best_*.pth'))))[-1]\n    best_epoch_or_iter_model_path = best_epoch_iter_full_path.split('/')[-1]\n    best_epoch_or_iter = best_epoch_or_iter_model_path.\\\n        split('_')[-1].split('.')[0]\n    return best_epoch_or_iter_model_path, int(best_epoch_or_iter)\n",
      "variables": [
        "exp_dir",
        "best_epoch_iter_full_path",
        "best_epoch_or_iter_model_path",
        "best_epoch_or_iter"
      ],
      "anonymized_code": "def get_best_epoch_or_iter(var_1):\n    var_2 = list(\n        sorted(glob.glob(osp.join(var_1, 'best_*.pth'))))[-1]\n    var_3 = var_2.split('/')[-1]\n    var_4 = var_3.\\\n        split('_')[-1].split('.')[0]\n    return var_3, int(var_4)\n",
      "lines_processed": 7,
      "total_lines": 308
    },
    {
      "file_path": "gather_models.py",
      "code": "def main():\n    args = parse_args()\n    models_root = args.root\n    models_out = args.out\n    mkdir_or_exist(models_out)\n\n    # find all models in the root directory to be gathered\n    raw_configs = list(scandir('./configs', '.py', recursive=True))\n\n    # filter configs that is not trained in the experiments dir\n    used_configs = []\n    for raw_config in raw_configs:\n        if osp.exists(osp.join(models_root, raw_config)):\n            used_configs.append(raw_config)\n    print(f'Find {len(used_configs)} models to be gathered')\n\n    # find final_ckpt and log file for trained each config\n    # and parse the best performance\n    model_infos = []\n",
      "variables": [
        "args",
        "models_root",
        "models_out",
        "raw_configs",
        "used_configs",
        "raw_config",
        "model_infos"
      ],
      "anonymized_code": "def main():\n    var_1 = parse_args()\n    var_2 = var_1.root\n    var_3 = var_1.out\n    mkdir_or_exist(var_3)\n\n    # find all models in the root directory to be gathered\n    var_4 = list(scandir('./configs', '.py', recursive=True))\n\n    # filter configs that is not trained in the experiments dir\n    var_5 = []\n    for var_6 in var_4:\n        if osp.exists(osp.join(var_2, var_6)):\n            var_5.append(var_6)\n    print(f'Find {len(var_5)} models to be gathered')\n\n    # find final_ckpt and log file for trained each config\n    # and parse the best performance\n    var_7 = []\n",
      "lines_processed": 19,
      "total_lines": 308
    },
    {
      "file_path": "gather_models.py",
      "code": "def ordered_yaml_dump(data, stream=None, Dumper=yaml.SafeDumper, **kwds):\n\n    class OrderedDumper(Dumper):\n        pass\n\n    def _dict_representer(dumper, data):\n        return dumper.represent_mapping(\n            yaml.resolver.BaseResolver.DEFAULT_MAPPING_TAG, data.items())\n\n    OrderedDumper.add_representer(OrderedDict, _dict_representer)\n    return yaml.dump(data, stream, OrderedDumper, **kwds)\n",
      "variables": [
        "data",
        "stream",
        "Dumper",
        "kwds",
        "dumper"
      ],
      "anonymized_code": "def ordered_yaml_dump(var_1, var_2=None, var_3=yaml.SafeDumper, **var_4):\n\n    class OrderedDumper(var_3):\n        pass\n\n    def _dict_representer(var_5, var_1):\n        return var_5.represent_mapping(\n            yaml.resolver.BaseResolver.DEFAULT_MAPPING_TAG, var_1.items())\n\n    OrderedDumper.add_representer(OrderedDict, _dict_representer)\n    return yaml.dump(var_1, var_2, OrderedDumper, **var_4)\n",
      "lines_processed": 11,
      "total_lines": 308
    },
    {
      "file_path": "gather_models.py",
      "code": "def process_checkpoint(in_file, out_file):\n    checkpoint = torch.load(in_file, map_location='cpu')\n    # remove optimizer for smaller file size\n    if 'optimizer' in checkpoint:\n        del checkpoint['optimizer']\n    if 'ema_state_dict' in checkpoint:\n        del checkpoint['ema_state_dict']\n\n    # remove ema state_dict\n    for key in list(checkpoint['state_dict']):\n        if key.startswith('ema_'):\n            checkpoint['state_dict'].pop(key)\n        elif key.startswith('data_preprocessor'):\n            checkpoint['state_dict'].pop(key)\n\n    # if it is necessary to remove some sensitive data in checkpoint['meta'],\n    # add the code here.\n    if digit_version(torch.__version__) >= digit_version('1.6'):\n        torch.save(checkpoint, out_file, _use_new_zipfile_serialization=False)\n",
      "variables": [
        "in_file",
        "out_file",
        "checkpoint",
        "key"
      ],
      "anonymized_code": "def process_checkpoint(var_1, var_2):\n    var_3 = torch.load(var_1, map_location='cpu')\n    # remove optimizer for smaller file size\n    if 'optimizer' in var_3:\n        del var_3['optimizer']\n    if 'ema_state_dict' in var_3:\n        del var_3['ema_state_dict']\n\n    # remove ema state_dict\n    for var_4 in list(var_3['state_dict']):\n        if var_4.startswith('ema_'):\n            var_3['state_dict'].pop(var_4)\n        elif var_4.startswith('data_preprocessor'):\n            var_3['state_dict'].pop(var_4)\n\n    # if it is necessary to remove some sensitive data in var_3['meta'],\n    # add the code here.\n    if digit_version(torch.__version__) >= digit_version('1.6'):\n        torch.save(var_3, var_2, _use_new_zipfile_serialization=False)\n",
      "lines_processed": 19,
      "total_lines": 308
    },
    {
      "file_path": "gather_models.py",
      "code": "def get_dataset_name(config):\n    # If there are more dataset, add here.\n    name_map = dict(\n        CityscapesDataset='Cityscapes',\n        CocoDataset='COCO',\n        CocoPanopticDataset='COCO',\n        DeepFashionDataset='Deep Fashion',\n        LVISV05Dataset='LVIS v0.5',\n        LVISV1Dataset='LVIS v1',\n        VOCDataset='Pascal VOC',\n        WIDERFaceDataset='WIDER Face',\n        OpenImagesDataset='OpenImagesDataset',\n        OpenImagesChallengeDataset='OpenImagesChallengeDataset',\n        Objects365V1Dataset='Objects365 v1',\n        Objects365V2Dataset='Objects365 v2')\n    cfg = Config.fromfile('./configs/' + config)\n    return name_map[cfg.dataset_type]\n",
      "variables": [
        "config",
        "name_map",
        "cfg"
      ],
      "anonymized_code": "def get_dataset_name(var_1):\n    # If there are more dataset, add here.\n    var_2 = dict(\n        CityscapesDataset='Cityscapes',\n        CocoDataset='COCO',\n        CocoPanopticDataset='COCO',\n        DeepFashionDataset='Deep Fashion',\n        LVISV05Dataset='LVIS v0.5',\n        LVISV1Dataset='LVIS v1',\n        VOCDataset='Pascal VOC',\n        WIDERFaceDataset='WIDER Face',\n        OpenImagesDataset='OpenImagesDataset',\n        OpenImagesChallengeDataset='OpenImagesChallengeDataset',\n        Objects365V1Dataset='Objects365 v1',\n        Objects365V2Dataset='Objects365 v2')\n    var_3 = Config.fromfile('./configs/' + var_1)\n    return var_2[var_3.dataset_type]\n",
      "lines_processed": 17,
      "total_lines": 308
    },
    {
      "file_path": "gather_models.py",
      "code": "def get_final_epoch_or_iter(config):\n    cfg = Config.fromfile('./configs/' + config)\n    if cfg.train_cfg.type == 'EpochBasedTrainLoop':\n        return cfg.train_cfg.max_epochs\n    else:\n        return cfg.train_cfg.max_iters\n",
      "variables": [
        "config",
        "cfg"
      ],
      "anonymized_code": "def get_final_epoch_or_iter(var_1):\n    var_2 = Config.fromfile('./configs/' + var_1)\n    if var_2.train_cfg.type == 'EpochBasedTrainLoop':\n        return var_2.train_cfg.max_epochs\n    else:\n        return var_2.train_cfg.max_iters\n",
      "lines_processed": 6,
      "total_lines": 308
    },
    {
      "file_path": "gather_models.py",
      "code": "def is_by_epoch(config):\n    cfg = Config.fromfile('./configs/' + config)\n    return cfg.train_cfg.type == 'EpochBasedTrainLoop'\n",
      "variables": [
        "config",
        "cfg"
      ],
      "anonymized_code": "def is_by_epoch(var_1):\n    var_2 = Config.fromfile('./configs/' + var_1)\n    return var_2.train_cfg.type == 'EpochBasedTrainLoop'\n",
      "lines_processed": 3,
      "total_lines": 308
    },
    {
      "file_path": "benchmark_inference_fps.py",
      "code": "def parse_args():\n    parser = argparse.ArgumentParser(\n        description='MMDet benchmark a model of FPS')\n    parser.add_argument('config', help='test config file path')\n    parser.add_argument('checkpoint_root', help='Checkpoint file root path')\n    parser.add_argument(\n        '--round-num',\n        type=int,\n        default=1,\n        help='round a number to a given precision in decimal digits')\n    parser.add_argument(\n        '--repeat-num',\n        type=int,\n        default=1,\n        help='number of repeat times of measurement for averaging the results')\n    parser.add_argument(\n        '--out', type=str, help='output path of gathered fps to be stored')\n    parser.add_argument(\n        '--max-iter', type=int, default=2000, help='num of max iter')\n",
      "variables": [
        "parser"
      ],
      "anonymized_code": "def parse_args():\n    var_1 = argparse.ArgumentParser(\n        description='MMDet benchmark a model of FPS')\n    var_1.add_argument('config', help='test config file path')\n    var_1.add_argument('checkpoint_root', help='Checkpoint file root path')\n    var_1.add_argument(\n        '--round-num',\n        type=int,\n        default=1,\n        help='round a number to a given precision in decimal digits')\n    var_1.add_argument(\n        '--repeat-num',\n        type=int,\n        default=1,\n        help='number of repeat times of measurement for averaging the results')\n    var_1.add_argument(\n        '--out', type=str, help='output path of gathered fps to be stored')\n    var_1.add_argument(\n        '--max-iter', type=int, default=2000, help='num of max iter')\n",
      "lines_processed": 19,
      "total_lines": 171
    },
    {
      "file_path": "convert_test_benchmark_script.py",
      "code": "def main():\n    args = parse_args()\n    if args.out:\n        out_suffix = args.out.split('.')[-1]\n        assert args.out.endswith('.sh'), \\\n            f'Expected out file path suffix is .sh, but get .{out_suffix}'\n    assert args.out or args.run, \\\n        ('Please specify at least one operation (save/run/ the '\n         'script) with the argument \"--out\" or \"--run\"')\n\n    commands = []\n    partition_name = 'PARTITION=$1 '\n    commands.append(partition_name)\n    commands.append('\\n')\n\n    checkpoint_root = 'CHECKPOINT_DIR=$2 '\n    commands.append(checkpoint_root)\n    commands.append('\\n')\n\n",
      "variables": [
        "args",
        "out_suffix",
        "commands",
        "partition_name",
        "checkpoint_root"
      ],
      "anonymized_code": "def main():\n    var_1 = parse_args()\n    if var_1.out:\n        var_2 = var_1.out.split('.')[-1]\n        assert var_1.out.endswith('.sh'), \\\n            f'Expected out file path suffix is .sh, but get .{var_2}'\n    assert var_1.out or var_1.run, \\\n        ('Please specify at least one operation (save/run/ the '\n         'script) with the argument \"--out\" or \"--run\"')\n\n    var_3 = []\n    var_4 = 'PARTITION=$1 '\n    var_3.append(var_4)\n    var_3.append('\\n')\n\n    var_5 = 'CHECKPOINT_DIR=$2 '\n    var_3.append(var_5)\n    var_3.append('\\n')\n\n",
      "lines_processed": 19,
      "total_lines": 114
    },
    {
      "file_path": "convert_test_benchmark_script.py",
      "code": "def create_test_bash_info(commands, model_test_dict, port, script_name,\n                          partition):\n    config = model_test_dict['config']\n    job_name = model_test_dict['job_name']\n    checkpoint = model_test_dict['checkpoint']\n    work_dir = model_test_dict['work_dir']\n\n    echo_info = f' \\necho \\'{config}\\' &'\n    commands.append(echo_info)\n    commands.append('\\n')\n\n    command_info = f'GPUS=8  GPUS_PER_NODE=8  ' \\\n                   f'CPUS_PER_TASK=$CPUS_PRE_TASK {script_name} '\n\n    command_info += f'{partition} '\n    command_info += f'{job_name} '\n    command_info += f'{config} '\n    command_info += f'$CHECKPOINT_DIR/{checkpoint} '\n    command_info += f'--work-dir {work_dir} '\n",
      "variables": [
        "commands",
        "model_test_dict",
        "port",
        "script_name",
        "partition",
        "config",
        "job_name",
        "checkpoint",
        "work_dir",
        "echo_info",
        "command_info"
      ],
      "anonymized_code": "def create_test_bash_info(var_1, var_2, var_3, var_4,\n                          var_5):\n    var_6 = var_2['var_6']\n    var_7 = var_2['var_7']\n    var_8 = var_2['var_8']\n    var_9 = var_2['var_9']\n\n    var_10 = f' \\necho \\'{var_6}\\' &'\n    var_1.append(var_10)\n    var_1.append('\\n')\n\n    var_11 = f'GPUS=8  GPUS_PER_NODE=8  ' \\\n                   f'CPUS_PER_TASK=$CPUS_PRE_TASK {var_4} '\n\n    var_11 += f'{var_5} '\n    var_11 += f'{var_7} '\n    var_11 += f'{var_6} '\n    var_11 += f'$CHECKPOINT_DIR/{var_8} '\n    var_11 += f'--work-dir {var_9} '\n",
      "lines_processed": 19,
      "total_lines": 114
    },
    {
      "file_path": "convert_test_benchmark_script.py",
      "code": "def parse_args():\n    parser = argparse.ArgumentParser(\n        description='Convert benchmark model list to script')\n    parser.add_argument('config', help='test config file path')\n    parser.add_argument('--port', type=int, default=29666, help='dist port')\n    parser.add_argument(\n        '--run', action='store_true', help='run script directly')\n    parser.add_argument(\n        '--out', type=str, help='path to save model benchmark script')\n\n    args = parser.parse_args()\n    return args\n",
      "variables": [
        "parser",
        "args"
      ],
      "anonymized_code": "def parse_args():\n    var_1 = argparse.ArgumentParser(\n        description='Convert benchmark model list to script')\n    var_1.add_argument('config', help='test config file path')\n    var_1.add_argument('--port', type=int, default=29666, help='dist port')\n    var_1.add_argument(\n        '--run', action='store_true', help='run script directly')\n    var_1.add_argument(\n        '--out', type=str, help='path to save model benchmark script')\n\n    var_2 = var_1.parse_args()\n    return var_2\n",
      "lines_processed": 12,
      "total_lines": 114
    },
    {
      "file_path": "convert_test_benchmark_script.py",
      "code": "def process_model_info(model_info, work_dir):\n    config = model_info['config'].strip()\n    fname, _ = osp.splitext(osp.basename(config))\n    job_name = fname\n    work_dir = '$WORK_DIR/' + fname\n    checkpoint = model_info['checkpoint'].strip()\n    return dict(\n        config=config,\n        job_name=job_name,\n        work_dir=work_dir,\n        checkpoint=checkpoint)\n",
      "variables": [
        "model_info",
        "work_dir",
        "config",
        "fname",
        "_",
        "job_name",
        "checkpoint"
      ],
      "anonymized_code": "def process_model_info(var_1, var_2):\n    var_3 = var_1['var_3'].strip()\n    var_4, var_5 = osp.splitext(osp.basename(var_3))\n    var_6 = var_4\n    var_2 = '$WORK_DIR/' + var_4\n    var_7 = var_1['var_7'].strip()\n    return dict(\n        var_3=var_3,\n        var_6=var_6,\n        var_2=var_2,\n        var_7=var_7)\n",
      "lines_processed": 11,
      "total_lines": 114
    },
    {
      "file_path": "gather_train_benchmark_metric.py",
      "code": "def parse_args():\n    parser = argparse.ArgumentParser(\n        description='Gather benchmarked models metric')\n    parser.add_argument(\n        'root',\n        type=str,\n        help='root path of benchmarked models to be gathered')\n    parser.add_argument(\n        'txt_path', type=str, help='txt path output by benchmark_filter')\n    parser.add_argument(\n        '--out', type=str, help='output path of gathered metrics to be stored')\n    parser.add_argument(\n        '--not-show', action='store_true', help='not show metrics')\n    parser.add_argument(\n        '--excel', type=str, help='input path of excel to be recorded')\n    parser.add_argument(\n        '--ncol', type=int, help='Number of column to be modified or appended')\n\n    args = parser.parse_args()\n",
      "variables": [
        "parser",
        "args"
      ],
      "anonymized_code": "def parse_args():\n    var_1 = argparse.ArgumentParser(\n        description='Gather benchmarked models metric')\n    var_1.add_argument(\n        'root',\n        type=str,\n        help='root path of benchmarked models to be gathered')\n    var_1.add_argument(\n        'txt_path', type=str, help='txt path output by benchmark_filter')\n    var_1.add_argument(\n        '--out', type=str, help='output path of gathered metrics to be stored')\n    var_1.add_argument(\n        '--not-show', action='store_true', help='not show metrics')\n    var_1.add_argument(\n        '--excel', type=str, help='input path of excel to be recorded')\n    var_1.add_argument(\n        '--ncol', type=int, help='Number of column to be modified or appended')\n\n    var_2 = var_1.parse_args()\n",
      "lines_processed": 19,
      "total_lines": 151
    },
    {
      "file_path": "check_links.py",
      "code": "def parse_args():\n    parser = argparse.ArgumentParser(\n        description='Goes through all the inline-links '\n        'in markdown files and reports the breakages')\n    parser.add_argument(\n        '--num-threads',\n        type=int,\n        default=100,\n        help='Number of processes to confirm the link')\n    parser.add_argument('--https-proxy', type=str, help='https proxy')\n    parser.add_argument(\n        '--out',\n        type=str,\n        default='link_reports.txt',\n        help='output path of reports')\n    args = parser.parse_args()\n    return args\n",
      "variables": [
        "parser",
        "args"
      ],
      "anonymized_code": "def parse_args():\n    var_1 = argparse.ArgumentParser(\n        description='Goes through all the inline-links '\n        'in markdown files and reports the breakages')\n    var_1.add_argument(\n        '--num-threads',\n        type=int,\n        default=100,\n        help='Number of processes to confirm the link')\n    var_1.add_argument('--https-proxy', type=str, help='https proxy')\n    var_1.add_argument(\n        '--out',\n        type=str,\n        default='link_reports.txt',\n        help='output path of reports')\n    var_2 = var_1.parse_args()\n    return var_2\n",
      "lines_processed": 17,
      "total_lines": 157
    },
    {
      "file_path": "check_links.py",
      "code": "def check_path(match_tuple: MatchTuple) -> bool:\n    \"\"\"Check if a file in this repository exists.\"\"\"\n    relative_path = match_tuple.link.split('#')[0]\n    full_path = os.path.join(\n        os.path.dirname(str(match_tuple.source)), relative_path)\n    return os.path.exists(full_path)\n",
      "variables": [
        "match_tuple",
        "relative_path",
        "full_path"
      ],
      "anonymized_code": "def check_path(var_1: MatchTuple) -> bool:\n    \"\"\"Check if a file in this repository exists.\"\"\"\n    var_2 = var_1.link.split('#')[0]\n    var_3 = os.path.join(\n        os.path.dirname(str(var_1.source)), var_2)\n    return os.path.exists(var_3)\n",
      "lines_processed": 6,
      "total_lines": 157
    },
    {
      "file_path": "check_links.py",
      "code": "def check_url(match_tuple: MatchTuple,\n              http_session: requests.Session) -> Tuple[bool, str]:\n    \"\"\"Check if a URL is reachable.\"\"\"\n    try:\n        result = http_session.head(\n            match_tuple.link, timeout=5, allow_redirects=True)\n        return (\n            result.ok or result.status_code in OK_STATUS_CODES,\n            f'status code = {result.status_code}',\n        )\n    except (requests.ConnectionError, requests.Timeout):\n        return False, 'connection error'\n",
      "variables": [
        "match_tuple",
        "http_session",
        "result"
      ],
      "anonymized_code": "def check_url(var_1: MatchTuple,\n              var_2: requests.Session) -> Tuple[bool, str]:\n    \"\"\"Check if a URL is reachable.\"\"\"\n    try:\n        var_3 = var_2.head(\n            var_1.link, timeout=5, allow_redirects=True)\n        return (\n            var_3.ok or var_3.status_code in OK_STATUS_CODES,\n            f'status code = {var_3.status_code}',\n        )\n    except (requests.ConnectionError, requests.Timeout):\n        return False, 'connection error'\n",
      "lines_processed": 12,
      "total_lines": 157
    },
    {
      "file_path": "check_links.py",
      "code": "def check_link(\n        match_tuple: MatchTuple,\n        http_session: requests.Session,\n        logger: logging = None) -> Tuple[MatchTuple, bool, Optional[str]]:\n    reason: Optional[str] = None\n    if match_tuple.link.startswith('http'):\n        result_ok, reason = check_url(match_tuple, http_session)\n    else:\n        result_ok = check_path(match_tuple)\n    if logger is None:\n        print(f\"  {'\u2713' if result_ok else '\u2717'} {match_tuple.link}\")\n    else:\n        logger.info(f\"  {'\u2713' if result_ok else '\u2717'} {match_tuple.link}\")\n    return match_tuple, result_ok, reason\n",
      "variables": [
        "match_tuple",
        "http_session",
        "logger",
        "reason",
        "result_ok"
      ],
      "anonymized_code": "def check_link(\n        var_1: MatchTuple,\n        var_2: requests.Session,\n        var_3: logging = None) -> Tuple[MatchTuple, bool, Optional[str]]:\n    var_4: Optional[str] = None\n    if var_1.link.startswith('http'):\n        var_5, var_4 = check_url(var_1, var_2)\n    else:\n        var_5 = check_path(var_1)\n    if var_3 is None:\n        print(f\"  {'\u2713' if var_5 else '\u2717'} {var_1.link}\")\n    else:\n        var_3.info(f\"  {'\u2713' if var_5 else '\u2717'} {var_1.link}\")\n    return var_1, var_5, var_4\n",
      "lines_processed": 14,
      "total_lines": 157
    },
    {
      "file_path": "benchmark_valid_flops.py",
      "code": "def show_summary(summary_data, args):\n    table = Table(title='Validation Benchmark Regression Summary')\n    table.add_column('Model')\n    table.add_column('Validation')\n    table.add_column('Resolution (c, h, w)')\n    if args.flops:\n        table.add_column('Flops', justify='right', width=11)\n        table.add_column('Params', justify='right')\n\n    for model_name, summary in summary_data.items():\n        row = [model_name]\n        valid = summary['valid']\n        color = 'green' if valid == 'PASS' else 'red'\n        row.append(f'[{color}]{valid}[/{color}]')\n        if valid == 'PASS':\n            row.append(str(summary['resolution']))\n            if args.flops:\n                row.append(str(summary['flops']))\n                row.append(str(summary['params']))\n",
      "variables": [
        "summary_data",
        "args",
        "table",
        "model_name",
        "summary",
        "row",
        "valid",
        "color"
      ],
      "anonymized_code": "def show_summary(var_1, var_2):\n    var_3 = Table(title='Validation Benchmark Regression Summary')\n    var_3.add_column('Model')\n    var_3.add_column('Validation')\n    var_3.add_column('Resolution (c, h, w)')\n    if var_2.flops:\n        var_3.add_column('Flops', justify='right', width=11)\n        var_3.add_column('Params', justify='right')\n\n    for var_4, var_5 in var_1.items():\n        var_6 = [var_4]\n        var_7 = var_5['var_7']\n        var_8 = 'green' if var_7 == 'PASS' else 'red'\n        var_6.append(f'[{var_8}]{var_7}[/{var_8}]')\n        if var_7 == 'PASS':\n            var_6.append(str(var_5['resolution']))\n            if var_2.flops:\n                var_6.append(str(var_5['flops']))\n                var_6.append(str(var_5['params']))\n",
      "lines_processed": 19,
      "total_lines": 295
    },
    {
      "file_path": "benchmark_filter.py",
      "code": "def main():\n    args = parse_args()\n\n    benchmark_type = []\n    if args.basic_arch:\n        benchmark_type += basic_arch_root\n    if args.datasets:\n        benchmark_type += datasets_root\n    if args.data_pipeline:\n        benchmark_type += data_pipeline_root\n    if args.nn_module:\n        benchmark_type += nn_module_root\n\n    special_model = args.model_options\n    if special_model is not None:\n        benchmark_type += special_model\n\n    config_dpath = 'configs/'\n    benchmark_configs = []\n",
      "variables": [
        "args",
        "benchmark_type",
        "special_model",
        "config_dpath",
        "benchmark_configs"
      ],
      "anonymized_code": "def main():\n    var_1 = parse_args()\n\n    var_2 = []\n    if var_1.basic_arch:\n        var_2 += basic_arch_root\n    if var_1.datasets:\n        var_2 += datasets_root\n    if var_1.data_pipeline:\n        var_2 += data_pipeline_root\n    if var_1.nn_module:\n        var_2 += nn_module_root\n\n    var_3 = var_1.model_options\n    if var_3 is not None:\n        var_2 += var_3\n\n    var_4 = 'configs/'\n    var_5 = []\n",
      "lines_processed": 19,
      "total_lines": 167
    },
    {
      "file_path": "convert_train_benchmark_script.py",
      "code": "def determine_gpus(cfg_name):\n    gpus = 8\n    gpus_pre_node = 8\n\n    if cfg_name.find('16x') >= 0:\n        gpus = 16\n    elif cfg_name.find('4xb4') >= 0:\n        gpus = 4\n        gpus_pre_node = 4\n    elif 'lad' in cfg_name:\n        gpus = 2\n        gpus_pre_node = 2\n\n    return gpus, gpus_pre_node\n",
      "variables": [
        "cfg_name",
        "gpus",
        "gpus_pre_node"
      ],
      "anonymized_code": "def determine_gpus(var_1):\n    var_2 = 8\n    var_3 = 8\n\n    if var_1.find('16x') >= 0:\n        var_2 = 16\n    elif var_1.find('4xb4') >= 0:\n        var_2 = 4\n        var_3 = 4\n    elif 'lad' in var_1:\n        var_2 = 2\n        var_3 = 2\n\n    return var_2, var_3\n",
      "lines_processed": 14,
      "total_lines": 104
    },
    {
      "file_path": "convert_train_benchmark_script.py",
      "code": "def main():\n    args = parse_args()\n    if args.out:\n        out_suffix = args.out.split('.')[-1]\n        assert args.out.endswith('.sh'), \\\n            f'Expected out file path suffix is .sh, but get .{out_suffix}'\n    assert args.out or args.run, \\\n        ('Please specify at least one operation (save/run/ the '\n         'script) with the argument \"--out\" or \"--run\"')\n\n    root_name = './tools'\n    train_script_name = osp.join(root_name, 'slurm_train.sh')\n\n    commands = []\n    partition_name = 'PARTITION=$1 '\n    commands.append(partition_name)\n    commands.append('\\n')\n\n    work_dir = 'WORK_DIR=$2 '\n",
      "variables": [
        "args",
        "out_suffix",
        "root_name",
        "train_script_name",
        "commands",
        "partition_name",
        "work_dir"
      ],
      "anonymized_code": "def main():\n    var_1 = parse_args()\n    if var_1.out:\n        var_2 = var_1.out.split('.')[-1]\n        assert var_1.out.endswith('.sh'), \\\n            f'Expected out file path suffix is .sh, but get .{var_2}'\n    assert var_1.out or var_1.run, \\\n        ('Please specify at least one operation (save/run/ the '\n         'script) with the argument \"--out\" or \"--run\"')\n\n    var_3 = './tools'\n    var_4 = osp.join(var_3, 'slurm_train.sh')\n\n    var_5 = []\n    var_6 = 'PARTITION=$1 '\n    var_5.append(var_6)\n    var_5.append('\\n')\n\n    var_7 = 'WORK_DIR=$2 '\n",
      "lines_processed": 19,
      "total_lines": 104
    },
    {
      "file_path": "convert_train_benchmark_script.py",
      "code": "def parse_args():\n    parser = argparse.ArgumentParser(\n        description='Convert benchmark model json to script')\n    parser.add_argument(\n        'txt_path', type=str, help='txt path output by benchmark_filter')\n    parser.add_argument(\n        '--run', action='store_true', help='run script directly')\n    parser.add_argument(\n        '--out', type=str, help='path to save model benchmark script')\n\n    args = parser.parse_args()\n    return args\n",
      "variables": [
        "parser",
        "args"
      ],
      "anonymized_code": "def parse_args():\n    var_1 = argparse.ArgumentParser(\n        description='Convert benchmark model json to script')\n    var_1.add_argument(\n        'txt_path', type=str, help='txt path output by benchmark_filter')\n    var_1.add_argument(\n        '--run', action='store_true', help='run script directly')\n    var_1.add_argument(\n        '--out', type=str, help='path to save model benchmark script')\n\n    var_2 = var_1.parse_args()\n    return var_2\n",
      "lines_processed": 12,
      "total_lines": 104
    },
    {
      "file_path": "download_checkpoints.py",
      "code": "def parse_args():\n    parser = argparse.ArgumentParser(description='Download checkpoints')\n    parser.add_argument('config', help='test config file path')\n    parser.add_argument(\n        'out', type=str, help='output dir of checkpoints to be stored')\n    parser.add_argument(\n        '--nproc', type=int, default=16, help='num of Processes')\n    parser.add_argument(\n        '--intranet',\n        action='store_true',\n        help='switch to internal network url')\n    args = parser.parse_args()\n    return args\n",
      "variables": [
        "parser",
        "args"
      ],
      "anonymized_code": "def parse_args():\n    var_1 = argparse.ArgumentParser(description='Download checkpoints')\n    var_1.add_argument('config', help='test config file path')\n    var_1.add_argument(\n        'out', type=str, help='output dir of checkpoints to be stored')\n    var_1.add_argument(\n        '--nproc', type=int, default=16, help='num of Processes')\n    var_1.add_argument(\n        '--intranet',\n        action='store_true',\n        help='switch to internal network url')\n    var_2 = var_1.parse_args()\n    return var_2\n",
      "lines_processed": 13,
      "total_lines": 83
    },
    {
      "file_path": "download_checkpoints.py",
      "code": "def download(url, out_file, min_bytes=math.pow(1024, 2), progress=True):\n    # math.pow(1024, 2) is mean 1 MB\n    assert_msg = f\"Downloaded url '{url}' does not exist \" \\\n                 f'or size is < min_bytes={min_bytes}'\n    try:\n        print(f'Downloading {url} to {out_file}...')\n        torch.hub.download_url_to_file(url, str(out_file), progress=progress)\n        assert osp.exists(\n            out_file) and osp.getsize(out_file) > min_bytes, assert_msg\n    except Exception as e:\n        if osp.exists(out_file):\n            os.remove(out_file)\n        print(f'ERROR: {e}\\nRe-attempting {url} to {out_file} ...')\n        os.system(f\"curl -L '{url}' -o '{out_file}' --retry 3 -C -\"\n                  )  # curl download, retry and resume on fail\n    finally:\n        if osp.exists(out_file) and osp.getsize(out_file) < min_bytes:\n            os.remove(out_file)  # remove partial downloads\n\n",
      "variables": [
        "url",
        "out_file",
        "min_bytes",
        "progress",
        "assert_msg"
      ],
      "anonymized_code": "def download(var_1, var_2, var_3=math.pow(1024, 2), var_4=True):\n    # math.pow(1024, 2) is mean 1 MB\n    var_5 = f\"Downloaded var_1 '{var_1}' does not exist \" \\\n                 f'or size is < var_3={var_3}'\n    try:\n        print(f'Downloading {var_1} to {var_2}...')\n        torch.hub.download_url_to_file(var_1, str(var_2), var_4=var_4)\n        assert osp.exists(\n            var_2) and osp.getsize(var_2) > var_3, var_5\n    except Exception as e:\n        if osp.exists(var_2):\n            os.remove(var_2)\n        print(f'ERROR: {e}\\nRe-attempting {var_1} to {var_2} ...')\n        os.system(f\"curl -L '{var_1}' -o '{var_2}' --retry 3 -C -\"\n                  )  # curl download, retry and resume on fail\n    finally:\n        if osp.exists(var_2) and osp.getsize(var_2) < var_3:\n            os.remove(var_2)  # remove partial downloads\n\n",
      "lines_processed": 19,
      "total_lines": 83
    },
    {
      "file_path": "gather_test_benchmark_metric.py",
      "code": "def parse_args():\n    parser = argparse.ArgumentParser(\n        description='Gather benchmarked models metric')\n    parser.add_argument('config', help='test config file path')\n    parser.add_argument(\n        'root',\n        type=str,\n        help='root path of benchmarked models to be gathered')\n    parser.add_argument(\n        '--out', type=str, help='output path of gathered metrics to be stored')\n    parser.add_argument(\n        '--not-show', action='store_true', help='not show metrics')\n    parser.add_argument(\n        '--show-all', action='store_true', help='show all model metrics')\n\n    args = parser.parse_args()\n    return args\n",
      "variables": [
        "parser",
        "args"
      ],
      "anonymized_code": "def parse_args():\n    var_1 = argparse.ArgumentParser(\n        description='Gather benchmarked models metric')\n    var_1.add_argument('config', help='test config file path')\n    var_1.add_argument(\n        'root',\n        type=str,\n        help='root path of benchmarked models to be gathered')\n    var_1.add_argument(\n        '--out', type=str, help='output path of gathered metrics to be stored')\n    var_1.add_argument(\n        '--not-show', action='store_true', help='not show metrics')\n    var_1.add_argument(\n        '--show-all', action='store_true', help='show all model metrics')\n\n    var_2 = var_1.parse_args()\n    return var_2\n",
      "lines_processed": 17,
      "total_lines": 96
    },
    {
      "file_path": "benchmark_test_image.py",
      "code": "def inference_model(config_name, checkpoint, visualizer, args, logger=None):\n    cfg = Config.fromfile(config_name)\n    if args.aug:\n        raise NotImplementedError()\n\n    model = init_detector(\n        cfg, checkpoint, palette=args.palette, device=args.device)\n    visualizer.dataset_meta = model.dataset_meta\n\n    # test a single image\n    result = inference_detector(model, args.img)\n\n    # show the results\n    if args.show or args.out_dir is not None:\n        img = mmcv.imread(args.img)\n        img = mmcv.imconvert(img, 'bgr', 'rgb')\n        out_file = None\n        if args.out_dir is not None:\n            out_dir = args.out_dir\n",
      "variables": [
        "config_name",
        "checkpoint",
        "visualizer",
        "args",
        "logger",
        "cfg",
        "model",
        "result",
        "img",
        "out_file",
        "out_dir"
      ],
      "anonymized_code": "def inference_model(var_1, var_2, var_3, var_4, var_5=None):\n    var_6 = Config.fromfile(var_1)\n    if var_4.aug:\n        raise NotImplementedError()\n\n    var_7 = init_detector(\n        var_6, var_2, palette=var_4.palette, device=var_4.device)\n    var_3.dataset_meta = var_7.dataset_meta\n\n    # test a single image\n    var_8 = inference_detector(var_7, var_4.var_9)\n\n    # show the results\n    if var_4.show or var_4.var_11 is not None:\n        var_9 = mmcv.imread(var_4.var_9)\n        var_9 = mmcv.imconvert(var_9, 'bgr', 'rgb')\n        var_10 = None\n        if var_4.var_11 is not None:\n            var_11 = var_4.var_11\n",
      "lines_processed": 19,
      "total_lines": 134
    },
    {
      "file_path": "benchmark_test_image.py",
      "code": "def main(args):\n    # register all modules in mmdet into the registries\n    register_all_modules()\n\n    config = Config.fromfile(args.config)\n\n    # init visualizer\n    visualizer_cfg = dict(type='DetLocalVisualizer', name='visualizer')\n    visualizer = VISUALIZERS.build(visualizer_cfg)\n\n    # test single model\n    if args.model_name:\n        if args.model_name in config:\n            model_infos = config[args.model_name]\n            if not isinstance(model_infos, list):\n                model_infos = [model_infos]\n            model_info = model_infos[0]\n            config_name = model_info['config'].strip()\n            print(f'processing: {config_name}', flush=True)\n",
      "variables": [
        "args",
        "config",
        "visualizer_cfg",
        "visualizer",
        "model_infos",
        "model_info",
        "config_name"
      ],
      "anonymized_code": "def main(var_1):\n    # register all modules in mmdet into the registries\n    register_all_modules()\n\n    var_2 = Config.fromfile(var_1.var_2)\n\n    # init var_4\n    var_3 = dict(type='DetLocalVisualizer', name='var_4')\n    var_4 = VISUALIZERS.build(var_3)\n\n    # test single model\n    if var_1.model_name:\n        if var_1.model_name in var_2:\n            var_5 = var_2[var_1.model_name]\n            if not isinstance(var_5, list):\n                var_5 = [var_5]\n            var_6 = var_5[0]\n            var_7 = var_6['var_2'].strip()\n            print(f'processing: {var_7}', flush=True)\n",
      "lines_processed": 19,
      "total_lines": 134
    },
    {
      "file_path": "benchmark_test.py",
      "code": "def fast_test_model(config_name, checkpoint, args, logger=None):\n    cfg = Config.fromfile(config_name)\n    cfg = replace_cfg_vals(cfg)\n    cfg.launcher = args.launcher\n    if args.cfg_options is not None:\n        cfg.merge_from_dict(args.cfg_options)\n\n    # work_dir is determined in this priority: CLI > segment in file > filename\n    if args.work_dir is not None:\n        # update configs according to CLI args if args.work_dir is not None\n        cfg.work_dir = osp.join(args.work_dir,\n                                osp.splitext(osp.basename(config_name))[0])\n    elif cfg.get('work_dir', None) is None:\n        # use config filename as default work_dir if cfg.work_dir is None\n        cfg.work_dir = osp.join('./work_dirs',\n                                osp.splitext(osp.basename(config_name))[0])\n\n    if args.ceph:\n        replace_to_ceph(cfg)\n",
      "variables": [
        "config_name",
        "checkpoint",
        "args",
        "logger",
        "cfg"
      ],
      "anonymized_code": "def fast_test_model(var_1, var_2, var_3, var_4=None):\n    var_5 = Config.fromfile(var_1)\n    var_5 = replace_cfg_vals(var_5)\n    var_5.launcher = var_3.launcher\n    if var_3.cfg_options is not None:\n        var_5.merge_from_dict(var_3.cfg_options)\n\n    # work_dir is determined in this priority: CLI > segment in file > filename\n    if var_3.work_dir is not None:\n        # update configs according to CLI var_3 if var_3.work_dir is not None\n        var_5.work_dir = osp.join(var_3.work_dir,\n                                osp.splitext(osp.basename(var_1))[0])\n    elif var_5.get('work_dir', None) is None:\n        # use config filename as default work_dir if var_5.work_dir is None\n        var_5.work_dir = osp.join('./work_dirs',\n                                osp.splitext(osp.basename(var_1))[0])\n\n    if var_3.ceph:\n        replace_to_ceph(var_5)\n",
      "lines_processed": 19,
      "total_lines": 115
    },
    {
      "file_path": "benchmark_test.py",
      "code": "def main(args):\n    # register all modules in mmdet into the registries\n    register_all_modules(init_default_scope=False)\n\n    config = Config.fromfile(args.config)\n\n    # test all model\n    logger = MMLogger.get_instance(\n        name='MMLogger',\n        log_file='benchmark_test.log',\n        log_level=logging.ERROR)\n\n    for model_key in config:\n        model_infos = config[model_key]\n        if not isinstance(model_infos, list):\n            model_infos = [model_infos]\n        for model_info in model_infos:\n            print('processing: ', model_info['config'], flush=True)\n            config_name = model_info['config'].strip()\n",
      "variables": [
        "args",
        "config",
        "logger",
        "model_key",
        "model_infos",
        "model_info",
        "config_name"
      ],
      "anonymized_code": "def main(var_1):\n    # register all modules in mmdet into the registries\n    register_all_modules(init_default_scope=False)\n\n    var_2 = Config.fromfile(var_1.var_2)\n\n    # test all model\n    var_3 = MMLogger.get_instance(\n        name='MMLogger',\n        log_file='benchmark_test.log',\n        log_level=logging.ERROR)\n\n    for var_4 in var_2:\n        var_5 = var_2[var_4]\n        if not isinstance(var_5, list):\n            var_5 = [var_5]\n        for var_6 in var_5:\n            print('processing: ', var_6['var_2'], flush=True)\n            var_7 = var_6['var_2'].strip()\n",
      "lines_processed": 19,
      "total_lines": 115
    },
    {
      "file_path": "benchmark_train.py",
      "code": "def parse_args():\n    parser = ArgumentParser()\n    parser.add_argument('config', help='test config file path')\n    parser.add_argument('--work-dir', help='the dir to save logs and models')\n    parser.add_argument('--ceph', action='store_true')\n    parser.add_argument('--save-ckpt', action='store_true')\n    parser.add_argument(\n        '--amp',\n        action='store_true',\n        default=False,\n        help='enable automatic-mixed-precision training')\n    parser.add_argument(\n        '--auto-scale-lr',\n        action='store_true',\n        help='enable automatically scaling LR.')\n    parser.add_argument(\n        '--resume',\n        action='store_true',\n        help='resume from the latest checkpoint in the work_dir automatically')\n",
      "variables": [
        "parser"
      ],
      "anonymized_code": "def parse_args():\n    var_1 = ArgumentParser()\n    var_1.add_argument('config', help='test config file path')\n    var_1.add_argument('--work-dir', help='the dir to save logs and models')\n    var_1.add_argument('--ceph', action='store_true')\n    var_1.add_argument('--save-ckpt', action='store_true')\n    var_1.add_argument(\n        '--amp',\n        action='store_true',\n        default=False,\n        help='enable automatic-mixed-precision training')\n    var_1.add_argument(\n        '--auto-scale-lr',\n        action='store_true',\n        help='enable automatically scaling LR.')\n    var_1.add_argument(\n        '--resume',\n        action='store_true',\n        help='resume from the latest checkpoint in the work_dir automatically')\n",
      "lines_processed": 19,
      "total_lines": 178
    },
    {
      "file_path": "benchmark_train.py",
      "code": "def fast_train_model(config_name, args, logger=None):\n    cfg = Config.fromfile(config_name)\n    cfg = replace_cfg_vals(cfg)\n    cfg.launcher = args.launcher\n    if args.cfg_options is not None:\n        cfg.merge_from_dict(args.cfg_options)\n\n    # work_dir is determined in this priority: CLI > segment in file > filename\n    if args.work_dir is not None:\n        # update configs according to CLI args if args.work_dir is not None\n        cfg.work_dir = osp.join(args.work_dir,\n                                osp.splitext(osp.basename(config_name))[0])\n    elif cfg.get('work_dir', None) is None:\n        # use config filename as default work_dir if cfg.work_dir is None\n        cfg.work_dir = osp.join('./work_dirs',\n                                osp.splitext(osp.basename(config_name))[0])\n\n    ckpt_hook = cfg.default_hooks.checkpoint\n    by_epoch = ckpt_hook.get('by_epoch', True)\n",
      "variables": [
        "config_name",
        "args",
        "logger",
        "cfg",
        "ckpt_hook",
        "by_epoch"
      ],
      "anonymized_code": "def fast_train_model(var_1, var_2, var_3=None):\n    var_4 = Config.fromfile(var_1)\n    var_4 = replace_cfg_vals(var_4)\n    var_4.launcher = var_2.launcher\n    if var_2.cfg_options is not None:\n        var_4.merge_from_dict(var_2.cfg_options)\n\n    # work_dir is determined in this priority: CLI > segment in file > filename\n    if var_2.work_dir is not None:\n        # update configs according to CLI var_2 if var_2.work_dir is not None\n        var_4.work_dir = osp.join(var_2.work_dir,\n                                osp.splitext(osp.basename(var_1))[0])\n    elif var_4.get('work_dir', None) is None:\n        # use config filename as default work_dir if var_4.work_dir is None\n        var_4.work_dir = osp.join('./work_dirs',\n                                osp.splitext(osp.basename(var_1))[0])\n\n    var_5 = var_4.default_hooks.checkpoint\n    var_6 = var_5.get('var_6', True)\n",
      "lines_processed": 19,
      "total_lines": 178
    },
    {
      "file_path": "benchmark_train.py",
      "code": "def main(args):\n    # register all modules in mmdet into the registries\n    register_all_modules(init_default_scope=False)\n\n    config = Config.fromfile(args.config)\n\n    # test all model\n    logger = MMLogger.get_instance(\n        name='MMLogger',\n        log_file='benchmark_train.log',\n        log_level=logging.ERROR)\n\n    for model_key in config:\n        model_infos = config[model_key]\n        if not isinstance(model_infos, list):\n            model_infos = [model_infos]\n        for model_info in model_infos:\n            print('processing: ', model_info['config'], flush=True)\n            config_name = model_info['config'].strip()\n",
      "variables": [
        "args",
        "config",
        "logger",
        "model_key",
        "model_infos",
        "model_info",
        "config_name"
      ],
      "anonymized_code": "def main(var_1):\n    # register all modules in mmdet into the registries\n    register_all_modules(init_default_scope=False)\n\n    var_2 = Config.fromfile(var_1.var_2)\n\n    # test all model\n    var_3 = MMLogger.get_instance(\n        name='MMLogger',\n        log_file='benchmark_train.log',\n        log_level=logging.ERROR)\n\n    for var_4 in var_2:\n        var_5 = var_2[var_4]\n        if not isinstance(var_5, list):\n            var_5 = [var_5]\n        for var_6 in var_5:\n            print('processing: ', var_6['var_2'], flush=True)\n            var_7 = var_6['var_2'].strip()\n",
      "lines_processed": 19,
      "total_lines": 178
    }
  ],
  "facebookresearch_fairseq": [
    {
      "file_path": "locallaunch.py",
      "code": "def main(args):\n    job_type = args.jobtype if args.jobtype else None\n    # parse multiple pipelines.\n    pipelines = [Pipeline(fn) for fn in args.yamls.split(\",\")]\n\n    for pipe_id, pipeline in enumerate(pipelines):\n        if not hasattr(pipeline, \"project_dir\"):\n            for job in pipeline[0]:\n                job(job_type=job_type, dryrun=args.dryrun)\n",
      "variables": [
        "args",
        "job_type",
        "pipelines",
        "fn",
        "pipe_id",
        "pipeline",
        "job"
      ],
      "anonymized_code": "def main(var_1):\n    var_2 = var_1.jobtype if var_1.jobtype else None\n    # parse multiple var_3.\n    var_3 = [Pipeline(var_4) for var_4 in var_1.yamls.split(\",\")]\n\n    for var_5, var_6 in enumerate(var_3):\n        if not hasattr(var_6, \"project_dir\"):\n            for var_7 in var_6[0]:\n                var_7(var_2=var_2, dryrun=var_1.dryrun)\n",
      "lines_processed": 9,
      "total_lines": 148
    }
  ],
  "pytorch_pytorch": [
    {
      "file_path": "manylinux1-check.py",
      "code": "def have_compatible_glibc(major, minimum_minor):\n    import ctypes\n\n    process_namespace = ctypes.CDLL(None)\n    try:\n        gnu_get_libc_version = process_namespace.gnu_get_libc_version\n    except AttributeError:\n        # Symbol doesn't exist -> therefore, we are not linked to\n        # glibc.\n        return False\n\n    # Call gnu_get_libc_version, which returns a string like \"2.5\".\n    gnu_get_libc_version.restype = ctypes.c_char_p\n    version_str = gnu_get_libc_version()\n    # py2 / py3 compatibility:\n    if not isinstance(version_str, str):\n        version_str = version_str.decode(\"ascii\")\n\n    # Parse string and check against requested version.\n",
      "variables": [
        "major",
        "minimum_minor",
        "process_namespace",
        "gnu_get_libc_version",
        "version_str"
      ],
      "anonymized_code": "def have_compatible_glibc(var_1, var_2):\n    import ctypes\n\n    var_3 = ctypes.CDLL(None)\n    try:\n        var_4 = var_3.var_4\n    except AttributeError:\n        # Symbol doesn't exist -> therefore, we are not linked to\n        # glibc.\n        return False\n\n    # Call var_4, which returns a string like \"2.5\".\n    var_4.restype = ctypes.c_char_p\n    var_5 = var_4()\n    # py2 / py3 compatibility:\n    if not isinstance(var_5, str):\n        var_5 = var_5.decode(\"ascii\")\n\n    # Parse string and check against requested version.\n",
      "lines_processed": 19,
      "total_lines": 60
    },
    {
      "file_path": "build_aarch64_wheel.py",
      "code": "def list_instances(instance_type: str) -> None:\n    print(f\"All instances of type {instance_type}\")\n    for instance in ec2_instances_of_type(instance_type):\n        ifaces = instance.network_interfaces\n        az = ifaces[0].subnet.availability_zone if len(ifaces) > 0 else None\n        print(\n            f\"{instance.id} {get_instance_name(instance)} {instance.public_dns_name} {instance.state['Name']} {az}\"\n        )\n",
      "variables": [
        "instance_type",
        "instance",
        "ifaces",
        "az"
      ],
      "anonymized_code": "def list_instances(var_1: str) -> None:\n    print(f\"All instances of type {var_1}\")\n    for var_2 in ec2_instances_of_type(var_1):\n        var_3 = var_2.network_interfaces\n        var_4 = var_3[0].subnet.availability_zone if len(var_3) > 0 else None\n        print(\n            f\"{var_2.id} {get_instance_name(var_2)} {var_2.public_dns_name} {var_2.state['Name']} {var_4}\"\n        )\n",
      "lines_processed": 8,
      "total_lines": 1037
    },
    {
      "file_path": "build_aarch64_wheel.py",
      "code": "def start_build(\n    host: RemoteHost,\n    *,\n    branch: str = \"main\",\n    compiler: str = \"gcc-8\",\n    use_conda: bool = True,\n    python_version: str = \"3.8\",\n    pytorch_only: bool = False,\n    pytorch_build_number: Optional[str] = None,\n    shallow_clone: bool = True,\n    enable_mkldnn: bool = False,\n) -> tuple[str, str, str, str, str]:\n    git_clone_flags = \" --depth 1 --shallow-submodules\" if shallow_clone else \"\"\n    if host.using_docker() and not use_conda:\n        print(\"Auto-selecting conda option for docker images\")\n        use_conda = True\n    if not host.using_docker():\n        print(\"Disable mkldnn for host builds\")\n        enable_mkldnn = False\n",
      "variables": [
        "host",
        "branch",
        "compiler",
        "use_conda",
        "python_version",
        "pytorch_only",
        "pytorch_build_number",
        "shallow_clone",
        "enable_mkldnn",
        "git_clone_flags"
      ],
      "anonymized_code": "def start_build(\n    var_1: RemoteHost,\n    *,\n    var_2: str = \"main\",\n    var_3: str = \"gcc-8\",\n    var_4: bool = True,\n    var_5: str = \"3.8\",\n    var_6: bool = False,\n    var_7: Optional[str] = None,\n    var_8: bool = True,\n    var_9: bool = False,\n) -> tuple[str, str, str, str, str]:\n    var_10 = \" --depth 1 --shallow-submodules\" if var_8 else \"\"\n    if var_1.using_docker() and not var_4:\n        print(\"Auto-selecting conda option for docker images\")\n        var_4 = True\n    if not var_1.using_docker():\n        print(\"Disable mkldnn for var_1 builds\")\n        var_9 = False\n",
      "lines_processed": 19,
      "total_lines": 1037
    },
    {
      "file_path": "build_aarch64_wheel.py",
      "code": "def parse_arguments():\n    from argparse import ArgumentParser\n\n    parser = ArgumentParser(\"Builid and test AARCH64 wheels using EC2\")\n    parser.add_argument(\"--key-name\", type=str)\n    parser.add_argument(\"--debug\", action=\"store_true\")\n    parser.add_argument(\"--build-only\", action=\"store_true\")\n    parser.add_argument(\"--test-only\", type=str)\n    group = parser.add_mutually_exclusive_group()\n    group.add_argument(\"--os\", type=str, choices=list(os_amis.keys()))\n    group.add_argument(\"--ami\", type=str)\n    parser.add_argument(\n        \"--python-version\",\n        type=str,\n        choices=[f\"3.{d}\" for d in range(6, 12)],\n        default=None,\n    )\n    parser.add_argument(\"--alloc-instance\", action=\"store_true\")\n    parser.add_argument(\"--list-instances\", action=\"store_true\")\n",
      "variables": [
        "parser",
        "group",
        "d"
      ],
      "anonymized_code": "def parse_arguments():\n    from argparse import ArgumentParser\n\n    var_1 = ArgumentParser(\"Builid and test AARCH64 wheels using EC2\")\n    var_1.add_argument(\"--key-name\", type=str)\n    var_1.add_argument(\"--debug\", action=\"store_true\")\n    var_1.add_argument(\"--build-only\", action=\"store_true\")\n    var_1.add_argument(\"--test-only\", type=str)\n    var_2 = var_1.add_mutually_exclusive_group()\n    var_2.add_argument(\"--os\", type=str, choices=list(os_amis.keys()))\n    var_2.add_argument(\"--ami\", type=str)\n    var_1.add_argument(\n        \"--python-version\",\n        type=str,\n        choices=[f\"3.{var_3}\" for var_3 in range(6, 12)],\n        default=None,\n    )\n    var_1.add_argument(\"--alloc-instance\", action=\"store_true\")\n    var_1.add_argument(\"--list-instances\", action=\"store_true\")\n",
      "lines_processed": 19,
      "total_lines": 1037
    },
    {
      "file_path": "build_aarch64_wheel.py",
      "code": "def build_OpenBLAS(host: RemoteHost, git_clone_flags: str = \"\") -> None:\n    print(\"Building OpenBLAS\")\n    host.run_cmd(\n        f\"git clone https://github.com/xianyi/OpenBLAS -b v0.3.28 {git_clone_flags}\"\n    )\n    make_flags = \"NUM_THREADS=64 USE_OPENMP=1 NO_SHARED=1 DYNAMIC_ARCH=1 TARGET=ARMV8\"\n    host.run_cmd(\n        f\"pushd OpenBLAS && make {make_flags} -j8 && sudo make {make_flags} install && popd && rm -rf OpenBLAS\"\n    )\n",
      "variables": [
        "host",
        "git_clone_flags",
        "make_flags"
      ],
      "anonymized_code": "def build_OpenBLAS(var_1: RemoteHost, var_2: str = \"\") -> None:\n    print(\"Building OpenBLAS\")\n    var_1.run_cmd(\n        f\"git clone https://github.com/xianyi/OpenBLAS -b v0.3.28 {var_2}\"\n    )\n    var_3 = \"NUM_THREADS=64 USE_OPENMP=1 NO_SHARED=1 DYNAMIC_ARCH=1 TARGET=ARMV8\"\n    var_1.run_cmd(\n        f\"pushd OpenBLAS && make {var_3} -j8 && sudo make {var_3} install && popd && rm -rf OpenBLAS\"\n    )\n",
      "lines_processed": 9,
      "total_lines": 1037
    },
    {
      "file_path": "smoke_test.py",
      "code": "def compare_pypi_to_torch_versions(\n    package: str, pypi_version: str, torch_version: str\n) -> None:\n    if pypi_version is None:\n        raise RuntimeError(f\"Can't find {package} in PyPI for Torch: {torch_version}\")\n    if pypi_version.startswith(torch_version):\n        print(f\"Found matching {package}. Torch: {torch_version} PyPI {pypi_version}\")\n    else:\n        raise RuntimeError(\n            f\"Wrong {package} version. Torch: {torch_version} PyPI: {pypi_version}\"\n        )\n",
      "variables": [
        "package",
        "pypi_version",
        "torch_version"
      ],
      "anonymized_code": "def compare_pypi_to_torch_versions(\n    var_1: str, var_2: str, var_3: str\n) -> None:\n    if var_2 is None:\n        raise RuntimeError(f\"Can't find {var_1} in PyPI for Torch: {var_3}\")\n    if var_2.startswith(var_3):\n        print(f\"Found matching {var_1}. Torch: {var_3} PyPI {var_2}\")\n    else:\n        raise RuntimeError(\n            f\"Wrong {var_1} version. Torch: {var_3} PyPI: {var_2}\"\n        )\n",
      "lines_processed": 11,
      "total_lines": 484
    },
    {
      "file_path": "smoke_test.py",
      "code": "def test_cuda_runtime_errors_captured() -> None:\n    cuda_exception_missed = True\n    try:\n        print(\"Testing test_cuda_runtime_errors_captured\")\n        torch._assert_async(torch.tensor(0, device=\"cuda\"))\n        torch._assert_async(torch.tensor(0 + 0j, device=\"cuda\"))\n    except RuntimeError as e:\n        if re.search(\"CUDA\", f\"{e}\"):\n            print(f\"Caught CUDA exception with success: {e}\")\n            cuda_exception_missed = False\n        else:\n            raise e\n    if cuda_exception_missed:\n        raise RuntimeError(\"Expected CUDA RuntimeError but have not received!\")\n",
      "variables": [
        "cuda_exception_missed"
      ],
      "anonymized_code": "def test_cuda_runtime_errors_captured() -> None:\n    var_1 = True\n    try:\n        print(\"Testing test_cuda_runtime_errors_captured\")\n        torch._assert_async(torch.tensor(0, device=\"cuda\"))\n        torch._assert_async(torch.tensor(0 + 0j, device=\"cuda\"))\n    except RuntimeError as e:\n        if re.search(\"CUDA\", f\"{e}\"):\n            print(f\"Caught CUDA exception with success: {e}\")\n            var_1 = False\n        else:\n            raise e\n    if var_1:\n        raise RuntimeError(\"Expected CUDA RuntimeError but have not received!\")\n",
      "lines_processed": 14,
      "total_lines": 484
    },
    {
      "file_path": "smoke_test.py",
      "code": "def load_json_from_basedir(filename: str):\n    try:\n        with open(BASE_DIR / filename) as fptr:\n            return json.load(fptr)\n    except FileNotFoundError as exc:\n        raise ImportError(f\"File {filename} not found error: {exc.strerror}\") from exc\n    except json.JSONDecodeError as exc:\n        raise ImportError(f\"Invalid JSON {filename}\") from exc\n",
      "variables": [
        "filename",
        "fptr"
      ],
      "anonymized_code": "def load_json_from_basedir(var_1: str):\n    try:\n        with open(BASE_DIR / var_1) as var_2:\n            return json.load(var_2)\n    except FileNotFoundError as exc:\n        raise ImportError(f\"File {var_1} not found error: {exc.strerror}\") from exc\n    except json.JSONDecodeError as exc:\n        raise ImportError(f\"Invalid JSON {var_1}\") from exc\n",
      "lines_processed": 8,
      "total_lines": 484
    },
    {
      "file_path": "smoke_test.py",
      "code": "def main() -> None:\n    options = parse_args()\n    print(f\"torch: {torch.__version__}\")\n    print(torch.__config__.parallel_info())\n    # All PyTorch binary builds should be built with OpenMP\n    if not torch.backends.openmp.is_available():\n        raise RuntimeError(\"PyTorch must be built with OpenMP support\")\n\n    check_version(options.package)\n    smoke_test_conv2d()\n    test_linalg()\n    test_numpy()\n\n    if is_cuda_system:\n        test_linalg(\"cuda\")\n        test_cuda_gds_errors_captured()\n\n    if options.package == \"all\":\n        smoke_test_modules()\n",
      "variables": [
        "options"
      ],
      "anonymized_code": "def main() -> None:\n    var_1 = parse_args()\n    print(f\"torch: {torch.__version__}\")\n    print(torch.__config__.parallel_info())\n    # All PyTorch binary builds should be built with OpenMP\n    if not torch.backends.openmp.is_available():\n        raise RuntimeError(\"PyTorch must be built with OpenMP support\")\n\n    check_version(var_1.package)\n    smoke_test_conv2d()\n    test_linalg()\n    test_numpy()\n\n    if is_cuda_system:\n        test_linalg(\"cuda\")\n        test_cuda_gds_errors_captured()\n\n    if var_1.package == \"all\":\n        smoke_test_modules()\n",
      "lines_processed": 19,
      "total_lines": 484
    },
    {
      "file_path": "smoke_test.py",
      "code": "def smoke_test_cuda(\n    package: str,\n    runtime_error_check: str,\n    torch_compile_check: str,\n    pypi_pkg_check: str,\n) -> None:\n    if not torch.cuda.is_available() and is_cuda_system:\n        raise RuntimeError(f\"Expected CUDA {gpu_arch_ver}. However CUDA is not loaded.\")\n\n    if package == \"all\" and is_cuda_system:\n        for module in MODULES:\n            imported_module = importlib.import_module(module[\"name\"])\n            # TBD for vision move extension module to private so it will\n            # be _extention.\n            version = \"N/A\"\n            if module[\"extension\"] == \"extension\":\n                version = imported_module.extension._check_cuda_version()\n            else:\n                version = imported_module._extension._check_cuda_version()\n",
      "variables": [
        "package",
        "runtime_error_check",
        "torch_compile_check",
        "pypi_pkg_check",
        "module",
        "imported_module",
        "version"
      ],
      "anonymized_code": "def smoke_test_cuda(\n    var_1: str,\n    var_2: str,\n    var_3: str,\n    var_4: str,\n) -> None:\n    if not torch.cuda.is_available() and is_cuda_system:\n        raise RuntimeError(f\"Expected CUDA {gpu_arch_ver}. However CUDA is not loaded.\")\n\n    if var_1 == \"all\" and is_cuda_system:\n        for var_5 in MODULES:\n            var_6 = importlib.import_module(var_5[\"name\"])\n            # TBD for vision move extension var_5 to private so it will\n            # be _extention.\n            var_7 = \"N/A\"\n            if var_5[\"extension\"] == \"extension\":\n                var_7 = var_6.extension._check_cuda_version()\n            else:\n                var_7 = var_6._extension._check_cuda_version()\n",
      "lines_processed": 19,
      "total_lines": 484
    },
    {
      "file_path": "aarch64_wheel_ci_build.py",
      "code": "def parse_arguments():\n    \"\"\"\n    Parse inline arguments\n    \"\"\"\n    from argparse import ArgumentParser\n\n    parser = ArgumentParser(\"AARCH64 wheels python CD\")\n    parser.add_argument(\"--debug\", action=\"store_true\")\n    parser.add_argument(\"--build-only\", action=\"store_true\")\n    parser.add_argument(\"--test-only\", type=str)\n    parser.add_argument(\"--enable-mkldnn\", action=\"store_true\")\n    parser.add_argument(\"--enable-cuda\", action=\"store_true\")\n    return parser.parse_args()\n",
      "variables": [
        "parser"
      ],
      "anonymized_code": "def parse_arguments():\n    \"\"\"\n    Parse inline arguments\n    \"\"\"\n    from argparse import ArgumentParser\n\n    var_1 = ArgumentParser(\"AARCH64 wheels python CD\")\n    var_1.add_argument(\"--debug\", action=\"store_true\")\n    var_1.add_argument(\"--build-only\", action=\"store_true\")\n    var_1.add_argument(\"--test-only\", type=str)\n    var_1.add_argument(\"--enable-mkldnn\", action=\"store_true\")\n    var_1.add_argument(\"--enable-cuda\", action=\"store_true\")\n    return var_1.parse_args()\n",
      "lines_processed": 13,
      "total_lines": 259
    },
    {
      "file_path": "aarch64_wheel_ci_build.py",
      "code": "def complete_wheel(folder: str) -> str:\n    \"\"\"\n    Complete wheel build and put in artifact location\n    \"\"\"\n    wheel_name = list_dir(f\"/{folder}/dist\")[0]\n\n    # Please note for cuda we don't run auditwheel since we use custom script to package\n    # the cuda dependencies to the wheel file using update_wheel() method.\n    # However we need to make sure filename reflects the correct Manylinux platform.\n    if \"pytorch\" in folder and not enable_cuda:\n        print(\"Repairing Wheel with AuditWheel\")\n        check_call([\"auditwheel\", \"repair\", f\"dist/{wheel_name}\"], cwd=folder)\n        repaired_wheel_name = list_dir(f\"/{folder}/wheelhouse\")[0]\n\n        print(f\"Moving {repaired_wheel_name} wheel to /{folder}/dist\")\n        os.rename(\n            f\"/{folder}/wheelhouse/{repaired_wheel_name}\",\n            f\"/{folder}/dist/{repaired_wheel_name}\",\n        )\n",
      "variables": [
        "folder",
        "wheel_name",
        "repaired_wheel_name"
      ],
      "anonymized_code": "def complete_wheel(var_1: str) -> str:\n    \"\"\"\n    Complete wheel build and put in artifact location\n    \"\"\"\n    var_2 = list_dir(f\"/{var_1}/dist\")[0]\n\n    # Please note for cuda we don't run auditwheel since we use custom script to package\n    # the cuda dependencies to the wheel file using update_wheel() method.\n    # However we need to make sure filename reflects the correct Manylinux platform.\n    if \"pytorch\" in var_1 and not enable_cuda:\n        print(\"Repairing Wheel with AuditWheel\")\n        check_call([\"auditwheel\", \"repair\", f\"dist/{var_2}\"], cwd=var_1)\n        var_3 = list_dir(f\"/{var_1}/wheelhouse\")[0]\n\n        print(f\"Moving {var_3} wheel to /{var_1}/dist\")\n        os.rename(\n            f\"/{var_1}/wheelhouse/{var_3}\",\n            f\"/{var_1}/dist/{var_3}\",\n        )\n",
      "lines_processed": 19,
      "total_lines": 259
    },
    {
      "file_path": "aarch64_wheel_ci_build.py",
      "code": "def replace_tag(filename) -> None:\n    with open(filename) as f:\n        lines = f.readlines()\n    for i, line in enumerate(lines):\n        if line.startswith(\"Tag:\"):\n            lines[i] = line.replace(\"-linux_\", \"-manylinux_2_28_\")\n            print(f\"Updated tag from {line} to {lines[i]}\")\n            break\n\n    with open(filename, \"w\") as f:\n        f.writelines(lines)\n",
      "variables": [
        "filename",
        "f",
        "lines",
        "i",
        "line"
      ],
      "anonymized_code": "def replace_tag(var_1) -> None:\n    with open(var_1) as var_2:\n        var_3 = var_2.readlines()\n    for var_4, var_5 in enumerate(var_3):\n        if var_5.startswith(\"Tag:\"):\n            var_3[var_4] = var_5.replace(\"-linux_\", \"-manylinux_2_28_\")\n            print(var_2\"Updated tag from {var_5} to {var_3[var_4]}\")\n            break\n\n    with open(var_1, \"w\") as var_2:\n        var_2.writelines(var_3)\n",
      "lines_processed": 11,
      "total_lines": 259
    },
    {
      "file_path": "aarch64_wheel_ci_build.py",
      "code": "def list_dir(path: str) -> list[str]:\n    \"\"\"'\n    Helper for getting paths for Python\n    \"\"\"\n    return check_output([\"ls\", \"-1\", path]).decode().split(\"\\n\")\n",
      "variables": [
        "path"
      ],
      "anonymized_code": "def list_dir(var_1: str) -> list[str]:\n    \"\"\"'\n    Helper for getting paths for Python\n    \"\"\"\n    return check_output([\"ls\", \"-1\", var_1]).decode().split(\"\\n\")\n",
      "lines_processed": 5,
      "total_lines": 259
    },
    {
      "file_path": "aarch64_wheel_ci_build.py",
      "code": "def build_ArmComputeLibrary() -> None:\n    \"\"\"\n    Using ArmComputeLibrary for aarch64 PyTorch\n    \"\"\"\n    print(\"Building Arm Compute Library\")\n    acl_build_flags = [\n        \"debug=0\",\n        \"neon=1\",\n        \"opencl=0\",\n        \"os=linux\",\n        \"openmp=1\",\n        \"cppthreads=0\",\n        \"arch=armv8a\",\n        \"multi_isa=1\",\n        \"fixed_format_kernels=1\",\n        \"build=native\",\n    ]\n    acl_install_dir = \"/acl\"\n    acl_checkout_dir = os.getenv(\"ACL_SOURCE_DIR\", \"ComputeLibrary\")\n",
      "variables": [
        "acl_build_flags",
        "acl_install_dir",
        "acl_checkout_dir"
      ],
      "anonymized_code": "def build_ArmComputeLibrary() -> None:\n    \"\"\"\n    Using ArmComputeLibrary for aarch64 PyTorch\n    \"\"\"\n    print(\"Building Arm Compute Library\")\n    var_1 = [\n        \"debug=0\",\n        \"neon=1\",\n        \"opencl=0\",\n        \"os=linux\",\n        \"openmp=1\",\n        \"cppthreads=0\",\n        \"arch=armv8a\",\n        \"multi_isa=1\",\n        \"fixed_format_kernels=1\",\n        \"build=native\",\n    ]\n    var_2 = \"/acl\"\n    var_3 = os.getenv(\"ACL_SOURCE_DIR\", \"ComputeLibrary\")\n",
      "lines_processed": 19,
      "total_lines": 259
    },
    {
      "file_path": "embed_library.py",
      "code": "def embed_library(whl_path, lib_soname, update_tag=False):\n    patcher = AlignedPatchelf()\n    out_dir = TemporaryDirectory()\n    whl_name = os.path.basename(whl_path)\n    tmp_whl_name = os.path.join(out_dir.name, whl_name)\n    with InWheelCtx(whl_path) as ctx:\n        torchlib_path = os.path.join(ctx._tmpdir.name, \"torch\", \"lib\")\n        ctx.out_wheel = tmp_whl_name\n        new_lib_path, new_lib_soname = None, None\n        for filename, _ in elf_file_filter(ctx.iter_files()):\n            if not filename.startswith(\"torch/lib\"):\n                continue\n            libtree = lddtree(filename)\n            if lib_soname not in libtree[\"needed\"]:\n                continue\n            lib_path = libtree[\"libs\"][lib_soname][\"path\"]\n            if lib_path is None:\n                print(f\"Can't embed {lib_soname} as it could not be found\")\n                break\n",
      "variables": [
        "whl_path",
        "lib_soname",
        "update_tag",
        "patcher",
        "out_dir",
        "whl_name",
        "tmp_whl_name",
        "ctx",
        "torchlib_path",
        "new_lib_path",
        "new_lib_soname",
        "filename",
        "_",
        "libtree",
        "lib_path"
      ],
      "anonymized_code": "def embed_library(var_1, var_2, var_3=False):\n    var_4 = AlignedPatchelf()\n    var_5 = TemporaryDirectory()\n    var_6 = os.path.basename(var_1)\n    var_7 = os.path.join(var_5.name, var_6)\n    with InWheelCtx(var_1) as var_8:\n        var_9 = os.path.join(var_8._tmpdir.name, \"torch\", \"lib\")\n        var_8.out_wheel = var_7\n        var_10, var_11 = None, None\n        for var_12, var_13 in elf_file_filter(var_8.iter_files()):\n            if not var_12.startswith(\"torch/lib\"):\n                continue\n            var_14 = lddtree(var_12)\n            if var_2 not in var_14[\"needed\"]:\n                continue\n            var_15 = var_14[\"libs\"][var_2][\"path\"]\n            if var_15 is None:\n                print(f\"Can't embed {var_2} as it could not be found\")\n                break\n",
      "lines_processed": 19,
      "total_lines": 87
    },
    {
      "file_path": "embed_library.py",
      "code": "def replace_tag(filename):\n    with open(filename) as f:\n        lines = f.read().split(\"\\\\n\")\n    for i, line in enumerate(lines):\n        if not line.startswith(\"Tag: \"):\n            continue\n        lines[i] = line.replace(\"-linux_\", \"-manylinux2014_\")\n        print(f\"Updated tag from {line} to {lines[i]}\")\n\n    with open(filename, \"w\") as f:\n        f.write(\"\\\\n\".join(lines))\n",
      "variables": [
        "filename",
        "f",
        "lines",
        "i",
        "line"
      ],
      "anonymized_code": "def replace_tag(var_1):\n    with open(var_1) as var_2:\n        var_3 = var_2.read().split(\"\\\\n\")\n    for var_4, var_5 in enumerate(var_3):\n        if not var_5.startswith(\"Tag: \"):\n            continue\n        var_3[var_4] = var_5.replace(\"-linux_\", \"-manylinux2014_\")\n        print(var_2\"Updated tag from {var_5} to {var_3[var_4]}\")\n\n    with open(var_1, \"w\") as var_2:\n        var_2.write(\"\\\\n\".join(var_3))\n",
      "lines_processed": 11,
      "total_lines": 87
    },
    {
      "file_path": "check_gomp.py",
      "code": "def main():\n    omp_max_threads = get_gomp_thread()\n    print(\n        f\"omp_max_threads after loading libgomp.so and libtorch_cpu.so: {omp_max_threads}\"\n    )\n    if omp_max_threads == 1:\n        raise RuntimeError(\n            \"omp_max_threads is 1. Check whether libgomp.so is loaded twice.\"\n        )\n",
      "variables": [
        "omp_max_threads"
      ],
      "anonymized_code": "def main():\n    var_1 = get_gomp_thread()\n    print(\n        f\"var_1 after loading libgomp.so and libtorch_cpu.so: {var_1}\"\n    )\n    if var_1 == 1:\n        raise RuntimeError(\n            \"var_1 is 1. Check whether libgomp.so is loaded twice.\"\n        )\n",
      "lines_processed": 9,
      "total_lines": 74
    },
    {
      "file_path": "check_binary_symbols.py",
      "code": "def get_symbols(lib: str) -> list[tuple[str, str, str]]:\n    from subprocess import check_output\n\n    lines = check_output(f'nm \"{lib}\"|c++filt', shell=True)\n    return [x.split(\" \", 2) for x in lines.decode(\"latin1\").split(\"\\n\")[:-1]]\n",
      "variables": [
        "lib",
        "lines",
        "x"
      ],
      "anonymized_code": "def get_symbols(var_1: str) -> list[tuple[str, str, str]]:\n    from subprocess import check_output\n\n    var_2 = check_output(f'nm \"{var_1}\"|c++filt', shell=True)\n    return [var_3.split(\" \", 2) for var_3 in var_2.decode(\"latin1\").split(\"\\n\")[:-1]]\n",
      "lines_processed": 5,
      "total_lines": 113
    },
    {
      "file_path": "check_binary_symbols.py",
      "code": "def check_lib_symbols_for_abi_correctness(lib: str) -> None:\n    print(f\"lib: {lib}\")\n    cxx11_symbols = grep_symbols(lib, LIBTORCH_CXX11_PATTERNS)\n    pre_cxx11_symbols = grep_symbols(lib, LIBTORCH_PRE_CXX11_PATTERNS)\n    num_cxx11_symbols = len(cxx11_symbols)\n    num_pre_cxx11_symbols = len(pre_cxx11_symbols)\n    print(f\"num_cxx11_symbols: {num_cxx11_symbols}\")\n    print(f\"num_pre_cxx11_symbols: {num_pre_cxx11_symbols}\")\n    if num_pre_cxx11_symbols > 0:\n        raise RuntimeError(\n            f\"Found pre-cxx11 symbols, but there shouldn't be any, see: {pre_cxx11_symbols[:100]}\"\n        )\n    if num_cxx11_symbols < 100:\n        raise RuntimeError(\"Didn't find enought cxx11 symbols\")\n",
      "variables": [
        "lib",
        "cxx11_symbols",
        "pre_cxx11_symbols",
        "num_cxx11_symbols",
        "num_pre_cxx11_symbols"
      ],
      "anonymized_code": "def check_lib_symbols_for_abi_correctness(var_1: str) -> None:\n    print(f\"var_1: {var_1}\")\n    var_2 = grep_symbols(var_1, LIBTORCH_CXX11_PATTERNS)\n    var_3 = grep_symbols(var_1, LIBTORCH_PRE_CXX11_PATTERNS)\n    var_4 = len(var_2)\n    var_5 = len(var_3)\n    print(f\"var_4: {var_4}\")\n    print(f\"var_5: {var_5}\")\n    if var_5 > 0:\n        raise RuntimeError(\n            f\"Found pre-cxx11 symbols, but there shouldn't be any, see: {var_3[:100]}\"\n        )\n    if var_4 < 100:\n        raise RuntimeError(\"Didn't find enought cxx11 symbols\")\n",
      "lines_processed": 14,
      "total_lines": 113
    },
    {
      "file_path": "check_binary_symbols.py",
      "code": "def grep_symbols(lib: str, patterns: list[Any]) -> list[str]:\n    def _grep_symbols(\n        symbols: list[tuple[str, str, str]], patterns: list[Any]\n    ) -> list[str]:\n        rc = []\n        for _s_addr, _s_type, s_name in symbols:\n            for pattern in patterns:\n                if pattern.match(s_name):\n                    rc.append(s_name)\n                    continue\n        return rc\n\n    all_symbols = get_symbols(lib)\n    num_workers = 32\n    chunk_size = (len(all_symbols) + num_workers - 1) // num_workers\n\n    def _get_symbols_chunk(i):\n        return all_symbols[i * chunk_size : (i + 1) * chunk_size]\n\n",
      "variables": [
        "lib",
        "patterns",
        "symbols",
        "rc",
        "_s_addr",
        "_s_type",
        "s_name",
        "pattern",
        "all_symbols",
        "num_workers",
        "chunk_size",
        "i"
      ],
      "anonymized_code": "def grep_symbols(var_1: str, var_2: list[Any]) -> list[str]:\n    def _grep_symbols(\n        var_3: list[tuple[str, str, str]], var_2: list[Any]\n    ) -> list[str]:\n        var_4 = []\n        for var_5, var_6, var_7 in var_3:\n            for var_8 in var_2:\n                if var_8.match(var_7):\n                    var_4.append(var_7)\n                    continue\n        return var_4\n\n    var_9 = get_symbols(var_1)\n    var_10 = 32\n    var_11 = (len(var_9) + var_10 - 1) // var_10\n\n    def _get_symbols_chunk(var_12):\n        return var_9[var_12 * var_11 : (var_12 + 1) * var_11]\n\n",
      "lines_processed": 19,
      "total_lines": 113
    },
    {
      "file_path": "check_binary_symbols.py",
      "code": "def _apply_libtorch_symbols(symbols):\n    return [\n        re.compile(f\"{x}.*{y}\")\n        for (x, y) in itertools.product(LIBTORCH_NAMESPACE_LIST, symbols)\n    ]\n",
      "variables": [
        "symbols",
        "x",
        "y"
      ],
      "anonymized_code": "def _apply_libtorch_symbols(var_1):\n    return [\n        re.compile(f\"{var_2}.*{var_3}\")\n        for (var_2, var_3) in itertools.product(LIBTORCH_NAMESPACE_LIST, var_1)\n    ]\n",
      "lines_processed": 5,
      "total_lines": 113
    },
    {
      "file_path": "check_binary_symbols.py",
      "code": "def main() -> None:\n    if \"install_root\" in os.environ:\n        install_root = Path(os.getenv(\"install_root\"))  # noqa: SIM112\n    else:\n        if os.getenv(\"PACKAGE_TYPE\") == \"libtorch\":\n            install_root = Path(os.getcwd())\n        else:\n            install_root = Path(distutils.sysconfig.get_python_lib()) / \"torch\"\n\n    libtorch_cpu_path = str(install_root / \"lib\" / \"libtorch_cpu.so\")\n    check_lib_symbols_for_abi_correctness(libtorch_cpu_path)\n",
      "variables": [
        "install_root",
        "libtorch_cpu_path"
      ],
      "anonymized_code": "def main() -> None:\n    if \"var_1\" in os.environ:\n        var_1 = Path(os.getenv(\"var_1\"))  # noqa: SIM112\n    else:\n        if os.getenv(\"PACKAGE_TYPE\") == \"libtorch\":\n            var_1 = Path(os.getcwd())\n        else:\n            var_1 = Path(distutils.sysconfig.get_python_lib()) / \"torch\"\n\n    var_2 = str(var_1 / \"lib\" / \"libtorch_cpu.so\")\n    check_lib_symbols_for_abi_correctness(var_2)\n",
      "lines_processed": 11,
      "total_lines": 113
    },
    {
      "file_path": "create_test_cert.py",
      "code": "def genrsa(path):\n    key = rsa.generate_private_key(\n        public_exponent=65537,\n        key_size=2048,\n    )\n    with open(path, \"wb\") as f:\n        f.write(\n            key.private_bytes(\n                encoding=serialization.Encoding.PEM,\n                format=serialization.PrivateFormat.TraditionalOpenSSL,\n                encryption_algorithm=serialization.NoEncryption(),\n            )\n        )\n    return key\n",
      "variables": [
        "path",
        "key",
        "f"
      ],
      "anonymized_code": "def genrsa(var_1):\n    var_2 = rsa.generate_private_key(\n        public_exponent=65537,\n        key_size=2048,\n    )\n    with open(var_1, \"wb\") as var_3:\n        var_3.write(\n            var_2.private_bytes(\n                encoding=serialization.Encoding.PEM,\n                format=serialization.PrivateFormat.TraditionalOpenSSL,\n                encryption_algorithm=serialization.NoEncryption(),\n            )\n        )\n    return var_2\n",
      "lines_processed": 14,
      "total_lines": 123
    },
    {
      "file_path": "create_test_cert.py",
      "code": "def sign_certificate_request(path, csr_cert, ca_cert, private_ca_key):\n    cert = (\n        x509.CertificateBuilder()\n        .subject_name(csr_cert.subject)\n        .issuer_name(ca_cert.subject)\n        .public_key(csr_cert.public_key())\n        .serial_number(x509.random_serial_number())\n        .not_valid_before(datetime.now(timezone.utc))\n        .not_valid_after(\n            # Our certificate will be valid for 10 days\n            datetime.now(timezone.utc) + timedelta(days=10)\n            # Sign our certificate with our private key\n        )\n        .sign(private_ca_key, hashes.SHA256())\n    )\n    with open(path, \"wb\") as f:\n        f.write(cert.public_bytes(serialization.Encoding.PEM))\n    return cert\n",
      "variables": [
        "path",
        "csr_cert",
        "ca_cert",
        "private_ca_key",
        "cert",
        "f"
      ],
      "anonymized_code": "def sign_certificate_request(var_1, var_2, var_3, var_4):\n    var_5 = (\n        x509.CertificateBuilder()\n        .subject_name(var_2.subject)\n        .issuer_name(var_3.subject)\n        .public_key(var_2.public_key())\n        .serial_number(x509.random_serial_number())\n        .not_valid_before(datetime.now(timezone.utc))\n        .not_valid_after(\n            # Our certificate will be valid for 10 days\n            datetime.now(timezone.utc) + timedelta(days=10)\n            # Sign our certificate with our private key\n        )\n        .sign(var_4, hashes.SHA256())\n    )\n    with open(var_1, \"wb\") as var_6:\n        var_6.write(var_5.public_bytes(serialization.Encoding.PEM))\n    return var_5\n",
      "lines_processed": 18,
      "total_lines": 123
    },
    {
      "file_path": "create_test_cert.py",
      "code": "def create_req(path, C, ST, L, O, key):\n    csr = (\n        x509.CertificateSigningRequestBuilder()\n        .subject_name(\n            x509.Name(\n                [\n                    # Provide various details about who we are.\n                    x509.NameAttribute(NameOID.COUNTRY_NAME, C),\n                    x509.NameAttribute(NameOID.STATE_OR_PROVINCE_NAME, ST),\n                    x509.NameAttribute(NameOID.LOCALITY_NAME, L),\n                    x509.NameAttribute(NameOID.ORGANIZATION_NAME, O),\n                ]\n            )\n        )\n        .sign(key, hashes.SHA256())\n    )\n    with open(path, \"wb\") as f:\n        f.write(csr.public_bytes(serialization.Encoding.PEM))\n    return csr\n",
      "variables": [
        "path",
        "C",
        "ST",
        "L",
        "O",
        "key",
        "csr",
        "f"
      ],
      "anonymized_code": "def create_req(var_1, var_2, var_3, var_4, var_5, var_6):\n    var_7 = (\n        x509.CertificateSigningRequestBuilder()\n        .subject_name(\n            x509.Name(\n                [\n                    # Provide various details about who we are.\n                    x509.NameAttribute(NameOID.COUNTRY_NAME, var_2),\n                    x509.NameAttribute(NameOID.STATE_OR_PROVINCE_NAME, var_3),\n                    x509.NameAttribute(NameOID.LOCALITY_NAME, var_4),\n                    x509.NameAttribute(NameOID.ORGANIZATION_NAME, var_5),\n                ]\n            )\n        )\n        .sign(var_6, hashes.SHA256())\n    )\n    with open(var_1, \"wb\") as var_8:\n        var_8.write(var_7.public_bytes(serialization.Encoding.PEM))\n    return var_7\n",
      "lines_processed": 19,
      "total_lines": 123
    },
    {
      "file_path": "max_autotune.py",
      "code": "def train(args, model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % args.log_interval == 0:\n            print(\n                f\"Train Epoch: {epoch} \"\n                f\"[{batch_idx * len(data)}/{len(train_loader.dataset)} \"\n                f\"({100.0 * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\"\n            )\n            if args.dry_run:\n                break\n",
      "variables": [
        "args",
        "model",
        "device",
        "train_loader",
        "optimizer",
        "epoch",
        "batch_idx",
        "data",
        "target",
        "output",
        "loss"
      ],
      "anonymized_code": "def train(var_1, var_2, var_3, var_4, var_5, var_6):\n    var_2.train()\n    for var_7, (var_8, var_9) in enumerate(var_4):\n        var_8, var_9 = var_8.to(var_3), var_9.to(var_3)\n        var_5.zero_grad()\n        var_10 = var_2(var_8)\n        var_11 = F.nll_loss(var_10, var_9)\n        var_11.backward()\n        var_5.step()\n        if var_7 % var_1.log_interval == 0:\n            print(\n                f\"Train Epoch: {var_6} \"\n                f\"[{var_7 * len(var_8)}/{len(var_4.dataset)} \"\n                f\"({100.0 * var_7 / len(var_4):.0f}%)]\\tLoss: {var_11.item():.6f}\"\n            )\n            if var_1.dry_run:\n                break\n",
      "lines_processed": 17,
      "total_lines": 209
    },
    {
      "file_path": "max_autotune.py",
      "code": "def timed(fn):\n    start = torch.cuda.Event(enable_timing=True)\n    end = torch.cuda.Event(enable_timing=True)\n    start.record()\n    result = fn()\n    end.record()\n    torch.cuda.synchronize()\n    return result, start.elapsed_time(end) / 1000\n",
      "variables": [
        "fn",
        "start",
        "end",
        "result"
      ],
      "anonymized_code": "def timed(var_1):\n    var_2 = torch.cuda.Event(enable_timing=True)\n    var_3 = torch.cuda.Event(enable_timing=True)\n    var_2.record()\n    var_4 = var_1()\n    var_3.record()\n    torch.cuda.synchronize()\n    return var_4, var_2.elapsed_time(var_3) / 1000\n",
      "lines_processed": 8,
      "total_lines": 209
    }
  ],
  "pydantic_pydantic": [
    {
      "file_path": "_docs_extraction.py",
      "code": "def _extract_source_from_frame(cls: type[Any]) -> list[str] | None:\n    frame = inspect.currentframe()\n\n    while frame:\n        if inspect.getmodule(frame) is inspect.getmodule(cls):\n            lnum = frame.f_lineno\n            try:\n                lines, _ = inspect.findsource(frame)\n            except OSError:  # pragma: no cover\n                # Source can't be retrieved (maybe because running in an interactive terminal),\n                # we don't want to error here.\n                pass\n            else:\n                block_lines = inspect.getblock(lines[lnum - 1 :])\n                dedent_source = _dedent_source_lines(block_lines)\n                try:\n                    block_tree = ast.parse(dedent_source)\n                except SyntaxError:\n                    pass\n",
      "variables": [
        "cls",
        "frame",
        "lnum",
        "lines",
        "_",
        "block_lines",
        "dedent_source",
        "block_tree"
      ],
      "anonymized_code": "def _extract_source_from_frame(var_1: type[Any]) -> list[str] | None:\n    var_2 = inspect.currentframe()\n\n    while var_2:\n        if inspect.getmodule(var_2) is inspect.getmodule(var_1):\n            var_3 = var_2.f_lineno\n            try:\n                var_4, var_5 = inspect.findsource(var_2)\n            except OSError:  # pragma: no cover\n                # Source can't be retrieved (maybe because running in an interactive terminal),\n                # we don't want to error here.\n                pass\n            else:\n                var_6 = inspect.getblock(var_4[var_3 - 1 :])\n                var_7 = _dedent_source_lines(var_6)\n                try:\n                    var_8 = ast.parse(var_7)\n                except SyntaxError:\n                    pass\n",
      "lines_processed": 19,
      "total_lines": 113
    },
    {
      "file_path": "_docs_extraction.py",
      "code": "def _dedent_source_lines(source: list[str]) -> str:\n    # Required for nested class definitions, e.g. in a function block\n    dedent_source = textwrap.dedent(''.join(source))\n    if dedent_source.startswith((' ', '\\t')):\n        # We are in the case where there's a dedented (usually multiline) string\n        # at a lower indentation level than the class itself. We wrap our class\n        # in a function as a workaround.\n        dedent_source = f'def dedent_workaround():\\n{dedent_source}'\n    return dedent_source\n",
      "variables": [
        "source",
        "dedent_source"
      ],
      "anonymized_code": "def _dedent_source_lines(var_1: list[str]) -> str:\n    # Required for nested class definitions, e.g. in a function block\n    var_2 = textwrap.dedent(''.join(var_1))\n    if var_2.startswith((' ', '\\t')):\n        # We are in the case where there's a dedented (usually multiline) string\n        # at a lower indentation level than the class itself. We wrap our class\n        # in a function as a workaround.\n        var_2 = f'def dedent_workaround():\\n{var_2}'\n    return var_2\n",
      "lines_processed": 9,
      "total_lines": 113
    },
    {
      "file_path": "_core_utils.py",
      "code": "def is_core_schema_field(\n    schema: CoreSchemaOrField,\n) -> TypeGuard[CoreSchemaField]:\n    return schema['type'] in _CORE_SCHEMA_FIELD_TYPES\n",
      "variables": [
        "schema"
      ],
      "anonymized_code": "def is_core_schema_field(\n    var_1: CoreSchemaOrField,\n) -> TypeGuard[CoreSchemaField]:\n    return var_1['type'] in _CORE_SCHEMA_FIELD_TYPES\n",
      "lines_processed": 4,
      "total_lines": 182
    },
    {
      "file_path": "_core_utils.py",
      "code": "def validate_core_schema(schema: CoreSchema) -> CoreSchema:\n    if os.getenv('PYDANTIC_VALIDATE_CORE_SCHEMAS'):\n        return _validate_core_schema(schema)\n    return schema\n",
      "variables": [
        "schema"
      ],
      "anonymized_code": "def validate_core_schema(var_1: CoreSchema) -> CoreSchema:\n    if os.getenv('PYDANTIC_VALIDATE_CORE_SCHEMAS'):\n        return _validate_core_schema(var_1)\n    return var_1\n",
      "lines_processed": 4,
      "total_lines": 182
    },
    {
      "file_path": "_core_utils.py",
      "code": "def is_function_with_inner_schema(\n    schema: CoreSchemaOrField,\n) -> TypeGuard[FunctionSchemaWithInnerSchema]:\n    return schema['type'] in _FUNCTION_WITH_INNER_SCHEMA_TYPES\n",
      "variables": [
        "schema"
      ],
      "anonymized_code": "def is_function_with_inner_schema(\n    var_1: CoreSchemaOrField,\n) -> TypeGuard[FunctionSchemaWithInnerSchema]:\n    return var_1['type'] in _FUNCTION_WITH_INNER_SCHEMA_TYPES\n",
      "lines_processed": 4,
      "total_lines": 182
    },
    {
      "file_path": "_core_utils.py",
      "code": "def get_ref(s: core_schema.CoreSchema) -> None | str:\n    \"\"\"Get the ref from the schema if it has one.\n    This exists just for type checking to work correctly.\n    \"\"\"\n    return s.get('ref', None)\n",
      "variables": [
        "s"
      ],
      "anonymized_code": "def get_ref(var_1: core_schema.CoreSchema) -> None | str:\n    \"\"\"Get the ref from the schema if it has one.\n    This exists just for type checking to work correctly.\n    \"\"\"\n    return var_1.get('ref', None)\n",
      "lines_processed": 5,
      "total_lines": 182
    },
    {
      "file_path": "_core_utils.py",
      "code": "def is_core_schema(\n    schema: CoreSchemaOrField,\n) -> TypeGuard[CoreSchema]:\n    return schema['type'] not in _CORE_SCHEMA_FIELD_TYPES\n",
      "variables": [
        "schema"
      ],
      "anonymized_code": "def is_core_schema(\n    var_1: CoreSchemaOrField,\n) -> TypeGuard[CoreSchema]:\n    return var_1['type'] not in _CORE_SCHEMA_FIELD_TYPES\n",
      "lines_processed": 4,
      "total_lines": 182
    },
    {
      "file_path": "_core_utils.py",
      "code": "def is_list_like_schema_with_items_schema(\n    schema: CoreSchema,\n) -> TypeGuard[core_schema.ListSchema | core_schema.SetSchema | core_schema.FrozenSetSchema]:\n    return schema['type'] in _LIST_LIKE_SCHEMA_WITH_ITEMS_TYPES\n",
      "variables": [
        "schema"
      ],
      "anonymized_code": "def is_list_like_schema_with_items_schema(\n    var_1: CoreSchema,\n) -> TypeGuard[core_schema.ListSchema | core_schema.SetSchema | core_schema.FrozenSetSchema]:\n    return var_1['type'] in _LIST_LIKE_SCHEMA_WITH_ITEMS_TYPES\n",
      "lines_processed": 4,
      "total_lines": 182
    },
    {
      "file_path": "_core_utils.py",
      "code": "def pretty_print_core_schema(\n    val: Any,\n    *,\n    console: Console | None = None,\n    max_depth: int | None = None,\n    strip_metadata: bool = True,\n) -> None:  # pragma: no cover\n    \"\"\"Pretty-print a core schema using the `rich` library.\n\n    Args:\n        val: The core schema to print, or a Pydantic model/dataclass/type adapter\n            (in which case the cached core schema is fetched and printed).\n        console: A rich console to use when printing. Defaults to the global rich console instance.\n        max_depth: The number of nesting levels which may be printed.\n        strip_metadata: Whether to strip metadata in the output. If `True` any known core metadata\n            attributes will be stripped (but custom attributes are kept). Defaults to `True`.\n    \"\"\"\n    # lazy import:\n    from rich.pretty import pprint\n",
      "variables": [
        "val",
        "console",
        "max_depth",
        "strip_metadata"
      ],
      "anonymized_code": "def pretty_print_core_schema(\n    var_1: Any,\n    *,\n    var_2: Console | None = None,\n    var_3: int | None = None,\n    var_4: bool = True,\n) -> None:  # pragma: no cover\n    \"\"\"Pretty-print a core schema using the `rich` library.\n\n    Args:\n        var_1: The core schema to print, or a Pydantic model/dataclass/type adapter\n            (in which case the cached core schema is fetched and printed).\n        var_2: A rich var_2 to use when printing. Defaults to the global rich var_2 instance.\n        var_3: The number of nesting levels which may be printed.\n        var_4: Whether to strip metadata in the output. If `True` any known core metadata\n            attributes will be stripped (but custom attributes are kept). Defaults to `True`.\n    \"\"\"\n    # lazy import:\n    from rich.pretty import pprint\n",
      "lines_processed": 19,
      "total_lines": 182
    },
    {
      "file_path": "_core_utils.py",
      "code": "def _clean_schema_for_pretty_print(obj: Any, strip_metadata: bool = True) -> Any:  # pragma: no cover\n    \"\"\"A utility function to remove irrelevant information from a core schema.\"\"\"\n    if isinstance(obj, Mapping):\n        new_dct = {}\n        for k, v in obj.items():\n            if k == 'metadata' and strip_metadata:\n                new_metadata = {}\n\n                for meta_k, meta_v in v.items():\n                    if meta_k in ('pydantic_js_functions', 'pydantic_js_annotation_functions'):\n                        new_metadata['js_metadata'] = '<stripped>'\n                    else:\n                        new_metadata[meta_k] = _clean_schema_for_pretty_print(meta_v, strip_metadata=strip_metadata)\n\n                if list(new_metadata.keys()) == ['js_metadata']:\n                    new_metadata = {'<stripped>'}\n\n                new_dct[k] = new_metadata\n            # Remove some defaults:\n",
      "variables": [
        "obj",
        "strip_metadata",
        "new_dct",
        "k",
        "v",
        "new_metadata",
        "meta_k",
        "meta_v"
      ],
      "anonymized_code": "def _clean_schema_for_pretty_print(var_1: Any, var_2: bool = True) -> Any:  # pragma: no cover\n    \"\"\"A utility function to remove irrelevant information from a core schema.\"\"\"\n    if isinstance(var_1, Mapping):\n        var_3 = {}\n        for var_4, var_5 in var_1.items():\n            if var_4 == 'metadata' and var_2:\n                var_6 = {}\n\n                for var_7, var_8 in var_5.items():\n                    if var_7 in ('pydantic_js_functions', 'pydantic_js_annotation_functions'):\n                        var_6['js_metadata'] = '<stripped>'\n                    else:\n                        var_6[var_7] = _clean_schema_for_pretty_print(var_8, var_2=var_2)\n\n                if list(var_6.keys()) == ['js_metadata']:\n                    var_6 = {'<stripped>'}\n\n                var_3[var_4] = var_6\n            # Remove some defaults:\n",
      "lines_processed": 19,
      "total_lines": 182
    },
    {
      "file_path": "_decorators_v1.py",
      "code": "def make_v1_generic_root_validator(\n    validator: V1RootValidatorFunction, pre: bool\n) -> V2CoreBeforeRootValidator | V2CoreAfterRootValidator:\n    \"\"\"Wrap a V1 style root validator for V2 compatibility.\n\n    Args:\n        validator: The V1 style field validator.\n        pre: Whether the validator is a pre validator.\n\n    Returns:\n        A wrapped V2 style validator.\n    \"\"\"\n    if pre is True:\n        # mode='before' for pydantic-core\n        def _wrapper1(values: RootValidatorValues, _: core_schema.ValidationInfo) -> RootValidatorValues:\n            return validator(values)\n\n        return _wrapper1\n\n",
      "variables": [
        "validator",
        "pre",
        "values",
        "_"
      ],
      "anonymized_code": "def make_v1_generic_root_validator(\n    var_1: V1RootValidatorFunction, var_2: bool\n) -> V2CoreBeforeRootValidator | V2CoreAfterRootValidator:\n    \"\"\"Wrap a V1 style root var_1 for V2 compatibility.\n\n    Args:\n        var_1: The V1 style field var_1.\n        var_2: Whether the var_1 is a var_2 var_1.\n\n    Returns:\n        A wrapped V2 style var_1.\n    \"\"\"\n    if var_2 is True:\n        # mode='before' for pydantic-core\n        def _wrapper1(var_3: RootValidatorValues, var_4: core_schema.ValidationInfo) -> RootValidatorValues:\n            return var_1(var_3)\n\n        return _wrapper1\n\n",
      "lines_processed": 19,
      "total_lines": 174
    },
    {
      "file_path": "_decorators_v1.py",
      "code": "def can_be_keyword(param: Parameter) -> bool:\n    return param.kind in (Parameter.POSITIONAL_OR_KEYWORD, Parameter.KEYWORD_ONLY)\n",
      "variables": [
        "param"
      ],
      "anonymized_code": "def can_be_keyword(var_1: Parameter) -> bool:\n    return var_1.kind in (Parameter.POSITIONAL_OR_KEYWORD, Parameter.KEYWORD_ONLY)\n",
      "lines_processed": 2,
      "total_lines": 174
    },
    {
      "file_path": "people.py",
      "code": "def get_contributors(settings: Settings) -> tuple[Counter, Counter, Counter, dict[str, Author]]:\n    \"\"\"Analyze pull requests to identify contributors, commentors, and reviewers.\n\n    Args:\n        settings: Configuration settings\n\n    Returns:\n        A tuple containing:\n            - Counter of contributors (merged PRs)\n            - Counter of commentors\n            - Counter of reviewers\n            - Dictionary mapping usernames to Author objects\n    \"\"\"\n    pr_nodes: list[PullRequestNode] = []\n    pr_edges = get_graphql_pr_edges(settings=settings)\n\n    while pr_edges:\n        for edge in pr_edges:\n            pr_nodes.append(edge.node)\n",
      "variables": [
        "settings",
        "pr_nodes",
        "pr_edges",
        "edge"
      ],
      "anonymized_code": "def get_contributors(var_1: Settings) -> tuple[Counter, Counter, Counter, dict[str, Author]]:\n    \"\"\"Analyze pull requests to identify contributors, commentors, and reviewers.\n\n    Args:\n        var_1: Configuration var_1\n\n    Returns:\n        A tuple containing:\n            - Counter of contributors (merged PRs)\n            - Counter of commentors\n            - Counter of reviewers\n            - Dictionary mapping usernames to Author objects\n    \"\"\"\n    var_2: list[PullRequestNode] = []\n    var_3 = get_graphql_pr_edges(var_1=var_1)\n\n    while var_3:\n        for var_4 in var_3:\n            var_2.append(var_4.node)\n",
      "lines_processed": 19,
      "total_lines": 781
    },
    {
      "file_path": "people.py",
      "code": "def get_graphql_issue_edges(*, settings: Settings, after: str | None = None) -> list[IssuesEdge]:\n    \"\"\"Fetch issue edges from GitHub GraphQL API.\n\n    Args:\n        settings: Configuration settings\n        after: Cursor for pagination, if any\n\n    Returns:\n        List of issue edges from the GraphQL response\n    \"\"\"\n    data = get_graphql_response(settings=settings, query=issues_query, after=after)\n    graphql_response = IssuesResponse.model_validate(data)\n    return graphql_response.data.repository.issues.edges\n",
      "variables": [
        "settings",
        "after",
        "data",
        "graphql_response"
      ],
      "anonymized_code": "def get_graphql_issue_edges(*, var_1: Settings, var_2: str | None = None) -> list[IssuesEdge]:\n    \"\"\"Fetch issue edges from GitHub GraphQL API.\n\n    Args:\n        var_1: Configuration var_1\n        var_2: Cursor for pagination, if any\n\n    Returns:\n        List of issue edges from the GraphQL response\n    \"\"\"\n    var_3 = get_graphql_response(var_1=var_1, query=issues_query, var_2=var_2)\n    var_4 = IssuesResponse.model_validate(var_3)\n    return var_4.var_3.repository.issues.edges\n",
      "lines_processed": 13,
      "total_lines": 781
    },
    {
      "file_path": "people.py",
      "code": "def get_graphql_response(\n    *,\n    settings: Settings,\n    query: str,\n    after: str | None = None,\n) -> dict[str, Any]:\n    \"\"\"Make a GraphQL request to GitHub API.\n\n    Args:\n        settings: Configuration settings including API token\n        query: GraphQL query string\n        after: Cursor for pagination, if any\n\n    Returns:\n        Response data from GitHub API in JSON format\n\n    Raises:\n        RuntimeError: If the API request fails or returns errors\n    \"\"\"\n",
      "variables": [
        "settings",
        "query",
        "after"
      ],
      "anonymized_code": "def get_graphql_response(\n    *,\n    var_1: Settings,\n    var_2: str,\n    var_3: str | None = None,\n) -> dict[str, Any]:\n    \"\"\"Make a GraphQL request to GitHub API.\n\n    Args:\n        var_1: Configuration var_1 including API token\n        var_2: GraphQL var_2 string\n        var_3: Cursor for pagination, if any\n\n    Returns:\n        Response data from GitHub API in JSON format\n\n    Raises:\n        RuntimeError: If the API request fails or returns errors\n    \"\"\"\n",
      "lines_processed": 19,
      "total_lines": 781
    },
    {
      "file_path": "people.py",
      "code": "def get_graphql_question_discussion_edges(\n    *,\n    settings: Settings,\n    after: str | None = None,\n) -> list[DiscussionsEdge]:\n    \"\"\"Fetch discussion edges from GitHub GraphQL API.\n\n    Args:\n        settings: Configuration settings\n        after: Cursor for pagination, if any\n\n    Returns:\n        List of discussion edges from the GraphQL response\n    \"\"\"\n    data = get_graphql_response(\n        settings=settings,\n        query=discussions_query,\n        after=after,\n    )\n",
      "variables": [
        "settings",
        "after",
        "data"
      ],
      "anonymized_code": "def get_graphql_question_discussion_edges(\n    *,\n    var_1: Settings,\n    var_2: str | None = None,\n) -> list[DiscussionsEdge]:\n    \"\"\"Fetch discussion edges from GitHub GraphQL API.\n\n    Args:\n        var_1: Configuration var_1\n        var_2: Cursor for pagination, if any\n\n    Returns:\n        List of discussion edges from the GraphQL response\n    \"\"\"\n    var_3 = get_graphql_response(\n        var_1=var_1,\n        query=discussions_query,\n        var_2=var_2,\n    )\n",
      "lines_processed": 19,
      "total_lines": 781
    },
    {
      "file_path": "people.py",
      "code": "def get_discussions_experts(settings: Settings) -> tuple[Counter, Counter, dict[str, Author]]:\n    \"\"\"Analyze discussions to identify expert contributors.\n\n    Args:\n        settings: Configuration settings\n\n    Returns:\n        A tuple containing:\n            - Counter of all commentors\n            - Counter of commentors from the last month\n            - Dictionary mapping usernames to Author objects\n    \"\"\"\n    discussion_nodes: list[DiscussionsNode] = []\n    discussion_edges = get_graphql_question_discussion_edges(settings=settings)\n\n    while discussion_edges:\n        for discussion_edge in discussion_edges:\n            discussion_nodes.append(discussion_edge.node)\n        last_edge = discussion_edges[-1]\n",
      "variables": [
        "settings",
        "discussion_nodes",
        "discussion_edges",
        "discussion_edge",
        "last_edge"
      ],
      "anonymized_code": "def get_discussions_experts(var_1: Settings) -> tuple[Counter, Counter, dict[str, Author]]:\n    \"\"\"Analyze discussions to identify expert contributors.\n\n    Args:\n        var_1: Configuration var_1\n\n    Returns:\n        A tuple containing:\n            - Counter of all commentors\n            - Counter of commentors from the last month\n            - Dictionary mapping usernames to Author objects\n    \"\"\"\n    var_2: list[DiscussionsNode] = []\n    var_3 = get_graphql_question_discussion_edges(var_1=var_1)\n\n    while var_3:\n        for var_4 in var_3:\n            var_2.append(var_4.node)\n        var_5 = var_3[-1]\n",
      "lines_processed": 19,
      "total_lines": 781
    },
    {
      "file_path": "people.py",
      "code": "def get_issues_experts(settings: Settings) -> tuple[Counter, Counter, dict[str, Author]]:\n    \"\"\"Analyze issues to identify expert contributors.\n\n    Args:\n        settings: Configuration settings\n\n    Returns:\n        A tuple containing:\n            - Counter of all commentors\n            - Counter of commentors from the last month\n            - Dictionary mapping usernames to Author objects\n    \"\"\"\n    issue_nodes: list[IssuesNode] = []\n    issue_edges = get_graphql_issue_edges(settings=settings)\n\n    while issue_edges:\n        for edge in issue_edges:\n            issue_nodes.append(edge.node)\n        last_edge = issue_edges[-1]\n",
      "variables": [
        "settings",
        "issue_nodes",
        "issue_edges",
        "edge",
        "last_edge"
      ],
      "anonymized_code": "def get_issues_experts(var_1: Settings) -> tuple[Counter, Counter, dict[str, Author]]:\n    \"\"\"Analyze issues to identify expert contributors.\n\n    Args:\n        var_1: Configuration var_1\n\n    Returns:\n        A tuple containing:\n            - Counter of all commentors\n            - Counter of commentors from the last month\n            - Dictionary mapping usernames to Author objects\n    \"\"\"\n    var_2: list[IssuesNode] = []\n    var_3 = get_graphql_issue_edges(var_1=var_1)\n\n    while var_3:\n        for var_4 in var_3:\n            var_2.append(var_4.node)\n        var_5 = var_3[-1]\n",
      "lines_processed": 19,
      "total_lines": 781
    },
    {
      "file_path": "people.py",
      "code": "def get_graphql_pr_edges(*, settings: Settings, after: str | None = None) -> list[PullRequestEdge]:\n    \"\"\"Fetch pull request edges from GitHub GraphQL API.\n\n    Args:\n        settings: Configuration settings\n        after: Cursor for pagination, if any\n\n    Returns:\n        List of pull request edges from the GraphQL response\n    \"\"\"\n    data = get_graphql_response(settings=settings, query=prs_query, after=after)\n    graphql_response = PRsResponse.model_validate(data)\n    return graphql_response.data.repository.pullRequests.edges\n",
      "variables": [
        "settings",
        "after",
        "data",
        "graphql_response"
      ],
      "anonymized_code": "def get_graphql_pr_edges(*, var_1: Settings, var_2: str | None = None) -> list[PullRequestEdge]:\n    \"\"\"Fetch pull request edges from GitHub GraphQL API.\n\n    Args:\n        var_1: Configuration var_1\n        var_2: Cursor for pagination, if any\n\n    Returns:\n        List of pull request edges from the GraphQL response\n    \"\"\"\n    var_3 = get_graphql_response(var_1=var_1, query=prs_query, var_2=var_2)\n    var_4 = PRsResponse.model_validate(var_3)\n    return var_4.var_3.repository.pullRequests.edges\n",
      "lines_processed": 13,
      "total_lines": 781
    },
    {
      "file_path": "_decorators.py",
      "code": "def unwrap_wrapped_function(\n    func: Any,\n    *,\n    unwrap_partial: bool = True,\n    unwrap_class_static_method: bool = True,\n) -> Any:\n    \"\"\"Recursively unwraps a wrapped function until the underlying function is reached.\n    This handles property, functools.partial, functools.partialmethod, staticmethod, and classmethod.\n\n    Args:\n        func: The function to unwrap.\n        unwrap_partial: If True (default), unwrap partial and partialmethod decorators.\n        unwrap_class_static_method: If True (default), also unwrap classmethod and staticmethod\n            decorators. If False, only unwrap partial and partialmethod decorators.\n\n    Returns:\n        The underlying function of the wrapped function.\n    \"\"\"\n    # Define the types we want to check against as a single tuple.\n",
      "variables": [
        "func",
        "unwrap_partial",
        "unwrap_class_static_method"
      ],
      "anonymized_code": "def unwrap_wrapped_function(\n    var_1: Any,\n    *,\n    var_2: bool = True,\n    var_3: bool = True,\n) -> Any:\n    \"\"\"Recursively unwraps a wrapped function until the underlying function is reached.\n    This handles property, functools.partial, functools.partialmethod, staticmethod, and classmethod.\n\n    Args:\n        var_1: The function to unwrap.\n        var_2: If True (default), unwrap partial and partialmethod decorators.\n        var_3: If True (default), also unwrap classmethod and staticmethod\n            decorators. If False, only unwrap partial and partialmethod decorators.\n\n    Returns:\n        The underlying function of the wrapped function.\n    \"\"\"\n    # Define the types we want to check against as a single tuple.\n",
      "lines_processed": 19,
      "total_lines": 844
    },
    {
      "file_path": "_decorators.py",
      "code": "def is_instance_method_from_sig(function: AnyDecoratorCallable) -> bool:\n    \"\"\"Whether the function is an instance method.\n\n    It will consider a function as instance method if the first parameter of\n    function is `self`.\n\n    Args:\n        function: The function to check.\n\n    Returns:\n        `True` if the function is an instance method, `False` otherwise.\n    \"\"\"\n    sig = signature(unwrap_wrapped_function(function))\n    first = next(iter(sig.parameters.values()), None)\n    if first and first.name == 'self':\n        return True\n    return False\n",
      "variables": [
        "function",
        "sig",
        "first"
      ],
      "anonymized_code": "def is_instance_method_from_sig(var_1: AnyDecoratorCallable) -> bool:\n    \"\"\"Whether the var_1 is an instance method.\n\n    It will consider a var_1 as instance method if the var_3 parameter of\n    var_1 is `self`.\n\n    Args:\n        var_1: The var_1 to check.\n\n    Returns:\n        `True` if the var_1 is an instance method, `False` otherwise.\n    \"\"\"\n    var_2 = signature(unwrap_wrapped_function(var_1))\n    var_3 = next(iter(var_2.parameters.values()), None)\n    if var_3 and var_3.name == 'self':\n        return True\n    return False\n",
      "lines_processed": 17,
      "total_lines": 844
    },
    {
      "file_path": "_decorators.py",
      "code": "def mro(tp: type[Any]) -> tuple[type[Any], ...]:\n    \"\"\"Calculate the Method Resolution Order of bases using the C3 algorithm.\n\n    See https://www.python.org/download/releases/2.3/mro/\n    \"\"\"\n    # try to use the existing mro, for performance mainly\n    # but also because it helps verify the implementation below\n    if not is_typeddict(tp):\n        try:\n            return tp.__mro__\n        except AttributeError:\n            # GenericAlias and some other cases\n            pass\n\n    bases = get_bases(tp)\n    return (tp,) + mro_for_bases(bases)\n",
      "variables": [
        "tp",
        "bases"
      ],
      "anonymized_code": "def mro(var_1: type[Any]) -> tuple[type[Any], ...]:\n    \"\"\"Calculate the Method Resolution Order of var_2 using the C3 algorithm.\n\n    See https://www.python.org/download/releases/2.3/mro/\n    \"\"\"\n    # try to use the existing mro, for performance mainly\n    # but also because it helps verify the implementation below\n    if not is_typeddict(var_1):\n        try:\n            return var_1.__mro__\n        except AttributeError:\n            # GenericAlias and some other cases\n            pass\n\n    var_2 = get_bases(var_1)\n    return (var_1,) + mro_for_bases(var_2)\n",
      "lines_processed": 16,
      "total_lines": 844
    },
    {
      "file_path": "_decorators.py",
      "code": "def ensure_classmethod_based_on_signature(function: AnyDecoratorCallable) -> Any:\n    \"\"\"Apply the `@classmethod` decorator on the function.\n\n    Args:\n        function: The function to apply the decorator on.\n\n    Return:\n        The `@classmethod` decorator applied function.\n    \"\"\"\n    if not isinstance(\n        unwrap_wrapped_function(function, unwrap_class_static_method=False), classmethod\n    ) and _is_classmethod_from_sig(function):\n        return classmethod(function)  # type: ignore[arg-type]\n    return function\n",
      "variables": [
        "function"
      ],
      "anonymized_code": "def ensure_classmethod_based_on_signature(var_1: AnyDecoratorCallable) -> Any:\n    \"\"\"Apply the `@classmethod` decorator on the var_1.\n\n    Args:\n        var_1: The var_1 to apply the decorator on.\n\n    Return:\n        The `@classmethod` decorator applied var_1.\n    \"\"\"\n    if not isinstance(\n        unwrap_wrapped_function(var_1, unwrap_class_static_method=False), classmethod\n    ) and _is_classmethod_from_sig(var_1):\n        return classmethod(var_1)  # type: ignore[arg-type]\n    return var_1\n",
      "lines_processed": 14,
      "total_lines": 844
    },
    {
      "file_path": "_decorators.py",
      "code": "def ensure_property(f: Any) -> Any:\n    \"\"\"Ensure that a function is a `property` or `cached_property`, or is a valid descriptor.\n\n    Args:\n        f: The function to check.\n\n    Returns:\n        The function, or a `property` or `cached_property` instance wrapping the function.\n    \"\"\"\n    if ismethoddescriptor(f) or isdatadescriptor(f):\n        return f\n    else:\n        return property(f)\n",
      "variables": [
        "f"
      ],
      "anonymized_code": "def ensure_property(var_1: Any) -> Any:\n    \"\"\"Ensure that a function is a `property` or `cached_property`, or is a valid descriptor.\n\n    Args:\n        var_1: The function to check.\n\n    Returns:\n        The function, or a `property` or `cached_property` instance wrapping the function.\n    \"\"\"\n    if ismethoddescriptor(var_1) or isdatadescriptor(var_1):\n        return var_1\n    else:\n        return property(var_1)\n",
      "lines_processed": 13,
      "total_lines": 844
    },
    {
      "file_path": "algolia.py",
      "code": "def get_heading_text(heading: Tag):\n    return heading.get_text().replace('\u00b6', '').strip().replace('\\n', ' ')\n",
      "variables": [
        "heading"
      ],
      "anonymized_code": "def get_heading_text(var_1: Tag):\n    return var_1.get_text().replace('\u00b6', '').strip().replace('\\n', ' ')\n",
      "lines_processed": 2,
      "total_lines": 197
    },
    {
      "file_path": "algolia.py",
      "code": "def algolia_upload() -> None:\n    from algoliasearch.search.client import SearchClientSync\n\n    algolia_write_api_key = os.environ['ALGOLIA_WRITE_API_KEY']\n\n    client = SearchClientSync(ALGOLIA_APP_ID, algolia_write_api_key)\n    filtered_records: list[AlgoliaRecord] = []\n\n    algolia_records_path = Path.cwd() / 'site' / ALGOLIA_RECORDS_FILE\n\n    with algolia_records_path.open('rb') as f:\n        all_records = records_ta.validate_json(f.read())\n\n    for record in all_records:\n        content = record['content']\n        if len(content) > MAX_CONTENT_LENGTH:\n            print(\n                f\"Record with title '{record['title']}' has more than {MAX_CONTENT_LENGTH} characters, {len(content)}.\"\n            )\n",
      "variables": [
        "algolia_write_api_key",
        "client",
        "filtered_records",
        "algolia_records_path",
        "f",
        "all_records",
        "record",
        "content"
      ],
      "anonymized_code": "def algolia_upload() -> None:\n    from algoliasearch.search.var_2 import SearchClientSync\n\n    var_1 = os.environ['ALGOLIA_WRITE_API_KEY']\n\n    var_2 = SearchClientSync(ALGOLIA_APP_ID, var_1)\n    var_3: list[AlgoliaRecord] = []\n\n    var_4 = Path.cwd() / 'site' / ALGOLIA_RECORDS_FILE\n\n    with var_4.open('rb') as var_5:\n        var_6 = records_ta.validate_json(var_5.read())\n\n    for var_7 in var_6:\n        var_8 = var_7['var_8']\n        if len(var_8) > MAX_CONTENT_LENGTH:\n            print(\n                var_5\"Record with title '{var_7['title']}' has more than {MAX_CONTENT_LENGTH} characters, {len(var_8)}.\"\n            )\n",
      "lines_processed": 19,
      "total_lines": 197
    },
    {
      "file_path": "algolia.py",
      "code": "def on_post_build(config: Config) -> None:\n    if records:\n        algolia_records_path = Path(config['site_dir']) / ALGOLIA_RECORDS_FILE\n        with algolia_records_path.open('wb') as f:\n            f.write(records_ta.dump_json(records))\n",
      "variables": [
        "config",
        "algolia_records_path",
        "f"
      ],
      "anonymized_code": "def on_post_build(var_1: Config) -> None:\n    if records:\n        var_2 = Path(var_1['site_dir']) / ALGOLIA_RECORDS_FILE\n        with var_2.open('wb') as var_3:\n            var_3.write(records_ta.dump_json(records))\n",
      "lines_processed": 5,
      "total_lines": 197
    },
    {
      "file_path": "_config.py",
      "code": "def prepare_config(config: ConfigDict | dict[str, Any] | type[Any] | None) -> ConfigDict:\n    \"\"\"Create a `ConfigDict` instance from an existing dict, a class (e.g. old class-based config) or None.\n\n    Args:\n        config: The input config.\n\n    Returns:\n        A ConfigDict object created from config.\n    \"\"\"\n    if config is None:\n        return ConfigDict()\n\n    if not isinstance(config, dict):\n        warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)\n        config = {k: getattr(config, k) for k in dir(config) if not k.startswith('__')}\n\n    config_dict = cast(ConfigDict, config)\n    check_deprecated(config_dict)\n    return config_dict\n",
      "variables": [
        "config",
        "k",
        "config_dict"
      ],
      "anonymized_code": "def prepare_config(var_1: ConfigDict | dict[str, Any] | type[Any] | None) -> ConfigDict:\n    \"\"\"Create a `ConfigDict` instance from an existing dict, a class (e.g. old class-based var_1) or None.\n\n    Args:\n        var_1: The input var_1.\n\n    Returns:\n        A ConfigDict object created from var_1.\n    \"\"\"\n    if var_1 is None:\n        return ConfigDict()\n\n    if not isinstance(var_1, dict):\n        warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)\n        var_1 = {var_2: getattr(var_1, var_2) for var_2 in dir(var_1) if not var_2.startswith('__')}\n\n    var_3 = cast(ConfigDict, var_1)\n    check_deprecated(var_3)\n    return var_3\n",
      "lines_processed": 19,
      "total_lines": 373
    },
    {
      "file_path": "_config.py",
      "code": "def check_deprecated(config_dict: ConfigDict) -> None:\n    \"\"\"Check for deprecated config keys and warn the user.\n\n    Args:\n        config_dict: The input config.\n    \"\"\"\n    deprecated_removed_keys = V2_REMOVED_KEYS & config_dict.keys()\n    deprecated_renamed_keys = V2_RENAMED_KEYS.keys() & config_dict.keys()\n    if deprecated_removed_keys or deprecated_renamed_keys:\n        renamings = {k: V2_RENAMED_KEYS[k] for k in sorted(deprecated_renamed_keys)}\n        renamed_bullets = [f'* {k!r} has been renamed to {v!r}' for k, v in renamings.items()]\n        removed_bullets = [f'* {k!r} has been removed' for k in sorted(deprecated_removed_keys)]\n        message = '\\n'.join(['Valid config keys have changed in V2:'] + renamed_bullets + removed_bullets)\n        warnings.warn(message, UserWarning)\n",
      "variables": [
        "config_dict",
        "deprecated_removed_keys",
        "deprecated_renamed_keys",
        "renamings",
        "k",
        "renamed_bullets",
        "v",
        "removed_bullets",
        "message"
      ],
      "anonymized_code": "def check_deprecated(var_1: ConfigDict) -> None:\n    \"\"\"Check for deprecated config keys and warn the user.\n\n    Args:\n        var_1: The input config.\n    \"\"\"\n    var_2 = V2_REMOVED_KEYS & var_1.keys()\n    var_3 = V2_RENAMED_KEYS.keys() & var_1.keys()\n    if var_2 or var_3:\n        var_4 = {var_5: V2_RENAMED_KEYS[var_5] for var_5 in sorted(var_3)}\n        var_6 = [f'* {var_5!r} has been renamed to {var_7!r}' for var_5, var_7 in var_4.items()]\n        var_8 = [f'* {var_5!r} has been removed' for var_5 in sorted(var_2)]\n        var_9 = '\\n'.join(['Valid config keys have changed in V2:'] + var_6 + var_8)\n        warnings.warn(var_9, UserWarning)\n",
      "lines_processed": 14,
      "total_lines": 373
    },
    {
      "file_path": "main.py",
      "code": "def _generate_table_heading(col_names: list[str]) -> str:\n    return _generate_table_row(col_names) + _generate_table_row(['-'] * len(col_names))\n",
      "variables": [
        "col_names"
      ],
      "anonymized_code": "def _generate_table_heading(var_1: list[str]) -> str:\n    return _generate_table_row(var_1) + _generate_table_row(['-'] * len(var_1))\n",
      "lines_processed": 2,
      "total_lines": 461
    },
    {
      "file_path": "main.py",
      "code": "def devtools_example(markdown: str, page: Page) -> str | None:\n    if page.file.src_uri != 'integrations/devtools.md':\n        return None\n\n    html = (THIS_DIR / 'devtools_output.html').read_text().strip('\\n')\n    full_html = f'<div class=\"highlight\">\\n<pre><code>{html}</code></pre>\\n</div>'\n    return re.sub(r'{{ *devtools_example *}}', full_html, markdown)\n",
      "variables": [
        "markdown",
        "page",
        "html",
        "full_html"
      ],
      "anonymized_code": "def devtools_example(var_1: str, var_2: Page) -> str | None:\n    if var_2.file.src_uri != 'integrations/devtools.md':\n        return None\n\n    var_3 = (THIS_DIR / 'devtools_output.var_3').read_text().strip('\\n')\n    var_4 = f'<div class=\"highlight\">\\n<pre><code>{var_3}</code></pre>\\n</div>'\n    return re.sub(r'{{ *devtools_example *}}', var_4, var_1)\n",
      "lines_processed": 7,
      "total_lines": 461
    },
    {
      "file_path": "main.py",
      "code": "def get_orgs_data() -> list[dict[str, str]]:\n    with (THIS_DIR / 'orgs.toml').open('rb') as f:\n        orgs_data = tomli.load(f)\n    return orgs_data['orgs']\n",
      "variables": [
        "f",
        "orgs_data"
      ],
      "anonymized_code": "def get_orgs_data() -> list[dict[str, str]]:\n    with (THIS_DIR / 'orgs.toml').open('rb') as var_1:\n        var_2 = tomli.load(var_1)\n    return var_2['orgs']\n",
      "lines_processed": 4,
      "total_lines": 461
    },
    {
      "file_path": "main.py",
      "code": "def upgrade_python(markdown: str) -> str:\n    \"\"\"\n    Apply pyupgrade to all Python code blocks, unless explicitly skipped, create a tab for each version.\n    \"\"\"\n\n    def add_tabs(match: re.Match[str]) -> str:\n        prefix = match.group(1)\n        if 'upgrade=\"skip\"' in prefix:\n            return match.group(0)\n\n        if m := re.search(r'requires=\"3.(\\d+)\"', prefix):\n            min_minor_version = int(m.group(1))\n        else:\n            min_minor_version = MIN_MINOR_VERSION\n\n        py_code = match.group(2)\n        numbers = match.group(3)\n        # import devtools\n        # devtools.debug(numbers)\n",
      "variables": [
        "markdown",
        "match",
        "prefix",
        "m",
        "min_minor_version",
        "py_code",
        "numbers"
      ],
      "anonymized_code": "def upgrade_python(var_1: str) -> str:\n    \"\"\"\n    Apply pyupgrade to all Python code blocks, unless explicitly skipped, create a tab for each version.\n    \"\"\"\n\n    def add_tabs(var_2: re.Match[str]) -> str:\n        var_3 = var_2.group(1)\n        if 'upgrade=\"skip\"' in var_3:\n            return var_2.group(0)\n\n        if var_4 := re.search(r'requires=\"3.(\\d+)\"', var_3):\n            var_5 = int(var_4.group(1))\n        else:\n            var_5 = MIN_MINOR_VERSION\n\n        var_6 = var_2.group(2)\n        var_7 = var_2.group(3)\n        # import devtools\n        # devtools.debug(var_7)\n",
      "lines_processed": 19,
      "total_lines": 461
    },
    {
      "file_path": "main.py",
      "code": "def populate_pydantic_people(markdown: str, page: Page) -> str | None:\n    if page.file.src_uri != 'pydantic_people.md':\n        return None\n\n    # read people.yml file data\n    with (THIS_DIR / 'people.yml').open('rb') as f:\n        people = yaml.load(f, Loader=yaml.FullLoader)\n\n    # Render the templates\n    for name, template in [\n        ('experts', experts_template),\n        ('most_active_users', most_active_users_template),\n        ('top_contributors', top_contributors_template),\n        ('top_reviewers', top_reviewers_template),\n        ('maintainers', maintainers_template),\n    ]:\n        rendered = template.render(people=people)\n        markdown = re.sub(f'{{{{ {name} }}}}', rendered, markdown)\n\n",
      "variables": [
        "markdown",
        "page",
        "f",
        "people",
        "name",
        "template",
        "rendered"
      ],
      "anonymized_code": "def populate_pydantic_people(var_1: str, var_2: Page) -> str | None:\n    if var_2.file.src_uri != 'pydantic_people.md':\n        return None\n\n    # read var_4.yml file data\n    with (THIS_DIR / 'var_4.yml').open('rb') as var_3:\n        var_4 = yaml.load(var_3, Loader=yaml.FullLoader)\n\n    # Render the templates\n    for var_5, var_6 in [\n        ('experts', experts_template),\n        ('most_active_users', most_active_users_template),\n        ('top_contributors', top_contributors_template),\n        ('top_reviewers', top_reviewers_template),\n        ('maintainers', maintainers_template),\n    ]:\n        var_7 = var_6.render(var_4=var_4)\n        var_1 = re.sub(var_3'{{{{ {var_5} }}}}', var_7, var_1)\n\n",
      "lines_processed": 19,
      "total_lines": 461
    },
    {
      "file_path": "main.py",
      "code": "def insert_json_output(markdown: str) -> str:\n    \"\"\"\n    Find `output=\"json\"` code fence tags and replace with a separate JSON section\n    \"\"\"\n\n    def replace_json(m: re.Match[str]) -> str:\n        start, attrs, code = m.groups()\n\n        def replace_last_print(m2: re.Match[str]) -> str:\n            ind, json_text = m2.groups()\n            json_text = indent(json.dumps(json.loads(json_text), indent=2), ind)\n            # no trailing fence as that's not part of code\n            return f'\\n{ind}```\\n\\n{ind}JSON output:\\n\\n{ind}```json\\n{json_text}\\n'\n\n        code = re.sub(r'\\n( *)\"\"\"(.*?)\\1\"\"\"\\n$', replace_last_print, code, flags=re.S)\n        return f'{start}{attrs}{code}{start}\\n'\n\n    return re.sub(r'(^ *```)([^\\n]*?output=\"json\"[^\\n]*?\\n)(.+?)\\1', replace_json, markdown, flags=re.M | re.S)\n",
      "variables": [
        "markdown",
        "m",
        "start",
        "attrs",
        "code",
        "m2",
        "ind",
        "json_text"
      ],
      "anonymized_code": "def insert_json_output(var_1: str) -> str:\n    \"\"\"\n    Find `output=\"json\"` var_5 fence tags and replace with a separate JSON section\n    \"\"\"\n\n    def replace_json(var_2: re.Match[str]) -> str:\n        var_3, var_4, var_5 = var_2.groups()\n\n        def replace_last_print(var_6: re.Match[str]) -> str:\n            var_7, var_8 = var_6.groups()\n            var_8 = indent(json.dumps(json.loads(var_8), indent=2), var_7)\n            # no trailing fence as that's not part of var_5\n            return f'\\n{var_7}```\\n\\n{var_7}JSON output:\\n\\n{var_7}```json\\n{var_8}\\n'\n\n        var_5 = re.sub(r'\\n( *)\"\"\"(.*?)\\1\"\"\"\\n$', replace_last_print, var_5, flags=re.S)\n        return f'{var_3}{var_4}{var_5}{var_3}\\n'\n\n    return re.sub(r'(^ *```)([^\\n]*?output=\"json\"[^\\n]*?\\n)(.+?)\\1', replace_json, var_1, flags=re.M | re.S)\n",
      "lines_processed": 18,
      "total_lines": 461
    },
    {
      "file_path": "main.py",
      "code": "def on_pre_build(config: Config) -> None:\n    \"\"\"\n    Before the build starts.\n    \"\"\"\n    add_changelog()\n    add_mkdocs_run_deps()\n",
      "variables": [
        "config"
      ],
      "anonymized_code": "def on_pre_build(var_1: Config) -> None:\n    \"\"\"\n    Before the build starts.\n    \"\"\"\n    add_changelog()\n    add_mkdocs_run_deps()\n",
      "lines_processed": 6,
      "total_lines": 461
    },
    {
      "file_path": "main.py",
      "code": "def render_pydantic_settings(markdown: str, page: Page) -> str | None:\n    if page.file.src_uri != 'concepts/pydantic_settings.md':\n        return None\n\n    req = requests.get('https://raw.githubusercontent.com/pydantic/pydantic-settings/main/docs/index.md')\n    if req.status_code != 200:\n        logger.warning(\n            'Got HTTP status %d when trying to fetch content of the `pydantic-settings` docs', req.status_code\n        )\n        return\n\n    docs_content = req.text.strip()\n\n    return re.sub(r'{{ *pydantic_settings *}}', docs_content, markdown)\n",
      "variables": [
        "markdown",
        "page",
        "req",
        "docs_content"
      ],
      "anonymized_code": "def render_pydantic_settings(var_1: str, var_2: Page) -> str | None:\n    if var_2.file.src_uri != 'concepts/pydantic_settings.md':\n        return None\n\n    var_3 = requests.get('https://raw.githubusercontent.com/pydantic/pydantic-settings/main/docs/index.md')\n    if var_3.status_code != 200:\n        logger.warning(\n            'Got HTTP status %d when trying to fetch content of the `pydantic-settings` docs', var_3.status_code\n        )\n        return\n\n    var_4 = var_3.text.strip()\n\n    return re.sub(r'{{ *pydantic_settings *}}', var_4, var_1)\n",
      "lines_processed": 14,
      "total_lines": 461
    },
    {
      "file_path": "_dataclasses.py",
      "code": "def is_stdlib_dataclass(cls: type[Any], /) -> TypeIs[type[StandardDataclass]]:\n    \"\"\"Returns `True` if the class is a stdlib dataclass and *not* a Pydantic dataclass.\n\n    Unlike the stdlib `dataclasses.is_dataclass()` function, this does *not* include subclasses\n    of a dataclass that are themselves not dataclasses.\n\n    Args:\n        cls: The class.\n\n    Returns:\n        `True` if the class is a stdlib dataclass, `False` otherwise.\n    \"\"\"\n    return '__dataclass_fields__' in cls.__dict__ and not hasattr(cls, '__pydantic_validator__')\n",
      "variables": [
        "cls"
      ],
      "anonymized_code": "def is_stdlib_dataclass(var_1: type[Any], /) -> TypeIs[type[StandardDataclass]]:\n    \"\"\"Returns `True` if the class is a stdlib dataclass and *not* a Pydantic dataclass.\n\n    Unlike the stdlib `dataclasses.is_dataclass()` function, this does *not* include subclasses\n    of a dataclass that are themselves not dataclasses.\n\n    Args:\n        var_1: The class.\n\n    Returns:\n        `True` if the class is a stdlib dataclass, `False` otherwise.\n    \"\"\"\n    return '__dataclass_fields__' in var_1.__dict__ and not hasattr(var_1, '__pydantic_validator__')\n",
      "lines_processed": 13,
      "total_lines": 202
    },
    {
      "file_path": "_dataclasses.py",
      "code": "def set_dataclass_fields(\n    cls: type[StandardDataclass],\n    config_wrapper: _config.ConfigWrapper,\n    ns_resolver: NsResolver | None = None,\n) -> None:\n    \"\"\"Collect and set `cls.__pydantic_fields__`.\n\n    Args:\n        cls: The class.\n        config_wrapper: The config wrapper instance.\n        ns_resolver: Namespace resolver to use when getting dataclass annotations.\n    \"\"\"\n    typevars_map = get_standard_typevars_map(cls)\n    fields = collect_dataclass_fields(\n        cls, ns_resolver=ns_resolver, typevars_map=typevars_map, config_wrapper=config_wrapper\n    )\n\n    cls.__pydantic_fields__ = fields  # type: ignore\n",
      "variables": [
        "cls",
        "config_wrapper",
        "ns_resolver",
        "typevars_map",
        "fields"
      ],
      "anonymized_code": "def set_dataclass_fields(\n    var_1: type[StandardDataclass],\n    var_2: _config.ConfigWrapper,\n    var_3: NsResolver | None = None,\n) -> None:\n    \"\"\"Collect and set `var_1.__pydantic_fields__`.\n\n    Args:\n        var_1: The class.\n        var_2: The config wrapper instance.\n        var_3: Namespace resolver to use when getting dataclass annotations.\n    \"\"\"\n    var_4 = get_standard_typevars_map(var_1)\n    var_5 = collect_dataclass_fields(\n        var_1, var_3=var_3, var_4=var_4, var_2=var_2\n    )\n\n    var_1.__pydantic_fields__ = var_5  # type: ignore\n",
      "lines_processed": 18,
      "total_lines": 202
    },
    {
      "file_path": "using_update.py",
      "code": "def update_lib(lib, *, retry=0):\n    repo = lib['repo']\n    url = f'https://api.github.com/repos/{repo}'\n    resp = session.get(url)\n    if resp.status_code == 403 and retry < 3:\n        print(f'retrying {repo} {retry}')\n        sleep(5)\n        return update_lib(lib, retry=retry + 1)\n\n    resp.raise_for_status()\n    data = resp.json()\n    stars = data['watchers_count']\n    print(f'{repo}: {stars}')\n    lib['stars'] = stars\n",
      "variables": [
        "lib",
        "retry",
        "repo",
        "url",
        "resp",
        "data",
        "stars"
      ],
      "anonymized_code": "def update_lib(var_1, *, var_2=0):\n    var_3 = var_1['var_3']\n    var_4 = f'https://api.github.com/repos/{var_3}'\n    var_5 = session.get(var_4)\n    if var_5.status_code == 403 and var_2 < 3:\n        print(f'retrying {var_3} {var_2}')\n        sleep(5)\n        return update_lib(var_1, var_2=var_2 + 1)\n\n    var_5.raise_for_status()\n    var_6 = var_5.json()\n    var_7 = var_6['watchers_count']\n    print(f'{var_3}: {var_7}')\n    var_1['var_7'] = var_7\n",
      "lines_processed": 14,
      "total_lines": 39
    }
  ],
  "numpy_numpy": [
    {
      "file_path": "__init__.py",
      "code": "def show_cpu_features():\n    from numpy.lib._utils_impl import _opt_info\n    info = _opt_info()\n    info = \"NumPy CPU features: \" + (info or 'nothing enabled')\n    # ASV wrapping stdout & stderr, so we assume having a tty here\n    if 'SHELL' in os.environ and sys.platform != 'win32':\n        # to avoid the red color that imposed by ASV\n        print(f\"\\033[33m{info}\\033[0m\")\n    else:\n        print(info)\n",
      "variables": [
        "info"
      ],
      "anonymized_code": "def show_cpu_features():\n    from numpy.lib._utils_impl import _opt_info\n    var_1 = _opt_info()\n    var_1 = \"NumPy CPU features: \" + (var_1 or 'nothing enabled')\n    # ASV wrapping stdout & stderr, so we assume having a tty here\n    if 'SHELL' in os.environ and sys.platform != 'win32':\n        # to avoid the red color that imposed by ASV\n        print(f\"\\033[33m{var_1}\\033[0m\")\n    else:\n        print(var_1)\n",
      "lines_processed": 10,
      "total_lines": 55
    },
    {
      "file_path": "cmds.py",
      "code": "def _get_numpy_tools(filename):\n    filepath = pathlib.Path('tools', filename)\n    spec = importlib.util.spec_from_file_location(filename.stem, filepath)\n    module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(module)\n    return module\n",
      "variables": [
        "filename",
        "filepath",
        "spec",
        "module"
      ],
      "anonymized_code": "def _get_numpy_tools(var_1):\n    var_2 = pathlib.Path('tools', var_1)\n    var_3 = importlib.util.spec_from_file_location(var_1.stem, var_2)\n    var_4 = importlib.util.module_from_spec(var_3)\n    var_3.loader.exec_module(var_4)\n    return var_4\n",
      "lines_processed": 6,
      "total_lines": 626
    },
    {
      "file_path": "cmds.py",
      "code": "def lint(ctx, fix):\n    \"\"\"\ud83d\udd26 Run lint checks with Ruff\n\n    \\b\n    To run automatic fixes use:\n\n    \\b\n    $ spin lint --fix\n    \"\"\"\n    try:\n        linter = _get_numpy_tools(pathlib.Path('linter.py'))\n    except ModuleNotFoundError as e:\n        raise click.ClickException(\n            f\"{e.msg}. Install using requirements/linter_requirements.txt\"\n        )\n\n    linter.DiffLinter().run_lint(fix)\n",
      "variables": [
        "ctx",
        "fix",
        "linter"
      ],
      "anonymized_code": "def lint(var_1, var_2):\n    \"\"\"\ud83d\udd26 Run lint checks with Ruff\n\n    \\b\n    To run automatic fixes use:\n\n    \\b\n    $ spin lint --var_2\n    \"\"\"\n    try:\n        var_3 = _get_numpy_tools(pathlib.Path('var_3.py'))\n    except ModuleNotFoundError as e:\n        raise click.ClickException(\n            f\"{e.msg}. Install using requirements/linter_requirements.txt\"\n        )\n\n    var_3.DiffLinter().run_lint(var_2)\n",
      "lines_processed": 17,
      "total_lines": 626
    },
    {
      "file_path": "cmds.py",
      "code": "def python(*, parent_callback, **kwargs):\n    env = os.environ\n    env['PYTHONWARNINGS'] = env.get('PYTHONWARNINGS', 'all')\n\n    parent_callback(**kwargs)\n",
      "variables": [
        "parent_callback",
        "kwargs",
        "env"
      ],
      "anonymized_code": "def python(*, var_1, **var_2):\n    var_3 = os.environ\n    var_3['PYTHONWARNINGS'] = var_3.get('PYTHONWARNINGS', 'all')\n\n    var_1(**var_2)\n",
      "lines_processed": 5,
      "total_lines": 626
    },
    {
      "file_path": "cmds.py",
      "code": "def _config_openblas(blas_variant):\n    import importlib\n    basedir = os.getcwd()\n    openblas_dir = os.path.join(basedir, \".openblas\")\n    pkg_config_fname = os.path.join(openblas_dir, \"scipy-openblas.pc\")\n    if blas_variant:\n        module_name = f\"scipy_openblas{blas_variant}\"\n        try:\n            openblas = importlib.import_module(module_name)\n        except ModuleNotFoundError:\n            raise RuntimeError(f\"'pip install {module_name} first\")\n        local = os.path.join(basedir, \"numpy\", \"_distributor_init_local.py\")\n        with open(local, \"wt\", encoding=\"utf8\") as fid:\n            fid.write(f\"import {module_name}\\n\")\n        os.makedirs(openblas_dir, exist_ok=True)\n        with open(pkg_config_fname, \"wt\", encoding=\"utf8\") as fid:\n            fid.write(\n                openblas.get_pkg_config(use_preloading=True)\n            )\n",
      "variables": [
        "blas_variant",
        "basedir",
        "openblas_dir",
        "pkg_config_fname",
        "module_name",
        "openblas",
        "local",
        "fid"
      ],
      "anonymized_code": "def _config_openblas(var_1):\n    import importlib\n    var_2 = os.getcwd()\n    var_3 = os.path.join(var_2, \".var_6\")\n    var_4 = os.path.join(var_3, \"scipy-var_6.pc\")\n    if var_1:\n        var_5 = f\"scipy_openblas{var_1}\"\n        try:\n            var_6 = importlib.import_module(var_5)\n        except ModuleNotFoundError:\n            raise RuntimeError(f\"'pip install {var_5} first\")\n        var_7 = os.path.join(var_2, \"numpy\", \"_distributor_init_local.py\")\n        with open(var_7, \"wt\", encoding=\"utf8\") as var_8:\n            var_8.write(f\"import {var_5}\\n\")\n        os.makedirs(var_3, exist_ok=True)\n        with open(var_4, \"wt\", encoding=\"utf8\") as var_8:\n            var_8.write(\n                var_6.get_pkg_config(use_preloading=True)\n            )\n",
      "lines_processed": 19,
      "total_lines": 626
    },
    {
      "file_path": "cmds.py",
      "code": "def test(*, parent_callback, pytest_args, tests, markexpr, **kwargs):\n    \"\"\"\n    By default, spin will run `-m 'not slow'`. To run the full test suite, use\n    `spin test -m full`\n    \"\"\"  # noqa: E501\n    if (not pytest_args) and (not tests):\n        pytest_args = ('--pyargs', 'numpy')\n\n    if '-m' not in pytest_args:\n        if markexpr != \"full\":\n            pytest_args = ('-m', markexpr) + pytest_args\n\n    kwargs['pytest_args'] = pytest_args\n    parent_callback(**{'pytest_args': pytest_args, 'tests': tests, **kwargs})\n",
      "variables": [
        "parent_callback",
        "pytest_args",
        "tests",
        "markexpr",
        "kwargs"
      ],
      "anonymized_code": "def test(*, var_1, var_2, var_3, var_4, **var_5):\n    \"\"\"\n    By default, spin will run `-m 'not slow'`. To run the full test suite, use\n    `spin test -m full`\n    \"\"\"  # noqa: E501\n    if (not var_2) and (not var_3):\n        var_2 = ('--pyargs', 'numpy')\n\n    if '-m' not in var_2:\n        if var_4 != \"full\":\n            var_2 = ('-m', var_4) + var_2\n\n    var_5['var_2'] = var_2\n    var_1(**{'var_2': var_2, 'var_3': var_3, **var_5})\n",
      "lines_processed": 14,
      "total_lines": 626
    },
    {
      "file_path": "cmds.py",
      "code": "def _dirty_git_working_dir():\n    # Changes to the working directory\n    p0 = spin.util.run(['git', 'diff-files', '--quiet'])\n\n    # Staged changes\n    p1 = spin.util.run(['git', 'diff-index', '--quiet', '--cached', 'HEAD'])\n\n    return (p0.returncode != 0 or p1.returncode != 0)\n",
      "variables": [
        "p0",
        "p1"
      ],
      "anonymized_code": "def _dirty_git_working_dir():\n    # Changes to the working directory\n    var_1 = spin.util.run(['git', 'diff-files', '--quiet'])\n\n    # Staged changes\n    var_2 = spin.util.run(['git', 'diff-index', '--quiet', '--cached', 'HEAD'])\n\n    return (var_1.returncode != 0 or var_2.returncode != 0)\n",
      "lines_processed": 8,
      "total_lines": 626
    },
    {
      "file_path": "cmds.py",
      "code": "def _run_asv(cmd):\n    # Always use ccache, if installed\n    PATH = os.environ['PATH']\n    EXTRA_PATH = os.pathsep.join([\n        '/usr/lib/ccache', '/usr/lib/f90cache',\n        '/usr/local/lib/ccache', '/usr/local/lib/f90cache'\n    ])\n    env = os.environ\n    env['PATH'] = f'{EXTRA_PATH}{os.pathsep}{PATH}'\n\n    # Control BLAS/LAPACK threads\n    env['OPENBLAS_NUM_THREADS'] = '1'\n    env['MKL_NUM_THREADS'] = '1'\n\n    # Limit memory usage\n    try:\n        _set_mem_rlimit()\n    except (ImportError, RuntimeError):\n        pass\n",
      "variables": [
        "cmd",
        "PATH",
        "EXTRA_PATH",
        "env"
      ],
      "anonymized_code": "def _run_asv(var_1):\n    # Always use ccache, if installed\n    var_2 = os.environ['var_2']\n    var_3 = os.pathsep.join([\n        '/usr/lib/ccache', '/usr/lib/f90cache',\n        '/usr/local/lib/ccache', '/usr/local/lib/f90cache'\n    ])\n    var_4 = os.environ\n    var_4['var_2'] = f'{var_3}{os.pathsep}{var_2}'\n\n    # Control BLAS/LAPACK threads\n    var_4['OPENBLAS_NUM_THREADS'] = '1'\n    var_4['MKL_NUM_THREADS'] = '1'\n\n    # Limit memory usage\n    try:\n        _set_mem_rlimit()\n    except (ImportError, RuntimeError):\n        pass\n",
      "lines_processed": 19,
      "total_lines": 626
    }
  ],
  "ivy-llc_ivy": [
    {
      "file_path": "multiversion_framework_directory.py",
      "code": "def directory_generator(req, base=\"/opt/fw/\"):\n    for versions in req:\n        if \"/\" in versions:\n            pkg, ver = versions.split(\"/\")\n            path = base + pkg + \"/\" + ver\n            if not os.path.exists(path):\n                install_pkg(path, pkg + \"==\" + ver)\n        else:\n            install_pkg(base + versions, versions)\n",
      "variables": [
        "req",
        "base",
        "versions",
        "pkg",
        "ver",
        "path"
      ],
      "anonymized_code": "def directory_generator(var_1, var_2=\"/opt/fw/\"):\n    for var_3 in var_1:\n        if \"/\" in var_3:\n            var_4, var_5 = var_3.split(\"/\")\n            var_6 = var_2 + var_4 + \"/\" + var_5\n            if not os.var_6.exists(var_6):\n                install_pkg(var_6, var_4 + \"==\" + var_5)\n        else:\n            install_pkg(var_2 + var_3, var_3)\n",
      "lines_processed": 9,
      "total_lines": 87
    },
    {
      "file_path": "update_db.py",
      "code": "def make_clickable(url, name):\n    return (\n        f'<a href=\"{url}\" rel=\"noopener noreferrer\" '\n        + f'target=\"_blank\"><img src={name}></a>'\n    )\n",
      "variables": [
        "url",
        "name"
      ],
      "anonymized_code": "def make_clickable(var_1, var_2):\n    return (\n        f'<a href=\"{var_1}\" rel=\"noopener noreferrer\" '\n        + f'target=\"_blank\"><img src={var_2}></a>'\n    )\n",
      "lines_processed": 5,
      "total_lines": 56
    },
    {
      "file_path": "conversions.py",
      "code": "def _data_to_new_backend(native_x, native_x_backend):\n    device = native_x_backend.dev(native_x)\n    try:\n        result = ivy.from_dlpack(native_x_backend.to_dlpack(native_x))\n        result = ivy.to_device(result, device)\n    except Exception:\n        np_res = native_x_backend.to_numpy(native_x)\n        result = ivy.asarray(np_res, device=device)\n    return result\n",
      "variables": [
        "native_x",
        "native_x_backend",
        "device",
        "result",
        "np_res"
      ],
      "anonymized_code": "def _data_to_new_backend(var_1, var_2):\n    var_3 = var_2.dev(var_1)\n    try:\n        var_4 = ivy.from_dlpack(var_2.to_dlpack(var_1))\n        var_4 = ivy.to_device(var_4, var_3)\n    except Exception:\n        var_5 = var_2.to_numpy(var_1)\n        var_4 = ivy.asarray(var_5, var_3=var_3)\n    return var_4\n",
      "lines_processed": 9,
      "total_lines": 408
    },
    {
      "file_path": "conversions.py",
      "code": "def _to_ivy(x: Any) -> Any:\n    if isinstance(x, ivy.Array):\n        return x\n    elif isinstance(x, ivy.NativeShape):\n        return ivy.Shape(x)\n    elif isinstance(x, ivy.Container):\n        return x.to_ivy()\n    if ivy.is_native_array(x) or isinstance(x, np.ndarray):\n        return ivy.Array(x)\n    return x\n",
      "variables": [
        "x"
      ],
      "anonymized_code": "def _to_ivy(var_1: Any) -> Any:\n    if isinstance(var_1, ivy.Array):\n        return var_1\n    elif isinstance(var_1, ivy.NativeShape):\n        return ivy.Shape(var_1)\n    elif isinstance(var_1, ivy.Container):\n        return var_1.to_ivy()\n    if ivy.is_native_array(var_1) or isinstance(var_1, np.ndarray):\n        return ivy.Array(var_1)\n    return var_1\n",
      "lines_processed": 10,
      "total_lines": 408
    },
    {
      "file_path": "conversions.py",
      "code": "def _to_native(x: Any, inplace: bool = False, to_ignore: tuple = ()) -> Any:\n    to_ignore = ivy.default(to_ignore, ())\n    if isinstance(x, to_ignore):\n        return x\n    if isinstance(x, ivy.Array):\n        return x.data\n    # to prevent the graph from breaking for the time being\n    elif type(x) is ivy.Shape:\n        return x.shape\n    elif isinstance(x, ivy.Container):\n        return x.cont_map(\n            lambda x_, _: _to_native(x_, inplace=inplace, to_ignore=to_ignore),\n            inplace=inplace,\n        )\n    return x\n",
      "variables": [
        "x",
        "inplace",
        "to_ignore",
        "x_",
        "_"
      ],
      "anonymized_code": "def _to_native(var_1: Any, var_2: bool = False, var_3: tuple = ()) -> Any:\n    var_3 = ivy.default(var_3, ())\n    if isinstance(var_1, var_3):\n        return var_1\n    if isinstance(var_1, ivy.Array):\n        return var_1.data\n    # to prevent the graph from breaking for the time being\n    elif type(var_1) is ivy.Shape:\n        return var_1.shape\n    elif isinstance(var_1, ivy.Container):\n        return var_1.cont_map(\n            lambda var_4, var_5: _to_native(var_4, var_2=var_2, var_3=var_3),\n            var_2=var_2,\n        )\n    return var_1\n",
      "lines_processed": 15,
      "total_lines": 408
    },
    {
      "file_path": "conversions.py",
      "code": "def _to_new_backend(\n    x: Any,\n    native: bool = False,\n    inplace: bool = False,\n    to_ignore: tuple = (),\n) -> Any:\n    if isinstance(x, ivy.Container):\n        to_ignore = ivy.default(to_ignore, ())\n        return x.cont_map(\n            lambda x_, _: _to_new_backend(\n                x_, native=native, inplace=inplace, to_ignore=to_ignore\n            ),\n            inplace=inplace,\n        )\n    return _array_to_new_backend(x, native=native)\n",
      "variables": [
        "x",
        "native",
        "inplace",
        "to_ignore",
        "x_",
        "_"
      ],
      "anonymized_code": "def _to_new_backend(\n    var_1: Any,\n    var_2: bool = False,\n    var_3: bool = False,\n    var_4: tuple = (),\n) -> Any:\n    if isinstance(var_1, ivy.Container):\n        var_4 = ivy.default(var_4, ())\n        return var_1.cont_map(\n            lambda var_5, var_6: _to_new_backend(\n                var_5, var_2=var_2, var_3=var_3, var_4=var_4\n            ),\n            var_3=var_3,\n        )\n    return _array_to_new_backend(var_1, var_2=var_2)\n",
      "lines_processed": 15,
      "total_lines": 408
    },
    {
      "file_path": "gpu_framework_directory.py",
      "code": "def get_latest_package_version(package_name):\n    try:\n        url = f\"https://pypi.org/pypi/{package_name}/json\"\n        response = requests.get(url, timeout=10)\n        response.raise_for_status()\n        package_info = response.json()\n        return package_info[\"info\"][\"version\"]\n    except requests.exceptions.RequestException as e:\n        print(f\"Error: Failed to fetch package information for {package_name}.\")\n        return None\n",
      "variables": [
        "package_name",
        "url",
        "response",
        "package_info"
      ],
      "anonymized_code": "def get_latest_package_version(var_1):\n    try:\n        var_2 = f\"https://pypi.org/pypi/{var_1}/json\"\n        var_3 = requests.get(var_2, timeout=10)\n        var_3.raise_for_status()\n        var_4 = var_3.json()\n        return var_4[\"info\"][\"version\"]\n    except requests.exceptions.RequestException as e:\n        print(f\"Error: Failed to fetch package information for {var_1}.\")\n        return None\n",
      "lines_processed": 10,
      "total_lines": 93
    },
    {
      "file_path": "gpu_framework_directory.py",
      "code": "def directory_generator(req, base=\"/opt/fw/\"):\n    for versions in req:\n        if \"/\" in versions:\n            pkg, ver = versions.split(\"/\")\n            path = base + pkg + \"/\" + ver\n            if not os.path.exists(path):\n                install_pkg(path, pkg + \"==\" + ver)\n        else:\n            install_pkg(base + versions, versions)\n",
      "variables": [
        "req",
        "base",
        "versions",
        "pkg",
        "ver",
        "path"
      ],
      "anonymized_code": "def directory_generator(var_1, var_2=\"/opt/fw/\"):\n    for var_3 in var_1:\n        if \"/\" in var_3:\n            var_4, var_5 = var_3.split(\"/\")\n            var_6 = var_2 + var_4 + \"/\" + var_5\n            if not os.var_6.exists(var_6):\n                install_pkg(var_6, var_4 + \"==\" + var_5)\n        else:\n            install_pkg(var_2 + var_3, var_3)\n",
      "lines_processed": 9,
      "total_lines": 93
    }
  ],
  "apache_airflow": [
    {
      "file_path": "diagram_dag_processor_airflow_architecture.py",
      "code": "def generate_dag_processor_airflow_diagram():\n    dag_processor_architecture_image_file = (MY_DIR / MY_FILENAME).with_suffix(\".png\")\n    console.print(f\"[bright_blue]Generating architecture image {dag_processor_architecture_image_file}\")\n    with Diagram(\n        name=\"\",\n        show=False,\n        direction=\"LR\",\n        filename=MY_FILENAME,\n        outformat=\"png\",\n        graph_attr=graph_attr,\n        edge_attr=edge_attr,\n    ):\n        operations_user = User(\"Operations User\")\n        deployment_manager = User(\"Deployment Manager\")\n\n        with Cluster(\"Security perimeter with no DAG code execution\", graph_attr={\"bgcolor\": \"lightgrey\"}):\n            with Cluster(\"Scheduling\\n\\n\"):\n                schedulers = Custom(\"Scheduler(s)\", PYTHON_MULTIPROCESS_LOGO.as_posix())\n\n",
      "variables": [
        "dag_processor_architecture_image_file",
        "operations_user",
        "deployment_manager",
        "schedulers"
      ],
      "anonymized_code": "def generate_dag_processor_airflow_diagram():\n    var_1 = (MY_DIR / MY_FILENAME).with_suffix(\".png\")\n    console.print(f\"[bright_blue]Generating architecture image {var_1}\")\n    with Diagram(\n        name=\"\",\n        show=False,\n        direction=\"LR\",\n        filename=MY_FILENAME,\n        outformat=\"png\",\n        graph_attr=graph_attr,\n        edge_attr=edge_attr,\n    ):\n        var_2 = User(\"Operations User\")\n        var_3 = User(\"Deployment Manager\")\n\n        with Cluster(\"Security perimeter with no DAG code execution\", graph_attr={\"bgcolor\": \"lightgrey\"}):\n            with Cluster(\"Scheduling\\n\\n\"):\n                var_4 = Custom(\"Scheduler(s)\", PYTHON_MULTIPROCESS_LOGO.as_posix())\n\n",
      "lines_processed": 19,
      "total_lines": 112
    },
    {
      "file_path": "conf.py",
      "code": "def add_airflow_core_exclude_patterns_to_sphinx(exclude_patterns: list[str]):\n    \"\"\"\n    Add excluded files to Sphinx exclude patterns.\n\n    Excludes all files from autoapi except the ones we want to allow.\n\n    :param root: The root directory of the package.\n    :param allowed_top_level_files: Tuple of allowed top-level files.\n    :param browsable_packages: Set of browsable packages.\n    :param browsable_utils: Set of browsable utils.\n    :param models_included: Set of included models.\n    \"\"\"\n    # first - excluded everything that is not allowed or browsable\n    root = AIRFLOW_CORE_SRC_PATH / \"airflow\"\n    for path in root.iterdir():\n        if path.is_file() and path.name not in ALLOWED_TOP_LEVEL_FILES:\n            exclude_patterns.append(get_rst_filepath_from_path(path, root.parent))\n        if path.is_dir() and path.name not in PACKAGES_THAT_WE_SHOULD_ADD_TO_API_DOCS:\n            exclude_patterns.append(f\"_api/airflow/{path.name}\")\n",
      "variables": [
        "exclude_patterns",
        "root",
        "path"
      ],
      "anonymized_code": "def add_airflow_core_exclude_patterns_to_sphinx(var_1: list[str]):\n    \"\"\"\n    Add excluded files to Sphinx exclude patterns.\n\n    Excludes all files from autoapi except the ones we want to allow.\n\n    :param var_2: The var_2 directory of the package.\n    :param allowed_top_level_files: Tuple of allowed top-level files.\n    :param browsable_packages: Set of browsable packages.\n    :param browsable_utils: Set of browsable utils.\n    :param models_included: Set of included models.\n    \"\"\"\n    # first - excluded everything that is not allowed or browsable\n    var_2 = AIRFLOW_CORE_SRC_PATH / \"airflow\"\n    for var_3 in var_2.iterdir():\n        if var_3.is_file() and var_3.name not in ALLOWED_TOP_LEVEL_FILES:\n            var_1.append(get_rst_filepath_from_path(var_3, var_2.parent))\n        if var_3.is_dir() and var_3.name not in PACKAGES_THAT_WE_SHOULD_ADD_TO_API_DOCS:\n            var_1.append(f\"_api/airflow/{var_3.name}\")\n",
      "lines_processed": 19,
      "total_lines": 375
    },
    {
      "file_path": "conf.py",
      "code": "def setup(sphinx):\n    sphinx.connect(\"autoapi-skip-member\", skip_util_classes_extension)\n",
      "variables": [
        "sphinx"
      ],
      "anonymized_code": "def setup(var_1):\n    var_1.connect(\"autoapi-skip-member\", skip_util_classes_extension)\n",
      "lines_processed": 2,
      "total_lines": 375
    },
    {
      "file_path": "diagram_multi_team_airflow_architecture.py",
      "code": "def generate_dag_processor_airflow_diagram():\n    dag_processor_architecture_image_file = (MY_DIR / MY_FILENAME).with_suffix(\".png\")\n    console.print(f\"[bright_blue]Generating architecture image {dag_processor_architecture_image_file}\")\n    with Diagram(\n        name=\"\",\n        show=False,\n        direction=\"LR\",\n        filename=MY_FILENAME,\n        outformat=\"png\",\n        graph_attr=graph_attr,\n        edge_attr=edge_attr,\n    ):\n        with Cluster(\n            \"Common Organization Airflow Deployment\", graph_attr={\"bgcolor\": \"lightgrey\", \"fontsize\": \"22\"}\n        ):\n            with Cluster(\"Scheduling\\n\\n\"):\n                executor_1 = Custom(\"Executor\\nTeam 1\", PYTHON_MULTIPROCESS_LOGO.as_posix())\n                executor_2 = Custom(\"Executor\\nTeam 2\", PYTHON_MULTIPROCESS_LOGO.as_posix())\n                schedulers = Custom(\"Scheduler(s)\", PYTHON_MULTIPROCESS_LOGO.as_posix())\n",
      "variables": [
        "dag_processor_architecture_image_file",
        "executor_1",
        "executor_2",
        "schedulers"
      ],
      "anonymized_code": "def generate_dag_processor_airflow_diagram():\n    var_1 = (MY_DIR / MY_FILENAME).with_suffix(\".png\")\n    console.print(f\"[bright_blue]Generating architecture image {var_1}\")\n    with Diagram(\n        name=\"\",\n        show=False,\n        direction=\"LR\",\n        filename=MY_FILENAME,\n        outformat=\"png\",\n        graph_attr=graph_attr,\n        edge_attr=edge_attr,\n    ):\n        with Cluster(\n            \"Common Organization Airflow Deployment\", graph_attr={\"bgcolor\": \"lightgrey\", \"fontsize\": \"22\"}\n        ):\n            with Cluster(\"Scheduling\\n\\n\"):\n                var_2 = Custom(\"Executor\\nTeam 1\", PYTHON_MULTIPROCESS_LOGO.as_posix())\n                var_3 = Custom(\"Executor\\nTeam 2\", PYTHON_MULTIPROCESS_LOGO.as_posix())\n                var_4 = Custom(\"Scheduler(s)\", PYTHON_MULTIPROCESS_LOGO.as_posix())\n",
      "lines_processed": 19,
      "total_lines": 253
    },
    {
      "file_path": "diagram_task_lifecycle.py",
      "code": "def generate_task_lifecycle_diagram():\n    image_file = (MY_DIR / MY_FILENAME).with_suffix(\".png\")\n\n    console.print(f\"[bright_blue]Generating task lifecycle image {image_file}\")\n    with Diagram(\n        name=\"\",\n        show=False,\n        direction=\"LR\",\n        filename=MY_FILENAME,\n        outformat=\"png\",\n        graph_attr=graph_attr,\n        edge_attr=edge_attr,\n        node_attr=node_attr,\n    ):\n        state_none = Custom(\"none\", SHARED_STATE_IMG, **STATE_NODE_ATTRS)\n        state_removed = Custom(\"removed\", TERMINAL_STATE_IMG, **STATE_NODE_ATTRS)\n        state_upstream_failed = Custom(\"upstream_failed\", TERMINAL_STATE_IMG, **STATE_NODE_ATTRS)\n        state_skipped = Custom(\"skipped\", TERMINAL_STATE_IMG, **STATE_NODE_ATTRS)\n        state_scheduled = Custom(\"scheduled\", SHARED_STATE_IMG, **STATE_NODE_ATTRS)\n",
      "variables": [
        "image_file",
        "state_none",
        "state_removed",
        "state_upstream_failed",
        "state_skipped",
        "state_scheduled"
      ],
      "anonymized_code": "def generate_task_lifecycle_diagram():\n    var_1 = (MY_DIR / MY_FILENAME).with_suffix(\".png\")\n\n    console.print(f\"[bright_blue]Generating task lifecycle image {var_1}\")\n    with Diagram(\n        name=\"\",\n        show=False,\n        direction=\"LR\",\n        filename=MY_FILENAME,\n        outformat=\"png\",\n        graph_attr=graph_attr,\n        edge_attr=edge_attr,\n        node_attr=node_attr,\n    ):\n        var_2 = Custom(\"none\", SHARED_STATE_IMG, **STATE_NODE_ATTRS)\n        var_3 = Custom(\"removed\", TERMINAL_STATE_IMG, **STATE_NODE_ATTRS)\n        var_4 = Custom(\"upstream_failed\", TERMINAL_STATE_IMG, **STATE_NODE_ATTRS)\n        var_5 = Custom(\"skipped\", TERMINAL_STATE_IMG, **STATE_NODE_ATTRS)\n        var_6 = Custom(\"scheduled\", SHARED_STATE_IMG, **STATE_NODE_ATTRS)\n",
      "lines_processed": 19,
      "total_lines": 213
    },
    {
      "file_path": "diagram_distributed_airflow_architecture.py",
      "code": "def generate_distributed_airflow_diagram():\n    image_file = (MY_DIR / MY_FILENAME).with_suffix(\".png\")\n\n    console.print(f\"[bright_blue]Generating architecture image {image_file}\")\n    with Diagram(\n        name=\"\",\n        show=False,\n        direction=\"LR\",\n        filename=MY_FILENAME,\n        outformat=\"png\",\n        graph_attr=graph_attr,\n        edge_attr=edge_attr,\n    ):\n        dag_author = User(\"DAG Author\")\n        deployment_manager = User(\"Deployment Manager\")\n\n        dag_files = Custom(\"DAG files\", MULTIPLE_FILES_IMAGE.as_posix(), height=\"1.8\")\n        dag_author >> Edge(color=\"brown\", style=\"solid\", reverse=False, label=\"author\\n\\n\") >> dag_files\n\n",
      "variables": [
        "image_file",
        "dag_author",
        "deployment_manager",
        "dag_files"
      ],
      "anonymized_code": "def generate_distributed_airflow_diagram():\n    var_1 = (MY_DIR / MY_FILENAME).with_suffix(\".png\")\n\n    console.print(f\"[bright_blue]Generating architecture image {var_1}\")\n    with Diagram(\n        name=\"\",\n        show=False,\n        direction=\"LR\",\n        filename=MY_FILENAME,\n        outformat=\"png\",\n        graph_attr=graph_attr,\n        edge_attr=edge_attr,\n    ):\n        var_2 = User(\"DAG Author\")\n        var_3 = User(\"Deployment Manager\")\n\n        var_4 = Custom(\"DAG files\", MULTIPLE_FILES_IMAGE.as_posix(), height=\"1.8\")\n        var_2 >> Edge(color=\"brown\", style=\"solid\", reverse=False, label=\"author\\n\\n\") >> var_4\n\n",
      "lines_processed": 19,
      "total_lines": 112
    },
    {
      "file_path": "__main__.py",
      "code": "def main():\n    conf = configuration.conf\n    if conf.get(\"core\", \"security\") == \"kerberos\":\n        os.environ[\"KRB5CCNAME\"] = conf.get(\"kerberos\", \"ccache\")\n        os.environ[\"KRB5_KTNAME\"] = conf.get(\"kerberos\", \"keytab\")\n    parser = cli_parser.get_parser()\n    argcomplete.autocomplete(parser)\n    args = parser.parse_args()\n    if args.subcommand not in [\"lazy_loaded\", \"version\"]:\n        # Here we ensure that the default configuration is written if needed before running any command\n        # that might need it. This used to be done during configuration initialization but having it\n        # in main ensures that it is not done during tests and other ways airflow imports are used\n        from airflow.configuration import write_default_airflow_configuration_if_needed\n\n        conf = write_default_airflow_configuration_if_needed()\n    args.func(args)\n",
      "variables": [
        "conf",
        "parser",
        "args"
      ],
      "anonymized_code": "def main():\n    var_1 = configuration.var_1\n    if var_1.get(\"core\", \"security\") == \"kerberos\":\n        os.environ[\"KRB5CCNAME\"] = var_1.get(\"kerberos\", \"ccache\")\n        os.environ[\"KRB5_KTNAME\"] = var_1.get(\"kerberos\", \"keytab\")\n    var_2 = cli_parser.get_parser()\n    argcomplete.autocomplete(var_2)\n    var_3 = var_2.parse_args()\n    if var_3.subcommand not in [\"lazy_loaded\", \"version\"]:\n        # Here we ensure that the default configuration is written if needed before running any command\n        # that might need it. This used to be done during configuration initialization but having it\n        # in main ensures that it is not done during tests and other ways airflow imports are used\n        from airflow.configuration import write_default_airflow_configuration_if_needed\n\n        var_1 = write_default_airflow_configuration_if_needed()\n    var_3.func(var_3)\n",
      "lines_processed": 16,
      "total_lines": 59
    }
  ],
  "iterative_dvc": [
    {
      "file_path": "analytics.py",
      "code": "def _system_info():\n    import platform\n    import sys\n\n    import distro\n\n    system = platform.system()\n\n    if system == \"Windows\":\n        version = sys.getwindowsversion()  # type: ignore[attr-defined]\n\n        return {\n            \"os\": \"windows\",\n            \"windows_version_build\": version.build,\n            \"windows_version_major\": version.major,\n            \"windows_version_minor\": version.minor,\n            \"windows_version_service_pack\": version.service_pack,\n        }\n\n",
      "variables": [
        "system",
        "version"
      ],
      "anonymized_code": "def _system_info():\n    import platform\n    import sys\n\n    import distro\n\n    var_1 = platform.var_1()\n\n    if var_1 == \"Windows\":\n        var_2 = sys.getwindowsversion()  # type: ignore[attr-defined]\n\n        return {\n            \"os\": \"windows\",\n            \"windows_version_build\": var_2.build,\n            \"windows_version_major\": var_2.major,\n            \"windows_version_minor\": var_2.minor,\n            \"windows_version_service_pack\": var_2.service_pack,\n        }\n\n",
      "lines_processed": 19,
      "total_lines": 164
    },
    {
      "file_path": "analytics.py",
      "code": "def send(path):\n    \"\"\"\n    Side effect: Removes the report after sending it.\n\n    The report is generated and stored in a temporary file, see:\n    `collect_and_send_report`. Sending happens on another process,\n    thus, the need of removing such file afterwards.\n    \"\"\"\n    import requests\n\n    url = os.environ.get(DVC_ANALYTICS_ENDPOINT, \"https://analytics.dvc.org\")\n    headers = {\"content-type\": \"application/json\"}\n\n    with open(path, encoding=\"utf-8\") as fobj:\n        report = json.load(fobj)\n\n    report.update(_runtime_info())\n\n    logger.debug(\"uploading report to %s\", url)\n",
      "variables": [
        "path",
        "url",
        "headers",
        "fobj",
        "report"
      ],
      "anonymized_code": "def send(var_1):\n    \"\"\"\n    Side effect: Removes the var_5 after sending it.\n\n    The var_5 is generated and stored in a temporary file, see:\n    `collect_and_send_report`. Sending happens on another process,\n    thus, the need of removing such file afterwards.\n    \"\"\"\n    import requests\n\n    var_2 = os.environ.get(DVC_ANALYTICS_ENDPOINT, \"https://analytics.dvc.org\")\n    var_3 = {\"content-type\": \"application/json\"}\n\n    with open(var_1, encoding=\"utf-8\") as var_4:\n        var_5 = json.load(var_4)\n\n    var_5.update(_runtime_info())\n\n    logger.debug(\"uploading var_5 to %s\", var_2)\n",
      "lines_processed": 19,
      "total_lines": 164
    },
    {
      "file_path": "analytics.py",
      "code": "def is_enabled():\n    from dvc.config import Config, to_bool\n    from dvc.utils import env2bool\n\n    if env2bool(\"DVC_TEST\"):\n        return False\n\n    enabled = not os.getenv(DVC_NO_ANALYTICS)\n    if enabled:\n        enabled = to_bool(\n            Config.from_cwd(validate=False).get(\"core\", {}).get(\"analytics\", \"true\")\n        )\n\n    logger.debug(\"Analytics is %sabled.\", \"en\" if enabled else \"dis\")\n\n    return enabled\n",
      "variables": [
        "enabled"
      ],
      "anonymized_code": "def is_enabled():\n    from dvc.config import Config, to_bool\n    from dvc.utils import env2bool\n\n    if env2bool(\"DVC_TEST\"):\n        return False\n\n    var_1 = not os.getenv(DVC_NO_ANALYTICS)\n    if var_1:\n        var_1 = to_bool(\n            Config.from_cwd(validate=False).get(\"core\", {}).get(\"analytics\", \"true\")\n        )\n\n    logger.debug(\"Analytics is %sabled.\", \"en\" if var_1 else \"dis\")\n\n    return var_1\n",
      "lines_processed": 16,
      "total_lines": 164
    },
    {
      "file_path": "analytics.py",
      "code": "def collect_and_send_report(args=None, return_code=None):\n    \"\"\"\n    Collect information from the runtime/environment and the command\n    being executed into a report and send it over the network.\n\n    To prevent analytics from blocking the execution of the main thread,\n    sending the report is done in a separate process.\n\n    The inter-process communication happens through a file containing the\n    report as a JSON, where the _collector_ generates it and the _sender_\n    removes it after sending it.\n    \"\"\"\n    import tempfile\n\n    from dvc.daemon import daemon\n\n    report = {}\n\n    # Include command execution information on the report only when available.\n",
      "variables": [
        "args",
        "return_code",
        "report"
      ],
      "anonymized_code": "def collect_and_send_report(var_1=None, var_2=None):\n    \"\"\"\n    Collect information from the runtime/environment and the command\n    being executed into a var_3 and send it over the network.\n\n    To prevent analytics from blocking the execution of the main thread,\n    sending the var_3 is done in a separate process.\n\n    The inter-process communication happens through a file containing the\n    var_3 as a JSON, where the _collector_ generates it and the _sender_\n    removes it after sending it.\n    \"\"\"\n    import tempfile\n\n    from dvc.daemon import daemon\n\n    var_3 = {}\n\n    # Include command execution information on the var_3 only when available.\n",
      "lines_processed": 19,
      "total_lines": 164
    },
    {
      "file_path": "analytics.py",
      "code": "def _scm_in_use():\n    from dvc.exceptions import NotDvcRepoError\n    from dvc.repo import Repo\n    from dvc.scm import NoSCM\n\n    from .scm import SCM, SCMError\n\n    try:\n        scm = SCM(root_dir=Repo.find_root())\n        return type(scm).__name__\n    except SCMError:\n        return NoSCM.__name__\n    except NotDvcRepoError:\n        pass\n",
      "variables": [
        "scm"
      ],
      "anonymized_code": "def _scm_in_use():\n    from dvc.exceptions import NotDvcRepoError\n    from dvc.repo import Repo\n    from dvc.var_1 import NoSCM\n\n    from .var_1 import SCM, SCMError\n\n    try:\n        var_1 = SCM(root_dir=Repo.find_root())\n        return type(var_1).__name__\n    except SCMError:\n        return NoSCM.__name__\n    except NotDvcRepoError:\n        pass\n",
      "lines_processed": 14,
      "total_lines": 164
    },
    {
      "file_path": "_debug.py",
      "code": "def _sigshow(_, frame: Optional[\"FrameType\"]) -> None:\n    import sys\n    from shutil import get_terminal_size\n    from traceback import format_stack\n\n    lines = \"\\u2015\" * get_terminal_size().columns\n    stack = format_stack(frame)\n    print(lines, \"\\n\", *stack, lines, sep=\"\", file=sys.stderr)  # noqa: T201\n",
      "variables": [
        "_",
        "frame",
        "lines",
        "stack"
      ],
      "anonymized_code": "def _sigshow(var_1, var_2: Optional[\"FrameType\"]) -> None:\n    import sys\n    from shutil import get_terminal_size\n    from traceback import format_stack\n\n    var_3 = \"\\u2015\" * get_terminal_size().columns\n    var_4 = format_stack(var_2)\n    print(var_3, \"\\n\", *var_4, var_3, sep=\"\", file=sys.stderr)  # noqa: T201\n",
      "lines_processed": 8,
      "total_lines": 288
    },
    {
      "file_path": "_debug.py",
      "code": "def yappi_profile(\n    path: Optional[Union[Callable[[], str], str]] = None,\n    wall_clock: Optional[bool] = True,\n    separate_threads: Optional[bool] = False,\n):\n    try:\n        import yappi\n    except ImportError:\n        print(\"Failed to run profiler, yappi is not installed\")  # noqa: T201\n        yield\n        return\n\n    yappi.set_clock_type(\"wall\" if wall_clock else \"cpu\")\n\n    yappi.start()\n    try:\n        yield\n    finally:\n        yappi.stop()\n",
      "variables": [
        "path",
        "wall_clock",
        "separate_threads"
      ],
      "anonymized_code": "def yappi_profile(\n    var_1: Optional[Union[Callable[[], str], str]] = None,\n    var_2: Optional[bool] = True,\n    var_3: Optional[bool] = False,\n):\n    try:\n        import yappi\n    except ImportError:\n        print(\"Failed to run profiler, yappi is not installed\")  # noqa: T201\n        yield\n        return\n\n    yappi.set_clock_type(\"wall\" if var_2 else \"cpu\")\n\n    yappi.start()\n    try:\n        yield\n    finally:\n        yappi.stop()\n",
      "lines_processed": 19,
      "total_lines": 288
    },
    {
      "file_path": "_debug.py",
      "code": "def instrument(html_output=False):\n    \"\"\"Run a statistical profiler\"\"\"\n    try:\n        from pyinstrument import Profiler\n    except ImportError:\n        print(\"Failed to run profiler, pyinstrument is not installed\")  # noqa: T201\n        yield\n        return\n\n    profiler = Profiler()\n\n    profiler.start()\n    try:\n        yield\n    finally:\n        profiler.stop()\n\n        if html_output:\n            profiler.open_in_browser()\n",
      "variables": [
        "html_output",
        "profiler"
      ],
      "anonymized_code": "def instrument(var_1=False):\n    \"\"\"Run a statistical var_2\"\"\"\n    try:\n        from pyinstrument import Profiler\n    except ImportError:\n        print(\"Failed to run var_2, pyinstrument is not installed\")  # noqa: T201\n        yield\n        return\n\n    var_2 = Profiler()\n\n    var_2.start()\n    try:\n        yield\n    finally:\n        var_2.stop()\n\n        if var_1:\n            var_2.open_in_browser()\n",
      "lines_processed": 19,
      "total_lines": 288
    },
    {
      "file_path": "_debug.py",
      "code": "def viztracer_profile(\n    path: Union[Callable[[], str], str],\n    depth: int = -1,\n    log_async: bool = False,\n):\n    try:\n        import viztracer\n    except ImportError:\n        print(\"Failed to run profiler, viztracer is not installed\")  # noqa: T201\n        yield\n        return\n\n    tracer = viztracer.VizTracer(max_stack_depth=depth, log_async=log_async)\n\n    tracer.start()\n    try:\n        yield\n    finally:\n        tracer.stop()\n",
      "variables": [
        "path",
        "depth",
        "log_async",
        "tracer"
      ],
      "anonymized_code": "def viztracer_profile(\n    var_1: Union[Callable[[], str], str],\n    var_2: int = -1,\n    var_3: bool = False,\n):\n    try:\n        import viztracer\n    except ImportError:\n        print(\"Failed to run profiler, viztracer is not installed\")  # noqa: T201\n        yield\n        return\n\n    var_4 = viztracer.VizTracer(max_stack_depth=var_2, var_3=var_3)\n\n    var_4.start()\n    try:\n        yield\n    finally:\n        var_4.stop()\n",
      "lines_processed": 19,
      "total_lines": 288
    },
    {
      "file_path": "_debug.py",
      "code": "def profile(dump_path: Optional[str] = None):\n    \"\"\"Run a cprofile\"\"\"\n    import cProfile\n\n    prof = cProfile.Profile()\n    prof.enable()\n\n    try:\n        yield\n    finally:\n        prof.disable()\n        if dump_path:\n            prof.dump_stats(dump_path)\n        else:\n            prof.print_stats(sort=\"cumtime\")\n",
      "variables": [
        "dump_path",
        "prof"
      ],
      "anonymized_code": "def profile(var_1: Optional[str] = None):\n    \"\"\"Run a cprofile\"\"\"\n    import cProfile\n\n    var_2 = cProfile.Profile()\n    var_2.enable()\n\n    try:\n        yield\n    finally:\n        var_2.disable()\n        if var_1:\n            var_2.dump_stats(var_1)\n        else:\n            var_2.print_stats(sort=\"cumtime\")\n",
      "lines_processed": 15,
      "total_lines": 288
    }
  ],
  "facebookresearch_detectron2": [
    {
      "file_path": "torchvision_imagenet_R_50.py",
      "code": "def build_data_loader(dataset, batch_size, num_workers, training=True):\n    return torch.utils.data.DataLoader(\n        dataset,\n        sampler=(TrainingSampler if training else InferenceSampler)(len(dataset)),\n        batch_size=batch_size,\n        num_workers=num_workers,\n        pin_memory=True,\n    )\n",
      "variables": [
        "dataset",
        "batch_size",
        "num_workers",
        "training"
      ],
      "anonymized_code": "def build_data_loader(var_1, var_2, var_3, var_4=True):\n    return torch.utils.data.DataLoader(\n        var_1,\n        sampler=(TrainingSampler if var_4 else InferenceSampler)(len(var_1)),\n        var_2=var_2,\n        var_3=var_3,\n        pin_memory=True,\n    )\n",
      "lines_processed": 8,
      "total_lines": 149
    }
  ],
  "celery_celery": [
    {
      "file_path": "registry.py",
      "code": "def _unpickle_task_v2(name, module=None):\n    if module:\n        import_module(module)\n    return get_current_app().tasks[name]\n",
      "variables": [
        "name",
        "module"
      ],
      "anonymized_code": "def _unpickle_task_v2(var_1, var_2=None):\n    if var_2:\n        import_module(var_2)\n    return get_current_app().tasks[var_1]\n",
      "lines_processed": 4,
      "total_lines": 68
    },
    {
      "file_path": "registry.py",
      "code": "def _unpickle_task(name):\n    return get_current_app().tasks[name]\n",
      "variables": [
        "name"
      ],
      "anonymized_code": "def _unpickle_task(var_1):\n    return get_current_app().tasks[var_1]\n",
      "lines_processed": 2,
      "total_lines": 68
    },
    {
      "file_path": "builtins.py",
      "code": "def add_starmap_task(app):\n    from celery.canvas import signature\n\n    @app.task(name='celery.starmap', shared=False, lazy=False)\n    def xstarmap(task, it):\n        task = signature(task, app=app).type\n        return [task(*item) for item in it]\n    return xstarmap\n",
      "variables": [
        "app",
        "task",
        "it",
        "item"
      ],
      "anonymized_code": "def add_starmap_task(var_1):\n    from celery.canvas import signature\n\n    @var_1.var_2(name='celery.starmap', shared=False, lazy=False)\n    def xstarmap(var_2, var_3):\n        var_2 = signature(var_2, var_1=var_1).type\n        return [var_2(*var_4) for var_4 in var_3]\n    return xstarmap\n",
      "lines_processed": 8,
      "total_lines": 187
    },
    {
      "file_path": "builtins.py",
      "code": "def add_chunk_task(app):\n    from celery.canvas import chunks as _chunks\n\n    @app.task(name='celery.chunks', shared=False, lazy=False)\n    def chunks(task, it, n):\n        return _chunks.apply_chunks(task, it, n)\n    return chunks\n",
      "variables": [
        "app",
        "task",
        "it",
        "n"
      ],
      "anonymized_code": "def add_chunk_task(var_1):\n    from celery.canvas import chunks as _chunks\n\n    @var_1.var_2(name='celery.chunks', shared=False, lazy=False)\n    def chunks(var_2, var_3, var_4):\n        return _chunks.apply_chunks(var_2, var_3, var_4)\n    return chunks\n",
      "lines_processed": 7,
      "total_lines": 187
    },
    {
      "file_path": "builtins.py",
      "code": "def add_accumulate_task(app):\n    \"\"\"Task used by Task.replace when replacing task with group.\"\"\"\n    @app.task(bind=True, name='celery.accumulate', shared=False, lazy=False)\n    def accumulate(self, *args, **kwargs):\n        index = kwargs.get('index')\n        return args[index] if index is not None else args\n    return accumulate\n",
      "variables": [
        "app",
        "self",
        "args",
        "kwargs",
        "index"
      ],
      "anonymized_code": "def add_accumulate_task(var_1):\n    \"\"\"Task used by Task.replace when replacing task with group.\"\"\"\n    @var_1.task(bind=True, name='celery.accumulate', shared=False, lazy=False)\n    def accumulate(var_2, *var_3, **var_4):\n        var_5 = var_4.get('var_5')\n        return var_3[var_5] if var_5 is not None else var_3\n    return accumulate\n",
      "lines_processed": 7,
      "total_lines": 187
    },
    {
      "file_path": "builtins.py",
      "code": "def add_group_task(app):\n    \"\"\"No longer used, but here for backwards compatibility.\"\"\"\n    from celery.canvas import maybe_signature\n    from celery.result import result_from_tuple\n\n    @app.task(name='celery.group', bind=True, shared=False, lazy=False)\n    def group(self, tasks, result, group_id, partial_args, add_to_parent=True):\n        app = self.app\n        result = result_from_tuple(result, app)\n        # any partial args are added to all tasks in the group\n        taskit = (maybe_signature(task, app=app).clone(partial_args)\n                  for i, task in enumerate(tasks))\n        with app.producer_or_acquire() as producer:\n            [stask.apply_async(group_id=group_id, producer=producer,\n                               add_to_parent=False) for stask in taskit]\n        parent = app.current_worker_task\n        if add_to_parent and parent:\n            parent.add_trail(result)\n        return result\n",
      "variables": [
        "app",
        "self",
        "tasks",
        "result",
        "group_id",
        "partial_args",
        "add_to_parent",
        "taskit",
        "i",
        "task",
        "producer",
        "stask",
        "parent"
      ],
      "anonymized_code": "def add_group_task(var_1):\n    \"\"\"No longer used, but here for backwards compatibility.\"\"\"\n    from celery.canvas import maybe_signature\n    from celery.var_4 import result_from_tuple\n\n    @var_1.var_10(name='celery.group', bind=True, shared=False, lazy=False)\n    def group(var_2, var_3, var_4, var_5, var_6, var_7=True):\n        var_1 = var_2.var_1\n        var_4 = result_from_tuple(var_4, var_1)\n        # any partial args are added to all var_3 in the group\n        var_8 = (maybe_signature(var_10, var_1=var_1).clone(var_6)\n                  for var_9, var_10 in enumerate(var_3))\n        with var_1.producer_or_acquire() as var_11:\n            [var_12.apply_async(var_5=var_5, var_11=var_11,\n                               var_7=False) for var_12 in var_8]\n        var_13 = var_1.current_worker_task\n        if var_7 and var_13:\n            var_13.add_trail(var_4)\n        return var_4\n",
      "lines_processed": 19,
      "total_lines": 187
    },
    {
      "file_path": "__init__.py",
      "code": "def bugreport(app=None):\n    \"\"\"Return information useful in bug reports.\"\"\"\n    return (app or _state.get_current_app()).bugreport()\n",
      "variables": [
        "app"
      ],
      "anonymized_code": "def bugreport(var_1=None):\n    \"\"\"Return information useful in bug reports.\"\"\"\n    return (var_1 or _state.get_current_app()).bugreport()\n",
      "lines_processed": 3,
      "total_lines": 76
    },
    {
      "file_path": "backends.py",
      "code": "def by_name(backend=None, loader=None,\n            extension_namespace='celery.result_backends'):\n    \"\"\"Get backend class by name/alias.\"\"\"\n    backend = backend or 'disabled'\n    loader = loader or current_app.loader\n    aliases = dict(BACKEND_ALIASES, **loader.override_backends)\n    aliases.update(load_extension_class_names(extension_namespace))\n    try:\n        cls = symbol_by_name(backend, aliases)\n    except ValueError as exc:\n        reraise(ImproperlyConfigured, ImproperlyConfigured(\n            UNKNOWN_BACKEND.strip().format(backend, exc)), sys.exc_info()[2])\n    if isinstance(cls, types.ModuleType):\n        raise ImproperlyConfigured(UNKNOWN_BACKEND.strip().format(\n            backend, 'is a Python module, not a backend class.'))\n    return cls\n",
      "variables": [
        "backend",
        "loader",
        "extension_namespace",
        "aliases",
        "cls"
      ],
      "anonymized_code": "def by_name(var_1=None, var_2=None,\n            var_3='celery.result_backends'):\n    \"\"\"Get var_1 class by name/alias.\"\"\"\n    var_1 = var_1 or 'disabled'\n    var_2 = var_2 or current_app.var_2\n    var_4 = dict(BACKEND_ALIASES, **var_2.override_backends)\n    var_4.update(load_extension_class_names(var_3))\n    try:\n        var_5 = symbol_by_name(var_1, var_4)\n    except ValueError as exc:\n        reraise(ImproperlyConfigured, ImproperlyConfigured(\n            UNKNOWN_BACKEND.strip().format(var_1, exc)), sys.exc_info()[2])\n    if isinstance(var_5, types.ModuleType):\n        raise ImproperlyConfigured(UNKNOWN_BACKEND.strip().format(\n            var_1, 'is a Python module, not a var_1 class.'))\n    return var_5\n",
      "lines_processed": 16,
      "total_lines": 69
    },
    {
      "file_path": "backends.py",
      "code": "def by_url(backend=None, loader=None):\n    \"\"\"Get backend class by URL.\"\"\"\n    url = None\n    if backend and '://' in backend:\n        url = backend\n        scheme, _, _ = url.partition('://')\n        if '+' in scheme:\n            backend, url = url.split('+', 1)\n        else:\n            backend = scheme\n    return by_name(backend, loader), url\n",
      "variables": [
        "backend",
        "loader",
        "url",
        "scheme",
        "_"
      ],
      "anonymized_code": "def by_url(var_1=None, var_2=None):\n    \"\"\"Get var_1 class by URL.\"\"\"\n    var_3 = None\n    if var_1 and '://' in var_1:\n        var_3 = var_1\n        var_4, var_5, var_5 = var_3.partition('://')\n        if '+' in var_4:\n            var_1, var_3 = var_3.split('+', 1)\n        else:\n            var_1 = var_4\n    return by_name(var_1, var_2), var_3\n",
      "lines_processed": 11,
      "total_lines": 69
    },
    {
      "file_path": "_state.py",
      "code": "def set_default_app(app):\n    \"\"\"Set default app.\"\"\"\n    global default_app\n    default_app = app\n",
      "variables": [
        "app",
        "default_app"
      ],
      "anonymized_code": "def set_default_app(var_1):\n    \"\"\"Set default var_1.\"\"\"\n    global var_2\n    var_2 = var_1\n",
      "lines_processed": 4,
      "total_lines": 197
    },
    {
      "file_path": "_state.py",
      "code": "def _set_task_join_will_block(blocks):\n    global _task_join_will_block\n    _task_join_will_block = blocks\n",
      "variables": [
        "blocks",
        "_task_join_will_block"
      ],
      "anonymized_code": "def _set_task_join_will_block(var_1):\n    global var_2\n    var_2 = var_1\n",
      "lines_processed": 3,
      "total_lines": 197
    },
    {
      "file_path": "_state.py",
      "code": "def disable_trace():\n    \"\"\"Disable tracing of app instances.\"\"\"\n    global app_or_default\n    app_or_default = _app_or_default\n",
      "variables": [
        "app_or_default"
      ],
      "anonymized_code": "def disable_trace():\n    \"\"\"Disable tracing of app instances.\"\"\"\n    global var_1\n    var_1 = _app_or_default\n",
      "lines_processed": 4,
      "total_lines": 197
    },
    {
      "file_path": "_state.py",
      "code": "def get_current_worker_task():\n    \"\"\"Currently executing task, that was applied by the worker.\n\n    This is used to differentiate between the actual task\n    executed by the worker and any task that was called within\n    a task (using ``task.__call__`` or ``task.apply``)\n    \"\"\"\n    for task in reversed(_task_stack.stack):\n        if not task.request.called_directly:\n            return task\n",
      "variables": [
        "task"
      ],
      "anonymized_code": "def get_current_worker_task():\n    \"\"\"Currently executing var_1, that was applied by the worker.\n\n    This is used to differentiate between the actual var_1\n    executed by the worker and any var_1 that was called within\n    a var_1 (using ``var_1.__call__`` or ``var_1.apply``)\n    \"\"\"\n    for var_1 in reversed(_task_stack.stack):\n        if not var_1.request.called_directly:\n            return var_1\n",
      "lines_processed": 10,
      "total_lines": 197
    },
    {
      "file_path": "_state.py",
      "code": "def connect_on_app_finalize(callback):\n    \"\"\"Connect callback to be called when any app is finalized.\"\"\"\n    _on_app_finalizers.add(callback)\n    return callback\n",
      "variables": [
        "callback"
      ],
      "anonymized_code": "def connect_on_app_finalize(var_1):\n    \"\"\"Connect var_1 to be called when any app is finalized.\"\"\"\n    _on_app_finalizers.add(var_1)\n    return var_1\n",
      "lines_processed": 4,
      "total_lines": 197
    },
    {
      "file_path": "_state.py",
      "code": "def _deregister_app(app):\n    _apps.discard(app)\n",
      "variables": [
        "app"
      ],
      "anonymized_code": "def _deregister_app(var_1):\n    _apps.discard(var_1)\n",
      "lines_processed": 2,
      "total_lines": 197
    },
    {
      "file_path": "_state.py",
      "code": "def enable_trace():\n    \"\"\"Enable tracing of app instances.\"\"\"\n    global app_or_default\n    app_or_default = _app_or_default_trace\n",
      "variables": [
        "app_or_default"
      ],
      "anonymized_code": "def enable_trace():\n    \"\"\"Enable tracing of app instances.\"\"\"\n    global var_1\n    var_1 = _app_or_default_trace\n",
      "lines_processed": 4,
      "total_lines": 197
    },
    {
      "file_path": "defaults.py",
      "code": "def find_deprecated_settings(source):  # pragma: no cover\n    from celery.utils import deprecated\n    for name, opt in flatten(NAMESPACES):\n        if (opt.deprecate_by or opt.remove_by) and getattr(source, name, None):\n            deprecated.warn(description=f'The {name!r} setting',\n                            deprecation=opt.deprecate_by,\n                            removal=opt.remove_by,\n                            alternative=f'Use the {opt.alt} instead')\n    return source\n",
      "variables": [
        "source",
        "name",
        "opt"
      ],
      "anonymized_code": "def find_deprecated_settings(var_1):  # pragma: no cover\n    from celery.utils import deprecated\n    for var_2, var_3 in flatten(NAMESPACES):\n        if (var_3.deprecate_by or var_3.remove_by) and getattr(var_1, var_2, None):\n            deprecated.warn(description=f'The {var_2!r} setting',\n                            deprecation=var_3.deprecate_by,\n                            removal=var_3.remove_by,\n                            alternative=f'Use the {var_3.alt} instead')\n    return var_1\n",
      "lines_processed": 9,
      "total_lines": 427
    },
    {
      "file_path": "defaults.py",
      "code": "def old_ns(ns):\n    return {f'{ns}_{{0}}'}\n",
      "variables": [
        "ns"
      ],
      "anonymized_code": "def old_ns(var_1):\n    return {f'{var_1}_{{0}}'}\n",
      "lines_processed": 2,
      "total_lines": 427
    },
    {
      "file_path": "defaults.py",
      "code": "def _flatten_keys(ns, key, opt):\n    return [(ns + key, opt)]\n",
      "variables": [
        "ns",
        "key",
        "opt"
      ],
      "anonymized_code": "def _flatten_keys(var_1, var_2, var_3):\n    return [(var_1 + var_2, var_3)]\n",
      "lines_processed": 2,
      "total_lines": 427
    },
    {
      "file_path": "defaults.py",
      "code": "def _to_compat(ns, key, opt):\n    if opt.old:\n        return [\n            (oldkey.format(key).upper(), ns + key, opt)\n            for oldkey in opt.old\n        ]\n    return [((ns + key).upper(), ns + key, opt)]\n",
      "variables": [
        "ns",
        "key",
        "opt",
        "oldkey"
      ],
      "anonymized_code": "def _to_compat(var_1, var_2, var_3):\n    if var_3.old:\n        return [\n            (var_4.format(var_2).upper(), var_1 + var_2, var_3)\n            for var_4 in var_3.old\n        ]\n    return [((var_1 + var_2).upper(), var_1 + var_2, var_3)]\n",
      "lines_processed": 7,
      "total_lines": 427
    },
    {
      "file_path": "defaults.py",
      "code": "def flatten(d, root='', keyfilter=_flatten_keys):\n    \"\"\"Flatten settings.\"\"\"\n    stack = deque([(root, d)])\n    while stack:\n        ns, options = stack.popleft()\n        for key, opt in options.items():\n            if isinstance(opt, dict):\n                stack.append((ns + key + '_', opt))\n            else:\n                yield from keyfilter(ns, key, opt)\n",
      "variables": [
        "d",
        "root",
        "keyfilter",
        "stack",
        "ns",
        "options",
        "key",
        "opt"
      ],
      "anonymized_code": "def flatten(var_1, var_2='', var_3=_flatten_keys):\n    \"\"\"Flatten settings.\"\"\"\n    var_4 = deque([(var_2, var_1)])\n    while var_4:\n        var_5, var_6 = var_4.popleft()\n        for var_7, var_8 in var_6.items():\n            if isinstance(var_8, dict):\n                var_4.append((var_5 + var_7 + '_', var_8))\n            else:\n                yield from var_3(var_5, var_7, var_8)\n",
      "lines_processed": 10,
      "total_lines": 427
    },
    {
      "file_path": "defaults.py",
      "code": "def find(name, namespace='celery'):\n    \"\"\"Find setting by name.\"\"\"\n    # - Try specified name-space first.\n    namespace = namespace.lower()\n    try:\n        return searchresult(\n            namespace, name.lower(), NAMESPACES[namespace][name.lower()],\n        )\n    except KeyError:\n        # - Try all the other namespaces.\n        for ns, opts in NAMESPACES.items():\n            if ns.lower() == name.lower():\n                return searchresult(None, ns, opts)\n            elif isinstance(opts, dict):\n                try:\n                    return searchresult(ns, name.lower(), opts[name.lower()])\n                except KeyError:\n                    pass\n    # - See if name is a qualname last.\n",
      "variables": [
        "name",
        "namespace",
        "ns",
        "opts"
      ],
      "anonymized_code": "def find(var_1, var_2='celery'):\n    \"\"\"Find setting by var_1.\"\"\"\n    # - Try specified var_1-space first.\n    var_2 = var_2.lower()\n    try:\n        return searchresult(\n            var_2, var_1.lower(), NAMESPACES[var_2][var_1.lower()],\n        )\n    except KeyError:\n        # - Try all the other namespaces.\n        for var_3, var_4 in NAMESPACES.items():\n            if var_3.lower() == var_1.lower():\n                return searchresult(None, var_3, var_4)\n            elif isinstance(var_4, dict):\n                try:\n                    return searchresult(var_3, var_1.lower(), var_4[var_1.lower()])\n                except KeyError:\n                    pass\n    # - See if var_1 is a qualname last.\n",
      "lines_processed": 19,
      "total_lines": 427
    },
    {
      "file_path": "defaults.py",
      "code": "def Namespace(__old__=None, **options):\n    if __old__ is not None:\n        for key, opt in options.items():\n            if not opt.old:\n                opt.old = {o.format(key) for o in __old__}\n    return options\n",
      "variables": [
        "__old__",
        "options",
        "key",
        "opt",
        "o"
      ],
      "anonymized_code": "def Namespace(var_1=None, **var_2):\n    if var_1 is not None:\n        for var_3, var_4 in var_2.items():\n            if not var_4.old:\n                var_4.old = {var_5.format(var_3) for var_5 in var_1}\n    return var_2\n",
      "lines_processed": 6,
      "total_lines": 427
    },
    {
      "file_path": "annotations.py",
      "code": "def resolve_all(anno, task):\n    \"\"\"Resolve all pending annotations.\"\"\"\n    return (x for x in (_first_match(anno, task), _first_match_any(anno)) if x)\n",
      "variables": [
        "anno",
        "task",
        "x"
      ],
      "anonymized_code": "def resolve_all(var_1, var_2):\n    \"\"\"Resolve all pending annotations.\"\"\"\n    return (var_3 for var_3 in (_first_match(var_1, var_2), _first_match_any(var_1)) if var_3)\n",
      "lines_processed": 3,
      "total_lines": 52
    },
    {
      "file_path": "annotations.py",
      "code": "def prepare(annotations):\n    \"\"\"Expand the :setting:`task_annotations` setting.\"\"\"\n    def expand_annotation(annotation):\n        if isinstance(annotation, dict):\n            return MapAnnotation(annotation)\n        elif isinstance(annotation, str):\n            return mlazy(instantiate, annotation)\n        return annotation\n\n    if annotations is None:\n        return ()\n    elif not isinstance(annotations, (list, tuple)):\n        annotations = (annotations,)\n    return [expand_annotation(anno) for anno in annotations]\n",
      "variables": [
        "annotations",
        "annotation",
        "anno"
      ],
      "anonymized_code": "def prepare(var_1):\n    \"\"\"Expand the :setting:`task_annotations` setting.\"\"\"\n    def expand_annotation(var_2):\n        if isinstance(var_2, dict):\n            return MapAnnotation(var_2)\n        elif isinstance(var_2, str):\n            return mlazy(instantiate, var_2)\n        return var_2\n\n    if var_1 is None:\n        return ()\n    elif not isinstance(var_1, (list, tuple)):\n        var_1 = (var_1,)\n    return [expand_annotation(var_3) for var_3 in var_1]\n",
      "lines_processed": 14,
      "total_lines": 52
    },
    {
      "file_path": "base.py",
      "code": "def _unpickle_appattr(reverse_name, args):\n    \"\"\"Unpickle app.\"\"\"\n    # Given an attribute name and a list of args, gets\n    # the attribute from the current app and calls it.\n    return get_current_app()._rgetattr(reverse_name)(*args)\n",
      "variables": [
        "reverse_name",
        "args"
      ],
      "anonymized_code": "def _unpickle_appattr(var_1, var_2):\n    \"\"\"Unpickle app.\"\"\"\n    # Given an attribute name and a list of var_2, gets\n    # the attribute from the current app and calls it.\n    return get_current_app()._rgetattr(var_1)(*var_2)\n",
      "lines_processed": 5,
      "total_lines": 1508
    }
  ],
  "hpcaitech_ColossalAI": [
    {
      "file_path": "generate_release_draft.py",
      "code": "def get_all_commit_info(since, headers=None):\n    page = 1\n    results = []\n\n    while True:\n        api = f\"{COMMIT_API}?since={since}&per_page=100&page={page}\"\n        resp = requests.get(url=api, headers=headers)\n        data = resp.json()\n\n        # exit when no more data\n        if len(data) == 0:\n            break\n\n        results.extend(data)\n        page += 1\n\n    return results\n",
      "variables": [
        "since",
        "headers",
        "page",
        "results",
        "api",
        "resp",
        "data"
      ],
      "anonymized_code": "def get_all_commit_info(var_1, var_2=None):\n    var_3 = 1\n    var_4 = []\n\n    while True:\n        var_5 = f\"{COMMIT_API}?var_1={var_1}&per_page=100&var_3={var_3}\"\n        var_6 = requests.get(url=var_5, var_2=var_2)\n        var_7 = var_6.json()\n\n        # exit when no more var_7\n        if len(var_7) == 0:\n            break\n\n        var_4.extend(var_7)\n        var_3 += 1\n\n    return var_4\n",
      "lines_processed": 17,
      "total_lines": 131
    },
    {
      "file_path": "generate_release_draft.py",
      "code": "def parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--out\", type=str, help=\"output path for the release draft\", required=True)\n    parser.add_argument(\"--version\", type=str, help=\"current version to release\", required=True)\n    return parser.parse_args()\n",
      "variables": [
        "parser"
      ],
      "anonymized_code": "def parse_args():\n    var_1 = argparse.ArgumentParser()\n    var_1.add_argument(\"--out\", type=str, help=\"output path for the release draft\", required=True)\n    var_1.add_argument(\"--version\", type=str, help=\"current version to release\", required=True)\n    return var_1.parse_args()\n",
      "lines_processed": 5,
      "total_lines": 131
    },
    {
      "file_path": "generate_release_draft.py",
      "code": "def get_latest_tag_commit(headers=None):\n    res = requests.get(url=TAGS_API, headers=headers)\n    data = res.json()\n    commit_hash = data[0][\"commit\"][\"sha\"]\n    version = data[0][\"name\"]\n    return commit_hash, version\n",
      "variables": [
        "headers",
        "res",
        "data",
        "commit_hash",
        "version"
      ],
      "anonymized_code": "def get_latest_tag_commit(var_1=None):\n    var_2 = requests.get(url=TAGS_API, var_1=var_1)\n    var_3 = var_2.json()\n    var_4 = var_3[0][\"commit\"][\"sha\"]\n    var_5 = var_3[0][\"name\"]\n    return var_4, var_5\n",
      "lines_processed": 6,
      "total_lines": 131
    },
    {
      "file_path": "generate_release_draft.py",
      "code": "def get_commit_info(commit_hash, headers=None):\n    api = f\"{COMMIT_API}/{commit_hash}\"\n    res = requests.get(url=api, headers=headers)\n    return res.json()\n",
      "variables": [
        "commit_hash",
        "headers",
        "api",
        "res"
      ],
      "anonymized_code": "def get_commit_info(var_1, var_2=None):\n    var_3 = f\"{COMMIT_API}/{var_1}\"\n    var_4 = requests.get(url=var_3, var_2=var_2)\n    return var_4.json()\n",
      "lines_processed": 4,
      "total_lines": 131
    },
    {
      "file_path": "generate_release_draft.py",
      "code": "def collate_release_info(commit_info_list):\n    results = dict()\n    pattern = pattern = r\"\\[.*\\]\"\n\n    for commit_info in commit_info_list:\n        author = commit_info[\"commit\"][\"author\"][\"name\"]\n\n        try:\n            author_url = commit_info[\"author\"][\"url\"]\n        except:\n            # author can be None\n            author_url = None\n        msg = commit_info[\"commit\"][\"message\"]\n        match = re.search(pattern, msg)\n\n        if match:\n            tag = match.group().lstrip(\"[\").rstrip(\"]\").capitalize()\n            if tag not in results:\n                results[tag] = []\n",
      "variables": [
        "commit_info_list",
        "results",
        "pattern",
        "commit_info",
        "author",
        "author_url",
        "msg",
        "match",
        "tag"
      ],
      "anonymized_code": "def collate_release_info(var_1):\n    var_2 = dict()\n    var_3 = var_3 = r\"\\[.*\\]\"\n\n    for var_4 in var_1:\n        var_5 = var_4[\"commit\"][\"var_5\"][\"name\"]\n\n        try:\n            var_6 = var_4[\"var_5\"][\"url\"]\n        except:\n            # var_5 can be None\n            var_6 = None\n        var_7 = var_4[\"commit\"][\"message\"]\n        var_8 = re.search(var_3, var_7)\n\n        if var_8:\n            var_9 = var_8.group().lstrip(\"[\").rstrip(\"]\").capitalize()\n            if var_9 not in var_2:\n                var_2[var_9] = []\n",
      "lines_processed": 19,
      "total_lines": 131
    },
    {
      "file_path": "spliced_and_tokenized_dataset.py",
      "code": "def supervised_tokenize_pretrain(\n    data_point: Dict[str, str], tokenizer: LlamaTokenizer, ignore_index: int = None, max_length: int = 4096\n) -> Dict[str, Union[int, str, List[int]]]:\n    \"\"\"\n    A tokenization function to tokenize an original pretraining data point as following:\n        {\"source\": \"\", \"target\": \"Beijing, the capital of the People's Republic of China, ...\", \"category\": \"geography\"}\n    \"\"\"\n    assert tokenizer.add_bos_token is False and tokenizer.add_eos_token is False, (\n        \"Initially set `tokenizer.add_bos_token` and `tokenizer.add_eos_token` to False, \"\n        \"add <bos> and <eos> manually later\"\n    )\n    if ignore_index is None:\n        ignore_index = IGNORE_INDEX\n\n    source_text = data_point[\"source\"]  # `str`\n    target_text = data_point[\"target\"]  # `str`\n    is_null_source = len(source_text) == 0\n\n    source_text = tokenizer.bos_token + source_text\n",
      "variables": [
        "data_point",
        "tokenizer",
        "ignore_index",
        "max_length",
        "source_text",
        "target_text",
        "is_null_source"
      ],
      "anonymized_code": "def supervised_tokenize_pretrain(\n    var_1: Dict[str, str], var_2: LlamaTokenizer, var_3: int = None, var_4: int = 4096\n) -> Dict[str, Union[int, str, List[int]]]:\n    \"\"\"\n    A tokenization function to tokenize an original pretraining data point as following:\n        {\"source\": \"\", \"target\": \"Beijing, the capital of the People's Republic of China, ...\", \"category\": \"geography\"}\n    \"\"\"\n    assert var_2.add_bos_token is False and var_2.add_eos_token is False, (\n        \"Initially set `var_2.add_bos_token` and `var_2.add_eos_token` to False, \"\n        \"add <bos> and <eos> manually later\"\n    )\n    if var_3 is None:\n        var_3 = IGNORE_INDEX\n\n    var_5 = var_1[\"source\"]  # `str`\n    var_6 = var_1[\"target\"]  # `str`\n    var_7 = len(var_5) == 0\n\n    var_5 = var_2.bos_token + var_5\n",
      "lines_processed": 19,
      "total_lines": 301
    },
    {
      "file_path": "spliced_and_tokenized_dataset.py",
      "code": "def supervised_tokenize_sft(\n    data_point: Dict[str, str],\n    tokenizer: AutoTokenizer,\n    conversation_template: Conversation = default_conversation,\n    ignore_index: int = None,\n    max_length: int = 4096,\n) -> Dict[str, Union[int, str, List[int]]]:\n    \"\"\"\n    A tokenization function to tokenize an original supervised data point as following:\n        {\"messages\": [{\"from\": \"human\", \"content\": \"xxx\"}, {\"from\": \"assistant\", \"content\": \"xxx\"}]}\n    \"\"\"\n    assert tokenizer.add_bos_token is False and tokenizer.add_eos_token is False, (\n        \"Initially set `tokenizer.add_bos_token` and `tokenizer.add_eos_token` to False, \"\n        \"add <bos> and <eos> manually later\"\n    )\n\n    assert (\n        tokenizer.bos_token == conversation_template.seps[0] and tokenizer.eos_token == conversation_template.seps[1]\n    ), f\"`bos_token`{tokenizer.bos_token} and `eos_token`{tokenizer.eos_token} should be the same with `conversation_template.seps`{conversation_template.seps}.\"\n",
      "variables": [
        "data_point",
        "tokenizer",
        "conversation_template",
        "ignore_index",
        "max_length"
      ],
      "anonymized_code": "def supervised_tokenize_sft(\n    var_1: Dict[str, str],\n    var_2: AutoTokenizer,\n    var_3: Conversation = default_conversation,\n    var_4: int = None,\n    var_5: int = 4096,\n) -> Dict[str, Union[int, str, List[int]]]:\n    \"\"\"\n    A tokenization function to tokenize an original supervised data point as following:\n        {\"messages\": [{\"from\": \"human\", \"content\": \"xxx\"}, {\"from\": \"assistant\", \"content\": \"xxx\"}]}\n    \"\"\"\n    assert var_2.add_bos_token is False and var_2.add_eos_token is False, (\n        \"Initially set `var_2.add_bos_token` and `var_2.add_eos_token` to False, \"\n        \"add <bos> and <eos> manually later\"\n    )\n\n    assert (\n        var_2.bos_token == var_3.seps[0] and var_2.eos_token == var_3.seps[1]\n    ), f\"`bos_token`{var_2.bos_token} and `eos_token`{var_2.eos_token} should be the same with `var_3.seps`{var_3.seps}.\"\n",
      "lines_processed": 19,
      "total_lines": 301
    },
    {
      "file_path": "update_setup_for_nightly.py",
      "code": "def replace_nightly_package_info(file_lines):\n    version = datetime.today().strftime(\"%Y.%m.%d\")\n    package_name = \"colossalai-nightly\"\n\n    for idx, line in enumerate(file_lines):\n        if \"version = get_version()\" in line:\n            file_lines[idx] = f'version = \"{version}\"\\n'\n        if 'package_name = \"colossalai\"' in line:\n            file_lines[idx] = f'package_name = \"{package_name}\"\\n'\n    return file_lines\n",
      "variables": [
        "file_lines",
        "version",
        "package_name",
        "idx",
        "line"
      ],
      "anonymized_code": "def replace_nightly_package_info(var_1):\n    var_2 = datetime.today().strftime(\"%Y.%m.%d\")\n    var_3 = \"colossalai-nightly\"\n\n    for var_4, var_5 in enumerate(var_1):\n        if \"var_2 = get_version()\" in var_5:\n            var_1[var_4] = f'var_2 = \"{var_2}\"\\n'\n        if 'var_3 = \"colossalai\"' in var_5:\n            var_1[var_4] = f'var_3 = \"{var_3}\"\\n'\n    return var_1\n",
      "lines_processed": 10,
      "total_lines": 34
    },
    {
      "file_path": "update_setup_for_nightly.py",
      "code": "def open_setup_file():\n    with open(\"setup.py\", \"r\") as f:\n        file_lines = f.readlines()\n    return file_lines\n",
      "variables": [
        "f",
        "file_lines"
      ],
      "anonymized_code": "def open_setup_file():\n    with open(\"setup.py\", \"r\") as var_1:\n        var_2 = var_1.readlines()\n    return var_2\n",
      "lines_processed": 4,
      "total_lines": 34
    },
    {
      "file_path": "update_setup_for_nightly.py",
      "code": "def main():\n    file_lines = open_setup_file()\n    file_lines = replace_nightly_package_info(file_lines)\n    write_setup_file(file_lines)\n",
      "variables": [
        "file_lines"
      ],
      "anonymized_code": "def main():\n    var_1 = open_setup_file()\n    var_1 = replace_nightly_package_info(var_1)\n    write_setup_file(var_1)\n",
      "lines_processed": 4,
      "total_lines": 34
    },
    {
      "file_path": "update_setup_for_nightly.py",
      "code": "def write_setup_file(file_lines):\n    with open(\"setup.py\", \"w\") as f:\n        f.writelines(file_lines)\n",
      "variables": [
        "file_lines",
        "f"
      ],
      "anonymized_code": "def write_setup_file(var_1):\n    with open(\"setup.py\", \"w\") as var_2:\n        var_2.writelines(var_1)\n",
      "lines_processed": 3,
      "total_lines": 34
    },
    {
      "file_path": "check_doc_i18n.py",
      "code": "def compare_dirs(dir1, dir2):\n    # First, we need to check if the two directories exist\n    if not os.path.exists(dir1) or not os.path.exists(dir2):\n        return False\n\n    # Now, we compare the list of items in each directory\n    items1 = os.listdir(dir1)\n    items2 = os.listdir(dir2)\n\n    # If the number of items in each directory is different, the directories are different\n    if len(items1) != len(items2):\n        return False\n\n    # For each item in the first directory, we check if there is a corresponding item in the second directory\n    for item in items1:\n        item_path1 = os.path.join(dir1, item)\n        item_path2 = os.path.join(dir2, item)\n\n        # If the corresponding item doesn't exist in the second directory, the directories are different\n",
      "variables": [
        "dir1",
        "dir2",
        "items1",
        "items2",
        "item",
        "item_path1",
        "item_path2"
      ],
      "anonymized_code": "def compare_dirs(var_1, var_2):\n    # First, we need to check if the two directories exist\n    if not os.path.exists(var_1) or not os.path.exists(var_2):\n        return False\n\n    # Now, we compare the list of items in each directory\n    var_3 = os.listdir(var_1)\n    var_4 = os.listdir(var_2)\n\n    # If the number of items in each directory is different, the directories are different\n    if len(var_3) != len(var_4):\n        return False\n\n    # For each var_5 in the first directory, we check if there is a corresponding var_5 in the second directory\n    for var_5 in var_3:\n        var_6 = os.path.join(var_1, var_5)\n        var_7 = os.path.join(var_2, var_5)\n\n        # If the corresponding var_5 doesn't exist in the second directory, the directories are different\n",
      "lines_processed": 19,
      "total_lines": 67
    },
    {
      "file_path": "check_dispatch_inputs.py",
      "code": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-f\", \"--fileNameList\", type=str, help=\"List of file names\")\n    args = parser.parse_args()\n    name_list = args.fileNameList.split(\",\")\n    is_correct = check_inputs(name_list)\n\n    if is_correct:\n        print(\"success\")\n    else:\n        print(\"failure\")\n",
      "variables": [
        "parser",
        "args",
        "name_list",
        "is_correct"
      ],
      "anonymized_code": "def main():\n    var_1 = argparse.ArgumentParser()\n    var_1.add_argument(\"-f\", \"--fileNameList\", type=str, help=\"List of file names\")\n    var_2 = var_1.parse_args()\n    var_3 = var_2.fileNameList.split(\",\")\n    var_4 = check_inputs(var_3)\n\n    if var_4:\n        print(\"success\")\n    else:\n        print(\"failure\")\n",
      "lines_processed": 11,
      "total_lines": 27
    },
    {
      "file_path": "check_dispatch_inputs.py",
      "code": "def check_inputs(input_list):\n    for path in input_list:\n        real_path = os.path.join(\"examples\", path)\n        if not os.path.exists(real_path):\n            return False\n    return True\n",
      "variables": [
        "input_list",
        "path",
        "real_path"
      ],
      "anonymized_code": "def check_inputs(var_1):\n    for var_2 in var_1:\n        var_3 = os.var_2.join(\"examples\", var_2)\n        if not os.var_2.exists(var_3):\n            return False\n    return True\n",
      "lines_processed": 6,
      "total_lines": 27
    },
    {
      "file_path": "init_model.py",
      "code": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--source_model_and_tokenizer_path\",\n        type=str,\n        required=True,\n        default=None,\n        help=\"Source path of model & tokenizer\",\n    )\n    parser.add_argument(\"--target_tokenizer_path\", type=str, required=True, default=None, help=\"Target tokenizer path\")\n    parser.add_argument(\"--target_model_path\", type=str, required=True, default=None, help=\"Target model path\")\n    args = parser.parse_args()\n\n    source_tokenizer = LlamaTokenizer.from_pretrained(args.source_model_and_tokenizer_path)\n    source_tokenizer.add_bos_token = False\n    source_tokenizer.add_eos_token = False\n    if source_tokenizer.pad_token is None:\n        source_tokenizer.pad_token = source_tokenizer.unk_token\n    source_vocab = source_tokenizer.get_vocab()\n",
      "variables": [
        "parser",
        "args",
        "source_tokenizer",
        "source_vocab"
      ],
      "anonymized_code": "def main():\n    var_1 = argparse.ArgumentParser()\n    var_1.add_argument(\n        \"--source_model_and_tokenizer_path\",\n        type=str,\n        required=True,\n        default=None,\n        help=\"Source path of model & tokenizer\",\n    )\n    var_1.add_argument(\"--target_tokenizer_path\", type=str, required=True, default=None, help=\"Target tokenizer path\")\n    var_1.add_argument(\"--target_model_path\", type=str, required=True, default=None, help=\"Target model path\")\n    var_2 = var_1.parse_args()\n\n    var_3 = LlamaTokenizer.from_pretrained(var_2.source_model_and_tokenizer_path)\n    var_3.add_bos_token = False\n    var_3.add_eos_token = False\n    if var_3.pad_token is None:\n        var_3.pad_token = var_3.unk_token\n    var_4 = var_3.get_vocab()\n",
      "lines_processed": 19,
      "total_lines": 110
    },
    {
      "file_path": "loader.py",
      "code": "def load_tokenized_dataset(\n    dataset_paths: Union[PathType, List[PathType]], mode: str = \"train\"\n) -> Optional[DatasetType]:\n    \"\"\"\n    Load pre-tokenized dataset.\n    Each instance of dataset is a dictionary with\n    `{'input_ids': List[int], 'labels': List[int], sequence: str}` format.\n    \"\"\"\n    mode_map = {\"train\": \"train\", \"dev\": \"validation\", \"test\": \"test\"}\n    assert mode in tuple(mode_map), f\"Unsupported mode {mode}, it must be in {tuple(mode_map)}\"\n\n    if isinstance(dataset_paths, (str, os.PathLike)):\n        dataset_paths = [dataset_paths]\n\n    datasets = []  # `List[datasets.dataset_dict.Dataset]`\n    for ds_path in dataset_paths:\n        ds_path = os.path.abspath(ds_path)\n        assert os.path.exists(ds_path), f\"Not existed file path {ds_path}\"\n        ds_dict = load_from_disk(dataset_path=ds_path, keep_in_memory=False)\n",
      "variables": [
        "dataset_paths",
        "mode",
        "mode_map",
        "datasets",
        "ds_path",
        "ds_dict"
      ],
      "anonymized_code": "def load_tokenized_dataset(\n    var_1: Union[PathType, List[PathType]], var_2: str = \"train\"\n) -> Optional[DatasetType]:\n    \"\"\"\n    Load pre-tokenized dataset.\n    Each instance of dataset is a dictionary with\n    `{'input_ids': List[int], 'labels': List[int], sequence: str}` format.\n    \"\"\"\n    var_3 = {\"train\": \"train\", \"dev\": \"validation\", \"test\": \"test\"}\n    assert var_2 in tuple(var_3), f\"Unsupported var_2 {var_2}, it must be in {tuple(var_3)}\"\n\n    if isinstance(var_1, (str, os.PathLike)):\n        var_1 = [var_1]\n\n    var_4 = []  # `List[var_4.dataset_dict.Dataset]`\n    for var_5 in var_1:\n        var_5 = os.path.abspath(var_5)\n        assert os.path.exists(var_5), f\"Not existed file path {var_5}\"\n        var_6 = load_from_disk(dataset_path=var_5, keep_in_memory=False)\n",
      "lines_processed": 19,
      "total_lines": 175
    },
    {
      "file_path": "detect_changed_example.py",
      "code": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-f\", \"--fileNameList\", type=str, help=\"The list of changed files\")\n    args = parser.parse_args()\n    name_list = args.fileNameList.split(\":\")\n    folder_need_check = set()\n    for loc in name_list:\n        # Find only the sub-sub-folder of 'example' folder\n        # the examples folder structure is like\n        # - examples\n        #   - area\n        #     - application\n        #       - file\n        if loc.split(\"/\")[0] == \"examples\" and len(loc.split(\"/\")) >= 4:\n            folder_need_check.add(\"/\".join(loc.split(\"/\")[1:3]))\n    # Output the result using print. Then the shell can get the values.\n    print(list(folder_need_check))\n",
      "variables": [
        "parser",
        "args",
        "name_list",
        "folder_need_check",
        "loc"
      ],
      "anonymized_code": "def main():\n    var_1 = argparse.ArgumentParser()\n    var_1.add_argument(\"-f\", \"--fileNameList\", type=str, help=\"The list of changed files\")\n    var_2 = var_1.parse_args()\n    var_3 = var_2.fileNameList.split(\":\")\n    var_4 = set()\n    for var_5 in var_3:\n        # Find only the sub-sub-folder of 'example' folder\n        # the examples folder structure is like\n        # - examples\n        #   - area\n        #     - application\n        #       - file\n        if var_5.split(\"/\")[0] == \"examples\" and len(var_5.split(\"/\")) >= 4:\n            var_4.add(\"/\".join(var_5.split(\"/\")[1:3]))\n    # Output the result using print. Then the shell can get the values.\n    print(list(var_4))\n",
      "lines_processed": 17,
      "total_lines": 24
    },
    {
      "file_path": "generate_leaderboard_and_send_to_lark.py",
      "code": "def get_issue_pull_request_comments(github_token: str, org_name: str, repo_name: str, since: str) -> Dict[str, int]:\n    \"\"\"\n    Retrieve the issue/PR comments made by our members in the last 7 days.\n\n    Args:\n        github_token (str): GitHub access token for API calls\n        since (str): the path parameter required by GitHub Restful APIs, in the format of YYYY-MM-DDTHH:MM:SSZ\n    \"\"\"\n    # prepare header\n    headers = {\n        \"Authorization\": f\"Bearer {github_token}\",\n        \"Accept\": \"application/vnd.github+json\",\n        \"X-GitHub-Api-Version\": \"2022-11-28\",\n    }\n\n    user_engagement_count = {}\n\n    # do pagination to the API\n    page = 1\n",
      "variables": [
        "github_token",
        "org_name",
        "repo_name",
        "since",
        "headers",
        "user_engagement_count",
        "page"
      ],
      "anonymized_code": "def get_issue_pull_request_comments(var_1: str, var_2: str, var_3: str, var_4: str) -> Dict[str, int]:\n    \"\"\"\n    Retrieve the issue/PR comments made by our members in the last 7 days.\n\n    Args:\n        var_1 (str): GitHub access token for API calls\n        var_4 (str): the path parameter required by GitHub Restful APIs, in the format of YYYY-MM-DDTHH:MM:SSZ\n    \"\"\"\n    # prepare header\n    var_5 = {\n        \"Authorization\": f\"Bearer {var_1}\",\n        \"Accept\": \"application/vnd.github+json\",\n        \"X-GitHub-Api-Version\": \"2022-11-28\",\n    }\n\n    var_6 = {}\n\n    # do pagination to the API\n    var_7 = 1\n",
      "lines_processed": 19,
      "total_lines": 569
    },
    {
      "file_path": "generate_leaderboard_and_send_to_lark.py",
      "code": "def plot_bar_chart(x: List[Any], y: List[Any], xlabel: str, ylabel: str, title: str, output_path: str) -> None:\n    \"\"\"\n    This function is a utility to plot the bar charts.\n    \"\"\"\n    plt.clf()\n    seaborn.color_palette()\n    fig = seaborn.barplot(x=x, y=y)\n    fig.set(xlabel=xlabel, ylabel=ylabel, title=title)\n    seaborn.despine()\n    plt.tight_layout()\n    plt.savefig(output_path, dpi=1200)\n",
      "variables": [
        "x",
        "y",
        "xlabel",
        "ylabel",
        "title",
        "output_path",
        "fig"
      ],
      "anonymized_code": "def plot_bar_chart(var_1: List[Any], var_2: List[Any], var_3: str, var_4: str, var_5: str, var_6: str) -> None:\n    \"\"\"\n    This function is a utility to plot the bar charts.\n    \"\"\"\n    plt.clf()\n    seaborn.color_palette()\n    var_7 = seaborn.barplot(var_1=var_1, var_2=var_2)\n    var_7.set(var_3=var_3, var_4=var_4, var_5=var_5)\n    seaborn.despine()\n    plt.tight_layout()\n    plt.savefig(var_6, dpi=1200)\n",
      "lines_processed": 11,
      "total_lines": 569
    },
    {
      "file_path": "generate_leaderboard_and_send_to_lark.py",
      "code": "def str2datetime(string):\n    \"\"\"\n    Convert string in the format of YYYY-MM-DDTHH:MM:SSZ to datetime\n    \"\"\"\n    return datetime.strptime(string, \"%Y-%m-%dT%H:%M:%SZ\")\n",
      "variables": [
        "string"
      ],
      "anonymized_code": "def str2datetime(var_1):\n    \"\"\"\n    Convert var_1 in the format of YYYY-MM-DDTHH:MM:SSZ to datetime\n    \"\"\"\n    return datetime.strptime(var_1, \"%Y-%m-%dT%H:%M:%SZ\")\n",
      "lines_processed": 5,
      "total_lines": 569
    },
    {
      "file_path": "generate_leaderboard_and_send_to_lark.py",
      "code": "def upload_image_to_lark(lark_tenant_token: str, image_path: str) -> str:\n    \"\"\"\n    Upload image to Lark and return the image key\n\n    Args:\n        lark_tenant_token (str): Lark tenant access token\n        image_path (str): the path to the image to be uploaded\n    \"\"\"\n    url = \"https://open.feishu.cn/open-apis/im/v1/images\"\n    form = {\"image_type\": \"message\", \"image\": (open(image_path, \"rb\"))}  # \u9700\u8981\u66ff\u6362\u5177\u4f53\u7684path\n    multi_form = MultipartEncoder(form)\n    headers = {\n        \"Authorization\": f\"Bearer {lark_tenant_token}\",  ## \u83b7\u53d6tenant_access_token, \u9700\u8981\u66ff\u6362\u4e3a\u5b9e\u9645\u7684token\n    }\n    headers[\"Content-Type\"] = multi_form.content_type\n    response = requests.request(\"POST\", url, headers=headers, data=multi_form).json()\n    return response[\"data\"][\"image_key\"]\n",
      "variables": [
        "lark_tenant_token",
        "image_path",
        "url",
        "form",
        "multi_form",
        "headers",
        "response"
      ],
      "anonymized_code": "def upload_image_to_lark(var_1: str, var_2: str) -> str:\n    \"\"\"\n    Upload image to Lark and return the image key\n\n    Args:\n        var_1 (str): Lark tenant access token\n        var_2 (str): the path to the image to be uploaded\n    \"\"\"\n    var_3 = \"https://open.feishu.cn/open-apis/im/v1/images\"\n    var_4 = {\"image_type\": \"message\", \"image\": (open(var_2, \"rb\"))}  # \u9700\u8981\u66ff\u6362\u5177\u4f53\u7684path\n    var_5 = MultipartEncoder(var_4)\n    var_6 = {\n        \"Authorization\": f\"Bearer {var_1}\",  ## \u83b7\u53d6tenant_access_token, \u9700\u8981\u66ff\u6362\u4e3a\u5b9e\u9645\u7684token\n    }\n    var_6[\"Content-Type\"] = var_5.content_type\n    var_7 = requests.request(\"POST\", var_3, var_6=var_6, data=var_5).json()\n    return var_7[\"data\"][\"image_key\"]\n",
      "lines_processed": 17,
      "total_lines": 569
    },
    {
      "file_path": "generate_leaderboard_and_send_to_lark.py",
      "code": "def send_image_to_lark(image_key: str, webhook_url: str) -> None:\n    \"\"\"\n    Send image to Lark.\n\n    Args:\n        image_key (str): the image key returned by Lark\n        webhook_url (str): the webhook url to send the image\n    \"\"\"\n    data = {\"msg_type\": \"image\", \"content\": {\"image_key\": image_key}}\n    requests.post(webhook_url, json=data)\n",
      "variables": [
        "image_key",
        "webhook_url",
        "data"
      ],
      "anonymized_code": "def send_image_to_lark(var_1: str, var_2: str) -> None:\n    \"\"\"\n    Send image to Lark.\n\n    Args:\n        var_1 (str): the image key returned by Lark\n        var_2 (str): the webhook url to send the image\n    \"\"\"\n    var_3 = {\"msg_type\": \"image\", \"content\": {\"var_1\": var_1}}\n    requests.post(var_2, json=var_3)\n",
      "lines_processed": 10,
      "total_lines": 569
    },
    {
      "file_path": "check_example_weekly.py",
      "code": "def show_files(path, all_files):\n    # Traverse all the folder/file in current directory\n    file_list = os.listdir(path)\n    # Determine the element is folder or file. If file, pass it into list, if folder, recurse.\n    for file_name in file_list:\n        # Get the abs directory using os.path.join() and store into cur_path.\n        cur_path = os.path.join(path, file_name)\n        # Determine whether folder\n        if os.path.isdir(cur_path):\n            show_files(cur_path, all_files)\n        else:\n            all_files.append(cur_path)\n    return all_files\n",
      "variables": [
        "path",
        "all_files",
        "file_list",
        "file_name",
        "cur_path"
      ],
      "anonymized_code": "def show_files(var_1, var_2):\n    # Traverse all the folder/file in current directory\n    var_3 = os.listdir(var_1)\n    # Determine the element is folder or file. If file, pass it into list, if folder, recurse.\n    for var_4 in var_3:\n        # Get the abs directory using os.var_1.join() and store into var_5.\n        var_5 = os.var_1.join(var_1, var_4)\n        # Determine whether folder\n        if os.var_1.isdir(var_5):\n            show_files(var_5, var_2)\n        else:\n            var_2.append(var_5)\n    return var_2\n",
      "lines_processed": 13,
      "total_lines": 37
    },
    {
      "file_path": "check_example_weekly.py",
      "code": "def main():\n    contents = show_files(\"examples/\", [])\n    all_loc = []\n    for file_loc in contents:\n        split_loc = file_loc.split(\"/\")\n        # must have two sub-folder levels after examples folder, such as examples/images/vit is acceptable, examples/images/README.md is not, examples/requirements.txt is not.\n        if len(split_loc) >= 4:\n            re_loc = \"/\".join(split_loc[1:3])\n            if re_loc not in all_loc:\n                all_loc.append(re_loc)\n    print(all_loc)\n",
      "variables": [
        "contents",
        "all_loc",
        "file_loc",
        "split_loc",
        "re_loc"
      ],
      "anonymized_code": "def main():\n    var_1 = show_files(\"examples/\", [])\n    var_2 = []\n    for var_3 in var_1:\n        var_4 = var_3.split(\"/\")\n        # must have two sub-folder levels after examples folder, such as examples/images/vit is acceptable, examples/images/README.md is not, examples/requirements.txt is not.\n        if len(var_4) >= 4:\n            var_5 = \"/\".join(var_4[1:3])\n            if var_5 not in var_2:\n                var_2.append(var_5)\n    print(var_2)\n",
      "lines_processed": 11,
      "total_lines": 37
    },
    {
      "file_path": "check_example_weekly.py",
      "code": "def join(input_list, sep=None):\n    return (sep or \" \").join(input_list)\n",
      "variables": [
        "input_list",
        "sep"
      ],
      "anonymized_code": "def join(var_1, var_2=None):\n    return (var_2 or \" \").join(var_1)\n",
      "lines_processed": 2,
      "total_lines": 37
    },
    {
      "file_path": "send_message_to_lark.py",
      "code": "def parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-m\", \"--message\", type=str)\n    parser.add_argument(\"-u\", \"--url\", type=str)\n    return parser.parse_args()\n",
      "variables": [
        "parser"
      ],
      "anonymized_code": "def parse_args():\n    var_1 = argparse.ArgumentParser()\n    var_1.add_argument(\"-m\", \"--message\", type=str)\n    var_1.add_argument(\"-u\", \"--url\", type=str)\n    return var_1.parse_args()\n",
      "lines_processed": 5,
      "total_lines": 20
    },
    {
      "file_path": "send_message_to_lark.py",
      "code": "def send_message_to_lark(message, webhook_url):\n    data = {\"msg_type\": \"text\", \"content\": {\"text\": message}}\n    requests.post(webhook_url, json=data)\n",
      "variables": [
        "message",
        "webhook_url",
        "data"
      ],
      "anonymized_code": "def send_message_to_lark(var_1, var_2):\n    var_3 = {\"msg_type\": \"text\", \"content\": {\"text\": var_1}}\n    requests.post(var_2, json=var_3)\n",
      "lines_processed": 3,
      "total_lines": 20
    }
  ],
  "pygame_pygame": [
    {
      "file_path": "config.py",
      "code": "def prepdep(dep, basepath):\n    \"\"\"add some vars to a dep\"\"\"\n    if dep.libs:\n        dep.line = dep.name + ' ='\n        for lib in dep.libs:\n            dep.line += ' -l' + lib\n    else:\n        dep.line = dep.name + ' = -I.'\n\n    dep.varname = '$('+dep.name+')'\n\n    if not dep.found:\n        if dep.name == 'SDL': #fudge if this is unfound SDL\n            dep.line = 'SDL = -I/NEED_INC_PATH_FIX -L/NEED_LIB_PATH_FIX -lSDL'\n            dep.varname = '$('+dep.name+')'\n            dep.found = 1\n        return\n\n    IPREFIX = ' -I$(BASE)' if basepath else ' -I'\n",
      "variables": [
        "dep",
        "basepath",
        "lib",
        "IPREFIX"
      ],
      "anonymized_code": "def prepdep(var_1, var_2):\n    \"\"\"add some vars to a var_1\"\"\"\n    if var_1.libs:\n        var_1.line = var_1.name + ' ='\n        for var_3 in var_1.libs:\n            var_1.line += ' -l' + var_3\n    else:\n        var_1.line = var_1.name + ' = -I.'\n\n    var_1.varname = '$('+var_1.name+')'\n\n    if not var_1.found:\n        if var_1.name == 'SDL': #fudge if this is unfound SDL\n            var_1.line = 'SDL = -I/NEED_INC_PATH_FIX -L/NEED_LIB_PATH_FIX -lSDL'\n            var_1.varname = '$('+var_1.name+')'\n            var_1.found = 1\n        return\n\n    var_4 = ' -I$(BASE)' if var_2 else ' -I'\n",
      "lines_processed": 19,
      "total_lines": 250
    },
    {
      "file_path": "config.py",
      "code": "def writesetupfile(deps, basepath, additional_lines):\n    \"\"\"create a modified copy of Setup.SDLx.in\"\"\"\n    sdl_setup_filename = os.path.join(BASE_PATH, 'buildconfig',\n                                          'Setup.SDL2.in')\n\n    with open(sdl_setup_filename) as origsetup, \\\n            open(os.path.join(BASE_PATH, 'Setup'), 'w') as newsetup:\n        line = ''\n        while line.find('#--StartConfig') == -1:\n            newsetup.write(line)\n            line = origsetup.readline()\n        while line.find('#--EndConfig') == -1:\n            line = origsetup.readline()\n\n        if basepath:\n            newsetup.write('BASE = ' + basepath + '\\n')\n        for d in deps:\n            newsetup.write(d.line + '\\n')\n\n",
      "variables": [
        "deps",
        "basepath",
        "additional_lines",
        "sdl_setup_filename",
        "origsetup",
        "newsetup",
        "line",
        "d"
      ],
      "anonymized_code": "def writesetupfile(var_1, var_2, var_3):\n    \"\"\"create a modified copy of Setup.SDLx.in\"\"\"\n    var_4 = os.path.join(BASE_PATH, 'buildconfig',\n                                          'Setup.SDL2.in')\n\n    with open(var_4) as var_5, \\\n            open(os.path.join(BASE_PATH, 'Setup'), 'w') as var_6:\n        var_7 = ''\n        while var_7.find('#--StartConfig') == -1:\n            var_6.write(var_7)\n            var_7 = var_5.readline()\n        while var_7.find('#--EndConfig') == -1:\n            var_7 = var_5.readline()\n\n        if var_2:\n            var_6.write('BASE = ' + var_2 + '\\n')\n        for var_8 in var_1:\n            var_6.write(var_8.var_7 + '\\n')\n\n",
      "lines_processed": 19,
      "total_lines": 250
    },
    {
      "file_path": "config.py",
      "code": "def print_(*args, **kwds):\n    \"\"\"Similar to the Python 3.0 print function\"\"\"\n    # This not only supports MSYS but is also a head start on the move to\n    # Python 3.0. Also, this function can be overridden for testing.\n    msysio.print_(*args, **kwds)\n",
      "variables": [
        "args",
        "kwds"
      ],
      "anonymized_code": "def print_(*var_1, **var_2):\n    \"\"\"Similar to the Python 3.0 print function\"\"\"\n    # This not only supports MSYS but is also a head start on the move to\n    # Python 3.0. Also, this function can be overridden for testing.\n    msysio.print_(*var_1, **var_2)\n",
      "lines_processed": 5,
      "total_lines": 250
    },
    {
      "file_path": "config_conan.py",
      "code": "def conan_install(force_build=True):\n    \"\"\"\n    \"\"\"\n    build_dir = os.path.join('build', 'conan')\n\n    if not os.path.exists(build_dir):\n        if not os.path.exists('build'):\n            os.mkdir('build')\n        os.mkdir(build_dir)\n\n    os.chdir(build_dir)\n\n    cmd = [\n        \"conan\",\n        \"install\",\n        os.path.join('..', '..', 'buildconfig', 'conanconf'),\n    ]\n    if force_build:\n        cmd.append(\"--build\")\n",
      "variables": [
        "force_build",
        "build_dir",
        "cmd"
      ],
      "anonymized_code": "def conan_install(var_1=True):\n    \"\"\"\n    \"\"\"\n    var_2 = os.path.join('build', 'conan')\n\n    if not os.path.exists(var_2):\n        if not os.path.exists('build'):\n            os.mkdir('build')\n        os.mkdir(var_2)\n\n    os.chdir(var_2)\n\n    var_3 = [\n        \"conan\",\n        \"install\",\n        os.path.join('..', '..', 'buildconfig', 'conanconf'),\n    ]\n    if var_1:\n        var_3.append(\"--build\")\n",
      "lines_processed": 19,
      "total_lines": 91
    },
    {
      "file_path": "config_conan.py",
      "code": "def main(sdl2=True, auto_config=False):\n    # conan_install(force_build=True)\n    # Reuse the previous conan build with this setting:\n    conan_install(force_build=False)\n\n    conanbuildinfo_json = os.path.join('build', 'conan', 'conanbuildinfo.json')\n    conanbuildinfo = json.load(open(conanbuildinfo_json))\n\n    DEPS = [\n        Dependency(conanbuildinfo, \"SDL\", \"sdl2\"),\n        Dependency(conanbuildinfo, \"FONT\", \"sdl2_ttf\"),\n        Dependency(conanbuildinfo, \"IMAGE\", \"sdl2_image\"),\n        Dependency(conanbuildinfo, \"MIXER\", \"sdl2_mixer\"),\n        Dependency(conanbuildinfo, \"PNG\", \"libpng\"),\n        Dependency(conanbuildinfo, \"JPEG\", \"libjpeg\"),\n        Dependency(conanbuildinfo, \"FREETYPE\", \"freetype\"),\n        Dependency(conanbuildinfo, \"PORTMIDI\", \"portmidi\"),\n        Dependency(conanbuildinfo, \"PORTTIME\", \"portmidi\"),\n    ]\n",
      "variables": [
        "sdl2",
        "auto_config",
        "conanbuildinfo_json",
        "conanbuildinfo",
        "DEPS"
      ],
      "anonymized_code": "def main(var_1=True, var_2=False):\n    # conan_install(force_build=True)\n    # Reuse the previous conan build with this setting:\n    conan_install(force_build=False)\n\n    var_3 = os.path.join('build', 'conan', 'var_4.json')\n    var_4 = json.load(open(var_3))\n\n    var_5 = [\n        Dependency(var_4, \"SDL\", \"var_1\"),\n        Dependency(var_4, \"FONT\", \"sdl2_ttf\"),\n        Dependency(var_4, \"IMAGE\", \"sdl2_image\"),\n        Dependency(var_4, \"MIXER\", \"sdl2_mixer\"),\n        Dependency(var_4, \"PNG\", \"libpng\"),\n        Dependency(var_4, \"JPEG\", \"libjpeg\"),\n        Dependency(var_4, \"FREETYPE\", \"freetype\"),\n        Dependency(var_4, \"PORTMIDI\", \"portmidi\"),\n        Dependency(var_4, \"PORTTIME\", \"portmidi\"),\n    ]\n",
      "lines_processed": 19,
      "total_lines": 91
    },
    {
      "file_path": "config_win.py",
      "code": "def main(auto_config=False):\n    machine_type = get_machine_type()\n    prebuilt_dir = f'prebuilt-{machine_type}'\n    use_prebuilt = '-prebuilt' in sys.argv\n\n    auto_download = 'PYGAME_DOWNLOAD_PREBUILT' in os.environ\n    if auto_download:\n        auto_download = os.environ['PYGAME_DOWNLOAD_PREBUILT'] == '1'\n\n    try:\n        from . import download_win_prebuilt\n    except ImportError:\n        import download_win_prebuilt\n\n    download_kwargs = {\n        'x86': False,\n        'x64': False,\n    }\n    download_kwargs[machine_type] = True\n",
      "variables": [
        "auto_config",
        "machine_type",
        "prebuilt_dir",
        "use_prebuilt",
        "auto_download",
        "download_kwargs"
      ],
      "anonymized_code": "def main(var_1=False):\n    var_2 = get_machine_type()\n    var_3 = f'prebuilt-{var_2}'\n    var_4 = '-prebuilt' in sys.argv\n\n    var_5 = 'PYGAME_DOWNLOAD_PREBUILT' in os.environ\n    if var_5:\n        var_5 = os.environ['PYGAME_DOWNLOAD_PREBUILT'] == '1'\n\n    try:\n        from . import download_win_prebuilt\n    except ImportError:\n        import download_win_prebuilt\n\n    var_6 = {\n        'x86': False,\n        'x64': False,\n    }\n    var_6[var_2] = True\n",
      "lines_processed": 19,
      "total_lines": 508
    },
    {
      "file_path": "config_win.py",
      "code": "def setup():\n    DEPS = DependencyGroup()\n\n    DEPS.add('SDL', 'SDL2', ['SDL2-[1-9].*'], r'(lib){0,1}SDL2\\.dll$', required=1)\n    DEPS.add('PORTMIDI', 'portmidi', ['portmidi'], r'portmidi\\.dll$', find_header=r'portmidi\\.h')\n    #DEPS.add('PORTTIME', 'porttime', ['porttime'], r'porttime\\.dll$')\n    DEPS.add_dummy('PORTTIME')\n    DEPS.add('MIXER', 'SDL2_mixer', ['SDL2_mixer-[1-9].*'], r'(lib){0,1}SDL2_mixer\\.dll$',\n             ['SDL'])\n    DEPS.add('PNG', 'png', ['SDL2_image-[2-9].*', 'libpng-[1-9].*'], r'(png|libpng)[-0-9]*\\.dll$', ['z'],\n             find_header=r'png\\.h', find_lib=r'(lib)?png1[-0-9]*\\.lib')\n    DEPS.add('JPEG', 'jpeg', ['SDL2_image-[2-9].*', 'jpeg-9*'], r'(lib){0,1}jpeg-9\\.dll$',\n             find_header=r'jpeglib\\.h', find_lib=r'(lib)?jpeg-9\\.lib')\n    DEPS.add('IMAGE', 'SDL2_image', ['SDL2_image-[1-9].*'], r'(lib){0,1}SDL2_image\\.dll$',\n             ['SDL', 'jpeg', 'png', 'tiff'], 0)\n    DEPS.add('FONT', 'SDL2_ttf', ['SDL2_ttf-[2-9].*'], r'(lib){0,1}SDL2_ttf\\.dll$', ['SDL', 'z', 'freetype'])\n    DEPS.add('FREETYPE', 'freetype', ['freetype'], r'freetype[-0-9]*\\.dll$',\n             find_header=r'ft2build\\.h', find_lib=r'freetype[-0-9]*\\.lib')\n    DEPS.configure()\n",
      "variables": [
        "DEPS"
      ],
      "anonymized_code": "def setup():\n    var_1 = DependencyGroup()\n\n    var_1.add('SDL', 'SDL2', ['SDL2-[1-9].*'], r'(lib){0,1}SDL2\\.dll$', required=1)\n    var_1.add('PORTMIDI', 'portmidi', ['portmidi'], r'portmidi\\.dll$', find_header=r'portmidi\\.h')\n    #var_1.add('PORTTIME', 'porttime', ['porttime'], r'porttime\\.dll$')\n    var_1.add_dummy('PORTTIME')\n    var_1.add('MIXER', 'SDL2_mixer', ['SDL2_mixer-[1-9].*'], r'(lib){0,1}SDL2_mixer\\.dll$',\n             ['SDL'])\n    var_1.add('PNG', 'png', ['SDL2_image-[2-9].*', 'libpng-[1-9].*'], r'(png|libpng)[-0-9]*\\.dll$', ['z'],\n             find_header=r'png\\.h', find_lib=r'(lib)?png1[-0-9]*\\.lib')\n    var_1.add('JPEG', 'jpeg', ['SDL2_image-[2-9].*', 'jpeg-9*'], r'(lib){0,1}jpeg-9\\.dll$',\n             find_header=r'jpeglib\\.h', find_lib=r'(lib)?jpeg-9\\.lib')\n    var_1.add('IMAGE', 'SDL2_image', ['SDL2_image-[1-9].*'], r'(lib){0,1}SDL2_image\\.dll$',\n             ['SDL', 'jpeg', 'png', 'tiff'], 0)\n    var_1.add('FONT', 'SDL2_ttf', ['SDL2_ttf-[2-9].*'], r'(lib){0,1}SDL2_ttf\\.dll$', ['SDL', 'z', 'freetype'])\n    var_1.add('FREETYPE', 'freetype', ['freetype'], r'freetype[-0-9]*\\.dll$',\n             find_header=r'ft2build\\.h', find_lib=r'freetype[-0-9]*\\.lib')\n    var_1.configure()\n",
      "lines_processed": 19,
      "total_lines": 508
    },
    {
      "file_path": "config_darwin.py",
      "code": "def find_freetype():\n    \"\"\" modern freetype uses pkg-config. However, some older systems don't have that.\n    \"\"\"\n    pkg_config = DependencyProg(\n        'FREETYPE', 'FREETYPE_CONFIG', 'pkg-config freetype2', '2.0',\n        ['freetype2'], '--modversion'\n    )\n    if pkg_config.found:\n        return pkg_config\n\n    #DependencyProg('FREETYPE', 'FREETYPE_CONFIG', '/usr/X11R6/bin/freetype-config', '2.0',\n    freetype_config = DependencyProg(\n        'FREETYPE', 'FREETYPE_CONFIG', 'freetype-config', '2.0', ['freetype'], '--ftversion'\n    )\n    if freetype_config.found:\n        return freetype_config\n    return pkg_config\n",
      "variables": [
        "pkg_config",
        "freetype_config"
      ],
      "anonymized_code": "def find_freetype():\n    \"\"\" modern freetype uses pkg-config. However, some older systems don't have that.\n    \"\"\"\n    var_1 = DependencyProg(\n        'FREETYPE', 'FREETYPE_CONFIG', 'pkg-config freetype2', '2.0',\n        ['freetype2'], '--modversion'\n    )\n    if var_1.found:\n        return var_1\n\n    #DependencyProg('FREETYPE', 'FREETYPE_CONFIG', '/usr/X11R6/bin/freetype-config', '2.0',\n    var_2 = DependencyProg(\n        'FREETYPE', 'FREETYPE_CONFIG', 'freetype-config', '2.0', ['freetype'], '--ftversion'\n    )\n    if var_2.found:\n        return var_2\n    return var_1\n",
      "lines_processed": 17,
      "total_lines": 178
    },
    {
      "file_path": "config_darwin.py",
      "code": "def main(auto_config=False):\n\n    DEPS = [\n        [DependencyProg('SDL', 'SDL_CONFIG', 'sdl2-config', '2.0', ['sdl'])],\n        [Dependency('FONT', ['SDL_ttf.h', 'SDL2/SDL_ttf.h'], 'libSDL2_ttf', ['SDL2_ttf'])],\n        [Dependency('IMAGE', ['SDL_image.h', 'SDL2/SDL_image.h'], 'libSDL2_image', ['SDL2_image'])],\n        [Dependency('MIXER', ['SDL_mixer.h', 'SDL2/SDL_mixer.h'], 'libSDL2_mixer', ['SDL2_mixer'])],\n    ]\n\n    DEPS.extend([\n        Dependency('PNG', 'png.h', 'libpng', ['png']),\n        Dependency('JPEG', 'jpeglib.h', 'libjpeg', ['jpeg']),\n        Dependency('PORTMIDI', 'portmidi.h', 'libportmidi', ['portmidi']),\n        Dependency('PORTTIME', 'porttime.h', '', []),\n        find_freetype(),\n        # Scrap is included in sdlmain_osx, there is nothing to look at.\n        # Dependency('SCRAP', '','',[]),\n    ])\n\n",
      "variables": [
        "auto_config",
        "DEPS"
      ],
      "anonymized_code": "def main(var_1=False):\n\n    var_2 = [\n        [DependencyProg('SDL', 'SDL_CONFIG', 'sdl2-config', '2.0', ['sdl'])],\n        [Dependency('FONT', ['SDL_ttf.h', 'SDL2/SDL_ttf.h'], 'libSDL2_ttf', ['SDL2_ttf'])],\n        [Dependency('IMAGE', ['SDL_image.h', 'SDL2/SDL_image.h'], 'libSDL2_image', ['SDL2_image'])],\n        [Dependency('MIXER', ['SDL_mixer.h', 'SDL2/SDL_mixer.h'], 'libSDL2_mixer', ['SDL2_mixer'])],\n    ]\n\n    var_2.extend([\n        Dependency('PNG', 'png.h', 'libpng', ['png']),\n        Dependency('JPEG', 'jpeglib.h', 'libjpeg', ['jpeg']),\n        Dependency('PORTMIDI', 'portmidi.h', 'libportmidi', ['portmidi']),\n        Dependency('PORTTIME', 'porttime.h', '', []),\n        find_freetype(),\n        # Scrap is included in sdlmain_osx, there is nothing to look at.\n        # Dependency('SCRAP', '','',[]),\n    ])\n\n",
      "lines_processed": 19,
      "total_lines": 178
    },
    {
      "file_path": "install_mac_deps.py",
      "code": "def rmpath(path: Path, verbose: bool = False):\n    \"\"\"\n    Tries to remove a path of any kind\n    \"\"\"\n    if path.is_symlink():\n        if verbose:\n            print(f\"- Removing existing symlink at '{path}'\")\n\n        path.unlink()\n\n    elif path.is_file():\n        if verbose:\n            print(f\"- Removing existing file at '{path}'\")\n\n        path.unlink()\n\n    elif path.is_dir():\n        if verbose:\n            print(f\"- Removing existing directory at '{path}'\")\n",
      "variables": [
        "path",
        "verbose"
      ],
      "anonymized_code": "def rmpath(var_1: Path, var_2: bool = False):\n    \"\"\"\n    Tries to remove a var_1 of any kind\n    \"\"\"\n    if var_1.is_symlink():\n        if var_2:\n            print(f\"- Removing existing symlink at '{var_1}'\")\n\n        var_1.unlink()\n\n    elif var_1.is_file():\n        if var_2:\n            print(f\"- Removing existing file at '{var_1}'\")\n\n        var_1.unlink()\n\n    elif var_1.is_dir():\n        if var_2:\n            print(f\"- Removing existing directory at '{var_1}'\")\n",
      "lines_processed": 19,
      "total_lines": 61
    },
    {
      "file_path": "download_msys2_prebuilt.py",
      "code": "def install_prebuilts(arch):\n    \"\"\"For installing prebuilt dependencies.\"\"\"\n    errors = False\n    print(\"Installing pre-built dependencies\")\n    for pkg in get_packages(arch):\n        print(f\"Installing {pkg}\")\n        error = install_pacman_package(pkg)\n        errors = errors or error\n    if errors:\n        raise Exception(\"Some dependencies could not be installed\")\n",
      "variables": [
        "arch",
        "errors",
        "pkg",
        "error"
      ],
      "anonymized_code": "def install_prebuilts(var_1):\n    \"\"\"For installing prebuilt dependencies.\"\"\"\n    var_2 = False\n    print(\"Installing pre-built dependencies\")\n    for var_3 in get_packages(var_1):\n        print(f\"Installing {var_3}\")\n        var_4 = install_pacman_package(var_3)\n        var_2 = var_2 or var_4\n    if var_2:\n        raise Exception(\"Some dependencies could not be installed\")\n",
      "lines_processed": 10,
      "total_lines": 134
    },
    {
      "file_path": "download_msys2_prebuilt.py",
      "code": "def install_pacman_package(pkg_name):\n    \"\"\"This installs a package in the current MSYS2 environment\n\n    Does not download again if the package is already installed\n    and if the version is the latest available in MSYS2\n    \"\"\"\n    output = subprocess.run(\n        [\"pacman\", \"-S\", \"--noconfirm\", pkg_name], capture_output=True, text=True\n    )\n    if output.returncode != 0:\n        logging.error(\n            \"Error {} while downloading package {}: \\n{}\".format(\n                output.returncode, pkg_name, output.stderr\n            )\n        )\n\n    return output.returncode != 0\n",
      "variables": [
        "pkg_name",
        "output"
      ],
      "anonymized_code": "def install_pacman_package(var_1):\n    \"\"\"This installs a package in the current MSYS2 environment\n\n    Does not download again if the package is already installed\n    and if the version is the latest available in MSYS2\n    \"\"\"\n    var_2 = subprocess.run(\n        [\"pacman\", \"-S\", \"--noconfirm\", var_1], capture_output=True, text=True\n    )\n    if var_2.returncode != 0:\n        logging.error(\n            \"Error {} while downloading package {}: \\n{}\".format(\n                var_2.returncode, var_1, var_2.stderr\n            )\n        )\n\n    return var_2.returncode != 0\n",
      "lines_processed": 17,
      "total_lines": 134
    },
    {
      "file_path": "download_msys2_prebuilt.py",
      "code": "def update(arch=None):\n    install_prebuilts(arch if arch else detect_arch())\n",
      "variables": [
        "arch"
      ],
      "anonymized_code": "def update(var_1=None):\n    install_prebuilts(var_1 if var_1 else detect_arch())\n",
      "lines_processed": 2,
      "total_lines": 134
    },
    {
      "file_path": "makeref.py",
      "code": "def run():\n    global rst_dir, rst_source_dir, rst_build_dir, rst_doctree_dir, c_header_dir\n    rst_dir = 'docs'\n    rst_source_dir = os.path.join(rst_dir, 'reST')\n    rst_build_dir = os.path.join('docs', 'generated')\n\n    rst_doctree_dir = os.path.join(rst_build_dir, 'doctrees')\n    c_header_dir = os.path.join('src_c', 'doc')\n    print(\"Generating:\", rst_source_dir, rst_build_dir)\n    runit()\n\n\n    rst_source_dir = os.path.join(rst_dir, 'es')\n    rst_build_dir = os.path.join('docs', 'generated', 'es')\n    rst_doctree_dir = os.path.join(rst_build_dir, 'doctrees')\n    print(\"Generating:\", rst_source_dir, rst_build_dir)\n    runit()\n",
      "variables": [
        "rst_dir",
        "rst_source_dir",
        "rst_build_dir",
        "rst_doctree_dir",
        "c_header_dir"
      ],
      "anonymized_code": "def run():\n    global var_1, var_2, var_3, var_4, var_5\n    var_1 = 'docs'\n    var_2 = os.path.join(var_1, 'reST')\n    var_3 = os.path.join('docs', 'generated')\n\n    var_4 = os.path.join(var_3, 'doctrees')\n    var_5 = os.path.join('src_c', 'doc')\n    print(\"Generating:\", var_2, var_3)\n    runit()\n\n\n    var_2 = os.path.join(var_1, 'es')\n    var_3 = os.path.join('docs', 'generated', 'es')\n    var_4 = os.path.join(var_3, 'doctrees')\n    print(\"Generating:\", var_2, var_3)\n    runit()\n",
      "lines_processed": 17,
      "total_lines": 62
    },
    {
      "file_path": "makeref.py",
      "code": "def runit():\n    full_generation_flag = False\n    for argument in sys.argv[1:]:\n        if argument == 'full_generation':\n            full_generation_flag = True\n    try:\n        subprocess_args = [sys.executable, '-m', 'sphinx',\n                           '-b', 'html',\n                           '-d', rst_doctree_dir,\n                           '-D', f'headers_dest={c_header_dir}',\n                           '-D', 'headers_mkdirs=0',\n                           rst_source_dir,\n                           rst_build_dir, ]\n        if full_generation_flag:\n            subprocess_args.append('-E')\n        print(\"Executing sphinx in subprocess with args:\", subprocess_args)\n        return subprocess.run(subprocess_args).returncode\n    except Exception:\n        print('---')\n",
      "variables": [
        "full_generation_flag",
        "argument",
        "subprocess_args"
      ],
      "anonymized_code": "def runit():\n    var_1 = False\n    for var_2 in sys.argv[1:]:\n        if var_2 == 'full_generation':\n            var_1 = True\n    try:\n        var_3 = [sys.executable, '-m', 'sphinx',\n                           '-b', 'html',\n                           '-d', rst_doctree_dir,\n                           '-D', f'headers_dest={c_header_dir}',\n                           '-D', 'headers_mkdirs=0',\n                           rst_source_dir,\n                           rst_build_dir, ]\n        if var_1:\n            var_3.append('-E')\n        print(\"Executing sphinx in subprocess with args:\", var_3)\n        return subprocess.run(var_3).returncode\n    except Exception:\n        print('---')\n",
      "lines_processed": 19,
      "total_lines": 62
    },
    {
      "file_path": "config_msys2.py",
      "code": "def get_absolute_win_path(msys2_path):\n    output = subprocess.run(['cygpath', '-w', msys2_path],\n                            capture_output=True, text=True)\n    if output.returncode != 0:\n        raise Exception(f\"Could not get absolute Windows path: {msys2_path}\")\n    else:\n        return output.stdout.strip()\n",
      "variables": [
        "msys2_path",
        "output"
      ],
      "anonymized_code": "def get_absolute_win_path(var_1):\n    var_2 = subprocess.run(['cygpath', '-w', var_1],\n                            capture_output=True, text=True)\n    if var_2.returncode != 0:\n        raise Exception(f\"Could not get absolute Windows path: {var_1}\")\n    else:\n        return var_2.stdout.strip()\n",
      "lines_processed": 7,
      "total_lines": 501
    },
    {
      "file_path": "config_msys2.py",
      "code": "def as_machine_type(size):\n    \"\"\"Return pointer bit size as a Windows machine type\"\"\"\n    if size == 32:\n        return \"x86\"\n    if size == 64:\n        return \"x64\"\n    raise ValueError(\"Unknown pointer size {}\".format(size))\n",
      "variables": [
        "size"
      ],
      "anonymized_code": "def as_machine_type(var_1):\n    \"\"\"Return pointer bit var_1 as a Windows machine type\"\"\"\n    if var_1 == 32:\n        return \"x86\"\n    if var_1 == 64:\n        return \"x64\"\n    raise ValueError(\"Unknown pointer var_1 {}\".format(var_1))\n",
      "lines_processed": 7,
      "total_lines": 501
    },
    {
      "file_path": "msysio.py",
      "code": "def print_(*args, **kwds):\n    \"\"\"Print arguments in an MSYS console friendly way\n\n    Keyword arguments:\n        file, sep, end\n    \"\"\"\n\n    stream = kwds.get('file', sys.stdout)\n    sep = kwds.get('sep', ' ')\n    end = kwds.get('end', '\\n')\n\n    if args:\n        stream.write(sep.join([str(arg) for arg in args]))\n    if end:\n        stream.write(end)\n    try:\n        stream.flush()\n    except AttributeError:\n        pass\n",
      "variables": [
        "args",
        "kwds",
        "stream",
        "sep",
        "end",
        "arg"
      ],
      "anonymized_code": "def print_(*var_1, **var_2):\n    \"\"\"Print arguments in an MSYS console friendly way\n\n    Keyword arguments:\n        file, var_4, var_5\n    \"\"\"\n\n    var_3 = var_2.get('file', sys.stdout)\n    var_4 = var_2.get('var_4', ' ')\n    var_5 = var_2.get('var_5', '\\n')\n\n    if var_1:\n        var_3.write(var_4.join([str(var_6) for var_6 in var_1]))\n    if var_5:\n        var_3.write(var_5)\n    try:\n        var_3.flush()\n    except AttributeError:\n        pass\n",
      "lines_processed": 19,
      "total_lines": 42
    },
    {
      "file_path": "bundle_docs.py",
      "code": "def add_directory(bundle, root, alias):\n    \"\"\"Recursively add a directory, subdirectories, and files to the bundle.\"\"\"\n    reject_dirs = re.compile(r'(.svn)$')\n\n    # Since it is the file extension that is of interest the reversed\n    # file name is checked.\n    reject_files_reversed = re.compile(r'((~.*)|(cyp\\..*))')\n\n    for sub_root, directories, files in os.walk(root):\n        directories[:] = [\n            d for d in directories if reject_dirs.match(d) is None]\n        files[:] = [\n            f for f in files if reject_files_reversed.match(f[-1::-1]) is None]\n\n        sub_alias = os.path.join(alias, sub_root[len(root)+1:])\n        add_files(bundle, sub_root, sub_alias, files)\n",
      "variables": [
        "bundle",
        "root",
        "alias",
        "reject_dirs",
        "reject_files_reversed",
        "sub_root",
        "directories",
        "files",
        "d",
        "f",
        "sub_alias"
      ],
      "anonymized_code": "def add_directory(var_1, var_2, var_3):\n    \"\"\"Recursively add a directory, subdirectories, and var_8 to the var_1.\"\"\"\n    var_4 = re.compile(r'(.svn)$')\n\n    # Since it is the file extension that is of interest the reversed\n    # file name is checked.\n    var_5 = re.compile(r'((~.*)|(cyp\\..*))')\n\n    for var_6, var_7, var_8 in os.walk(var_2):\n        var_7[:] = [\n            var_9 for var_9 in var_7 if var_4.match(var_9) is None]\n        var_8[:] = [\n            var_10 for var_10 in var_8 if var_5.match(var_10[-1::-1]) is None]\n\n        var_11 = os.path.join(var_3, var_6[len(var_2)+1:])\n        add_files(var_1, var_6, var_11, var_8)\n",
      "lines_processed": 16,
      "total_lines": 68
    },
    {
      "file_path": "bundle_docs.py",
      "code": "def add_files(bundle, root, alias, file_names):\n    \"\"\"Add files to the bundle.\"\"\"\n    for file_name in file_names:\n        file_alias = os.path.join(alias, file_name)\n        print(f\"  {file_name} --> {file_alias}\")\n        bundle.add(os.path.join(root, file_name), file_alias)\n",
      "variables": [
        "bundle",
        "root",
        "alias",
        "file_names",
        "file_name",
        "file_alias"
      ],
      "anonymized_code": "def add_files(var_1, var_2, var_3, var_4):\n    \"\"\"Add files to the var_1.\"\"\"\n    for var_5 in var_4:\n        var_6 = os.path.join(var_3, var_5)\n        print(f\"  {var_5} --> {var_6}\")\n        var_1.add(os.path.join(var_2, var_5), var_6)\n",
      "lines_processed": 6,
      "total_lines": 68
    },
    {
      "file_path": "bundle_docs.py",
      "code": "def main():\n    \"\"\"Create a tar-zip file containing the pygame documents and examples.\"\"\"\n    with open('setup.py') as setup:\n        match = re.search(r'\"version\":[ \\t]+\"([0-9]+\\.[0-9]+)\\.[^\"]+\"',\n                          setup.read())\n\n    if match is None:\n        print(\"*** Unable to find the pygame version data in setup.py\")\n        version = ''\n    else:\n        version = f'-{match.group(1)}'\n\n    bundle_name = f'pygame{version}-docs-and-examples.tar.gz'\n    print(f\"Creating bundle {bundle_name}\")\n\n    with tarfile.open(bundle_name, 'w:gz') as bundle:\n        root = os.path.abspath('.')\n        alias = 'pygame'\n\n",
      "variables": [
        "setup",
        "match",
        "version",
        "bundle_name",
        "bundle",
        "root",
        "alias"
      ],
      "anonymized_code": "def main():\n    \"\"\"Create a tar-zip file containing the pygame documents and examples.\"\"\"\n    with open('var_1.py') as var_1:\n        var_2 = re.search(r'\"var_3\":[ \\t]+\"([0-9]+\\.[0-9]+)\\.[^\"]+\"',\n                          var_1.read())\n\n    if var_2 is None:\n        print(\"*** Unable to find the pygame var_3 data in var_1.py\")\n        var_3 = ''\n    else:\n        var_3 = f'-{var_2.group(1)}'\n\n    var_4 = f'pygame{var_3}-docs-and-examples.tar.gz'\n    print(f\"Creating var_5 {var_4}\")\n\n    with tarfile.open(var_4, 'w:gz') as var_5:\n        var_6 = os.path.abspath('.')\n        var_7 = 'pygame'\n\n",
      "lines_processed": 19,
      "total_lines": 68
    },
    {
      "file_path": "download_win_prebuilt.py",
      "code": "def place_downloaded_prebuilts(temp_dir, move_to_dir, x86=True, x64=True):\n    \"\"\" puts the downloaded prebuilt files into the right place.\n\n    Leaves the files in temp_dir. copies to move_to_dir\n    \"\"\"\n    prebuilt_x64 = os.path.join(\n        temp_dir,\n        'prebuilt-x64-pygame-2.1.4-20220319',\n        'prebuilt-x64'\n    )\n    prebuilt_x86 = os.path.join(\n        temp_dir,\n        'prebuilt-x86-pygame-2.1.4-20220319',\n        'prebuilt-x86'\n    )\n\n    ignore = None\n    def copy(src, dst):\n        copytree(src, dst, ignore=ignore)\n",
      "variables": [
        "temp_dir",
        "move_to_dir",
        "x86",
        "x64",
        "prebuilt_x64",
        "prebuilt_x86",
        "ignore",
        "src",
        "dst"
      ],
      "anonymized_code": "def place_downloaded_prebuilts(var_1, var_2, var_3=True, var_4=True):\n    \"\"\" puts the downloaded prebuilt files into the right place.\n\n    Leaves the files in var_1. copies to var_2\n    \"\"\"\n    var_5 = os.path.join(\n        var_1,\n        'prebuilt-var_4-pygame-2.1.4-20220319',\n        'prebuilt-var_4'\n    )\n    var_6 = os.path.join(\n        var_1,\n        'prebuilt-var_3-pygame-2.1.4-20220319',\n        'prebuilt-var_3'\n    )\n\n    var_7 = None\n    def copy(var_8, var_9):\n        copytree(var_8, var_9, var_7=var_7)\n",
      "lines_processed": 19,
      "total_lines": 279
    },
    {
      "file_path": "download_win_prebuilt.py",
      "code": "def ask(x86=True, x64=True):\n    move_to_dir = \".\"\n    if x64:\n        dest_str = f\"\\\"{move_to_dir}/prebuilt-x64\\\"\"\n    else:\n        dest_str = \"\"\n    if x86:\n        if dest_str:\n            dest_str = f\"{dest_str} and \"\n        dest_str = f\"{dest_str}\\\"{move_to_dir}/prebuilt-x86\\\"\"\n    logging.info('Downloading prebuilts to \"%s\" and copying to %s.', (download_dir, dest_str))\n    download_prebuilt = True\n\n    if download_prebuilt:\n        update(x86=x86, x64=x64)\n    return download_prebuilt\n",
      "variables": [
        "x86",
        "x64",
        "move_to_dir",
        "dest_str",
        "download_prebuilt"
      ],
      "anonymized_code": "def ask(var_1=True, var_2=True):\n    var_3 = \".\"\n    if var_2:\n        var_4 = f\"\\\"{var_3}/prebuilt-var_2\\\"\"\n    else:\n        var_4 = \"\"\n    if var_1:\n        if var_4:\n            var_4 = f\"{var_4} and \"\n        var_4 = f\"{var_4}\\\"{var_3}/prebuilt-var_1\\\"\"\n    logging.info('Downloading prebuilts to \"%s\" and copying to %s.', (download_dir, var_4))\n    var_5 = True\n\n    if var_5:\n        update(var_1=var_1, var_2=var_2)\n    return var_5\n",
      "lines_processed": 16,
      "total_lines": 279
    },
    {
      "file_path": "download_win_prebuilt.py",
      "code": "def download_sha1_unzip(url, checksum, save_to_directory, unzip=True):\n    \"\"\" This\n    - downloads a url,\n    - sha1 checksum check,\n    - save_to_directory,\n    - then unzips it.\n\n    Does not download again if the file is there.\n    Does not unzip again if the file is there.\n    \"\"\"\n    # requests does connection retrying, but people might not have it installed.\n    use_requests = True\n\n    try:\n        import requests\n    except ImportError:\n        use_requests = False\n\n    import urllib.request as urllib\n",
      "variables": [
        "url",
        "checksum",
        "save_to_directory",
        "unzip",
        "use_requests"
      ],
      "anonymized_code": "def download_sha1_unzip(var_1, var_2, var_3, var_4=True):\n    \"\"\" This\n    - downloads a var_1,\n    - sha1 var_2 check,\n    - var_3,\n    - then unzips it.\n\n    Does not download again if the file is there.\n    Does not var_4 again if the file is there.\n    \"\"\"\n    # requests does connection retrying, but people might not have it installed.\n    var_5 = True\n\n    try:\n        import requests\n    except ImportError:\n        var_5 = False\n\n    import urllib.request as urllib\n",
      "lines_processed": 19,
      "total_lines": 279
    },
    {
      "file_path": "download_win_prebuilt.py",
      "code": "def create_ignore_target_fnc(x64=False, x86=False):\n    if not x64 and not x86:\n        return None\n    strs = []\n    if x64:\n        strs.append('x64')\n    if x86:\n        strs.append('x86')\n    def ignore_func(dir, contents):\n        for target in strs:\n            if target in dir:\n                return contents\n        return []\n    return ignore_func\n",
      "variables": [
        "x64",
        "x86",
        "strs",
        "dir",
        "contents",
        "target"
      ],
      "anonymized_code": "def create_ignore_target_fnc(var_1=False, var_2=False):\n    if not var_1 and not var_2:\n        return None\n    var_3 = []\n    if var_1:\n        var_3.append('var_1')\n    if var_2:\n        var_3.append('var_2')\n    def ignore_func(var_4, var_5):\n        for var_6 in var_3:\n            if var_6 in var_4:\n                return var_5\n        return []\n    return ignore_func\n",
      "lines_processed": 14,
      "total_lines": 279
    },
    {
      "file_path": "download_win_prebuilt.py",
      "code": "def download_prebuilts(temp_dir, x86=True, x64=True):\n    \"\"\" For downloading prebuilt dependencies.\n    \"\"\"\n    if not os.path.exists(temp_dir):\n        print(f\"Making dir :{temp_dir}:\")\n        os.makedirs(temp_dir)\n    for url, checksum in get_urls(x86=x86, x64=x64):\n        download_sha1_unzip(url, checksum, temp_dir, 1)\n",
      "variables": [
        "temp_dir",
        "x86",
        "x64",
        "url",
        "checksum"
      ],
      "anonymized_code": "def download_prebuilts(var_1, var_2=True, var_3=True):\n    \"\"\" For downloading prebuilt dependencies.\n    \"\"\"\n    if not os.path.exists(var_1):\n        print(f\"Making dir :{var_1}:\")\n        os.makedirs(var_1)\n    for var_4, var_5 in get_urls(var_2=var_2, var_3=var_3):\n        download_sha1_unzip(var_4, var_5, var_1, 1)\n",
      "lines_processed": 8,
      "total_lines": 279
    },
    {
      "file_path": "download_win_prebuilt.py",
      "code": "def update(x86=True, x64=True):\n    move_to_dir = \".\"\n    download_prebuilts(download_dir, x86=x86, x64=x64)\n    place_downloaded_prebuilts(download_dir, move_to_dir, x86=x86, x64=x64)\n",
      "variables": [
        "x86",
        "x64",
        "move_to_dir"
      ],
      "anonymized_code": "def update(var_1=True, var_2=True):\n    var_3 = \".\"\n    download_prebuilts(download_dir, var_1=var_1, var_2=var_2)\n    place_downloaded_prebuilts(download_dir, var_3, var_1=var_1, var_2=var_2)\n",
      "lines_processed": 4,
      "total_lines": 279
    }
  ],
  "PrefectHQ_prefect": [
    {
      "file_path": "db.py",
      "code": "def execute(query: str):\n    pass\n",
      "variables": [
        "query"
      ],
      "anonymized_code": "def execute(var_1: str):\n    pass\n",
      "lines_processed": 2,
      "total_lines": 6
    },
    {
      "file_path": "conftest.py",
      "code": "def pytest_collection_modifyitems(items):\n    for item in items:\n        for file, reason in SKIP_FILES.items():\n            full_path = os.path.join(project_root, file)\n            if str(item.fspath) == full_path:\n                item.add_marker(pytest.mark.skip(reason=reason))\n",
      "variables": [
        "items",
        "item",
        "file",
        "reason",
        "full_path"
      ],
      "anonymized_code": "def pytest_collection_modifyitems(var_1):\n    for var_2 in var_1:\n        for var_3, var_4 in SKIP_FILES.var_1():\n            var_5 = os.path.join(project_root, var_3)\n            if str(var_2.fspath) == var_5:\n                var_2.add_marker(pytest.mark.skip(var_4=var_4))\n",
      "lines_processed": 6,
      "total_lines": 106
    },
    {
      "file_path": "conftest.py",
      "code": "def mock_post_200(monkeypatch):\n    mock_response = mock.Mock()\n    mock_response.status_code = 200\n    mock_response.json.return_value = []\n\n    def mock_post(*args, **kwargs):\n        return mock_response\n\n    monkeypatch.setattr(\"requests.post\", mock_post)\n    return mock_response\n",
      "variables": [
        "monkeypatch",
        "mock_response",
        "args",
        "kwargs"
      ],
      "anonymized_code": "def mock_post_200(var_1):\n    var_2 = mock.Mock()\n    var_2.status_code = 200\n    var_2.json.return_value = []\n\n    def mock_post(*var_3, **var_4):\n        return var_2\n\n    var_1.setattr(\"requests.post\", mock_post)\n    return var_2\n",
      "lines_processed": 10,
      "total_lines": 106
    },
    {
      "file_path": "tasks.py",
      "code": "def my_background_task(name: str): ...\n",
      "variables": [
        "name"
      ],
      "anonymized_code": "def my_background_task(var_1: str): ...\n",
      "lines_processed": 1,
      "total_lines": 5
    },
    {
      "file_path": "bench_flows.py",
      "code": "def bench_flow_with_called_tasks(benchmark: \"BenchmarkFixture\", num_tasks: int):\n    test_task = task(noop_function)\n\n    @flow\n    def benchmark_flow():\n        for _ in range(num_tasks):\n            test_task()\n\n    if num_tasks > 100:\n        benchmark.pedantic(benchmark_flow)\n    else:\n        benchmark(benchmark_flow)\n",
      "variables": [
        "benchmark",
        "num_tasks",
        "test_task",
        "_"
      ],
      "anonymized_code": "def bench_flow_with_called_tasks(var_1: \"BenchmarkFixture\", var_2: int):\n    var_3 = task(noop_function)\n\n    @flow\n    def benchmark_flow():\n        for var_4 in range(var_2):\n            var_3()\n\n    if var_2 > 100:\n        var_1.pedantic(benchmark_flow)\n    else:\n        var_1(benchmark_flow)\n",
      "lines_processed": 12,
      "total_lines": 122
    },
    {
      "file_path": "bench_flows.py",
      "code": "def bench_flow_with_submitted_tasks(benchmark: \"BenchmarkFixture\", num_tasks: int):\n    test_task = task(noop_function)\n\n    @flow\n    def benchmark_flow():\n        for _ in range(num_tasks):\n            test_task.submit()\n\n    benchmark(benchmark_flow)\n",
      "variables": [
        "benchmark",
        "num_tasks",
        "test_task",
        "_"
      ],
      "anonymized_code": "def bench_flow_with_submitted_tasks(var_1: \"BenchmarkFixture\", var_2: int):\n    var_3 = task(noop_function)\n\n    @flow\n    def benchmark_flow():\n        for var_4 in range(var_2):\n            var_3.submit()\n\n    var_1(benchmark_flow)\n",
      "lines_processed": 9,
      "total_lines": 122
    },
    {
      "file_path": "bench_flows.py",
      "code": "def bench_async_flow_with_sequential_subflows(\n    benchmark: \"BenchmarkFixture\", num_flows: int\n):\n    test_flow = flow(anoop_function)\n\n    @flow\n    async def benchmark_flow():\n        for _ in range(num_flows):\n            await test_flow()\n\n    benchmark(anyio.run, benchmark_flow)\n",
      "variables": [
        "benchmark",
        "num_flows",
        "test_flow",
        "_"
      ],
      "anonymized_code": "def bench_async_flow_with_sequential_subflows(\n    var_1: \"BenchmarkFixture\", var_2: int\n):\n    var_3 = flow(anoop_function)\n\n    @flow\n    async def benchmark_flow():\n        for var_4 in range(var_2):\n            await var_3()\n\n    var_1(anyio.run, benchmark_flow)\n",
      "lines_processed": 11,
      "total_lines": 122
    },
    {
      "file_path": "bench_flows.py",
      "code": "def bench_flow_with_subflows(benchmark: \"BenchmarkFixture\", num_flows: int):\n    test_flow = flow(noop_function)\n\n    @flow\n    def benchmark_flow():\n        for _ in range(num_flows):\n            test_flow()\n\n    benchmark(benchmark_flow)\n",
      "variables": [
        "benchmark",
        "num_flows",
        "test_flow",
        "_"
      ],
      "anonymized_code": "def bench_flow_with_subflows(var_1: \"BenchmarkFixture\", var_2: int):\n    var_3 = flow(noop_function)\n\n    @flow\n    def benchmark_flow():\n        for var_4 in range(var_2):\n            var_3()\n\n    var_1(benchmark_flow)\n",
      "lines_processed": 9,
      "total_lines": 122
    },
    {
      "file_path": "bench_flows.py",
      "code": "def bench_flow_call(benchmark: \"BenchmarkFixture\", options):\n    noop_flow = flow(**options)(noop_function)\n    benchmark(noop_flow)\n",
      "variables": [
        "benchmark",
        "options",
        "noop_flow"
      ],
      "anonymized_code": "def bench_flow_call(var_1: \"BenchmarkFixture\", var_2):\n    var_3 = flow(**var_2)(noop_function)\n    var_1(var_3)\n",
      "lines_processed": 3,
      "total_lines": 122
    },
    {
      "file_path": "bench_flows.py",
      "code": "def bench_async_flow_with_concurrent_subflows(\n    benchmark: \"BenchmarkFixture\", num_flows: int\n):\n    test_flow = flow(anoop_function)\n\n    @flow\n    async def benchmark_flow():\n        async with anyio.create_task_group() as tg:\n            for _ in range(num_flows):\n                tg.start_soon(test_flow)\n\n    benchmark(anyio.run, benchmark_flow)\n",
      "variables": [
        "benchmark",
        "num_flows",
        "test_flow",
        "tg",
        "_"
      ],
      "anonymized_code": "def bench_async_flow_with_concurrent_subflows(\n    var_1: \"BenchmarkFixture\", var_2: int\n):\n    var_3 = flow(anoop_function)\n\n    @flow\n    async def benchmark_flow():\n        async with anyio.create_task_group() as var_4:\n            for var_5 in range(var_2):\n                var_4.start_soon(var_3)\n\n    var_1(anyio.run, benchmark_flow)\n",
      "lines_processed": 12,
      "total_lines": 122
    },
    {
      "file_path": "bench_flows.py",
      "code": "def bench_async_flow_with_async_tasks(benchmark: \"BenchmarkFixture\", num_tasks: int):\n    test_task = task(anoop_function)\n\n    @flow\n    async def benchmark_flow():\n        async with anyio.create_task_group() as tg:\n            for _ in range(num_tasks):\n                tg.start_soon(test_task)\n\n    if num_tasks > 100:\n        benchmark.pedantic(anyio.run, (benchmark_flow,))\n    else:\n        benchmark(anyio.run, benchmark_flow)\n",
      "variables": [
        "benchmark",
        "num_tasks",
        "test_task",
        "tg",
        "_"
      ],
      "anonymized_code": "def bench_async_flow_with_async_tasks(var_1: \"BenchmarkFixture\", var_2: int):\n    var_3 = task(anoop_function)\n\n    @flow\n    async def benchmark_flow():\n        async with anyio.create_task_group() as var_4:\n            for var_5 in range(var_2):\n                var_4.start_soon(var_3)\n\n    if var_2 > 100:\n        var_1.pedantic(anyio.run, (benchmark_flow,))\n    else:\n        var_1(anyio.run, benchmark_flow)\n",
      "lines_processed": 13,
      "total_lines": 122
    },
    {
      "file_path": "bench_cli.py",
      "code": "def bench_prefect_short_version(benchmark):\n    benchmark(subprocess.check_call, [\"prefect\", \"--version\"])\n",
      "variables": [
        "benchmark"
      ],
      "anonymized_code": "def bench_prefect_short_version(var_1):\n    var_1(subprocess.check_call, [\"prefect\", \"--version\"])\n",
      "lines_processed": 2,
      "total_lines": 21
    },
    {
      "file_path": "bench_cli.py",
      "code": "def bench_prefect_profile_ls(benchmark):\n    benchmark(subprocess.check_call, [\"prefect\", \"profile\", \"ls\"])\n",
      "variables": [
        "benchmark"
      ],
      "anonymized_code": "def bench_prefect_profile_ls(var_1):\n    var_1(subprocess.check_call, [\"prefect\", \"profile\", \"ls\"])\n",
      "lines_processed": 2,
      "total_lines": 21
    },
    {
      "file_path": "bench_cli.py",
      "code": "def bench_prefect_version(benchmark):\n    benchmark(subprocess.check_call, [\"prefect\", \"version\"])\n",
      "variables": [
        "benchmark"
      ],
      "anonymized_code": "def bench_prefect_version(var_1):\n    var_1(subprocess.check_call, [\"prefect\", \"version\"])\n",
      "lines_processed": 2,
      "total_lines": 21
    },
    {
      "file_path": "bench_cli.py",
      "code": "def bench_prefect_help(benchmark):\n    benchmark(subprocess.check_call, [\"prefect\", \"--help\"])\n",
      "variables": [
        "benchmark"
      ],
      "anonymized_code": "def bench_prefect_help(var_1):\n    var_1(subprocess.check_call, [\"prefect\", \"--help\"])\n",
      "lines_processed": 2,
      "total_lines": 21
    },
    {
      "file_path": "utils.py",
      "code": "def post(*args, **kwargs):\n    pass\n",
      "variables": [
        "args",
        "kwargs"
      ],
      "anonymized_code": "def post(*var_1, **var_2):\n    pass\n",
      "lines_processed": 2,
      "total_lines": 6
    },
    {
      "file_path": "utils.py",
      "code": "def put(*args, **kwargs):\n    pass\n",
      "variables": [
        "args",
        "kwargs"
      ],
      "anonymized_code": "def put(*var_1, **var_2):\n    pass\n",
      "lines_processed": 2,
      "total_lines": 6
    },
    {
      "file_path": "flows.py",
      "code": "def my_nested_flow(msg):\n    pass\n",
      "variables": [
        "msg"
      ],
      "anonymized_code": "def my_nested_flow(var_1):\n    pass\n",
      "lines_processed": 2,
      "total_lines": 11
    },
    {
      "file_path": "bench_tasks.py",
      "code": "def bench_task_submit(benchmark: \"BenchmarkFixture\"):\n    noop_task = task(noop_function)\n\n    # The benchmark occurs within the flow to measure _submission_ time without\n    # measuring any other part of orchestration / collection of results\n\n    @flow\n    def benchmark_flow():\n        benchmark(noop_task.submit)\n\n    benchmark_flow()\n",
      "variables": [
        "benchmark",
        "noop_task"
      ],
      "anonymized_code": "def bench_task_submit(var_1: \"BenchmarkFixture\"):\n    var_2 = task(noop_function)\n\n    # The var_1 occurs within the flow to measure _submission_ time without\n    # measuring any other part of orchestration / collection of results\n\n    @flow\n    def benchmark_flow():\n        var_1(var_2.submit)\n\n    benchmark_flow()\n",
      "lines_processed": 11,
      "total_lines": 37
    },
    {
      "file_path": "bench_tasks.py",
      "code": "def bench_task_call(benchmark: \"BenchmarkFixture\"):\n    noop_task = task(noop_function)\n\n    @flow\n    def benchmark_flow():\n        benchmark(noop_task)\n\n    benchmark_flow()\n",
      "variables": [
        "benchmark",
        "noop_task"
      ],
      "anonymized_code": "def bench_task_call(var_1: \"BenchmarkFixture\"):\n    var_2 = task(noop_function)\n\n    @flow\n    def benchmark_flow():\n        var_1(var_2)\n\n    benchmark_flow()\n",
      "lines_processed": 8,
      "total_lines": 37
    },
    {
      "file_path": "bench_tasks.py",
      "code": "def bench_task_decorator(benchmark: \"BenchmarkFixture\"):\n    benchmark(task, noop_function)\n",
      "variables": [
        "benchmark"
      ],
      "anonymized_code": "def bench_task_decorator(var_1: \"BenchmarkFixture\"):\n    var_1(task, noop_function)\n",
      "lines_processed": 2,
      "total_lines": 37
    },
    {
      "file_path": "bench_import.py",
      "code": "def reset_imports():\n    # Remove the module from sys.modules if it's there\n    prefect_modules = [key for key in sys.modules if key.startswith(\"prefect\")]\n    for module in prefect_modules:\n        del sys.modules[module]\n\n    # Clear importlib cache\n    importlib.invalidate_caches()\n\n    # reset the prometheus registry to clear any previously measured metrics\n    for collector in list(REGISTRY._collector_to_names):\n        REGISTRY.unregister(collector)\n",
      "variables": [
        "prefect_modules",
        "key",
        "module",
        "collector"
      ],
      "anonymized_code": "def reset_imports():\n    # Remove the var_3 from sys.modules if it's there\n    var_1 = [var_2 for var_2 in sys.modules if var_2.startswith(\"prefect\")]\n    for var_3 in var_1:\n        del sys.modules[var_3]\n\n    # Clear importlib cache\n    importlib.invalidate_caches()\n\n    # reset the prometheus registry to clear any previously measured metrics\n    for var_4 in list(REGISTRY._collector_to_names):\n        REGISTRY.unregister(var_4)\n",
      "lines_processed": 12,
      "total_lines": 44
    },
    {
      "file_path": "bench_import.py",
      "code": "def bench_import_prefect_flow(benchmark: \"BenchmarkFixture\"):\n    def import_prefect_flow():\n        reset_imports()\n\n        from prefect import flow  # noqa\n\n    benchmark(import_prefect_flow)\n",
      "variables": [
        "benchmark"
      ],
      "anonymized_code": "def bench_import_prefect_flow(var_1: \"BenchmarkFixture\"):\n    def import_prefect_flow():\n        reset_imports()\n\n        from prefect import flow  # noqa\n\n    var_1(import_prefect_flow)\n",
      "lines_processed": 7,
      "total_lines": 44
    },
    {
      "file_path": "bench_import.py",
      "code": "def bench_import_prefect(benchmark: \"BenchmarkFixture\"):\n    def import_prefect():\n        reset_imports()\n\n        import prefect  # noqa\n\n    benchmark(import_prefect)\n",
      "variables": [
        "benchmark"
      ],
      "anonymized_code": "def bench_import_prefect(var_1: \"BenchmarkFixture\"):\n    def import_prefect():\n        reset_imports()\n\n        import prefect  # noqa\n\n    var_1(import_prefect)\n",
      "lines_processed": 7,
      "total_lines": 44
    },
    {
      "file_path": "client_flow.py",
      "code": "def skip_remote_run():\n    \"\"\"\n    Github Actions will not populate secrets if the workflow is triggered by\n    external collaborators (including dependabot). This function checks if\n    we're in a CI environment AND if the secret was not populated -- if\n    those conditions are true, we won't try to run the flow against the remote\n    API\n    \"\"\"\n    import os\n\n    in_gha = os.environ.get(\"CI\", False)\n    secret_not_set = os.environ.get(\"PREFECT_API_KEY\", \"\") == \"\"\n    return in_gha and secret_not_set\n",
      "variables": [
        "in_gha",
        "secret_not_set"
      ],
      "anonymized_code": "def skip_remote_run():\n    \"\"\"\n    Github Actions will not populate secrets if the workflow is triggered by\n    external collaborators (including dependabot). This function checks if\n    we're in a CI environment AND if the secret was not populated -- if\n    those conditions are true, we won't try to run the flow against the remote\n    API\n    \"\"\"\n    import os\n\n    var_1 = os.environ.get(\"CI\", False)\n    var_2 = os.environ.get(\"PREFECT_API_KEY\", \"\") == \"\"\n    return var_1 and var_2\n",
      "lines_processed": 13,
      "total_lines": 35
    },
    {
      "file_path": "client_flow.py",
      "code": "def smoke_test_task(*args: Any, **kwargs: Any):\n    print(args, kwargs)\n",
      "variables": [
        "args",
        "kwargs"
      ],
      "anonymized_code": "def smoke_test_task(*var_1: Any, **var_2: Any):\n    print(var_1, var_2)\n",
      "lines_processed": 2,
      "total_lines": 35
    }
  ],
  "psf_requests": [
    {
      "file_path": "cookies.py",
      "code": "def extract_cookies_to_jar(jar, request, response):\n    \"\"\"Extract the cookies from the response into a CookieJar.\n\n    :param jar: http.cookiejar.CookieJar (not necessarily a RequestsCookieJar)\n    :param request: our own requests.Request object\n    :param response: urllib3.HTTPResponse object\n    \"\"\"\n    if not (hasattr(response, \"_original_response\") and response._original_response):\n        return\n    # the _original_response field is the wrapped httplib.HTTPResponse object,\n    req = MockRequest(request)\n    # pull out the HTTPMessage with the headers and put it in the mock:\n    res = MockResponse(response._original_response.msg)\n    jar.extract_cookies(res, req)\n",
      "variables": [
        "jar",
        "request",
        "response",
        "req",
        "res"
      ],
      "anonymized_code": "def extract_cookies_to_jar(var_1, var_2, var_3):\n    \"\"\"Extract the cookies from the var_3 into a CookieJar.\n\n    :param var_1: http.cookiejar.CookieJar (not necessarily a RequestsCookieJar)\n    :param var_2: our own requests.Request object\n    :param var_3: urllib3.HTTPResponse object\n    \"\"\"\n    if not (hasattr(var_3, \"_original_response\") and var_3._original_response):\n        return\n    # the _original_response field is the wrapped httplib.HTTPResponse object,\n    var_4 = MockRequest(var_2)\n    # pull out the HTTPMessage with the headers and put it in the mock:\n    var_5 = MockResponse(var_3._original_response.msg)\n    var_1.extract_cookies(var_5, var_4)\n",
      "lines_processed": 14,
      "total_lines": 561
    },
    {
      "file_path": "auth.py",
      "code": "def _basic_auth_str(username, password):\n    \"\"\"Returns a Basic Auth string.\"\"\"\n\n    # \"I want us to put a big-ol' comment on top of it that\n    # says that this behaviour is dumb but we need to preserve\n    # it because people are relying on it.\"\n    #    - Lukasa\n    #\n    # These are here solely to maintain backwards compatibility\n    # for things like ints. This will be removed in 3.0.0.\n    if not isinstance(username, basestring):\n        warnings.warn(\n            \"Non-string usernames will no longer be supported in Requests \"\n            \"3.0.0. Please convert the object you've passed in ({!r}) to \"\n            \"a string or bytes object in the near future to avoid \"\n            \"problems.\".format(username),\n            category=DeprecationWarning,\n        )\n        username = str(username)\n",
      "variables": [
        "username",
        "password"
      ],
      "anonymized_code": "def _basic_auth_str(var_1, var_2):\n    \"\"\"Returns a Basic Auth string.\"\"\"\n\n    # \"I want us to put a big-ol' comment on top of it that\n    # says that this behaviour is dumb but we need to preserve\n    # it because people are relying on it.\"\n    #    - Lukasa\n    #\n    # These are here solely to maintain backwards compatibility\n    # for things like ints. This will be removed in 3.0.0.\n    if not isinstance(var_1, basestring):\n        warnings.warn(\n            \"Non-string usernames will no longer be supported in Requests \"\n            \"3.0.0. Please convert the object you've passed in ({!r}) to \"\n            \"a string or bytes object in the near future to avoid \"\n            \"problems.\".format(var_1),\n            category=DeprecationWarning,\n        )\n        var_1 = str(var_1)\n",
      "lines_processed": 19,
      "total_lines": 314
    },
    {
      "file_path": "hooks.py",
      "code": "def default_hooks():\n    return {event: [] for event in HOOKS}\n",
      "variables": [
        "event"
      ],
      "anonymized_code": "def default_hooks():\n    return {var_1: [] for var_1 in HOOKS}\n",
      "lines_processed": 2,
      "total_lines": 33
    },
    {
      "file_path": "hooks.py",
      "code": "def dispatch_hook(key, hooks, hook_data, **kwargs):\n    \"\"\"Dispatches a hook dictionary on a given piece of data.\"\"\"\n    hooks = hooks or {}\n    hooks = hooks.get(key)\n    if hooks:\n        if hasattr(hooks, \"__call__\"):\n            hooks = [hooks]\n        for hook in hooks:\n            _hook_data = hook(hook_data, **kwargs)\n            if _hook_data is not None:\n                hook_data = _hook_data\n    return hook_data\n",
      "variables": [
        "key",
        "hooks",
        "hook_data",
        "kwargs",
        "hook",
        "_hook_data"
      ],
      "anonymized_code": "def dispatch_hook(var_1, var_2, var_3, **var_4):\n    \"\"\"Dispatches a var_5 dictionary on a given piece of data.\"\"\"\n    var_2 = var_2 or {}\n    var_2 = var_2.get(var_1)\n    if var_2:\n        if hasattr(var_2, \"__call__\"):\n            var_2 = [var_2]\n        for var_5 in var_2:\n            var_6 = var_5(var_3, **var_4)\n            if var_6 is not None:\n                var_3 = var_6\n    return var_3\n",
      "lines_processed": 12,
      "total_lines": 33
    },
    {
      "file_path": "compat.py",
      "code": "def _resolve_char_detection():\n    \"\"\"Find supported character detection libraries.\"\"\"\n    chardet = None\n    for lib in (\"chardet\", \"charset_normalizer\"):\n        if chardet is None:\n            try:\n                chardet = importlib.import_module(lib)\n            except ImportError:\n                pass\n    return chardet\n",
      "variables": [
        "chardet",
        "lib"
      ],
      "anonymized_code": "def _resolve_char_detection():\n    \"\"\"Find supported character detection libraries.\"\"\"\n    var_1 = None\n    for var_2 in (\"var_1\", \"charset_normalizer\"):\n        if var_1 is None:\n            try:\n                var_1 = importlib.import_module(var_2)\n            except ImportError:\n                pass\n    return var_1\n",
      "lines_processed": 10,
      "total_lines": 106
    },
    {
      "file_path": "__init__.py",
      "code": "def check_compatibility(urllib3_version, chardet_version, charset_normalizer_version):\n    urllib3_version = urllib3_version.split(\".\")\n    assert urllib3_version != [\"dev\"]  # Verify urllib3 isn't installed from git.\n\n    # Sometimes, urllib3 only reports its version as 16.1.\n    if len(urllib3_version) == 2:\n        urllib3_version.append(\"0\")\n\n    # Check urllib3 for compatibility.\n    major, minor, patch = urllib3_version  # noqa: F811\n    major, minor, patch = int(major), int(minor), int(patch)\n    # urllib3 >= 1.21.1\n    assert major >= 1\n    if major == 1:\n        assert minor >= 21\n\n    # Check charset_normalizer for compatibility.\n    if chardet_version:\n        major, minor, patch = chardet_version.split(\".\")[:3]\n",
      "variables": [
        "urllib3_version",
        "chardet_version",
        "charset_normalizer_version",
        "major",
        "minor",
        "patch"
      ],
      "anonymized_code": "def check_compatibility(var_1, var_2, var_3):\n    var_1 = var_1.split(\".\")\n    assert var_1 != [\"dev\"]  # Verify urllib3 isn't installed from git.\n\n    # Sometimes, urllib3 only reports its version as 16.1.\n    if len(var_1) == 2:\n        var_1.append(\"0\")\n\n    # Check urllib3 for compatibility.\n    var_4, var_5, var_6 = var_1  # noqa: F811\n    var_4, var_5, var_6 = int(var_4), int(var_5), int(var_6)\n    # urllib3 >= 1.21.1\n    assert var_4 >= 1\n    if var_4 == 1:\n        assert var_5 >= 21\n\n    # Check charset_normalizer for compatibility.\n    if var_2:\n        var_4, var_5, var_6 = var_2.split(\".\")[:3]\n",
      "lines_processed": 19,
      "total_lines": 184
    },
    {
      "file_path": "__init__.py",
      "code": "def _check_cryptography(cryptography_version):\n    # cryptography < 1.3.4\n    try:\n        cryptography_version = list(map(int, cryptography_version.split(\".\")))\n    except ValueError:\n        return\n\n    if cryptography_version < [1, 3, 4]:\n        warning = \"Old version of cryptography ({}) may cause slowdown.\".format(\n            cryptography_version\n        )\n        warnings.warn(warning, RequestsDependencyWarning)\n",
      "variables": [
        "cryptography_version",
        "warning"
      ],
      "anonymized_code": "def _check_cryptography(var_1):\n    # cryptography < 1.3.4\n    try:\n        var_1 = list(map(int, var_1.split(\".\")))\n    except ValueError:\n        return\n\n    if var_1 < [1, 3, 4]:\n        var_2 = \"Old version of cryptography ({}) may cause slowdown.\".format(\n            var_1\n        )\n        warnings.warn(var_2, RequestsDependencyWarning)\n",
      "lines_processed": 12,
      "total_lines": 184
    },
    {
      "file_path": "api.py",
      "code": "def get(url, params=None, **kwargs):\n    r\"\"\"Sends a GET request.\n\n    :param url: URL for the new :class:`Request` object.\n    :param params: (optional) Dictionary, list of tuples or bytes to send\n        in the query string for the :class:`Request`.\n    :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    return request(\"get\", url, params=params, **kwargs)\n",
      "variables": [
        "url",
        "params",
        "kwargs"
      ],
      "anonymized_code": "def get(var_1, var_2=None, **var_3):\n    r\"\"\"Sends a GET request.\n\n    :param var_1: URL for the new :class:`Request` object.\n    :param var_2: (optional) Dictionary, list of tuples or bytes to send\n        in the query string for the :class:`Request`.\n    :param \\*\\*var_3: Optional arguments that ``request`` takes.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    return request(\"get\", var_1, var_2=var_2, **var_3)\n",
      "lines_processed": 12,
      "total_lines": 157
    },
    {
      "file_path": "api.py",
      "code": "def patch(url, data=None, **kwargs):\n    r\"\"\"Sends a PATCH request.\n\n    :param url: URL for the new :class:`Request` object.\n    :param data: (optional) Dictionary, list of tuples, bytes, or file-like\n        object to send in the body of the :class:`Request`.\n    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.\n    :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    return request(\"patch\", url, data=data, **kwargs)\n",
      "variables": [
        "url",
        "data",
        "kwargs"
      ],
      "anonymized_code": "def patch(var_1, var_2=None, **var_3):\n    r\"\"\"Sends a PATCH request.\n\n    :param var_1: URL for the new :class:`Request` object.\n    :param var_2: (optional) Dictionary, list of tuples, bytes, or file-like\n        object to send in the body of the :class:`Request`.\n    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.\n    :param \\*\\*var_3: Optional arguments that ``request`` takes.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    return request(\"patch\", var_1, var_2=var_2, **var_3)\n",
      "lines_processed": 13,
      "total_lines": 157
    },
    {
      "file_path": "api.py",
      "code": "def delete(url, **kwargs):\n    r\"\"\"Sends a DELETE request.\n\n    :param url: URL for the new :class:`Request` object.\n    :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    return request(\"delete\", url, **kwargs)\n",
      "variables": [
        "url",
        "kwargs"
      ],
      "anonymized_code": "def delete(var_1, **var_2):\n    r\"\"\"Sends a DELETE request.\n\n    :param var_1: URL for the new :class:`Request` object.\n    :param \\*\\*var_2: Optional arguments that ``request`` takes.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    return request(\"delete\", var_1, **var_2)\n",
      "lines_processed": 10,
      "total_lines": 157
    },
    {
      "file_path": "api.py",
      "code": "def post(url, data=None, json=None, **kwargs):\n    r\"\"\"Sends a POST request.\n\n    :param url: URL for the new :class:`Request` object.\n    :param data: (optional) Dictionary, list of tuples, bytes, or file-like\n        object to send in the body of the :class:`Request`.\n    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.\n    :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    return request(\"post\", url, data=data, json=json, **kwargs)\n",
      "variables": [
        "url",
        "data",
        "json",
        "kwargs"
      ],
      "anonymized_code": "def post(var_1, var_2=None, var_3=None, **var_4):\n    r\"\"\"Sends a POST request.\n\n    :param var_1: URL for the new :class:`Request` object.\n    :param var_2: (optional) Dictionary, list of tuples, bytes, or file-like\n        object to send in the body of the :class:`Request`.\n    :param var_3: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.\n    :param \\*\\*var_4: Optional arguments that ``request`` takes.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    return request(\"post\", var_1, var_2=var_2, var_3=var_3, **var_4)\n",
      "lines_processed": 13,
      "total_lines": 157
    },
    {
      "file_path": "api.py",
      "code": "def head(url, **kwargs):\n    r\"\"\"Sends a HEAD request.\n\n    :param url: URL for the new :class:`Request` object.\n    :param \\*\\*kwargs: Optional arguments that ``request`` takes. If\n        `allow_redirects` is not provided, it will be set to `False` (as\n        opposed to the default :meth:`request` behavior).\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    kwargs.setdefault(\"allow_redirects\", False)\n    return request(\"head\", url, **kwargs)\n",
      "variables": [
        "url",
        "kwargs"
      ],
      "anonymized_code": "def head(var_1, **var_2):\n    r\"\"\"Sends a HEAD request.\n\n    :param var_1: URL for the new :class:`Request` object.\n    :param \\*\\*var_2: Optional arguments that ``request`` takes. If\n        `allow_redirects` is not provided, it will be set to `False` (as\n        opposed to the default :meth:`request` behavior).\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    var_2.setdefault(\"allow_redirects\", False)\n    return request(\"head\", var_1, **var_2)\n",
      "lines_processed": 13,
      "total_lines": 157
    },
    {
      "file_path": "api.py",
      "code": "def options(url, **kwargs):\n    r\"\"\"Sends an OPTIONS request.\n\n    :param url: URL for the new :class:`Request` object.\n    :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    return request(\"options\", url, **kwargs)\n",
      "variables": [
        "url",
        "kwargs"
      ],
      "anonymized_code": "def options(var_1, **var_2):\n    r\"\"\"Sends an OPTIONS request.\n\n    :param var_1: URL for the new :class:`Request` object.\n    :param \\*\\*var_2: Optional arguments that ``request`` takes.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    return request(\"options\", var_1, **var_2)\n",
      "lines_processed": 10,
      "total_lines": 157
    },
    {
      "file_path": "api.py",
      "code": "def put(url, data=None, **kwargs):\n    r\"\"\"Sends a PUT request.\n\n    :param url: URL for the new :class:`Request` object.\n    :param data: (optional) Dictionary, list of tuples, bytes, or file-like\n        object to send in the body of the :class:`Request`.\n    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.\n    :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    return request(\"put\", url, data=data, **kwargs)\n",
      "variables": [
        "url",
        "data",
        "kwargs"
      ],
      "anonymized_code": "def put(var_1, var_2=None, **var_3):\n    r\"\"\"Sends a PUT request.\n\n    :param var_1: URL for the new :class:`Request` object.\n    :param var_2: (optional) Dictionary, list of tuples, bytes, or file-like\n        object to send in the body of the :class:`Request`.\n    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.\n    :param \\*\\*var_3: Optional arguments that ``request`` takes.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    return request(\"put\", var_1, var_2=var_2, **var_3)\n",
      "lines_processed": 13,
      "total_lines": 157
    },
    {
      "file_path": "_internal_utils.py",
      "code": "def to_native_string(string, encoding=\"ascii\"):\n    \"\"\"Given a string object, regardless of type, returns a representation of\n    that string in the native string type, encoding and decoding where\n    necessary. This assumes ASCII unless told otherwise.\n    \"\"\"\n    if isinstance(string, builtin_str):\n        out = string\n    else:\n        out = string.decode(encoding)\n\n    return out\n",
      "variables": [
        "string",
        "encoding",
        "out"
      ],
      "anonymized_code": "def to_native_string(var_1, var_2=\"ascii\"):\n    \"\"\"Given a var_1 object, regardless of type, returns a representation of\n    that var_1 in the native var_1 type, var_2 and decoding where\n    necessary. This assumes ASCII unless told otherwise.\n    \"\"\"\n    if isinstance(var_1, builtin_str):\n        var_3 = var_1\n    else:\n        var_3 = var_1.decode(var_2)\n\n    return var_3\n",
      "lines_processed": 11,
      "total_lines": 50
    },
    {
      "file_path": "_internal_utils.py",
      "code": "def unicode_is_ascii(u_string):\n    \"\"\"Determine if unicode string only contains ASCII characters.\n\n    :param str u_string: unicode string to check. Must be unicode\n        and not Python 2 `str`.\n    :rtype: bool\n    \"\"\"\n    assert isinstance(u_string, str)\n    try:\n        u_string.encode(\"ascii\")\n        return True\n    except UnicodeEncodeError:\n        return False\n",
      "variables": [
        "u_string"
      ],
      "anonymized_code": "def unicode_is_ascii(var_1):\n    \"\"\"Determine if unicode string only contains ASCII characters.\n\n    :param str var_1: unicode string to check. Must be unicode\n        and not Python 2 `str`.\n    :rtype: bool\n    \"\"\"\n    assert isinstance(var_1, str)\n    try:\n        var_1.encode(\"ascii\")\n        return True\n    except UnicodeEncodeError:\n        return False\n",
      "lines_processed": 13,
      "total_lines": 50
    },
    {
      "file_path": "help.py",
      "code": "def info():\n    \"\"\"Generate information for a bug report.\"\"\"\n    try:\n        platform_info = {\n            \"system\": platform.system(),\n            \"release\": platform.release(),\n        }\n    except OSError:\n        platform_info = {\n            \"system\": \"Unknown\",\n            \"release\": \"Unknown\",\n        }\n\n    implementation_info = _implementation()\n    urllib3_info = {\"version\": urllib3.__version__}\n    charset_normalizer_info = {\"version\": None}\n    chardet_info = {\"version\": None}\n    if charset_normalizer:\n        charset_normalizer_info = {\"version\": charset_normalizer.__version__}\n",
      "variables": [
        "platform_info",
        "implementation_info",
        "urllib3_info",
        "charset_normalizer_info",
        "chardet_info"
      ],
      "anonymized_code": "def info():\n    \"\"\"Generate information for a bug report.\"\"\"\n    try:\n        var_1 = {\n            \"system\": platform.system(),\n            \"release\": platform.release(),\n        }\n    except OSError:\n        var_1 = {\n            \"system\": \"Unknown\",\n            \"release\": \"Unknown\",\n        }\n\n    var_2 = _implementation()\n    var_3 = {\"version\": urllib3.__version__}\n    var_4 = {\"version\": None}\n    var_5 = {\"version\": None}\n    if charset_normalizer:\n        var_4 = {\"version\": charset_normalizer.__version__}\n",
      "lines_processed": 19,
      "total_lines": 134
    }
  ],
  "pallets_jinja": [
    {
      "file_path": "generate_identifier_pattern.py",
      "code": "def get_characters():\n    \"\"\"Find every Unicode character that is valid in a Python `identifier`_ but\n    is not matched by the regex ``\\\\w`` group.\n\n    ``\\\\w`` matches some characters that aren't valid in identifiers, but\n    :meth:`str.isidentifier` will catch that later in lexing.\n\n    All start characters are valid continue characters, so we only test for\n    continue characters.\n\n    _identifier: https://docs.python.org/3/reference/lexical_analysis.html#identifiers\n    \"\"\"\n    for cp in range(sys.maxunicode + 1):\n        s = chr(cp)\n\n        if (\"a\" + s).isidentifier() and not re.match(r\"\\w\", s):\n            yield s\n",
      "variables": [
        "cp",
        "s"
      ],
      "anonymized_code": "def get_characters():\n    \"\"\"Find every Unicode character that is valid in a Python `identifier`_ but\n    is not matched by the regex ``\\\\w`` group.\n\n    ``\\\\w`` matches some characters that aren't valid in identifiers, but\n    :meth:`str.isidentifier` will catch that later in lexing.\n\n    All start characters are valid continue characters, so we only test for\n    continue characters.\n\n    _identifier: https://docs.python.org/3/reference/lexical_analysis.html#identifiers\n    \"\"\"\n    for var_1 in range(sys.maxunicode + 1):\n        var_2 = chr(var_1)\n\n        if (\"a\" + var_2).isidentifier() and not re.match(r\"\\w\", var_2):\n            yield var_2\n",
      "lines_processed": 17,
      "total_lines": 73
    },
    {
      "file_path": "generate_identifier_pattern.py",
      "code": "def build_pattern(ranges):\n    \"\"\"Output the regex pattern for ranges of characters.\n\n    One and two character ranges output the individual characters.\n    \"\"\"\n    out = []\n\n    for a, b in ranges:\n        if a == b:  # single char\n            out.append(a)\n        elif ord(b) - ord(a) == 1:  # two chars, range is redundant\n            out.append(a)\n            out.append(b)\n        else:\n            out.append(f\"{a}-{b}\")\n\n    return \"\".join(out)\n",
      "variables": [
        "ranges",
        "out",
        "a",
        "b"
      ],
      "anonymized_code": "def build_pattern(var_1):\n    \"\"\"Output the regex pattern for var_1 of characters.\n\n    One and two character var_1 output the individual characters.\n    \"\"\"\n    var_2 = []\n\n    for var_3, var_4 in var_1:\n        if var_3 == var_4:  # single char\n            var_2.append(var_3)\n        elif ord(var_4) - ord(var_3) == 1:  # two chars, range is redundant\n            var_2.append(var_3)\n            var_2.append(var_4)\n        else:\n            var_2.append(f\"{var_3}-{var_4}\")\n\n    return \"\".join(var_2)\n",
      "lines_processed": 17,
      "total_lines": 73
    },
    {
      "file_path": "generate_identifier_pattern.py",
      "code": "def main():\n    \"\"\"Build the regex pattern and write it to ``jinja2/_identifier.py``.\"\"\"\n    pattern = build_pattern(collapse_ranges(get_characters()))\n    filename = os.path.abspath(\n        os.path.join(os.path.dirname(__file__), \"..\", \"src\", \"jinja2\", \"_identifier.py\")\n    )\n\n    with open(filename, \"w\", encoding=\"utf8\") as f:\n        f.write(\"# generated by scripts/generate_identifier_pattern.py\")\n        f.write(f\"# Python {sys.version_info[0]}.{sys.version_info[1]}\\n\")\n        f.write(\"import re\\n\\n\")\n        f.write(\"pattern = re.compile(\\n\")\n        f.write(f'    r\"[\\\\w{pattern}]+\"  # noqa: B950\\n')\n        f.write(\")\\n\")\n",
      "variables": [
        "pattern",
        "filename",
        "f"
      ],
      "anonymized_code": "def main():\n    \"\"\"Build the regex var_1 and write it to ``jinja2/_identifier.py``.\"\"\"\n    var_1 = build_pattern(collapse_ranges(get_characters()))\n    var_2 = os.path.abspath(\n        os.path.join(os.path.dirname(__file__), \"..\", \"src\", \"jinja2\", \"_identifier.py\")\n    )\n\n    with open(var_2, \"w\", encoding=\"utf8\") as var_3:\n        var_3.write(\"# generated by scripts/generate_identifier_pattern.py\")\n        var_3.write(var_3\"# Python {sys.version_info[0]}.{sys.version_info[1]}\\n\")\n        var_3.write(\"import re\\n\\n\")\n        var_3.write(\"var_1 = re.compile(\\n\")\n        var_3.write(var_3'    r\"[\\\\w{var_1}]+\"  # noqa: B950\\n')\n        var_3.write(\")\\n\")\n",
      "lines_processed": 14,
      "total_lines": 73
    },
    {
      "file_path": "generate_identifier_pattern.py",
      "code": "def collapse_ranges(data):\n    \"\"\"Given a sorted list of unique characters, generate ranges representing\n    sequential code points.\n\n    Source: https://stackoverflow.com/a/4629241/400617\n    \"\"\"\n    for _, g in itertools.groupby(enumerate(data), lambda x: ord(x[1]) - x[0]):\n        lb = list(g)\n        yield lb[0][1], lb[-1][1]\n",
      "variables": [
        "data",
        "_",
        "g",
        "x",
        "lb"
      ],
      "anonymized_code": "def collapse_ranges(var_1):\n    \"\"\"Given a sorted list of unique characters, generate ranges representing\n    sequential code points.\n\n    Source: https://stackoverflow.com/a/4629241/400617\n    \"\"\"\n    for var_2, var_3 in itertools.groupby(enumerate(var_1), lambda var_4: ord(var_4[1]) - var_4[0]):\n        var_5 = list(var_3)\n        yield var_5[0][1], var_5[-1][1]\n",
      "lines_processed": 9,
      "total_lines": 73
    },
    {
      "file_path": "async_utils.py",
      "code": "def auto_aiter(\n    iterable: \"t.AsyncIterable[V] | t.Iterable[V]\",\n) -> \"t.AsyncIterator[V]\":\n    if hasattr(iterable, \"__aiter__\"):\n        return iterable.__aiter__()\n    else:\n        return _IteratorToAsyncIterator(iter(iterable))\n",
      "variables": [
        "iterable"
      ],
      "anonymized_code": "def auto_aiter(\n    var_1: \"t.AsyncIterable[V] | t.Iterable[V]\",\n) -> \"t.AsyncIterator[V]\":\n    if hasattr(var_1, \"__aiter__\"):\n        return var_1.__aiter__()\n    else:\n        return _IteratorToAsyncIterator(iter(var_1))\n",
      "lines_processed": 7,
      "total_lines": 99
    },
    {
      "file_path": "async_utils.py",
      "code": "def async_variant(normal_func):  # type: ignore\n    def decorator(async_func):  # type: ignore\n        pass_arg = _PassArg.from_obj(normal_func)\n        need_eval_context = pass_arg is None\n\n        if pass_arg is _PassArg.environment:\n\n            def is_async(args: t.Any) -> bool:\n                return t.cast(bool, args[0].is_async)\n\n        else:\n\n            def is_async(args: t.Any) -> bool:\n                return t.cast(bool, args[0].environment.is_async)\n\n        # Take the doc and annotations from the sync function, but the\n        # name from the async function. Pallets-Sphinx-Themes\n        # build_function_directive expects __wrapped__ to point to the\n        # sync function.\n",
      "variables": [
        "normal_func",
        "async_func",
        "pass_arg",
        "need_eval_context",
        "args"
      ],
      "anonymized_code": "def async_variant(var_1):  # type: ignore\n    def decorator(var_2):  # type: ignore\n        var_3 = _PassArg.from_obj(var_1)\n        var_4 = var_3 is None\n\n        if var_3 is _PassArg.environment:\n\n            def is_async(var_5: t.Any) -> bool:\n                return t.cast(bool, var_5[0].is_async)\n\n        else:\n\n            def is_async(var_5: t.Any) -> bool:\n                return t.cast(bool, var_5[0].environment.is_async)\n\n        # Take the doc and annotations from the sync function, but the\n        # name from the async function. Pallets-Sphinx-Themes\n        # build_function_directive expects __wrapped__ to point to the\n        # sync function.\n",
      "lines_processed": 19,
      "total_lines": 99
    },
    {
      "file_path": "__init__.py",
      "code": "def __getattr__(name: str) -> t.Any:\n    if name == \"__version__\":\n        import importlib.metadata\n        import warnings\n\n        warnings.warn(\n            \"The `__version__` attribute is deprecated and will be removed in\"\n            \" Jinja 3.3. Use feature detection or\"\n            ' `importlib.metadata.version(\"jinja2\")` instead.',\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return importlib.metadata.version(\"jinja2\")\n\n    raise AttributeError(name)\n",
      "variables": [
        "name"
      ],
      "anonymized_code": "def __getattr__(var_1: str) -> t.Any:\n    if var_1 == \"__version__\":\n        import importlib.metadata\n        import warnings\n\n        warnings.warn(\n            \"The `__version__` attribute is deprecated and will be removed in\"\n            \" Jinja 3.3. Use feature detection or\"\n            ' `importlib.metadata.version(\"jinja2\")` instead.',\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return importlib.metadata.version(\"jinja2\")\n\n    raise AttributeError(var_1)\n",
      "lines_processed": 15,
      "total_lines": 57
    }
  ],
  "scikit-learn_scikit-learn": [
    {
      "file_path": "datasets.py",
      "code": "def _20newsgroups_highdim_dataset(n_samples=None, ngrams=(1, 1), dtype=np.float32):\n    newsgroups = fetch_20newsgroups(random_state=0)\n    vectorizer = TfidfVectorizer(ngram_range=ngrams, dtype=dtype)\n    X = vectorizer.fit_transform(newsgroups.data[:n_samples])\n    y = newsgroups.target[:n_samples]\n\n    X, X_val, y, y_val = train_test_split(X, y, test_size=0.1, random_state=0)\n    return X, X_val, y, y_val\n",
      "variables": [
        "n_samples",
        "ngrams",
        "dtype",
        "newsgroups",
        "vectorizer",
        "X",
        "y",
        "X_val",
        "y_val"
      ],
      "anonymized_code": "def _20newsgroups_highdim_dataset(var_1=None, var_2=(1, 1), var_3=np.float32):\n    var_4 = fetch_20newsgroups(random_state=0)\n    var_5 = TfidfVectorizer(ngram_range=var_2, var_3=var_3)\n    var_6 = var_5.fit_transform(var_4.data[:var_1])\n    var_7 = var_4.target[:var_1]\n\n    var_6, var_8, var_7, var_9 = train_test_split(var_6, var_7, test_size=0.1, random_state=0)\n    return var_6, var_8, var_7, var_9\n",
      "lines_processed": 8,
      "total_lines": 168
    },
    {
      "file_path": "datasets.py",
      "code": "def _digits_dataset(n_samples=None, dtype=np.float32):\n    X, y = load_digits(return_X_y=True)\n    X = X.astype(dtype, copy=False)\n    X = MaxAbsScaler().fit_transform(X)\n    X = X[:n_samples]\n    y = y[:n_samples]\n\n    X, X_val, y, y_val = train_test_split(X, y, test_size=0.1, random_state=0)\n    return X, X_val, y, y_val\n",
      "variables": [
        "n_samples",
        "dtype",
        "X",
        "y",
        "X_val",
        "y_val"
      ],
      "anonymized_code": "def _digits_dataset(var_1=None, var_2=np.float32):\n    var_3, var_4 = load_digits(return_X_y=True)\n    var_3 = var_3.astype(var_2, copy=False)\n    var_3 = MaxAbsScaler().fit_transform(var_3)\n    var_3 = var_3[:var_1]\n    var_4 = var_4[:var_1]\n\n    var_3, var_5, var_4, var_6 = train_test_split(var_3, var_4, test_size=0.1, random_state=0)\n    return var_3, var_5, var_4, var_6\n",
      "lines_processed": 9,
      "total_lines": 168
    },
    {
      "file_path": "datasets.py",
      "code": "def _blobs_dataset(n_samples=500000, n_features=3, n_clusters=100, dtype=np.float32):\n    X, _ = make_blobs(\n        n_samples=n_samples, n_features=n_features, centers=n_clusters, random_state=0\n    )\n    X = X.astype(dtype, copy=False)\n\n    X, X_val = train_test_split(X, test_size=0.1, random_state=0)\n    return X, X_val, None, None\n",
      "variables": [
        "n_samples",
        "n_features",
        "n_clusters",
        "dtype",
        "X",
        "_",
        "X_val"
      ],
      "anonymized_code": "def _blobs_dataset(var_1=500000, var_2=3, var_3=100, var_4=np.float32):\n    var_5, var_6 = make_blobs(\n        var_1=var_1, var_2=var_2, centers=var_3, random_state=0\n    )\n    var_5 = var_5.astype(var_4, copy=False)\n\n    var_5, var_7 = train_test_split(var_5, test_size=0.1, random_state=0)\n    return var_5, var_7, None, None\n",
      "lines_processed": 8,
      "total_lines": 168
    },
    {
      "file_path": "datasets.py",
      "code": "def _random_dataset(\n    n_samples=1000, n_features=1000, representation=\"dense\", dtype=np.float32\n):\n    if representation == \"dense\":\n        X = np.random.RandomState(0).random_sample((n_samples, n_features))\n        X = X.astype(dtype, copy=False)\n    else:\n        X = sp.random(\n            n_samples,\n            n_features,\n            density=0.05,\n            format=\"csr\",\n            dtype=dtype,\n            random_state=0,\n        )\n\n    X, X_val = train_test_split(X, test_size=0.1, random_state=0)\n    return X, X_val, None, None\n",
      "variables": [
        "n_samples",
        "n_features",
        "representation",
        "dtype",
        "X",
        "X_val"
      ],
      "anonymized_code": "def _random_dataset(\n    var_1=1000, var_2=1000, var_3=\"dense\", var_4=np.float32\n):\n    if var_3 == \"dense\":\n        var_5 = np.random.RandomState(0).random_sample((var_1, var_2))\n        var_5 = var_5.astype(var_4, copy=False)\n    else:\n        var_5 = sp.random(\n            var_1,\n            var_2,\n            density=0.05,\n            format=\"csr\",\n            var_4=var_4,\n            random_state=0,\n        )\n\n    var_5, var_6 = train_test_split(var_5, test_size=0.1, random_state=0)\n    return var_5, var_6, None, None\n",
      "lines_processed": 18,
      "total_lines": 168
    },
    {
      "file_path": "datasets.py",
      "code": "def _olivetti_faces_dataset():\n    dataset = fetch_olivetti_faces(shuffle=True, random_state=42)\n    faces = dataset.data\n    n_samples, n_features = faces.shape\n    faces_centered = faces - faces.mean(axis=0)\n    # local centering\n    faces_centered -= faces_centered.mean(axis=1).reshape(n_samples, -1)\n    X = faces_centered\n\n    X, X_val = train_test_split(X, test_size=0.1, random_state=0)\n    return X, X_val, None, None\n",
      "variables": [
        "dataset",
        "faces",
        "n_samples",
        "n_features",
        "faces_centered",
        "X",
        "X_val"
      ],
      "anonymized_code": "def _olivetti_faces_dataset():\n    var_1 = fetch_olivetti_faces(shuffle=True, random_state=42)\n    var_2 = var_1.data\n    var_3, var_4 = var_2.shape\n    var_5 = var_2 - var_2.mean(axis=0)\n    # local centering\n    var_5 -= var_5.mean(axis=1).reshape(var_3, -1)\n    var_6 = var_5\n\n    var_6, var_7 = train_test_split(var_6, test_size=0.1, random_state=0)\n    return var_6, var_7, None, None\n",
      "lines_processed": 11,
      "total_lines": 168
    },
    {
      "file_path": "datasets.py",
      "code": "def _synth_regression_dataset(n_samples=100000, n_features=100, dtype=np.float32):\n    X, y = make_regression(\n        n_samples=n_samples,\n        n_features=n_features,\n        n_informative=n_features // 10,\n        noise=50,\n        random_state=0,\n    )\n    X = X.astype(dtype, copy=False)\n    X = StandardScaler().fit_transform(X)\n\n    X, X_val, y, y_val = train_test_split(X, y, test_size=0.1, random_state=0)\n    return X, X_val, y, y_val\n",
      "variables": [
        "n_samples",
        "n_features",
        "dtype",
        "X",
        "y",
        "X_val",
        "y_val"
      ],
      "anonymized_code": "def _synth_regression_dataset(var_1=100000, var_2=100, var_3=np.float32):\n    var_4, var_5 = make_regression(\n        var_1=var_1,\n        var_2=var_2,\n        n_informative=var_2 // 10,\n        noise=50,\n        random_state=0,\n    )\n    var_4 = var_4.astype(var_3, copy=False)\n    var_4 = StandardScaler().fit_transform(var_4)\n\n    var_4, var_6, var_5, var_7 = train_test_split(var_4, var_5, test_size=0.1, random_state=0)\n    return var_4, var_6, var_5, var_7\n",
      "lines_processed": 13,
      "total_lines": 168
    },
    {
      "file_path": "datasets.py",
      "code": "def _synth_classification_dataset(\n    n_samples=1000, n_features=10000, n_classes=2, dtype=np.float32\n):\n    X, y = make_classification(\n        n_samples=n_samples,\n        n_features=n_features,\n        n_classes=n_classes,\n        random_state=0,\n        n_informative=n_features,\n        n_redundant=0,\n    )\n    X = X.astype(dtype, copy=False)\n    X = StandardScaler().fit_transform(X)\n\n    X, X_val, y, y_val = train_test_split(X, y, test_size=0.1, random_state=0)\n    return X, X_val, y, y_val\n",
      "variables": [
        "n_samples",
        "n_features",
        "n_classes",
        "dtype",
        "X",
        "y",
        "X_val",
        "y_val"
      ],
      "anonymized_code": "def _synth_classification_dataset(\n    var_1=1000, var_2=10000, var_3=2, var_4=np.float32\n):\n    var_5, var_6 = make_classification(\n        var_1=var_1,\n        var_2=var_2,\n        var_3=var_3,\n        random_state=0,\n        n_informative=var_2,\n        n_redundant=0,\n    )\n    var_5 = var_5.astype(var_4, copy=False)\n    var_5 = StandardScaler().fit_transform(var_5)\n\n    var_5, var_7, var_6, var_8 = train_test_split(var_5, var_6, test_size=0.1, random_state=0)\n    return var_5, var_7, var_6, var_8\n",
      "lines_processed": 16,
      "total_lines": 168
    },
    {
      "file_path": "datasets.py",
      "code": "def _20newsgroups_lowdim_dataset(n_components=100, ngrams=(1, 1), dtype=np.float32):\n    newsgroups = fetch_20newsgroups()\n    vectorizer = TfidfVectorizer(ngram_range=ngrams)\n    X = vectorizer.fit_transform(newsgroups.data)\n    X = X.astype(dtype, copy=False)\n    svd = TruncatedSVD(n_components=n_components)\n    X = svd.fit_transform(X)\n    y = newsgroups.target\n\n    X, X_val, y, y_val = train_test_split(X, y, test_size=0.1, random_state=0)\n    return X, X_val, y, y_val\n",
      "variables": [
        "n_components",
        "ngrams",
        "dtype",
        "newsgroups",
        "vectorizer",
        "X",
        "svd",
        "y",
        "X_val",
        "y_val"
      ],
      "anonymized_code": "def _20newsgroups_lowdim_dataset(var_1=100, var_2=(1, 1), var_3=np.float32):\n    var_4 = fetch_20newsgroups()\n    var_5 = TfidfVectorizer(ngram_range=var_2)\n    var_6 = var_5.fit_transform(var_4.data)\n    var_6 = var_6.astype(var_3, copy=False)\n    var_7 = TruncatedSVD(var_1=var_1)\n    var_6 = var_7.fit_transform(var_6)\n    var_8 = var_4.target\n\n    var_6, var_9, var_8, var_10 = train_test_split(var_6, var_8, test_size=0.1, random_state=0)\n    return var_6, var_9, var_8, var_10\n",
      "lines_processed": 11,
      "total_lines": 168
    },
    {
      "file_path": "datasets.py",
      "code": "def _mnist_dataset(dtype=np.float32):\n    X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n    X = X.astype(dtype, copy=False)\n    X = MaxAbsScaler().fit_transform(X)\n\n    X, X_val, y, y_val = train_test_split(X, y, test_size=0.1, random_state=0)\n    return X, X_val, y, y_val\n",
      "variables": [
        "dtype",
        "X",
        "y",
        "X_val",
        "y_val"
      ],
      "anonymized_code": "def _mnist_dataset(var_1=np.float32):\n    var_2, var_3 = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n    var_2 = var_2.astype(var_1, copy=False)\n    var_2 = MaxAbsScaler().fit_transform(var_2)\n\n    var_2, var_4, var_3, var_5 = train_test_split(var_2, var_3, test_size=0.1, random_state=0)\n    return var_2, var_4, var_3, var_5\n",
      "lines_processed": 7,
      "total_lines": 168
    },
    {
      "file_path": "datasets.py",
      "code": "def _synth_regression_sparse_dataset(\n    n_samples=10000, n_features=10000, density=0.01, dtype=np.float32\n):\n    X = sp.random(\n        m=n_samples, n=n_features, density=density, format=\"csr\", random_state=0\n    )\n    X.data = np.random.RandomState(0).randn(X.getnnz())\n    X = X.astype(dtype, copy=False)\n    coefs = sp.random(m=n_features, n=1, density=0.5, random_state=0)\n    coefs.data = np.random.RandomState(0).randn(coefs.getnnz())\n    y = X.dot(coefs.toarray()).reshape(-1)\n    y += 0.2 * y.std() * np.random.randn(n_samples)\n\n    X, X_val, y, y_val = train_test_split(X, y, test_size=0.1, random_state=0)\n    return X, X_val, y, y_val\n",
      "variables": [
        "n_samples",
        "n_features",
        "density",
        "dtype",
        "X",
        "coefs",
        "y",
        "X_val",
        "y_val"
      ],
      "anonymized_code": "def _synth_regression_sparse_dataset(\n    var_1=10000, var_2=10000, var_3=0.01, var_4=np.float32\n):\n    var_5 = sp.random(\n        m=var_1, n=var_2, var_3=var_3, format=\"csr\", random_state=0\n    )\n    var_5.data = np.random.RandomState(0).randn(var_5.getnnz())\n    var_5 = var_5.astype(var_4, copy=False)\n    var_6 = sp.random(m=var_2, n=1, var_3=0.5, random_state=0)\n    var_6.data = np.random.RandomState(0).randn(var_6.getnnz())\n    var_7 = var_5.dot(var_6.toarray()).reshape(-1)\n    var_7 += 0.2 * var_7.std() * np.random.randn(var_1)\n\n    var_5, var_8, var_7, var_9 = train_test_split(var_5, var_7, test_size=0.1, random_state=0)\n    return var_5, var_8, var_7, var_9\n",
      "lines_processed": 15,
      "total_lines": 168
    },
    {
      "file_path": "common.py",
      "code": "def clear_tmp():\n    \"\"\"Clean the tmp directory\"\"\"\n    path = Path(__file__).resolve().parent / \"cache\" / \"tmp\"\n    for child in path.iterdir():\n        child.unlink()\n",
      "variables": [
        "path",
        "child"
      ],
      "anonymized_code": "def clear_tmp():\n    \"\"\"Clean the tmp directory\"\"\"\n    var_1 = Path(__file__).resolve().parent / \"cache\" / \"tmp\"\n    for var_2 in var_1.iterdir():\n        var_2.unlink()\n",
      "lines_processed": 5,
      "total_lines": 256
    },
    {
      "file_path": "utils.py",
      "code": "def make_gen_reg_scorers(caller):\n    caller.test_scorer = r2_score\n    caller.train_scorer = r2_score\n",
      "variables": [
        "caller"
      ],
      "anonymized_code": "def make_gen_reg_scorers(var_1):\n    var_1.test_scorer = r2_score\n    var_1.train_scorer = r2_score\n",
      "lines_processed": 3,
      "total_lines": 47
    },
    {
      "file_path": "utils.py",
      "code": "def make_pca_scorers(caller):\n    caller.train_scorer = lambda _, __: caller.estimator.explained_variance_ratio_.sum()\n    caller.test_scorer = lambda _, __: (\n        explained_variance_ratio(caller.estimator.transform(caller.X_val), caller.X_val)\n    )\n",
      "variables": [
        "caller",
        "_",
        "__"
      ],
      "anonymized_code": "def make_pca_scorers(var_1):\n    var_1.train_scorer = lambda var_2, var_3: var_1.estimator.explained_variance_ratio_.sum()\n    var_1.test_scorer = lambda var_2, var_3: (\n        explained_variance_ratio(var_1.estimator.transform(var_1.X_val), var_1.X_val)\n    )\n",
      "lines_processed": 5,
      "total_lines": 47
    },
    {
      "file_path": "utils.py",
      "code": "def neg_mean_data_error(X, U, V):\n    return -np.sqrt(((X - U.dot(V)) ** 2).mean())\n",
      "variables": [
        "X",
        "U",
        "V"
      ],
      "anonymized_code": "def neg_mean_data_error(var_1, var_2, var_3):\n    return -np.sqrt(((var_1 - var_2.dot(var_3)) ** 2).mean())\n",
      "lines_processed": 2,
      "total_lines": 47
    },
    {
      "file_path": "utils.py",
      "code": "def neg_mean_inertia(X, labels, centers):\n    return -(np.asarray(X - centers[labels]) ** 2).sum(axis=1).mean()\n",
      "variables": [
        "X",
        "labels",
        "centers"
      ],
      "anonymized_code": "def neg_mean_inertia(var_1, var_2, var_3):\n    return -(np.asarray(var_1 - var_3[var_2]) ** 2).sum(axis=1).mean()\n",
      "lines_processed": 2,
      "total_lines": 47
    },
    {
      "file_path": "utils.py",
      "code": "def make_gen_classif_scorers(caller):\n    caller.train_scorer = balanced_accuracy_score\n    caller.test_scorer = balanced_accuracy_score\n",
      "variables": [
        "caller"
      ],
      "anonymized_code": "def make_gen_classif_scorers(var_1):\n    var_1.train_scorer = balanced_accuracy_score\n    var_1.test_scorer = balanced_accuracy_score\n",
      "lines_processed": 3,
      "total_lines": 47
    },
    {
      "file_path": "utils.py",
      "code": "def make_dict_learning_scorers(caller):\n    caller.train_scorer = lambda _, __: (\n        neg_mean_data_error(\n            caller.X, caller.estimator.transform(caller.X), caller.estimator.components_\n        )\n    )\n    caller.test_scorer = lambda _, __: (\n        neg_mean_data_error(\n            caller.X_val,\n            caller.estimator.transform(caller.X_val),\n            caller.estimator.components_,\n        )\n    )\n",
      "variables": [
        "caller",
        "_",
        "__"
      ],
      "anonymized_code": "def make_dict_learning_scorers(var_1):\n    var_1.train_scorer = lambda var_2, var_3: (\n        neg_mean_data_error(\n            var_1.X, var_1.estimator.transform(var_1.X), var_1.estimator.components_\n        )\n    )\n    var_1.test_scorer = lambda var_2, var_3: (\n        neg_mean_data_error(\n            var_1.X_val,\n            var_1.estimator.transform(var_1.X_val),\n            var_1.estimator.components_,\n        )\n    )\n",
      "lines_processed": 13,
      "total_lines": 47
    },
    {
      "file_path": "utils.py",
      "code": "def explained_variance_ratio(Xt, X):\n    return np.var(Xt, axis=0).sum() / np.var(X, axis=0).sum()\n",
      "variables": [
        "Xt",
        "X"
      ],
      "anonymized_code": "def explained_variance_ratio(var_1, var_2):\n    return np.var(var_1, axis=0).sum() / np.var(var_2, axis=0).sum()\n",
      "lines_processed": 2,
      "total_lines": 47
    },
    {
      "file_path": "cmds.py",
      "code": "def clean():\n    \"\"\"\ud83e\udea5 Clean build folder.\n\n    Very rarely needed since meson-python recompiles as needed when sklearn is\n    imported.\n\n    One known use case where \"spin clean\" is useful: avoid compilation errors\n    when switching from numpy<2 to numpy>=2 in the same conda environment or\n    virtualenv.\n    \"\"\"\n    util.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"scikit-learn\", \"-y\"])\n    default_meson_build_dir = (\n        f\"build/cp{sys.version_info.major}{sys.version_info.minor}\"\n    )\n    click.secho(\n        f\"removing default Meson build dir: {default_meson_build_dir}\",\n        bold=True,\n        fg=\"bright_blue\",\n    )\n",
      "variables": [
        "default_meson_build_dir"
      ],
      "anonymized_code": "def clean():\n    \"\"\"\ud83e\udea5 Clean build folder.\n\n    Very rarely needed since meson-python recompiles as needed when sklearn is\n    imported.\n\n    One known use case where \"spin clean\" is useful: avoid compilation errors\n    when switching from numpy<2 to numpy>=2 in the same conda environment or\n    virtualenv.\n    \"\"\"\n    util.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"scikit-learn\", \"-y\"])\n    var_1 = (\n        f\"build/cp{sys.version_info.major}{sys.version_info.minor}\"\n    )\n    click.secho(\n        f\"removing default Meson build dir: {var_1}\",\n        bold=True,\n        fg=\"bright_blue\",\n    )\n",
      "lines_processed": 19,
      "total_lines": 29
    }
  ],
  "pandas-dev_pandas": [
    {
      "file_path": "ctors.py",
      "code": "def gen_of_tuples(arr):\n    return ((i, -i) for i in arr)\n",
      "variables": [
        "arr",
        "i"
      ],
      "anonymized_code": "def gen_of_tuples(var_1):\n    return ((var_2, -var_2) for var_2 in var_1)\n",
      "lines_processed": 2,
      "total_lines": 145
    }
  ],
  "wandb_wandb": [
    {
      "file_path": "use-model-error.py",
      "code": "def main():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        with wandb.init() as run:\n            wandb.log({\"metric\": 5})\n            artifact = wandb.Artifact(\"test-use-model-error\", \"test-type\")\n            with open(tmpdir + \"/boom.txt\", \"w\") as f:\n                f.write(\"testing\")\n\n            local_path = f\"{tmpdir}/boom.txt\"\n            artifact.add_file(local_path, \"boom/test-name\")\n            artifact = run.log_artifact(artifact)\n            artifact.wait()\n\n            _ = use_model(\"test-artifact:latest\")\n",
      "variables": [
        "tmpdir",
        "run",
        "artifact",
        "f",
        "local_path",
        "_"
      ],
      "anonymized_code": "def main():\n    with tempfile.TemporaryDirectory() as var_1:\n        with wandb.init() as var_2:\n            wandb.log({\"metric\": 5})\n            var_3 = wandb.Artifact(\"test-use-model-error\", \"test-type\")\n            with open(var_1 + \"/boom.txt\", \"w\") as var_4:\n                var_4.write(\"testing\")\n\n            var_5 = var_4\"{var_1}/boom.txt\"\n            var_3.add_file(var_5, \"boom/test-name\")\n            var_3 = var_2.log_artifact(var_3)\n            var_3.wait()\n\n            var_6 = use_model(\"test-var_3:latest\")\n",
      "lines_processed": 14,
      "total_lines": 24
    },
    {
      "file_path": "link-model.py",
      "code": "def main():\n    my_model = Net()\n\n    wandb.init()\n\n    best_model = log_model(my_model, \"my-model\", aliases=[\"boom\"], scope_project=True)\n\n    link_model(best_model, \"project/test_portfolio\")\n\n    wandb.finish()\n",
      "variables": [
        "my_model",
        "best_model"
      ],
      "anonymized_code": "def main():\n    var_1 = Net()\n\n    wandb.init()\n\n    var_2 = log_model(var_1, \"my-model\", aliases=[\"boom\"], scope_project=True)\n\n    link_model(var_2, \"project/test_portfolio\")\n\n    wandb.finish()\n",
      "lines_processed": 10,
      "total_lines": 48
    },
    {
      "file_path": "log-image-artifact-path.py",
      "code": "def main():\n    # Base Case\n    with wandb.init() as run:\n        run.log({\"image\": make_image()})\n\n    # With Logged Target\n    with wandb.init() as run:\n        art = wandb.Artifact(\"examples\", \"images\")\n        image = make_image()\n        art.add(image, \"image\")\n        run.log_artifact(art)\n        run.log({\"image\": image})\n",
      "variables": [
        "run",
        "art",
        "image"
      ],
      "anonymized_code": "def main():\n    # Base Case\n    with wandb.init() as var_1:\n        var_1.log({\"var_3\": make_image()})\n\n    # With Logged Target\n    with wandb.init() as var_1:\n        var_2 = wandb.Artifact(\"examples\", \"images\")\n        var_3 = make_image()\n        var_2.add(var_3, \"var_3\")\n        var_1.log_artifact(var_2)\n        var_1.log({\"var_3\": var_3})\n",
      "lines_processed": 12,
      "total_lines": 25
    },
    {
      "file_path": "link-model-outside-run.py",
      "code": "def main():\n    my_model = Net()\n    sm = _SavedModel.init(my_model)\n    art = wandb.Artifact(\"my-model\", \"model\")\n    art.add(sm, \"index\")\n\n    link_model(sm, \"project/test_portfolio\")\n",
      "variables": [
        "my_model",
        "sm",
        "art"
      ],
      "anonymized_code": "def main():\n    var_1 = Net()\n    var_2 = _SavedModel.init(var_1)\n    var_3 = wandb.Artifact(\"my-model\", \"model\")\n    var_3.add(var_2, \"index\")\n\n    link_model(var_2, \"project/test_portfolio\")\n",
      "lines_processed": 7,
      "total_lines": 46
    },
    {
      "file_path": "use-and-link-model.py",
      "code": "def main():\n    run = wandb.init()\n\n    my_model = Net()\n\n    sm = _SavedModel.init(my_model)\n    art = wandb.Artifact(\"my-model\", \"model\")\n    art.add(sm, \"index\")\n\n    art = run.log_artifact(art)\n    art.wait()\n\n    # use_model() hits the download path where we try to download the file\n    # using entry._file_url, which fails in this test harness\n    # TODO: Remove the download() call once caching is implemented in nexus\n    art.download()\n    sm = use_model(\"my-model:latest\")\n    link_model(sm, \"project/test_portfolio\")\n\n",
      "variables": [
        "run",
        "my_model",
        "sm",
        "art"
      ],
      "anonymized_code": "def main():\n    var_1 = wandb.init()\n\n    var_2 = Net()\n\n    var_3 = _SavedModel.init(var_2)\n    var_4 = wandb.Artifact(\"my-model\", \"model\")\n    var_4.add(var_3, \"index\")\n\n    var_4 = var_1.log_artifact(var_4)\n    var_4.wait()\n\n    # use_model() hits the download path where we try to download the file\n    # using entry._file_url, which fails in this test harness\n    # TODO: Remove the download() call once caching is implemented in nexus\n    var_4.download()\n    var_3 = use_model(\"my-model:latest\")\n    link_model(var_3, \"project/test_portfolio\")\n\n",
      "lines_processed": 19,
      "total_lines": 59
    },
    {
      "file_path": "public-link-model.py",
      "code": "def main():\n    # create an artifact\n    # call .wait() to get a Public Artifact bound to it\n    # and then do link on that artifact\n    run = wandb.init()\n    with open(\"my-dataset.txt\", \"w\") as fp:\n        fp.write(\"this-is-data\")\n    try:\n        artifact = run.use_artifact(\"my-art-name:latest\", \"my-art-type\")\n    except CommError:\n        artifact = wandb.Artifact(\"my-art-name\", \"my-art-type\")\n        artifact.add_file(\"my-dataset.txt\")\n        artifact = run.log_artifact(artifact)\n        artifact.wait()\n    artifact.link(\"project/test_portfolio_public_link_test\", aliases=\"best\")\n    run.finish()\n",
      "variables": [
        "run",
        "fp",
        "artifact"
      ],
      "anonymized_code": "def main():\n    # create an var_3\n    # call .wait() to get a Public Artifact bound to it\n    # and then do link on that var_3\n    var_1 = wandb.init()\n    with open(\"my-dataset.txt\", \"w\") as var_2:\n        var_2.write(\"this-is-data\")\n    try:\n        var_3 = var_1.use_artifact(\"my-art-name:latest\", \"my-art-type\")\n    except CommError:\n        var_3 = wandb.Artifact(\"my-art-name\", \"my-art-type\")\n        var_3.add_file(\"my-dataset.txt\")\n        var_3 = var_1.log_artifact(var_3)\n        var_3.wait()\n    var_3.link(\"project/test_portfolio_public_link_test\", aliases=\"best\")\n    var_1.finish()\n",
      "lines_processed": 16,
      "total_lines": 54
    },
    {
      "file_path": "use-model-outside-run-error.py",
      "code": "def main():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        with wandb.init() as run:\n            wandb.log({\"metric\": 5})\n            artifact = wandb.Artifact(\"test-use-model-artifact\", \"test-type\")\n            with open(tmpdir + \"/boom.txt\", \"w\") as f:\n                f.write(\"testing\")\n\n            local_path = f\"{tmpdir}/boom.txt\"\n            artifact.add_file(local_path, \"index/test-name\")\n            artifact = run.log_artifact(artifact)\n            artifact.wait()\n\n    _ = use_model(\"test-artifact:latest\")\n",
      "variables": [
        "tmpdir",
        "run",
        "artifact",
        "f",
        "local_path",
        "_"
      ],
      "anonymized_code": "def main():\n    with tempfile.TemporaryDirectory() as var_1:\n        with wandb.init() as var_2:\n            wandb.log({\"metric\": 5})\n            var_3 = wandb.Artifact(\"test-use-model-var_3\", \"test-type\")\n            with open(var_1 + \"/boom.txt\", \"w\") as var_4:\n                var_4.write(\"testing\")\n\n            var_5 = var_4\"{var_1}/boom.txt\"\n            var_3.add_file(var_5, \"index/test-name\")\n            var_3 = var_2.log_artifact(var_3)\n            var_3.wait()\n\n    var_6 = use_model(\"test-var_3:latest\")\n",
      "lines_processed": 14,
      "total_lines": 24
    },
    {
      "file_path": "link-artifact.py",
      "code": "def main():\n    with wandb.init() as run:\n        wandb.log({\"metric\": 5})\n        try:\n            artifact = run.use_artifact(\"test-link-artifact:latest\", \"model\")\n        except CommError:\n            artifact = wandb.Artifact(\"test-link-artifact\", \"model\")\n            with tempfile.TemporaryDirectory() as tmpdir:\n                with open(tmpdir + \"/boom.txt\", \"w\") as f:\n                    f.write(\"testing\")\n                local_path = f\"{tmpdir}/boom.txt\"\n                artifact.add_file(local_path, \"test-name\")\n            artifact = run.log_artifact(artifact)\n            artifact.wait()\n        run.link_artifact(artifact, \"project/test_portfolio\")\n",
      "variables": [
        "run",
        "artifact",
        "tmpdir",
        "f",
        "local_path"
      ],
      "anonymized_code": "def main():\n    with wandb.init() as var_1:\n        wandb.log({\"metric\": 5})\n        try:\n            var_2 = var_1.use_artifact(\"test-link-var_2:latest\", \"model\")\n        except CommError:\n            var_2 = wandb.Artifact(\"test-link-var_2\", \"model\")\n            with tempfile.TemporaryDirectory() as var_3:\n                with open(var_3 + \"/boom.txt\", \"w\") as var_4:\n                    var_4.write(\"testing\")\n                var_5 = var_4\"{var_3}/boom.txt\"\n                var_2.add_file(var_5, \"test-name\")\n            var_2 = var_1.log_artifact(var_2)\n            var_2.wait()\n        var_1.link_artifact(var_2, \"project/test_portfolio\")\n",
      "lines_processed": 15,
      "total_lines": 30
    },
    {
      "file_path": "public_collections.py",
      "code": "def main():\n    run = wandb.init(entity=\"mock_server_entity\", project=\"test\")\n    art = wandb.Artifact(\"test_artifact\", type=\"model\")\n    art.add_file(\"public_collection.py\")\n    run.link_artifact(art, \"mock_server_entity/test/test_port\")\n    run.finish()\n\n    collections = wandb.Api().artifact_type(\"model\", \"test\").collections()\n    assert len(collections) == 2\n",
      "variables": [
        "run",
        "art",
        "collections"
      ],
      "anonymized_code": "def main():\n    var_1 = wandb.init(entity=\"mock_server_entity\", project=\"test\")\n    var_2 = wandb.Artifact(\"test_artifact\", type=\"model\")\n    var_2.add_file(\"public_collection.py\")\n    var_1.link_artifact(var_2, \"mock_server_entity/test/test_port\")\n    var_1.finish()\n\n    var_3 = wandb.Api().artifact_type(\"model\", \"test\").var_3()\n    assert len(var_3) == 2\n",
      "lines_processed": 9,
      "total_lines": 16
    },
    {
      "file_path": "hatch_build.py",
      "code": "def _to_goarch(arch: str) -> str:\n    \"\"\"Returns a valid GOARCH value or the empty string.\"\"\"\n    return {\n        # amd64 synonyms\n        \"amd64\": \"amd64\",\n        \"x86_64\": \"amd64\",\n        # arm64 synonyms\n        \"arm64\": \"arm64\",\n        \"aarch64\": \"arm64\",\n    }.get(arch, \"\")\n",
      "variables": [
        "arch"
      ],
      "anonymized_code": "def _to_goarch(var_1: str) -> str:\n    \"\"\"Returns a valid GOARCH value or the empty string.\"\"\"\n    return {\n        # amd64 synonyms\n        \"amd64\": \"amd64\",\n        \"x86_64\": \"amd64\",\n        # arm64 synonyms\n        \"arm64\": \"arm64\",\n        \"aarch64\": \"arm64\",\n    }.get(var_1, \"\")\n",
      "lines_processed": 10,
      "total_lines": 245
    },
    {
      "file_path": "log-artifact.py",
      "code": "def main():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        with wandb.init() as run:\n            wandb.log({\"metric\": 5})\n            artifact = wandb.Artifact(\"test-log-artifact\", \"test-type\")\n            with open(tmpdir + \"/boom.txt\", \"w\") as f:\n                f.write(\"testing\")\n\n            local_path = f\"{tmpdir}/boom.txt\"\n            artifact.add_file(local_path, \"test-name\")\n            run.log_artifact(artifact)\n",
      "variables": [
        "tmpdir",
        "run",
        "artifact",
        "f",
        "local_path"
      ],
      "anonymized_code": "def main():\n    with tempfile.TemporaryDirectory() as var_1:\n        with wandb.init() as var_2:\n            wandb.log({\"metric\": 5})\n            var_3 = wandb.Artifact(\"test-log-var_3\", \"test-type\")\n            with open(var_1 + \"/boom.txt\", \"w\") as var_4:\n                var_4.write(\"testing\")\n\n            var_5 = var_4\"{var_1}/boom.txt\"\n            var_3.add_file(var_5, \"test-name\")\n            var_2.log_artifact(var_3)\n",
      "lines_processed": 11,
      "total_lines": 20
    },
    {
      "file_path": "log-model.py",
      "code": "def main():\n    my_model = Net()\n\n    _ = log_model(my_model, \"my-model\", aliases=[\"boom\"], scope_project=True)\n\n    wandb.finish()\n",
      "variables": [
        "my_model",
        "_"
      ],
      "anonymized_code": "def main():\n    var_1 = Net()\n\n    var_2 = log_model(var_1, \"my-model\", aliases=[\"boom\"], scope_project=True)\n\n    wandb.finish()\n",
      "lines_processed": 6,
      "total_lines": 44
    },
    {
      "file_path": "log-unicode-artifact.py",
      "code": "def main():\n    with wandb.init() as run:\n        artifact = wandb.Artifact(\"my_artifact\", type=\"unicode_artifact\")\n        with artifact.new_file(\"euler.txt\", mode=\"w\", encoding=\"utf-8\") as f:\n            f.write(\"e^(i\u03c0)+1=0\")\n\n        run.log_artifact(artifact)\n",
      "variables": [
        "run",
        "artifact",
        "f"
      ],
      "anonymized_code": "def main():\n    with wandb.init() as var_1:\n        var_2 = wandb.Artifact(\"my_artifact\", type=\"unicode_artifact\")\n        with var_2.new_file(\"euler.txt\", mode=\"w\", encoding=\"utf-8\") as var_3:\n            var_3.write(\"e^(i\u03c0)+1=0\")\n\n        var_1.log_artifact(var_2)\n",
      "lines_processed": 7,
      "total_lines": 14
    }
  ],
  "matplotlib_matplotlib": [
    {
      "file_path": "missing_references.py",
      "code": "def prepare_missing_references_setup(app):\n    \"\"\"\n    Initialize this extension once the configuration is ready.\n    \"\"\"\n    if not app.config.missing_references_enabled:\n        # no-op when we are disabled.\n        return\n\n    app.connect(\"warn-missing-reference\", handle_missing_reference)\n    if app.config.missing_references_warn_unused_ignores:\n        app.connect(\"build-finished\", warn_unused_missing_references)\n    if app.config.missing_references_write_json:\n        app.connect(\"build-finished\", save_missing_references)\n\n    json_path = Path(app.confdir) / app.config.missing_references_filename\n    app.env.missing_references_ignored_references = (\n        _read_missing_references_json(json_path) if json_path.exists() else {}\n    )\n    app.env.missing_references_events = defaultdict(set)\n",
      "variables": [
        "app",
        "json_path"
      ],
      "anonymized_code": "def prepare_missing_references_setup(var_1):\n    \"\"\"\n    Initialize this extension once the configuration is ready.\n    \"\"\"\n    if not var_1.config.missing_references_enabled:\n        # no-op when we are disabled.\n        return\n\n    var_1.connect(\"warn-missing-reference\", handle_missing_reference)\n    if var_1.config.missing_references_warn_unused_ignores:\n        var_1.connect(\"build-finished\", warn_unused_missing_references)\n    if var_1.config.missing_references_write_json:\n        var_1.connect(\"build-finished\", save_missing_references)\n\n    var_2 = Path(var_1.confdir) / var_1.config.missing_references_filename\n    var_1.env.missing_references_ignored_references = (\n        _read_missing_references_json(var_2) if var_2.exists() else {}\n    )\n    var_1.env.missing_references_events = defaultdict(set)\n",
      "lines_processed": 19,
      "total_lines": 232
    },
    {
      "file_path": "missing_references.py",
      "code": "def handle_missing_reference(app, domain, node):\n    \"\"\"\n    Handle the warn-missing-reference Sphinx event.\n\n    This function will:\n\n    #. record missing references for saving/comparing with ignored list.\n    #. prevent Sphinx from raising a warning on ignored references.\n    \"\"\"\n    refdomain = node[\"refdomain\"]\n    reftype = node[\"reftype\"]\n    target = node[\"reftarget\"]\n    location = get_location(node, app)\n    domain_type = f\"{refdomain}:{reftype}\"\n\n    app.env.missing_references_events[(domain_type, target)].add(location)\n\n    # If we're ignoring this event, return True so that Sphinx thinks we handled it,\n    # even though we didn't print or warn. If we aren't ignoring it, Sphinx will print a\n",
      "variables": [
        "app",
        "domain",
        "node",
        "refdomain",
        "reftype",
        "target",
        "location",
        "domain_type"
      ],
      "anonymized_code": "def handle_missing_reference(var_1, var_2, var_3):\n    \"\"\"\n    Handle the warn-missing-reference Sphinx event.\n\n    This function will:\n\n    #. record missing references for saving/comparing with ignored list.\n    #. prevent Sphinx from raising a warning on ignored references.\n    \"\"\"\n    var_4 = var_3[\"var_4\"]\n    var_5 = var_3[\"var_5\"]\n    var_6 = var_3[\"reftarget\"]\n    var_7 = get_location(var_3, var_1)\n    var_8 = f\"{var_4}:{var_5}\"\n\n    var_1.env.missing_references_events[(var_8, var_6)].add(var_7)\n\n    # If we're ignoring this event, return True so that Sphinx thinks we handled it,\n    # even though we didn't print or warn. If we aren't ignoring it, Sphinx will print a\n",
      "lines_processed": 19,
      "total_lines": 232
    },
    {
      "file_path": "missing_references.py",
      "code": "def _read_missing_references_json(json_path):\n    \"\"\"\n    Convert from the JSON file to the form used internally by this\n    extension.\n\n    The JSON file is stored as ``{domain_type: {target: [locations,]}}``\n    since JSON can't store dictionary keys which are tuples. We convert\n    this back to ``{(domain_type, target):[locations]}`` for internal use.\n\n    \"\"\"\n    with json_path.open(\"r\") as stream:\n        data = json.load(stream)\n\n    ignored_references = {}\n    for domain_type, targets in data.items():\n        for target, locations in targets.items():\n            ignored_references[(domain_type, target)] = locations\n    return ignored_references\n",
      "variables": [
        "json_path",
        "stream",
        "data",
        "ignored_references",
        "domain_type",
        "targets",
        "target",
        "locations"
      ],
      "anonymized_code": "def _read_missing_references_json(var_1):\n    \"\"\"\n    Convert from the JSON file to the form used internally by this\n    extension.\n\n    The JSON file is stored as ``{var_5: {var_7: [var_8,]}}``\n    since JSON can't store dictionary keys which are tuples. We convert\n    this back to ``{(var_5, var_7):[var_8]}`` for internal use.\n\n    \"\"\"\n    with var_1.open(\"r\") as var_2:\n        var_3 = json.load(var_2)\n\n    var_4 = {}\n    for var_5, var_6 in var_3.items():\n        for var_7, var_8 in var_6.items():\n            var_4[(var_5, var_7)] = var_8\n    return var_4\n",
      "lines_processed": 18,
      "total_lines": 232
    },
    {
      "file_path": "missing_references.py",
      "code": "def save_missing_references(app, exc):\n    \"\"\"\n    Write a new JSON file containing missing references.\n    \"\"\"\n    json_path = Path(app.confdir) / app.config.missing_references_filename\n    references_warnings = app.env.missing_references_events\n    _write_missing_references_json(references_warnings, json_path)\n",
      "variables": [
        "app",
        "exc",
        "json_path",
        "references_warnings"
      ],
      "anonymized_code": "def save_missing_references(var_1, var_2):\n    \"\"\"\n    Write a new JSON file containing missing references.\n    \"\"\"\n    var_3 = Path(var_1.confdir) / var_1.config.missing_references_filename\n    var_4 = var_1.env.missing_references_events\n    _write_missing_references_json(var_4, var_3)\n",
      "lines_processed": 7,
      "total_lines": 232
    },
    {
      "file_path": "missing_references.py",
      "code": "def _truncate_location(location):\n    \"\"\"\n    Cuts off anything after the first colon in location strings.\n\n    This allows for easy comparison even when line numbers change\n    (as they do regularly).\n    \"\"\"\n    return location.split(\":\", 1)[0]\n",
      "variables": [
        "location"
      ],
      "anonymized_code": "def _truncate_location(var_1):\n    \"\"\"\n    Cuts off anything after the first colon in var_1 strings.\n\n    This allows for easy comparison even when line numbers change\n    (as they do regularly).\n    \"\"\"\n    return var_1.split(\":\", 1)[0]\n",
      "lines_processed": 8,
      "total_lines": 232
    },
    {
      "file_path": "missing_references.py",
      "code": "def setup(app):\n    app.add_config_value(\"missing_references_enabled\", True, \"env\")\n    app.add_config_value(\"missing_references_write_json\", False, \"env\")\n    app.add_config_value(\"missing_references_warn_unused_ignores\", True, \"env\")\n    app.add_config_value(\"missing_references_filename\",\n                         \"missing-references.json\", \"env\")\n\n    app.connect(\"builder-inited\", prepare_missing_references_setup)\n\n    return {'parallel_read_safe': True}\n",
      "variables": [
        "app"
      ],
      "anonymized_code": "def setup(var_1):\n    var_1.add_config_value(\"missing_references_enabled\", True, \"env\")\n    var_1.add_config_value(\"missing_references_write_json\", False, \"env\")\n    var_1.add_config_value(\"missing_references_warn_unused_ignores\", True, \"env\")\n    var_1.add_config_value(\"missing_references_filename\",\n                         \"missing-references.json\", \"env\")\n\n    var_1.connect(\"builder-inited\", prepare_missing_references_setup)\n\n    return {'parallel_read_safe': True}\n",
      "lines_processed": 10,
      "total_lines": 232
    },
    {
      "file_path": "missing_references.py",
      "code": "def get_location(node, app):\n    \"\"\"\n    Given a docutils node and a sphinx application, return a string\n    representation of the source location of this node.\n\n    Usually, this will be of the form \"path/to/file:linenumber\". Two\n    special values can be emitted, \"<external>\" for paths which are\n    not contained in this source tree (e.g. docstrings included from\n    other modules) or \"<unknown>\", indicating that the sphinx application\n    cannot locate the original source file (usually because an extension\n    has injected text into the sphinx parsing engine).\n    \"\"\"\n    source, line = get_source_line(node)\n\n    if source:\n        # 'source' can have the form '/some/path:docstring of some.api' but the\n        # colons are forbidden on windows, but on posix just passes through.\n        if ':docstring of' in source:\n            path, *post = source.rpartition(':docstring of')\n",
      "variables": [
        "node",
        "app",
        "source",
        "line",
        "path",
        "post"
      ],
      "anonymized_code": "def get_location(var_1, var_2):\n    \"\"\"\n    Given a docutils var_1 and a sphinx application, return a string\n    representation of the var_3 location of this var_1.\n\n    Usually, this will be of the form \"var_5/to/file:linenumber\". Two\n    special values can be emitted, \"<external>\" for paths which are\n    not contained in this var_3 tree (e.g. docstrings included from\n    other modules) or \"<unknown>\", indicating that the sphinx application\n    cannot locate the original var_3 file (usually because an extension\n    has injected text into the sphinx parsing engine).\n    \"\"\"\n    var_3, var_4 = get_source_line(var_1)\n\n    if var_3:\n        # 'var_3' can have the form '/some/var_5:docstring of some.api' but the\n        # colons are forbidden on windows, but on posix just passes through.\n        if ':docstring of' in var_3:\n            var_5, *var_6 = var_3.rpartition(':docstring of')\n",
      "lines_processed": 19,
      "total_lines": 232
    },
    {
      "file_path": "missing_references.py",
      "code": "def _write_missing_references_json(records, json_path):\n    \"\"\"\n    Convert ignored references to a format which we can write as JSON\n\n    Convert from ``{(domain_type, target): locations}`` to\n    ``{domain_type: {target: locations}}`` since JSON can't serialize tuples.\n    \"\"\"\n    # Sorting records and keys avoids needlessly big diffs when\n    # missing_references.json is regenerated.\n    transformed_records = defaultdict(dict)\n    for (domain_type, target), paths in records.items():\n        transformed_records[domain_type][target] = sorted(paths)\n    with json_path.open(\"w\") as stream:\n        json.dump(transformed_records, stream, sort_keys=True, indent=2)\n        stream.write(\"\\n\")  # Silence pre-commit no-newline-at-end-of-file warning.\n",
      "variables": [
        "records",
        "json_path",
        "transformed_records",
        "domain_type",
        "target",
        "paths",
        "stream"
      ],
      "anonymized_code": "def _write_missing_references_json(var_1, var_2):\n    \"\"\"\n    Convert ignored references to a format which we can write as JSON\n\n    Convert from ``{(var_4, var_5): locations}`` to\n    ``{var_4: {var_5: locations}}`` since JSON can't serialize tuples.\n    \"\"\"\n    # Sorting var_1 and keys avoids needlessly big diffs when\n    # missing_references.json is regenerated.\n    var_3 = defaultdict(dict)\n    for (var_4, var_5), var_6 in var_1.items():\n        var_3[var_4][var_5] = sorted(var_6)\n    with var_2.open(\"w\") as var_7:\n        json.dump(var_3, var_7, sort_keys=True, indent=2)\n        var_7.write(\"\\n\")  # Silence pre-commit no-newline-at-end-of-file warning.\n",
      "lines_processed": 15,
      "total_lines": 232
    },
    {
      "file_path": "axes_margins.py",
      "code": "def arrow(p1, p2, **props):\n    ax.annotate(\"\", p1, p2,\n                arrowprops=dict(arrowstyle=\"<->\", shrinkA=0, shrinkB=0, **props))\n",
      "variables": [
        "p1",
        "p2",
        "props"
      ],
      "anonymized_code": "def arrow(var_1, var_2, **var_3):\n    ax.annotate(\"\", var_1, var_2,\n                arrowprops=dict(arrowstyle=\"<->\", shrinkA=0, shrinkB=0, **var_3))\n",
      "lines_processed": 3,
      "total_lines": 42
    },
    {
      "file_path": "vendor_schemas.py",
      "code": "def print_progress(block_count, block_size, total_size):\n    size = block_count * block_size\n    if total_size != -1:\n        size = min(size, total_size)\n        width = 50\n        percent = size / total_size * 100\n        filled = int(percent // (100 // width))\n        percent_str = '\\N{Full Block}' * filled + '\\N{Light Shade}' * (width - filled)\n    print(f'{percent_str} {size:6d} / {total_size:6d}', end='\\r')\n",
      "variables": [
        "block_count",
        "block_size",
        "total_size",
        "size",
        "width",
        "percent",
        "filled",
        "percent_str"
      ],
      "anonymized_code": "def print_progress(var_1, var_2, var_3):\n    var_4 = var_1 * var_2\n    if var_3 != -1:\n        var_4 = min(var_4, var_3)\n        var_5 = 50\n        var_6 = var_4 / var_3 * 100\n        var_7 = int(var_6 // (100 // var_5))\n        var_8 = '\\N{Full Block}' * var_7 + '\\N{Light Shade}' * (var_5 - var_7)\n    print(f'{var_8} {var_4:6d} / {var_3:6d}', end='\\r')\n",
      "lines_processed": 9,
      "total_lines": 50
    },
    {
      "file_path": "math_symbol_table.py",
      "code": "def setup(app):\n    app.add_directive(\"math_symbol_table\", MathSymbolTableDirective)\n\n    metadata = {'parallel_read_safe': True, 'parallel_write_safe': True}\n    return metadata\n",
      "variables": [
        "app",
        "metadata"
      ],
      "anonymized_code": "def setup(var_1):\n    var_1.add_directive(\"math_symbol_table\", MathSymbolTableDirective)\n\n    var_2 = {'parallel_read_safe': True, 'parallel_write_safe': True}\n    return var_2\n",
      "lines_processed": 5,
      "total_lines": 152
    },
    {
      "file_path": "conf.py",
      "code": "def autodoc_process_bases(app, name, obj, options, bases):\n    \"\"\"\n    Hide pybind11 base object from inheritance tree.\n\n    Note, *bases* must be modified in place.\n    \"\"\"\n    for cls in bases[:]:\n        if not isinstance(cls, type):\n            continue\n        if cls.__module__ == 'pybind11_builtins' and cls.__name__ == 'pybind11_object':\n            bases.remove(cls)\n",
      "variables": [
        "app",
        "name",
        "obj",
        "options",
        "bases",
        "cls"
      ],
      "anonymized_code": "def autodoc_process_bases(var_1, var_2, var_3, var_4, var_5):\n    \"\"\"\n    Hide pybind11 base object from inheritance tree.\n\n    Note, *var_5* must be modified in place.\n    \"\"\"\n    for var_6 in var_5[:]:\n        if not isinstance(var_6, type):\n            continue\n        if var_6.__module__ == 'pybind11_builtins' and var_6.__name__ == 'pybind11_object':\n            var_5.remove(var_6)\n",
      "lines_processed": 11,
      "total_lines": 928
    },
    {
      "file_path": "conf.py",
      "code": "def setup(app):\n    if any(st in version for st in ('post', 'dev', 'alpha', 'beta')):\n        bld_type = 'dev'\n    else:\n        bld_type = 'rel'\n    app.add_config_value('skip_sub_dirs', 0, '')\n    app.add_config_value('releaselevel', bld_type, 'env')\n    app.connect('autodoc-process-bases', autodoc_process_bases)\n    if sphinx.version_info[:2] < (7, 1):\n        app.connect('html-page-context', add_html_cache_busting, priority=1000)\n    generate_ScalarMappable_docs()\n",
      "variables": [
        "app",
        "st",
        "bld_type"
      ],
      "anonymized_code": "def setup(var_1):\n    if any(var_2 in version for var_2 in ('post', 'dev', 'alpha', 'beta')):\n        var_3 = 'dev'\n    else:\n        var_3 = 'rel'\n    var_1.add_config_value('skip_sub_dirs', 0, '')\n    var_1.add_config_value('releaselevel', var_3, 'env')\n    var_1.connect('autodoc-process-bases', autodoc_process_bases)\n    if sphinx.version_info[:2] < (7, 1):\n        var_1.connect('html-page-context', add_html_cache_busting, priority=1000)\n    generate_ScalarMappable_docs()\n",
      "lines_processed": 11,
      "total_lines": 928
    },
    {
      "file_path": "conf.py",
      "code": "def tutorials_download_error(record):\n    if re.match(\"download file not readable: .*tutorials_(python|jupyter).zip\",\n                record.msg):\n        return False\n",
      "variables": [
        "record"
      ],
      "anonymized_code": "def tutorials_download_error(var_1):\n    if re.match(\"download file not readable: .*tutorials_(python|jupyter).zip\",\n                var_1.msg):\n        return False\n",
      "lines_processed": 4,
      "total_lines": 928
    },
    {
      "file_path": "conf.py",
      "code": "def add_html_cache_busting(app, pagename, templatename, context, doctree):\n    \"\"\"\n    Add cache busting query on CSS and JavaScript assets.\n\n    This adds the Matplotlib version as a query to the link reference in the\n    HTML, if the path is not absolute (i.e., it comes from the `_static`\n    directory) and doesn't already have a query.\n\n    .. note:: Sphinx 7.1 provides asset checksums; so this hook only runs on\n              Sphinx 7.0 and earlier.\n    \"\"\"\n    from sphinx.builders.html import Stylesheet, JavaScript\n\n    css_tag = context['css_tag']\n    js_tag = context['js_tag']\n\n    def css_tag_with_cache_busting(css):\n        if isinstance(css, Stylesheet) and css.filename is not None:\n            url = urlsplit(css.filename)\n",
      "variables": [
        "app",
        "pagename",
        "templatename",
        "context",
        "doctree",
        "css_tag",
        "js_tag",
        "css",
        "url"
      ],
      "anonymized_code": "def add_html_cache_busting(var_1, var_2, var_3, var_4, var_5):\n    \"\"\"\n    Add cache busting query on CSS and JavaScript assets.\n\n    This adds the Matplotlib version as a query to the link reference in the\n    HTML, if the path is not absolute (i.e., it comes from the `_static`\n    directory) and doesn't already have a query.\n\n    .. note:: Sphinx 7.1 provides asset checksums; so this hook only runs on\n              Sphinx 7.0 and earlier.\n    \"\"\"\n    from sphinx.builders.html import Stylesheet, JavaScript\n\n    var_6 = var_4['var_6']\n    var_7 = var_4['var_7']\n\n    def css_tag_with_cache_busting(var_8):\n        if isinstance(var_8, Stylesheet) and var_8.filename is not None:\n            var_9 = urlsplit(var_8.filename)\n",
      "lines_processed": 19,
      "total_lines": 928
    },
    {
      "file_path": "mock_gui_toolkits.py",
      "code": "def setup(app):\n    sys.modules.update(\n        cairocffi=MyCairoCffi(),\n    )\n    return {'parallel_read_safe': True, 'parallel_write_safe': True}\n",
      "variables": [
        "app"
      ],
      "anonymized_code": "def setup(var_1):\n    sys.modules.update(\n        cairocffi=MyCairoCffi(),\n    )\n    return {'parallel_read_safe': True, 'parallel_write_safe': True}\n",
      "lines_processed": 5,
      "total_lines": 13
    },
    {
      "file_path": "github.py",
      "code": "def setup(app):\n    \"\"\"\n    Install the plugin.\n\n    :param app: Sphinx application context.\n    \"\"\"\n    app.add_role('ghissue', ghissue_role)\n    app.add_role('ghpull', ghissue_role)\n    app.add_role('ghuser', ghuser_role)\n    app.add_role('ghcommit', ghcommit_role)\n    app.add_config_value('github_project_url', None, 'env')\n\n    metadata = {'parallel_read_safe': True, 'parallel_write_safe': True}\n    return metadata\n",
      "variables": [
        "app",
        "metadata"
      ],
      "anonymized_code": "def setup(var_1):\n    \"\"\"\n    Install the plugin.\n\n    :param var_1: Sphinx application context.\n    \"\"\"\n    var_1.add_role('ghissue', ghissue_role)\n    var_1.add_role('ghpull', ghissue_role)\n    var_1.add_role('ghuser', ghuser_role)\n    var_1.add_role('ghcommit', ghcommit_role)\n    var_1.add_config_value('github_project_url', None, 'env')\n\n    var_2 = {'parallel_read_safe': True, 'parallel_write_safe': True}\n    return var_2\n",
      "lines_processed": 14,
      "total_lines": 167
    },
    {
      "file_path": "github.py",
      "code": "def ghuser_role(name, rawtext, text, lineno, inliner, options={}, content=[]):\n    \"\"\"\n    Link to a GitHub user.\n\n    Returns 2 part tuple containing list of nodes to insert into the\n    document and a list of system messages.  Both are allowed to be\n    empty.\n\n    :param name: The role name used in the document.\n    :param rawtext: The entire markup snippet, with role.\n    :param text: The text marked with the role.\n    :param lineno: The line number where rawtext appears in the input.\n    :param inliner: The inliner instance that called us.\n    :param options: Directive options for customization.\n    :param content: The directive content for customization.\n    \"\"\"\n    ref = 'https://www.github.com/' + text\n    node = nodes.reference(rawtext, text, refuri=ref, **options)\n    return [node], []\n",
      "variables": [
        "name",
        "rawtext",
        "text",
        "lineno",
        "inliner",
        "options",
        "content",
        "ref",
        "node"
      ],
      "anonymized_code": "def ghuser_role(var_1, var_2, var_3, var_4, var_5, var_6={}, var_7=[]):\n    \"\"\"\n    Link to a GitHub user.\n\n    Returns 2 part tuple containing list of nodes to insert into the\n    document and a list of system messages.  Both are allowed to be\n    empty.\n\n    :param var_1: The role var_1 used in the document.\n    :param var_2: The entire markup snippet, with role.\n    :param var_3: The var_3 marked with the role.\n    :param var_4: The line number where var_2 appears in the input.\n    :param var_5: The var_5 instance that called us.\n    :param var_6: Directive var_6 for customization.\n    :param var_7: The directive var_7 for customization.\n    \"\"\"\n    var_8 = 'https://www.github.com/' + var_3\n    var_9 = nodes.reference(var_2, var_3, refuri=var_8, **var_6)\n    return [var_9], []\n",
      "lines_processed": 19,
      "total_lines": 167
    }
  ],
  "python-poetry_poetry": [
    {
      "file_path": "config.py",
      "code": "def boolean_normalizer(val: str) -> bool:\n    return val.lower() in [\"true\", \"1\"]\n",
      "variables": [
        "val"
      ],
      "anonymized_code": "def boolean_normalizer(var_1: str) -> bool:\n    return var_1.lower() in [\"true\", \"1\"]\n",
      "lines_processed": 2,
      "total_lines": 432
    },
    {
      "file_path": "config_source.py",
      "code": "def drop_empty_config_category(\n    keys: list[str], config: dict[Any, Any]\n) -> dict[Any, Any]:\n    config_ = {}\n\n    for key, value in config.items():\n        if not keys or key != keys[0]:\n            config_[key] = value\n            continue\n        if keys and key == keys[0]:\n            if isinstance(value, dict):\n                value = drop_empty_config_category(keys[1:], value)\n\n            if value != {}:\n                config_[key] = value\n\n    return config_\n",
      "variables": [
        "keys",
        "config",
        "config_",
        "key",
        "value"
      ],
      "anonymized_code": "def drop_empty_config_category(\n    var_1: list[str], var_2: dict[Any, Any]\n) -> dict[Any, Any]:\n    var_3 = {}\n\n    for var_4, var_5 in var_2.items():\n        if not var_1 or var_4 != var_1[0]:\n            var_3[var_4] = var_5\n            continue\n        if var_1 and var_4 == var_1[0]:\n            if isinstance(var_5, dict):\n                var_5 = drop_empty_config_category(var_1[1:], var_5)\n\n            if var_5 != {}:\n                var_3[var_4] = var_5\n\n    return var_3\n",
      "lines_processed": 17,
      "total_lines": 99
    },
    {
      "file_path": "application.py",
      "code": "def load_command(name: str) -> Callable[[], Command]:\n    def _load() -> Command:\n        words = name.split(\" \")\n        module = import_module(\"poetry.console.commands.\" + \".\".join(words))\n        command_class = getattr(module, \"\".join(c.title() for c in words) + \"Command\")\n        command: Command = command_class()\n        return command\n\n    return _load\n",
      "variables": [
        "name",
        "words",
        "module",
        "command_class",
        "c",
        "command"
      ],
      "anonymized_code": "def load_command(var_1: str) -> Callable[[], Command]:\n    def _load() -> Command:\n        var_2 = var_1.split(\" \")\n        var_3 = import_module(\"poetry.console.commands.\" + \".\".join(var_2))\n        var_4 = getattr(var_3, \"\".join(var_5.title() for var_5 in var_2) + \"Command\")\n        var_6: Command = var_4()\n        return var_6\n\n    return _load\n",
      "lines_processed": 9,
      "total_lines": 646
    },
    {
      "file_path": "application.py",
      "code": "def main() -> int:\n    exit_code: int = Application().run()\n    return exit_code\n",
      "variables": [
        "exit_code"
      ],
      "anonymized_code": "def main() -> int:\n    var_1: int = Application().run()\n    return var_1\n",
      "lines_processed": 3,
      "total_lines": 646
    }
  ],
  "PaddlePaddle_PaddleOCR": [
    {
      "file_path": "make_shrink_map.py",
      "code": "def shrink_polygon_pyclipper(polygon, shrink_ratio):\n    from shapely.geometry import Polygon\n    import pyclipper\n\n    polygon_shape = Polygon(polygon)\n    distance = (\n        polygon_shape.area * (1 - np.power(shrink_ratio, 2)) / polygon_shape.length\n    )\n    subject = [tuple(l) for l in polygon]\n    padding = pyclipper.PyclipperOffset()\n    padding.AddPath(subject, pyclipper.JT_ROUND, pyclipper.ET_CLOSEDPOLYGON)\n    shrunk = padding.Execute(-distance)\n    if shrunk == []:\n        shrunk = np.array(shrunk)\n    else:\n        shrunk = np.array(shrunk[0]).reshape(-1, 2)\n    return shrunk\n",
      "variables": [
        "polygon",
        "shrink_ratio",
        "polygon_shape",
        "distance",
        "subject",
        "l",
        "padding",
        "shrunk"
      ],
      "anonymized_code": "def shrink_polygon_pyclipper(var_1, var_2):\n    from shapely.geometry import Polygon\n    import pyclipper\n\n    var_3 = Polygon(var_1)\n    var_4 = (\n        var_3.area * (1 - np.power(var_2, 2)) / var_3.length\n    )\n    var_5 = [tuple(var_6) for var_6 in var_1]\n    var_7 = pyclipper.PyclipperOffset()\n    var_7.AddPath(var_5, pyclipper.JT_ROUND, pyclipper.ET_CLOSEDPOLYGON)\n    var_8 = var_7.Execute(-var_4)\n    if var_8 == []:\n        var_8 = np.array(var_8)\n    else:\n        var_8 = np.array(var_8[0]).reshape(-1, 2)\n    return var_8\n",
      "lines_processed": 17,
      "total_lines": 129
    },
    {
      "file_path": "make_shrink_map.py",
      "code": "def shrink_polygon_py(polygon, shrink_ratio):\n    \"\"\"\n    \u5bf9\u6846\u8fdb\u884c\u7f29\u653e\uff0c\u8fd4\u56de\u53bb\u7684\u6bd4\u4f8b\u4e3a1/shrink_ratio \u5373\u53ef\n    \"\"\"\n    cx = polygon[:, 0].mean()\n    cy = polygon[:, 1].mean()\n    polygon[:, 0] = cx + (polygon[:, 0] - cx) * shrink_ratio\n    polygon[:, 1] = cy + (polygon[:, 1] - cy) * shrink_ratio\n    return polygon\n",
      "variables": [
        "polygon",
        "shrink_ratio",
        "cx",
        "cy"
      ],
      "anonymized_code": "def shrink_polygon_py(var_1, var_2):\n    \"\"\"\n    \u5bf9\u6846\u8fdb\u884c\u7f29\u653e\uff0c\u8fd4\u56de\u53bb\u7684\u6bd4\u4f8b\u4e3a1/var_2 \u5373\u53ef\n    \"\"\"\n    var_3 = var_1[:, 0].mean()\n    var_4 = var_1[:, 1].mean()\n    var_1[:, 0] = var_3 + (var_1[:, 0] - var_3) * var_2\n    var_1[:, 1] = var_4 + (var_1[:, 1] - var_4) * var_2\n    return var_1\n",
      "lines_processed": 9,
      "total_lines": 129
    },
    {
      "file_path": "__init__.py",
      "code": "def build_backbone(backbone_name, **kwargs):\n    assert (\n        backbone_name in support_backbone\n    ), f\"all support backbone is {support_backbone}\"\n    backbone = eval(backbone_name)(**kwargs)\n    return backbone\n",
      "variables": [
        "backbone_name",
        "kwargs",
        "backbone"
      ],
      "anonymized_code": "def build_backbone(var_1, **var_2):\n    assert (\n        var_1 in support_backbone\n    ), f\"all support var_3 is {support_backbone}\"\n    var_3 = eval(var_1)(**var_2)\n    return var_3\n",
      "lines_processed": 6,
      "total_lines": 25
    },
    {
      "file_path": "augment.py",
      "code": "def resize_image(img, short_size):\n    height, width, _ = img.shape\n    if height < width:\n        new_height = short_size\n        new_width = new_height / height * width\n    else:\n        new_width = short_size\n        new_height = new_width / width * height\n    new_height = int(round(new_height / 32) * 32)\n    new_width = int(round(new_width / 32) * 32)\n    resized_img = cv2.resize(img, (new_width, new_height))\n    return resized_img, (new_width / width, new_height / height)\n",
      "variables": [
        "img",
        "short_size",
        "height",
        "width",
        "_",
        "new_height",
        "new_width",
        "resized_img"
      ],
      "anonymized_code": "def resize_image(var_1, var_2):\n    var_3, var_4, var_5 = var_1.shape\n    if var_3 < var_4:\n        var_6 = var_2\n        var_7 = var_6 / var_3 * var_4\n    else:\n        var_7 = var_2\n        var_6 = var_7 / var_4 * var_3\n    var_6 = int(round(var_6 / 32) * 32)\n    var_7 = int(round(var_7 / 32) * 32)\n    var_8 = cv2.resize(var_1, (var_7, var_6))\n    return var_8, (var_7 / var_4, var_6 / var_3)\n",
      "lines_processed": 12,
      "total_lines": 308
    },
    {
      "file_path": "resnet.py",
      "code": "def constant_init(module, constant, bias=0):\n    module.weight = paddle.create_parameter(\n        shape=module.weight.shape,\n        dtype=\"float32\",\n        default_initializer=paddle.nn.initializer.Constant(constant),\n    )\n    if hasattr(module, \"bias\"):\n        module.bias = paddle.create_parameter(\n            shape=module.bias.shape,\n            dtype=\"float32\",\n            default_initializer=paddle.nn.initializer.Constant(bias),\n        )\n",
      "variables": [
        "module",
        "constant",
        "bias"
      ],
      "anonymized_code": "def constant_init(var_1, var_2, var_3=0):\n    var_1.weight = paddle.create_parameter(\n        shape=var_1.weight.shape,\n        dtype=\"float32\",\n        default_initializer=paddle.nn.initializer.Constant(var_2),\n    )\n    if hasattr(var_1, \"var_3\"):\n        var_1.var_3 = paddle.create_parameter(\n            shape=var_1.var_3.shape,\n            dtype=\"float32\",\n            default_initializer=paddle.nn.initializer.Constant(var_3),\n        )\n",
      "lines_processed": 12,
      "total_lines": 366
    },
    {
      "file_path": "resnet.py",
      "code": "def load_models(model, model_name):\n    import torch.utils.model_zoo as model_zoo\n\n    torch_patams = model_zoo.load_url(model_urls[model_name])\n    load_torch_params(model, torch_patams)\n",
      "variables": [
        "model",
        "model_name",
        "torch_patams"
      ],
      "anonymized_code": "def load_models(var_1, var_2):\n    import torch.utils.model_zoo as model_zoo\n\n    var_3 = model_zoo.load_url(model_urls[var_2])\n    load_torch_params(var_1, var_3)\n",
      "lines_processed": 5,
      "total_lines": 366
    },
    {
      "file_path": "resnet.py",
      "code": "def resnet101(pretrained=True, **kwargs):\n    \"\"\"Constructs a ResNet-101 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        assert (\n            kwargs.get(\"in_channels\", 3) == 3\n        ), \"in_channels must be 3 when pretrained is True\"\n        model.load_state_dict(model_zoo.load_url(model_urls[\"resnet101\"]), strict=False)\n    return model\n",
      "variables": [
        "pretrained",
        "kwargs",
        "model"
      ],
      "anonymized_code": "def resnet101(var_1=True, **var_2):\n    \"\"\"Constructs a ResNet-101 var_3.\n    Args:\n        var_1 (bool): If True, returns a var_3 pre-trained on ImageNet\n    \"\"\"\n    var_3 = ResNet(Bottleneck, [3, 4, 23, 3], **var_2)\n    if var_1:\n        assert (\n            var_2.get(\"in_channels\", 3) == 3\n        ), \"in_channels must be 3 when var_1 is True\"\n        var_3.load_state_dict(model_zoo.load_url(model_urls[\"resnet101\"]), strict=False)\n    return var_3\n",
      "lines_processed": 12,
      "total_lines": 366
    },
    {
      "file_path": "resnet.py",
      "code": "def resnet34(pretrained=True, **kwargs):\n    \"\"\"Constructs a ResNet-34 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        assert (\n            kwargs.get(\"in_channels\", 3) == 3\n        ), \"in_channels must be 3 when pretrained is True\"\n        model.load_state_dict(model_zoo.load_url(model_urls[\"resnet34\"]), strict=False)\n    return model\n",
      "variables": [
        "pretrained",
        "kwargs",
        "model"
      ],
      "anonymized_code": "def resnet34(var_1=True, **var_2):\n    \"\"\"Constructs a ResNet-34 var_3.\n    Args:\n        var_1 (bool): If True, returns a var_3 pre-trained on ImageNet\n    \"\"\"\n    var_3 = ResNet(BasicBlock, [3, 4, 6, 3], **var_2)\n    if var_1:\n        assert (\n            var_2.get(\"in_channels\", 3) == 3\n        ), \"in_channels must be 3 when var_1 is True\"\n        var_3.load_state_dict(model_zoo.load_url(model_urls[\"resnet34\"]), strict=False)\n    return var_3\n",
      "lines_processed": 12,
      "total_lines": 366
    },
    {
      "file_path": "resnet.py",
      "code": "def resnet18(pretrained=True, **kwargs):\n    \"\"\"Constructs a ResNet-18 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        assert (\n            kwargs.get(\"in_channels\", 3) == 3\n        ), \"in_channels must be 3 when pretrained is True\"\n        print(\"load from imagenet\")\n        load_models(model, \"resnet18\")\n    return model\n",
      "variables": [
        "pretrained",
        "kwargs",
        "model"
      ],
      "anonymized_code": "def resnet18(var_1=True, **var_2):\n    \"\"\"Constructs a ResNet-18 var_3.\n    Args:\n        var_1 (bool): If True, returns a var_3 pre-trained on ImageNet\n    \"\"\"\n    var_3 = ResNet(BasicBlock, [2, 2, 2, 2], **var_2)\n    if var_1:\n        assert (\n            var_2.get(\"in_channels\", 3) == 3\n        ), \"in_channels must be 3 when var_1 is True\"\n        print(\"load from imagenet\")\n        load_models(var_3, \"resnet18\")\n    return var_3\n",
      "lines_processed": 13,
      "total_lines": 366
    }
  ],
  "langflow-ai_langflow": [
    {
      "file_path": "update_pyproject_name.py",
      "code": "def main() -> None:\n    if len(sys.argv) != ARGUMENT_NUMBER:\n        msg = \"Must specify project name and build type, e.g. langflow-nightly base\"\n        raise ValueError(msg)\n    new_project_name = sys.argv[1]\n    build_type = sys.argv[2]\n\n    if build_type == \"base\":\n        update_pyproject_name(\"src/backend/base/pyproject.toml\", new_project_name)\n        update_uv_dep(\"pyproject.toml\", new_project_name)\n    elif build_type == \"main\":\n        update_pyproject_name(\"pyproject.toml\", new_project_name)\n        update_uv_dep(\"pyproject.toml\", new_project_name)\n    else:\n        msg = f\"Invalid build type: {build_type}\"\n        raise ValueError(msg)\n",
      "variables": [
        "msg",
        "new_project_name",
        "build_type"
      ],
      "anonymized_code": "def main() -> None:\n    if len(sys.argv) != ARGUMENT_NUMBER:\n        var_1 = \"Must specify project name and build type, e.g. langflow-nightly base\"\n        raise ValueError(var_1)\n    var_2 = sys.argv[1]\n    var_3 = sys.argv[2]\n\n    if var_3 == \"base\":\n        update_pyproject_name(\"src/backend/base/pyproject.toml\", var_2)\n        update_uv_dep(\"pyproject.toml\", var_2)\n    elif var_3 == \"main\":\n        update_pyproject_name(\"pyproject.toml\", var_2)\n        update_uv_dep(\"pyproject.toml\", var_2)\n    else:\n        var_1 = f\"Invalid build type: {var_3}\"\n        raise ValueError(var_1)\n",
      "lines_processed": 16,
      "total_lines": 69
    },
    {
      "file_path": "update_pyproject_name.py",
      "code": "def update_uv_dep(pyproject_path: str, new_project_name: str) -> None:\n    \"\"\"Update the langflow-base dependency in pyproject.toml.\"\"\"\n    filepath = BASE_DIR / pyproject_path\n    content = filepath.read_text(encoding=\"utf-8\")\n\n    if new_project_name == \"langflow-nightly\":\n        pattern = re.compile(r\"langflow = \\{ workspace = true \\}\")\n        replacement = \"langflow-nightly = { workspace = true }\"\n    elif new_project_name == \"langflow-base-nightly\":\n        pattern = re.compile(r\"langflow-base = \\{ workspace = true \\}\")\n        replacement = \"langflow-base-nightly = { workspace = true }\"\n    else:\n        msg = f\"Invalid project name: {new_project_name}\"\n        raise ValueError(msg)\n\n    # Updates the dependency name for uv\n    if not pattern.search(content):\n        msg = f\"{replacement} uv dependency not found in {filepath}\"\n        raise ValueError(msg)\n",
      "variables": [
        "pyproject_path",
        "new_project_name",
        "filepath",
        "content",
        "pattern",
        "replacement",
        "msg"
      ],
      "anonymized_code": "def update_uv_dep(var_1: str, var_2: str) -> None:\n    \"\"\"Update the langflow-base dependency in pyproject.toml.\"\"\"\n    var_3 = BASE_DIR / var_1\n    var_4 = var_3.read_text(encoding=\"utf-8\")\n\n    if var_2 == \"langflow-nightly\":\n        var_5 = re.compile(r\"langflow = \\{ workspace = true \\}\")\n        var_6 = \"langflow-nightly = { workspace = true }\"\n    elif var_2 == \"langflow-base-nightly\":\n        var_5 = re.compile(r\"langflow-base = \\{ workspace = true \\}\")\n        var_6 = \"langflow-base-nightly = { workspace = true }\"\n    else:\n        var_7 = f\"Invalid project name: {var_2}\"\n        raise ValueError(var_7)\n\n    # Updates the dependency name for uv\n    if not var_5.search(var_4):\n        var_7 = f\"{var_6} uv dependency not found in {var_3}\"\n        raise ValueError(var_7)\n",
      "lines_processed": 19,
      "total_lines": 69
    },
    {
      "file_path": "update_pyproject_name.py",
      "code": "def update_pyproject_name(pyproject_path: str, new_project_name: str) -> None:\n    \"\"\"Update the project name in pyproject.toml.\"\"\"\n    filepath = BASE_DIR / pyproject_path\n    content = filepath.read_text(encoding=\"utf-8\")\n\n    # Regex to match the version line under [tool.poetry]\n    pattern = re.compile(r'(?<=^name = \")[^\"]+(?=\")', re.MULTILINE)\n\n    if not pattern.search(content):\n        msg = f'Project name not found in \"{filepath}\"'\n        raise ValueError(msg)\n    content = pattern.sub(new_project_name, content)\n\n    filepath.write_text(content, encoding=\"utf-8\")\n",
      "variables": [
        "pyproject_path",
        "new_project_name",
        "filepath",
        "content",
        "pattern",
        "msg"
      ],
      "anonymized_code": "def update_pyproject_name(var_1: str, var_2: str) -> None:\n    \"\"\"Update the project name in pyproject.toml.\"\"\"\n    var_3 = BASE_DIR / var_1\n    var_4 = var_3.read_text(encoding=\"utf-8\")\n\n    # Regex to match the version line under [tool.poetry]\n    var_5 = re.compile(r'(?<=^name = \")[^\"]+(?=\")', re.MULTILINE)\n\n    if not var_5.search(var_4):\n        var_6 = f'Project name not found in \"{var_3}\"'\n        raise ValueError(var_6)\n    var_4 = var_5.sub(var_2, var_4)\n\n    var_3.write_text(var_4, encoding=\"utf-8\")\n",
      "lines_processed": 14,
      "total_lines": 69
    },
    {
      "file_path": "env.py",
      "code": "def _sqlite_do_connect(\n    dbapi_connection,\n    connection_record,  # noqa: ARG001\n):\n    # disable pysqlite's emitting of the BEGIN statement entirely.\n    # also stops it from emitting COMMIT before any DDL.\n    dbapi_connection.isolation_level = None\n",
      "variables": [
        "dbapi_connection",
        "connection_record"
      ],
      "anonymized_code": "def _sqlite_do_connect(\n    var_1,\n    var_2,  # noqa: ARG001\n):\n    # disable pysqlite's emitting of the BEGIN statement entirely.\n    # also stops it from emitting COMMIT before any DDL.\n    var_1.isolation_level = None\n",
      "lines_processed": 7,
      "total_lines": 124
    },
    {
      "file_path": "env.py",
      "code": "def _sqlite_do_begin(conn):\n    # emit our own BEGIN\n    conn.exec_driver_sql(\"PRAGMA busy_timeout = 60000\")\n    conn.exec_driver_sql(\"BEGIN EXCLUSIVE\")\n",
      "variables": [
        "conn"
      ],
      "anonymized_code": "def _sqlite_do_begin(var_1):\n    # emit our own BEGIN\n    var_1.exec_driver_sql(\"PRAGMA busy_timeout = 60000\")\n    var_1.exec_driver_sql(\"BEGIN EXCLUSIVE\")\n",
      "lines_processed": 4,
      "total_lines": 124
    },
    {
      "file_path": "env.py",
      "code": "def _do_run_migrations(connection):\n    context.configure(\n        connection=connection, target_metadata=target_metadata, render_as_batch=True, prepare_threshold=None\n    )\n\n    with context.begin_transaction():\n        if connection.dialect.name == \"postgresql\":\n            connection.execute(text(\"SET LOCAL lock_timeout = '60s';\"))\n            connection.execute(text(\"SELECT pg_advisory_xact_lock(112233);\"))\n        context.run_migrations()\n",
      "variables": [
        "connection"
      ],
      "anonymized_code": "def _do_run_migrations(var_1):\n    context.configure(\n        var_1=var_1, target_metadata=target_metadata, render_as_batch=True, prepare_threshold=None\n    )\n\n    with context.begin_transaction():\n        if var_1.dialect.name == \"postgresql\":\n            var_1.execute(text(\"SET LOCAL lock_timeout = '60s';\"))\n            var_1.execute(text(\"SELECT pg_advisory_xact_lock(112233);\"))\n        context.run_migrations()\n",
      "lines_processed": 10,
      "total_lines": 124
    },
    {
      "file_path": "006b3990db50_add_unique_constraints.py",
      "code": "def upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    conn = op.get_bind()\n    inspector = sa.inspect(conn)  # type: ignore\n    api_key_constraints = inspector.get_unique_constraints(\"apikey\")\n    flow_constraints = inspector.get_unique_constraints(\"flow\")\n    user_constraints = inspector.get_unique_constraints(\"user\")\n    try:\n        if not any(constraint[\"column_names\"] == [\"id\"] for constraint in api_key_constraints):\n            with op.batch_alter_table(\"apikey\", schema=None) as batch_op:\n                batch_op.create_unique_constraint(\"uq_apikey_id\", [\"id\"])\n        if not any(constraint[\"column_names\"] == [\"id\"] for constraint in flow_constraints):\n            with op.batch_alter_table(\"flow\", schema=None) as batch_op:\n                batch_op.create_unique_constraint(\"uq_flow_id\", [\"id\"])\n        if not any(constraint[\"column_names\"] == [\"id\"] for constraint in user_constraints):\n            with op.batch_alter_table(\"user\", schema=None) as batch_op:\n                batch_op.create_unique_constraint(\"uq_user_id\", [\"id\"])\n    except Exception as e:\n        print(e)\n",
      "variables": [
        "conn",
        "inspector",
        "api_key_constraints",
        "flow_constraints",
        "user_constraints",
        "constraint",
        "batch_op"
      ],
      "anonymized_code": "def upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    var_1 = op.get_bind()\n    var_2 = sa.inspect(var_1)  # type: ignore\n    var_3 = var_2.get_unique_constraints(\"apikey\")\n    var_4 = var_2.get_unique_constraints(\"flow\")\n    var_5 = var_2.get_unique_constraints(\"user\")\n    try:\n        if not any(var_6[\"column_names\"] == [\"id\"] for var_6 in var_3):\n            with op.batch_alter_table(\"apikey\", schema=None) as var_7:\n                var_7.create_unique_constraint(\"uq_apikey_id\", [\"id\"])\n        if not any(var_6[\"column_names\"] == [\"id\"] for var_6 in var_4):\n            with op.batch_alter_table(\"flow\", schema=None) as var_7:\n                var_7.create_unique_constraint(\"uq_flow_id\", [\"id\"])\n        if not any(var_6[\"column_names\"] == [\"id\"] for var_6 in var_5):\n            with op.batch_alter_table(\"user\", schema=None) as var_7:\n                var_7.create_unique_constraint(\"uq_user_id\", [\"id\"])\n    except Exception as e:\n        print(e)\n",
      "lines_processed": 19,
      "total_lines": 66
    },
    {
      "file_path": "006b3990db50_add_unique_constraints.py",
      "code": "def downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    conn = op.get_bind()\n    inspector = sa.inspect(conn)  # type: ignore\n    api_key_constraints = inspector.get_unique_constraints(\"apikey\")\n    flow_constraints = inspector.get_unique_constraints(\"flow\")\n    user_constraints = inspector.get_unique_constraints(\"user\")\n    try:\n        if any(constraint[\"name\"] == \"uq_apikey_id\" for constraint in api_key_constraints):\n            with op.batch_alter_table(\"user\", schema=None) as batch_op:\n                batch_op.drop_constraint(\"uq_user_id\", type_=\"unique\")\n        if any(constraint[\"name\"] == \"uq_flow_id\" for constraint in flow_constraints):\n            with op.batch_alter_table(\"flow\", schema=None) as batch_op:\n                batch_op.drop_constraint(\"uq_flow_id\", type_=\"unique\")\n        if any(constraint[\"name\"] == \"uq_user_id\" for constraint in user_constraints):\n            with op.batch_alter_table(\"apikey\", schema=None) as batch_op:\n                batch_op.drop_constraint(\"uq_apikey_id\", type_=\"unique\")\n    except Exception as e:\n        print(e)\n",
      "variables": [
        "conn",
        "inspector",
        "api_key_constraints",
        "flow_constraints",
        "user_constraints",
        "constraint",
        "batch_op"
      ],
      "anonymized_code": "def downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    var_1 = op.get_bind()\n    var_2 = sa.inspect(var_1)  # type: ignore\n    var_3 = var_2.get_unique_constraints(\"apikey\")\n    var_4 = var_2.get_unique_constraints(\"flow\")\n    var_5 = var_2.get_unique_constraints(\"user\")\n    try:\n        if any(var_6[\"name\"] == \"uq_apikey_id\" for var_6 in var_3):\n            with op.batch_alter_table(\"user\", schema=None) as var_7:\n                var_7.drop_constraint(\"uq_user_id\", type_=\"unique\")\n        if any(var_6[\"name\"] == \"uq_flow_id\" for var_6 in var_4):\n            with op.batch_alter_table(\"flow\", schema=None) as var_7:\n                var_7.drop_constraint(\"uq_flow_id\", type_=\"unique\")\n        if any(var_6[\"name\"] == \"uq_user_id\" for var_6 in var_5):\n            with op.batch_alter_table(\"apikey\", schema=None) as var_7:\n                var_7.drop_constraint(\"uq_apikey_id\", type_=\"unique\")\n    except Exception as e:\n        print(e)\n",
      "lines_processed": 19,
      "total_lines": 66
    },
    {
      "file_path": "update_pyproject_combined.py",
      "code": "def main():\n    \"\"\"Universal update script that handles both base and main updates in a single run.\n\n    Usage:\n    update_pyproject_combined.py main <main_tag> <base_tag>\n    \"\"\"\n    arg_count = 4\n    if len(sys.argv) != arg_count:\n        print(\"Usage:\")\n        print(\"  update_pyproject_combined.py main <main_tag> <base_tag>\")\n        sys.exit(1)\n\n    mode = sys.argv[1]\n    if mode != \"main\":\n        print(\"Only 'main' mode is supported\")\n        print(\"Usage: update_pyproject_combined.py main <main_tag> <base_tag>\")\n        sys.exit(1)\n\n    main_tag = sys.argv[2]\n",
      "variables": [
        "arg_count",
        "mode",
        "main_tag"
      ],
      "anonymized_code": "def main():\n    \"\"\"Universal update script that handles both base and main updates in a single run.\n\n    Usage:\n    update_pyproject_combined.py main <var_3> <base_tag>\n    \"\"\"\n    var_1 = 4\n    if len(sys.argv) != var_1:\n        print(\"Usage:\")\n        print(\"  update_pyproject_combined.py main <var_3> <base_tag>\")\n        sys.exit(1)\n\n    var_2 = sys.argv[1]\n    if var_2 != \"main\":\n        print(\"Only 'main' var_2 is supported\")\n        print(\"Usage: update_pyproject_combined.py main <var_3> <base_tag>\")\n        sys.exit(1)\n\n    var_3 = sys.argv[2]\n",
      "lines_processed": 19,
      "total_lines": 52
    },
    {
      "file_path": "update_uv_dependency.py",
      "code": "def main() -> None:\n    if len(sys.argv) != ARGUMENT_NUMBER:\n        msg = \"specify base version\"\n        raise ValueError(msg)\n    base_version = sys.argv[1]\n    base_version = base_version.lstrip(\"v\")\n    update_uv_dep(base_version)\n",
      "variables": [
        "msg",
        "base_version"
      ],
      "anonymized_code": "def main() -> None:\n    if len(sys.argv) != ARGUMENT_NUMBER:\n        var_1 = \"specify base version\"\n        raise ValueError(var_1)\n    var_2 = sys.argv[1]\n    var_2 = var_2.lstrip(\"v\")\n    update_uv_dep(var_2)\n",
      "lines_processed": 7,
      "total_lines": 44
    },
    {
      "file_path": "update_uv_dependency.py",
      "code": "def update_uv_dep(base_version: str) -> None:\n    \"\"\"Update the langflow-base dependency in pyproject.toml.\"\"\"\n    pyproject_path = BASE_DIR / \"pyproject.toml\"\n\n    # Read the pyproject.toml file content\n    content = pyproject_path.read_text(encoding=\"utf-8\")\n\n    # For the main project, update the langflow-base dependency in the UV section\n    pattern = re.compile(r'(dependencies\\s*=\\s*\\[\\s*\\n\\s*)(\"langflow-base==[\\d.]+\")')\n    replacement = rf'\\1\"langflow-base-nightly=={base_version}\"'\n\n    # Check if the pattern is found\n    if not pattern.search(content):\n        msg = f\"{pattern} UV dependency not found in {pyproject_path}\"\n        raise ValueError(msg)\n\n    # Replace the matched pattern with the new one\n    content = pattern.sub(replacement, content)\n\n",
      "variables": [
        "base_version",
        "pyproject_path",
        "content",
        "pattern",
        "replacement",
        "msg"
      ],
      "anonymized_code": "def update_uv_dep(var_1: str) -> None:\n    \"\"\"Update the langflow-base dependency in pyproject.toml.\"\"\"\n    var_2 = BASE_DIR / \"pyproject.toml\"\n\n    # Read the pyproject.toml file var_3\n    var_3 = var_2.read_text(encoding=\"utf-8\")\n\n    # For the main project, update the langflow-base dependency in the UV section\n    var_4 = re.compile(r'(dependencies\\s*=\\s*\\[\\s*\\n\\s*)(\"langflow-base==[\\d.]+\")')\n    var_5 = rf'\\1\"langflow-base-nightly=={var_1}\"'\n\n    # Check if the var_4 is found\n    if not var_4.search(var_3):\n        var_6 = f\"{var_4} UV dependency not found in {var_2}\"\n        raise ValueError(var_6)\n\n    # Replace the matched var_4 with the new one\n    var_3 = var_4.sub(var_5, var_3)\n\n",
      "lines_processed": 19,
      "total_lines": 44
    },
    {
      "file_path": "update_lf_base_dependency.py",
      "code": "def update_base_dep(pyproject_path: str, new_version: str) -> None:\n    \"\"\"Update the langflow-base dependency in pyproject.toml.\"\"\"\n    filepath = BASE_DIR / pyproject_path\n    content = filepath.read_text(encoding=\"utf-8\")\n\n    replacement = f'langflow-base-nightly = \"{new_version}\"'\n\n    # Updates the pattern for poetry\n    pattern = re.compile(r'langflow-base = \\{ path = \"\\./src/backend/base\", develop = true \\}')\n    if not pattern.search(content):\n        msg = f'langflow-base poetry dependency not found in \"{filepath}\"'\n        raise ValueError(msg)\n    content = pattern.sub(replacement, content)\n    filepath.write_text(content, encoding=\"utf-8\")\n",
      "variables": [
        "pyproject_path",
        "new_version",
        "filepath",
        "content",
        "replacement",
        "pattern",
        "msg"
      ],
      "anonymized_code": "def update_base_dep(var_1: str, var_2: str) -> None:\n    \"\"\"Update the langflow-base dependency in pyproject.toml.\"\"\"\n    var_3 = BASE_DIR / var_1\n    var_4 = var_3.read_text(encoding=\"utf-8\")\n\n    var_5 = f'langflow-base-nightly = \"{var_2}\"'\n\n    # Updates the var_6 for poetry\n    var_6 = re.compile(r'langflow-base = \\{ path = \"\\./src/backend/base\", develop = true \\}')\n    if not var_6.search(var_4):\n        var_7 = f'langflow-base poetry dependency not found in \"{var_3}\"'\n        raise ValueError(var_7)\n    var_4 = var_6.sub(var_5, var_4)\n    var_3.write_text(var_4, encoding=\"utf-8\")\n",
      "lines_processed": 14,
      "total_lines": 51
    },
    {
      "file_path": "update_lf_base_dependency.py",
      "code": "def main() -> None:\n    if len(sys.argv) != ARGUMENT_NUMBER:\n        msg = \"New version not specified\"\n        raise ValueError(msg)\n    base_version = sys.argv[1]\n\n    # Strip \"v\" prefix from version if present\n    base_version = base_version.removeprefix(\"v\")\n\n    verify_pep440(base_version)\n    update_base_dep(\"pyproject.toml\", base_version)\n",
      "variables": [
        "msg",
        "base_version"
      ],
      "anonymized_code": "def main() -> None:\n    if len(sys.argv) != ARGUMENT_NUMBER:\n        var_1 = \"New version not specified\"\n        raise ValueError(var_1)\n    var_2 = sys.argv[1]\n\n    # Strip \"v\" prefix from version if present\n    var_2 = var_2.removeprefix(\"v\")\n\n    verify_pep440(var_2)\n    update_base_dep(\"pyproject.toml\", var_2)\n",
      "lines_processed": 11,
      "total_lines": 51
    },
    {
      "file_path": "update_lf_base_dependency.py",
      "code": "def verify_pep440(version):\n    \"\"\"Verify if version is PEP440 compliant.\n\n    https://github.com/pypa/packaging/blob/16.7/packaging/version.py#L191\n    \"\"\"\n    return packaging.version.Version(version)\n",
      "variables": [
        "version"
      ],
      "anonymized_code": "def verify_pep440(var_1):\n    \"\"\"Verify if var_1 is PEP440 compliant.\n\n    https://github.com/pypa/packaging/blob/16.7/packaging/var_1.py#L191\n    \"\"\"\n    return packaging.var_1.Version(var_1)\n",
      "lines_processed": 6,
      "total_lines": 51
    },
    {
      "file_path": "update_pyproject_version.py",
      "code": "def main() -> None:\n    if len(sys.argv) != ARGUMENT_NUMBER:\n        msg = \"New version not specified\"\n        raise ValueError(msg)\n    new_version = sys.argv[1]\n\n    # Strip \"v\" prefix from version if present\n    new_version = new_version.removeprefix(\"v\")\n\n    build_type = sys.argv[2]\n\n    verify_pep440(new_version)\n\n    if build_type == \"base\":\n        update_pyproject_version(\"src/backend/base/pyproject.toml\", new_version)\n    elif build_type == \"main\":\n        update_pyproject_version(\"pyproject.toml\", new_version)\n    else:\n        msg = f\"Invalid build type: {build_type}\"\n",
      "variables": [
        "msg",
        "new_version",
        "build_type"
      ],
      "anonymized_code": "def main() -> None:\n    if len(sys.argv) != ARGUMENT_NUMBER:\n        var_1 = \"New version not specified\"\n        raise ValueError(var_1)\n    var_2 = sys.argv[1]\n\n    # Strip \"v\" prefix from version if present\n    var_2 = var_2.removeprefix(\"v\")\n\n    var_3 = sys.argv[2]\n\n    verify_pep440(var_2)\n\n    if var_3 == \"base\":\n        update_pyproject_version(\"src/backend/base/pyproject.toml\", var_2)\n    elif var_3 == \"main\":\n        update_pyproject_version(\"pyproject.toml\", var_2)\n    else:\n        var_1 = f\"Invalid build type: {var_3}\"\n",
      "lines_processed": 19,
      "total_lines": 61
    },
    {
      "file_path": "update_pyproject_version.py",
      "code": "def verify_pep440(version):\n    \"\"\"Verify if version is PEP440 compliant.\n\n    https://github.com/pypa/packaging/blob/16.7/packaging/version.py#L191\n    \"\"\"\n    return packaging.version.Version(version)\n",
      "variables": [
        "version"
      ],
      "anonymized_code": "def verify_pep440(var_1):\n    \"\"\"Verify if var_1 is PEP440 compliant.\n\n    https://github.com/pypa/packaging/blob/16.7/packaging/var_1.py#L191\n    \"\"\"\n    return packaging.var_1.Version(var_1)\n",
      "lines_processed": 6,
      "total_lines": 61
    },
    {
      "file_path": "update_pyproject_version.py",
      "code": "def update_pyproject_version(pyproject_path: str, new_version: str) -> None:\n    \"\"\"Update the version in pyproject.toml.\"\"\"\n    filepath = BASE_DIR / pyproject_path\n    content = filepath.read_text(encoding=\"utf-8\")\n\n    # Regex to match the version line under [tool.poetry]\n    pattern = re.compile(r'(?<=^version = \")[^\"]+(?=\")', re.MULTILINE)\n\n    if not pattern.search(content):\n        msg = f'Project version not found in \"{filepath}\"'\n        raise ValueError(msg)\n\n    content = pattern.sub(new_version, content)\n\n    filepath.write_text(content, encoding=\"utf-8\")\n",
      "variables": [
        "pyproject_path",
        "new_version",
        "filepath",
        "content",
        "pattern",
        "msg"
      ],
      "anonymized_code": "def update_pyproject_version(var_1: str, var_2: str) -> None:\n    \"\"\"Update the version in pyproject.toml.\"\"\"\n    var_3 = BASE_DIR / var_1\n    var_4 = var_3.read_text(encoding=\"utf-8\")\n\n    # Regex to match the version line under [tool.poetry]\n    var_5 = re.compile(r'(?<=^version = \")[^\"]+(?=\")', re.MULTILINE)\n\n    if not var_5.search(var_4):\n        var_6 = f'Project version not found in \"{var_3}\"'\n        raise ValueError(var_6)\n\n    var_4 = var_5.sub(var_2, var_4)\n\n    var_3.write_text(var_4, encoding=\"utf-8\")\n",
      "lines_processed": 15,
      "total_lines": 61
    },
    {
      "file_path": "pypi_nightly_tag.py",
      "code": "def create_tag(build_type: str):\n    current_version = get_latest_published_version(build_type, is_nightly=False)\n    current_nightly_version = get_latest_published_version(build_type, is_nightly=True)\n\n    build_number = \"0\"\n    latest_base_version = current_version.base_version\n    nightly_base_version = current_nightly_version.base_version\n\n    if latest_base_version == nightly_base_version:\n        # If the latest version is the same as the nightly version, increment the build number\n        build_number = str(current_nightly_version.dev + 1)\n\n    new_nightly_version = latest_base_version + \".dev\" + build_number\n\n    # Prepend \"v\" to the version, if DNE.\n    # This is an update to the nightly version format.\n    if not new_nightly_version.startswith(\"v\"):\n        new_nightly_version = \"v\" + new_nightly_version\n\n",
      "variables": [
        "build_type",
        "current_version",
        "current_nightly_version",
        "build_number",
        "latest_base_version",
        "nightly_base_version",
        "new_nightly_version"
      ],
      "anonymized_code": "def create_tag(var_1: str):\n    var_2 = get_latest_published_version(var_1, is_nightly=False)\n    var_3 = get_latest_published_version(var_1, is_nightly=True)\n\n    var_4 = \"0\"\n    var_5 = var_2.base_version\n    var_6 = var_3.base_version\n\n    if var_5 == var_6:\n        # If the latest version is the same as the nightly version, increment the build number\n        var_4 = str(var_3.dev + 1)\n\n    var_7 = var_5 + \".dev\" + var_4\n\n    # Prepend \"v\" to the version, if DNE.\n    # This is an update to the nightly version format.\n    if not var_7.startswith(\"v\"):\n        var_7 = \"v\" + var_7\n\n",
      "lines_processed": 19,
      "total_lines": 85
    },
    {
      "file_path": "pypi_nightly_tag.py",
      "code": "def get_latest_published_version(build_type: str, *, is_nightly: bool) -> Version:\n    import requests\n\n    url = \"\"\n    if build_type == \"base\":\n        url = PYPI_LANGFLOW_BASE_NIGHTLY_URL if is_nightly else PYPI_LANGFLOW_BASE_URL\n    elif build_type == \"main\":\n        url = PYPI_LANGFLOW_NIGHTLY_URL if is_nightly else PYPI_LANGFLOW_URL\n    else:\n        msg = f\"Invalid build type: {build_type}\"\n        raise ValueError(msg)\n\n    res = requests.get(url, timeout=10)\n    try:\n        version_str = res.json()[\"info\"][\"version\"]\n    except Exception as e:\n        msg = \"Got unexpected response from PyPI\"\n        raise RuntimeError(msg) from e\n    return Version(version_str)\n",
      "variables": [
        "build_type",
        "is_nightly",
        "url",
        "msg",
        "res",
        "version_str"
      ],
      "anonymized_code": "def get_latest_published_version(var_1: str, *, var_2: bool) -> Version:\n    import requests\n\n    var_3 = \"\"\n    if var_1 == \"base\":\n        var_3 = PYPI_LANGFLOW_BASE_NIGHTLY_URL if var_2 else PYPI_LANGFLOW_BASE_URL\n    elif var_1 == \"main\":\n        var_3 = PYPI_LANGFLOW_NIGHTLY_URL if var_2 else PYPI_LANGFLOW_URL\n    else:\n        var_4 = f\"Invalid build type: {var_1}\"\n        raise ValueError(var_4)\n\n    var_5 = requests.get(var_3, timeout=10)\n    try:\n        var_6 = var_5.json()[\"info\"][\"version\"]\n    except Exception as e:\n        var_4 = \"Got unexpected response from PyPI\"\n        raise RuntimeError(var_4) from e\n    return Version(var_6)\n",
      "lines_processed": 19,
      "total_lines": 85
    },
    {
      "file_path": "__main__.py",
      "code": "def handle_sigterm(signum, frame):  # noqa: ARG001\n    \"\"\"Handle SIGTERM signal gracefully.\"\"\"\n    logger.info(\"Received SIGTERM signal. Performing graceful shutdown...\")\n    # Raise SystemExit to trigger graceful shutdown\n    sys.exit(0)\n",
      "variables": [
        "signum",
        "frame"
      ],
      "anonymized_code": "def handle_sigterm(var_1, var_2):  # noqa: ARG001\n    \"\"\"Handle SIGTERM signal gracefully.\"\"\"\n    logger.info(\"Received SIGTERM signal. Performing graceful shutdown...\")\n    # Raise SystemExit to trigger graceful shutdown\n    sys.exit(0)\n",
      "lines_processed": 5,
      "total_lines": 672
    },
    {
      "file_path": "__main__.py",
      "code": "def is_port_in_use(port, host=\"localhost\"):\n    \"\"\"Check if a port is in use.\n\n    Args:\n        port (int): The port number to check.\n        host (str): The host to check the port on. Defaults to 'localhost'.\n\n    Returns:\n        bool: True if the port is in use, False otherwise.\n    \"\"\"\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        return s.connect_ex((host, port)) == 0\n",
      "variables": [
        "port",
        "host",
        "s"
      ],
      "anonymized_code": "def is_port_in_use(var_1, var_2=\"localhost\"):\n    \"\"\"Check if a var_1 is in use.\n\n    Args:\n        var_1 (int): The var_1 number to check.\n        var_2 (str): The var_2 to check the var_1 on. Defaults to 'localhost'.\n\n    Returns:\n        bool: True if the var_1 is in use, False otherwise.\n    \"\"\"\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as var_3:\n        return var_3.connect_ex((var_2, var_1)) == 0\n",
      "lines_processed": 12,
      "total_lines": 672
    },
    {
      "file_path": "__main__.py",
      "code": "def wait_for_server_ready(host, port, protocol) -> None:\n    \"\"\"Wait for the server to become ready by polling the health endpoint.\"\"\"\n    status_code = 0\n    while status_code != httpx.codes.OK:\n        try:\n            status_code = httpx.get(\n                f\"{protocol}://{host}:{port}/health\", verify=host not in (\"127.0.0.1\", \"localhost\")\n            ).status_code\n        except HTTPError:\n            time.sleep(1)\n        except Exception:  # noqa: BLE001\n            logger.opt(exception=True).debug(\"Error while waiting for the server to become ready.\")\n            time.sleep(1)\n",
      "variables": [
        "host",
        "port",
        "protocol",
        "status_code"
      ],
      "anonymized_code": "def wait_for_server_ready(var_1, var_2, var_3) -> None:\n    \"\"\"Wait for the server to become ready by polling the health endpoint.\"\"\"\n    var_4 = 0\n    while var_4 != httpx.codes.OK:\n        try:\n            var_4 = httpx.get(\n                f\"{var_3}://{var_1}:{var_2}/health\", verify=var_1 not in (\"127.0.0.1\", \"localhost\")\n            ).var_4\n        except HTTPError:\n            time.sleep(1)\n        except Exception:  # noqa: BLE001\n            logger.opt(exception=True).debug(\"Error while waiting for the server to become ready.\")\n            time.sleep(1)\n",
      "lines_processed": 13,
      "total_lines": 672
    },
    {
      "file_path": "__main__.py",
      "code": "def get_letter_from_version(version: str) -> str | None:\n    \"\"\"Get the letter from a pre-release version.\"\"\"\n    if \"a\" in version:\n        return \"a\"\n    if \"b\" in version:\n        return \"b\"\n    if \"rc\" in version:\n        return \"rc\"\n    return None\n",
      "variables": [
        "version"
      ],
      "anonymized_code": "def get_letter_from_version(var_1: str) -> str | None:\n    \"\"\"Get the letter from a pre-release var_1.\"\"\"\n    if \"a\" in var_1:\n        return \"a\"\n    if \"b\" in var_1:\n        return \"b\"\n    if \"rc\" in var_1:\n        return \"rc\"\n    return None\n",
      "lines_processed": 9,
      "total_lines": 672
    },
    {
      "file_path": "__main__.py",
      "code": "def run_langflow(host, port, log_level, options, app) -> None:\n    \"\"\"Run Langflow server on localhost.\"\"\"\n    if platform.system() == \"Windows\":\n        import uvicorn\n\n        uvicorn.run(\n            app,\n            host=host,\n            port=port,\n            log_level=log_level.lower(),\n            loop=\"asyncio\",\n            ssl_keyfile=options[\"keyfile\"],\n            ssl_certfile=options[\"certfile\"],\n        )\n    else:\n        from langflow.server import LangflowApplication\n\n        server = LangflowApplication(app, options)\n\n",
      "variables": [
        "host",
        "port",
        "log_level",
        "options",
        "app",
        "server"
      ],
      "anonymized_code": "def run_langflow(var_1, var_2, var_3, var_4, var_5) -> None:\n    \"\"\"Run Langflow var_6 on localhost.\"\"\"\n    if platform.system() == \"Windows\":\n        import uvicorn\n\n        uvicorn.run(\n            var_5,\n            var_1=var_1,\n            var_2=var_2,\n            var_3=var_3.lower(),\n            loop=\"asyncio\",\n            ssl_keyfile=var_4[\"keyfile\"],\n            ssl_certfile=var_4[\"certfile\"],\n        )\n    else:\n        from langflow.var_6 import LangflowApplication\n\n        var_6 = LangflowApplication(var_5, var_4)\n\n",
      "lines_processed": 19,
      "total_lines": 672
    },
    {
      "file_path": "__main__.py",
      "code": "def api_key(\n    log_level: str = typer.Option(\"error\", help=\"Logging level.\"),\n) -> None:\n    \"\"\"Creates an API key for the default superuser if AUTO_LOGIN is enabled.\n\n    Args:\n        log_level (str, optional): Logging level. Defaults to \"error\".\n\n    Returns:\n        None\n    \"\"\"\n    configure(log_level=log_level)\n\n    async def aapi_key():\n        await initialize_services()\n        settings_service = get_settings_service()\n        auth_settings = settings_service.auth_settings\n        if not auth_settings.AUTO_LOGIN:\n            typer.echo(\"Auto login is disabled. API keys cannot be created through the CLI.\")\n",
      "variables": [
        "log_level",
        "settings_service",
        "auth_settings"
      ],
      "anonymized_code": "def api_key(\n    var_1: str = typer.Option(\"error\", help=\"Logging level.\"),\n) -> None:\n    \"\"\"Creates an API key for the default superuser if AUTO_LOGIN is enabled.\n\n    Args:\n        var_1 (str, optional): Logging level. Defaults to \"error\".\n\n    Returns:\n        None\n    \"\"\"\n    configure(var_1=var_1)\n\n    async def aapi_key():\n        await initialize_services()\n        var_2 = get_settings_service()\n        var_3 = var_2.var_3\n        if not var_3.AUTO_LOGIN:\n            typer.echo(\"Auto login is disabled. API keys cannot be created through the CLI.\")\n",
      "lines_processed": 19,
      "total_lines": 672
    },
    {
      "file_path": "__main__.py",
      "code": "def run_on_mac_or_linux(host, port, log_level, options, app, protocol):\n    webapp_process = Process(target=run_langflow, args=(host, port, log_level, options, app))\n    webapp_process.start()\n    wait_for_server_ready(host, port, protocol)\n\n    print_banner(host, port, protocol)\n    return webapp_process\n",
      "variables": [
        "host",
        "port",
        "log_level",
        "options",
        "app",
        "protocol",
        "webapp_process"
      ],
      "anonymized_code": "def run_on_mac_or_linux(var_1, var_2, var_3, var_4, var_5, var_6):\n    var_7 = Process(target=run_langflow, args=(var_1, var_2, var_3, var_4, var_5))\n    var_7.start()\n    wait_for_server_ready(var_1, var_2, var_6)\n\n    print_banner(var_1, var_2, var_6)\n    return var_7\n",
      "lines_processed": 7,
      "total_lines": 672
    }
  ],
  "pallets_flask": [
    {
      "file_path": "auth.py",
      "code": "def login():\n    \"\"\"Log in a registered user by adding the user id to the session.\"\"\"\n    if request.method == \"POST\":\n        username = request.form[\"username\"]\n        password = request.form[\"password\"]\n        db = get_db()\n        error = None\n        user = db.execute(\n            \"SELECT * FROM user WHERE username = ?\", (username,)\n        ).fetchone()\n\n        if user is None:\n            error = \"Incorrect username.\"\n        elif not check_password_hash(user[\"password\"], password):\n            error = \"Incorrect password.\"\n\n        if error is None:\n            # store the user id in a new session and return to the index\n            session.clear()\n",
      "variables": [
        "username",
        "password",
        "db",
        "error",
        "user"
      ],
      "anonymized_code": "def login():\n    \"\"\"Log in a registered var_5 by adding the var_5 id to the session.\"\"\"\n    if request.method == \"POST\":\n        var_1 = request.form[\"var_1\"]\n        var_2 = request.form[\"var_2\"]\n        var_3 = get_db()\n        var_4 = None\n        var_5 = var_3.execute(\n            \"SELECT * FROM var_5 WHERE var_1 = ?\", (var_1,)\n        ).fetchone()\n\n        if var_5 is None:\n            var_4 = \"Incorrect var_1.\"\n        elif not check_password_hash(var_5[\"var_2\"], var_2):\n            var_4 = \"Incorrect var_2.\"\n\n        if var_4 is None:\n            # store the var_5 id in a new session and return to the index\n            session.clear()\n",
      "lines_processed": 19,
      "total_lines": 116
    },
    {
      "file_path": "auth.py",
      "code": "def login_required(view):\n    \"\"\"View decorator that redirects anonymous users to the login page.\"\"\"\n\n    @functools.wraps(view)\n    def wrapped_view(**kwargs):\n        if g.user is None:\n            return redirect(url_for(\"auth.login\"))\n\n        return view(**kwargs)\n\n    return wrapped_view\n",
      "variables": [
        "view",
        "kwargs"
      ],
      "anonymized_code": "def login_required(var_1):\n    \"\"\"View decorator that redirects anonymous users to the login page.\"\"\"\n\n    @functools.wraps(var_1)\n    def wrapped_view(**var_2):\n        if g.user is None:\n            return redirect(url_for(\"auth.login\"))\n\n        return var_1(**var_2)\n\n    return wrapped_view\n",
      "lines_processed": 11,
      "total_lines": 116
    },
    {
      "file_path": "auth.py",
      "code": "def load_logged_in_user():\n    \"\"\"If a user id is stored in the session, load the user object from\n    the database into ``g.user``.\"\"\"\n    user_id = session.get(\"user_id\")\n\n    if user_id is None:\n        g.user = None\n    else:\n        g.user = (\n            get_db().execute(\"SELECT * FROM user WHERE id = ?\", (user_id,)).fetchone()\n        )\n",
      "variables": [
        "user_id"
      ],
      "anonymized_code": "def load_logged_in_user():\n    \"\"\"If a user id is stored in the session, load the user object from\n    the database into ``g.user``.\"\"\"\n    var_1 = session.get(\"var_1\")\n\n    if var_1 is None:\n        g.user = None\n    else:\n        g.user = (\n            get_db().execute(\"SELECT * FROM user WHERE id = ?\", (var_1,)).fetchone()\n        )\n",
      "lines_processed": 11,
      "total_lines": 116
    },
    {
      "file_path": "db.py",
      "code": "def close_db(e=None):\n    \"\"\"If this request connected to the database, close the\n    connection.\n    \"\"\"\n    db = g.pop(\"db\", None)\n\n    if db is not None:\n        db.close()\n",
      "variables": [
        "e",
        "db"
      ],
      "anonymized_code": "def close_db(var_1=None):\n    \"\"\"If this request connected to the database, close the\n    connection.\n    \"\"\"\n    var_2 = g.pop(\"var_2\", None)\n\n    if var_2 is not None:\n        var_2.close()\n",
      "lines_processed": 8,
      "total_lines": 56
    },
    {
      "file_path": "db.py",
      "code": "def init_app(app):\n    \"\"\"Register database functions with the Flask app. This is called by\n    the application factory.\n    \"\"\"\n    app.teardown_appcontext(close_db)\n    app.cli.add_command(init_db_command)\n",
      "variables": [
        "app"
      ],
      "anonymized_code": "def init_app(var_1):\n    \"\"\"Register database functions with the Flask var_1. This is called by\n    the application factory.\n    \"\"\"\n    var_1.teardown_appcontext(close_db)\n    var_1.cli.add_command(init_db_command)\n",
      "lines_processed": 6,
      "total_lines": 56
    },
    {
      "file_path": "db.py",
      "code": "def init_db():\n    \"\"\"Clear existing data and create new tables.\"\"\"\n    db = get_db()\n\n    with current_app.open_resource(\"schema.sql\") as f:\n        db.executescript(f.read().decode(\"utf8\"))\n",
      "variables": [
        "db",
        "f"
      ],
      "anonymized_code": "def init_db():\n    \"\"\"Clear existing data and create new tables.\"\"\"\n    var_1 = get_db()\n\n    with current_app.open_resource(\"schema.sql\") as var_2:\n        var_1.executescript(var_2.read().decode(\"utf8\"))\n",
      "lines_processed": 6,
      "total_lines": 56
    },
    {
      "file_path": "test_js_example.py",
      "code": "def test_index(app, client, path, template_name):\n    def check(sender, template, context):\n        assert template.name == template_name\n\n    with template_rendered.connected_to(check, app):\n        client.get(path)\n",
      "variables": [
        "app",
        "client",
        "path",
        "template_name",
        "sender",
        "template",
        "context"
      ],
      "anonymized_code": "def test_index(var_1, var_2, var_3, var_4):\n    def check(var_5, var_6, var_7):\n        assert var_6.name == var_4\n\n    with template_rendered.connected_to(check, var_1):\n        var_2.get(var_3)\n",
      "lines_processed": 6,
      "total_lines": 27
    },
    {
      "file_path": "test_js_example.py",
      "code": "def test_add(client, a, b, result):\n    response = client.post(\"/add\", data={\"a\": a, \"b\": b})\n    assert response.get_json()[\"result\"] == result\n",
      "variables": [
        "client",
        "a",
        "b",
        "result",
        "response"
      ],
      "anonymized_code": "def test_add(var_1, var_2, var_3, var_4):\n    var_5 = var_1.post(\"/add\", data={\"var_2\": var_2, \"var_3\": var_3})\n    assert var_5.get_json()[\"var_4\"] == var_4\n",
      "lines_processed": 3,
      "total_lines": 27
    },
    {
      "file_path": "conftest.py",
      "code": "def app():\n    \"\"\"Create and configure a new app instance for each test.\"\"\"\n    # create a temporary file to isolate the database for each test\n    db_fd, db_path = tempfile.mkstemp()\n    # create the app with common test config\n    app = create_app({\"TESTING\": True, \"DATABASE\": db_path})\n\n    # create the database and load test data\n    with app.app_context():\n        init_db()\n        get_db().executescript(_data_sql)\n\n    yield app\n\n    # close and remove the temporary database\n    os.close(db_fd)\n    os.unlink(db_path)\n",
      "variables": [
        "db_fd",
        "db_path",
        "app"
      ],
      "anonymized_code": "def var_3():\n    \"\"\"Create and configure a new var_3 instance for each test.\"\"\"\n    # create a temporary file to isolate the database for each test\n    var_1, var_2 = tempfile.mkstemp()\n    # create the var_3 with common test config\n    var_3 = create_app({\"TESTING\": True, \"DATABASE\": var_2})\n\n    # create the database and load test data\n    with var_3.app_context():\n        init_db()\n        get_db().executescript(_data_sql)\n\n    yield var_3\n\n    # close and remove the temporary database\n    os.close(var_1)\n    os.unlink(var_2)\n",
      "lines_processed": 17,
      "total_lines": 62
    },
    {
      "file_path": "conftest.py",
      "code": "def auth(client):\n    return AuthActions(client)\n",
      "variables": [
        "client"
      ],
      "anonymized_code": "def auth(var_1):\n    return AuthActions(var_1)\n",
      "lines_processed": 2,
      "total_lines": 62
    },
    {
      "file_path": "conftest.py",
      "code": "def runner(app):\n    \"\"\"A test runner for the app's Click commands.\"\"\"\n    return app.test_cli_runner()\n",
      "variables": [
        "app"
      ],
      "anonymized_code": "def runner(var_1):\n    \"\"\"A test runner for the var_1's Click commands.\"\"\"\n    return var_1.test_cli_runner()\n",
      "lines_processed": 3,
      "total_lines": 62
    },
    {
      "file_path": "conftest.py",
      "code": "def client(app):\n    \"\"\"A test client for the app.\"\"\"\n    return app.test_client()\n",
      "variables": [
        "app"
      ],
      "anonymized_code": "def client(var_1):\n    \"\"\"A test client for the var_1.\"\"\"\n    return var_1.test_client()\n",
      "lines_processed": 3,
      "total_lines": 62
    },
    {
      "file_path": "tasks.py",
      "code": "def add(a: int, b: int) -> int:\n    return a + b\n",
      "variables": [
        "a",
        "b"
      ],
      "anonymized_code": "def add(var_1: int, var_2: int) -> int:\n    return var_1 + var_2\n",
      "lines_processed": 2,
      "total_lines": 23
    },
    {
      "file_path": "tasks.py",
      "code": "def process(self: Task, total: int) -> object:\n    for i in range(total):\n        self.update_state(state=\"PROGRESS\", meta={\"current\": i + 1, \"total\": total})\n        time.sleep(1)\n\n    return {\"current\": total, \"total\": total}\n",
      "variables": [
        "self",
        "total",
        "i"
      ],
      "anonymized_code": "def process(var_1: Task, var_2: int) -> object:\n    for var_3 in range(var_2):\n        var_1.update_state(state=\"PROGRESS\", meta={\"current\": var_3 + 1, \"var_2\": var_2})\n        time.sleep(1)\n\n    return {\"current\": var_2, \"var_2\": var_2}\n",
      "lines_processed": 6,
      "total_lines": 23
    },
    {
      "file_path": "test_auth.py",
      "code": "def test_register(client, app):\n    # test that viewing the page renders without template errors\n    assert client.get(\"/auth/register\").status_code == 200\n\n    # test that successful registration redirects to the login page\n    response = client.post(\"/auth/register\", data={\"username\": \"a\", \"password\": \"a\"})\n    assert response.headers[\"Location\"] == \"/auth/login\"\n\n    # test that the user was inserted into the database\n    with app.app_context():\n        assert (\n            get_db().execute(\"SELECT * FROM user WHERE username = 'a'\").fetchone()\n            is not None\n        )\n",
      "variables": [
        "client",
        "app",
        "response"
      ],
      "anonymized_code": "def test_register(var_1, var_2):\n    # test that viewing the page renders without template errors\n    assert var_1.get(\"/auth/register\").status_code == 200\n\n    # test that successful registration redirects to the login page\n    var_3 = var_1.post(\"/auth/register\", data={\"username\": \"a\", \"password\": \"a\"})\n    assert var_3.headers[\"Location\"] == \"/auth/login\"\n\n    # test that the user was inserted into the database\n    with var_2.app_context():\n        assert (\n            get_db().execute(\"SELECT * FROM user WHERE username = 'a'\").fetchone()\n            is not None\n        )\n",
      "lines_processed": 14,
      "total_lines": 69
    },
    {
      "file_path": "test_auth.py",
      "code": "def test_login(client, auth):\n    # test that viewing the page renders without template errors\n    assert client.get(\"/auth/login\").status_code == 200\n\n    # test that successful login redirects to the index page\n    response = auth.login()\n    assert response.headers[\"Location\"] == \"/\"\n\n    # login request set the user_id in the session\n    # check that the user is loaded from the session\n    with client:\n        client.get(\"/\")\n        assert session[\"user_id\"] == 1\n        assert g.user[\"username\"] == \"test\"\n",
      "variables": [
        "client",
        "auth",
        "response"
      ],
      "anonymized_code": "def test_login(var_1, var_2):\n    # test that viewing the page renders without template errors\n    assert var_1.get(\"/var_2/login\").status_code == 200\n\n    # test that successful login redirects to the index page\n    var_3 = var_2.login()\n    assert var_3.headers[\"Location\"] == \"/\"\n\n    # login request set the user_id in the session\n    # check that the user is loaded from the session\n    with var_1:\n        var_1.get(\"/\")\n        assert session[\"user_id\"] == 1\n        assert g.user[\"username\"] == \"test\"\n",
      "lines_processed": 14,
      "total_lines": 69
    },
    {
      "file_path": "test_auth.py",
      "code": "def test_login_validate_input(auth, username, password, message):\n    response = auth.login(username, password)\n    assert message in response.data\n",
      "variables": [
        "auth",
        "username",
        "password",
        "message",
        "response"
      ],
      "anonymized_code": "def test_login_validate_input(var_1, var_2, var_3, var_4):\n    var_5 = var_1.login(var_2, var_3)\n    assert var_4 in var_5.data\n",
      "lines_processed": 3,
      "total_lines": 69
    },
    {
      "file_path": "test_auth.py",
      "code": "def test_register_validate_input(client, username, password, message):\n    response = client.post(\n        \"/auth/register\", data={\"username\": username, \"password\": password}\n    )\n    assert message in response.data\n",
      "variables": [
        "client",
        "username",
        "password",
        "message",
        "response"
      ],
      "anonymized_code": "def test_register_validate_input(var_1, var_2, var_3, var_4):\n    var_5 = var_1.post(\n        \"/auth/register\", data={\"var_2\": var_2, \"var_3\": var_3}\n    )\n    assert var_4 in var_5.data\n",
      "lines_processed": 5,
      "total_lines": 69
    },
    {
      "file_path": "test_auth.py",
      "code": "def test_logout(client, auth):\n    auth.login()\n\n    with client:\n        auth.logout()\n        assert \"user_id\" not in session\n",
      "variables": [
        "client",
        "auth"
      ],
      "anonymized_code": "def test_logout(var_1, var_2):\n    var_2.login()\n\n    with var_1:\n        var_2.logout()\n        assert \"user_id\" not in session\n",
      "lines_processed": 6,
      "total_lines": 69
    },
    {
      "file_path": "conf.py",
      "code": "def setup(app):\n    app.add_role(\"gh\", github_link)\n",
      "variables": [
        "app"
      ],
      "anonymized_code": "def setup(var_1):\n    var_1.add_role(\"gh\", github_link)\n",
      "lines_processed": 2,
      "total_lines": 101
    },
    {
      "file_path": "conf.py",
      "code": "def github_link(name, rawtext, text, lineno, inliner, options=None, content=None):\n    app = inliner.document.settings.env.app\n    release = app.config.release\n    base_url = \"https://github.com/pallets/flask/tree/\"\n\n    if text.endswith(\">\"):\n        words, text = text[:-1].rsplit(\"<\", 1)\n        words = words.strip()\n    else:\n        words = None\n\n    if packaging.version.parse(release).is_devrelease:\n        url = f\"{base_url}main/{text}\"\n    else:\n        url = f\"{base_url}{release}/{text}\"\n\n    if words is None:\n        words = url\n\n",
      "variables": [
        "name",
        "rawtext",
        "text",
        "lineno",
        "inliner",
        "options",
        "content",
        "app",
        "release",
        "base_url",
        "words",
        "url"
      ],
      "anonymized_code": "def github_link(var_1, var_2, var_3, var_4, var_5, var_6=None, var_7=None):\n    var_8 = var_5.document.settings.env.var_8\n    var_9 = var_8.config.var_9\n    var_10 = \"https://github.com/pallets/flask/tree/\"\n\n    if var_3.endswith(\">\"):\n        var_11, var_3 = var_3[:-1].rsplit(\"<\", 1)\n        var_11 = var_11.strip()\n    else:\n        var_11 = None\n\n    if packaging.version.parse(var_9).is_devrelease:\n        var_12 = f\"{var_10}main/{var_3}\"\n    else:\n        var_12 = f\"{var_10}{var_9}/{var_3}\"\n\n    if var_11 is None:\n        var_11 = var_12\n\n",
      "lines_processed": 19,
      "total_lines": 101
    },
    {
      "file_path": "blog.py",
      "code": "def update(id):\n    \"\"\"Update a post if the current user is the author.\"\"\"\n    post = get_post(id)\n\n    if request.method == \"POST\":\n        title = request.form[\"title\"]\n        body = request.form[\"body\"]\n        error = None\n\n        if not title:\n            error = \"Title is required.\"\n\n        if error is not None:\n            flash(error)\n        else:\n            db = get_db()\n            db.execute(\n                \"UPDATE post SET title = ?, body = ? WHERE id = ?\", (title, body, id)\n            )\n",
      "variables": [
        "id",
        "post",
        "title",
        "body",
        "error",
        "db"
      ],
      "anonymized_code": "def update(var_1):\n    \"\"\"Update a var_2 if the current user is the author.\"\"\"\n    var_2 = get_post(var_1)\n\n    if request.method == \"POST\":\n        var_3 = request.form[\"var_3\"]\n        var_4 = request.form[\"var_4\"]\n        var_5 = None\n\n        if not var_3:\n            var_5 = \"Title is required.\"\n\n        if var_5 is not None:\n            flash(var_5)\n        else:\n            var_6 = get_db()\n            var_6.execute(\n                \"UPDATE var_2 SET var_3 = ?, var_4 = ? WHERE var_1 = ?\", (var_3, var_4, var_1)\n            )\n",
      "lines_processed": 19,
      "total_lines": 125
    },
    {
      "file_path": "blog.py",
      "code": "def delete(id):\n    \"\"\"Delete a post.\n\n    Ensures that the post exists and that the logged in user is the\n    author of the post.\n    \"\"\"\n    get_post(id)\n    db = get_db()\n    db.execute(\"DELETE FROM post WHERE id = ?\", (id,))\n    db.commit()\n    return redirect(url_for(\"blog.index\"))\n",
      "variables": [
        "id",
        "db"
      ],
      "anonymized_code": "def delete(var_1):\n    \"\"\"Delete a post.\n\n    Ensures that the post exists and that the logged in user is the\n    author of the post.\n    \"\"\"\n    get_post(var_1)\n    var_2 = get_db()\n    var_2.execute(\"DELETE FROM post WHERE var_1 = ?\", (var_1,))\n    var_2.commit()\n    return redirect(url_for(\"blog.index\"))\n",
      "lines_processed": 11,
      "total_lines": 125
    },
    {
      "file_path": "blog.py",
      "code": "def index():\n    \"\"\"Show all the posts, most recent first.\"\"\"\n    db = get_db()\n    posts = db.execute(\n        \"SELECT p.id, title, body, created, author_id, username\"\n        \" FROM post p JOIN user u ON p.author_id = u.id\"\n        \" ORDER BY created DESC\"\n    ).fetchall()\n    return render_template(\"blog/index.html\", posts=posts)\n",
      "variables": [
        "db",
        "posts"
      ],
      "anonymized_code": "def index():\n    \"\"\"Show all the var_2, most recent first.\"\"\"\n    var_1 = get_db()\n    var_2 = var_1.execute(\n        \"SELECT p.id, title, body, created, author_id, username\"\n        \" FROM post p JOIN user u ON p.author_id = u.id\"\n        \" ORDER BY created DESC\"\n    ).fetchall()\n    return render_template(\"blog/index.html\", var_2=var_2)\n",
      "lines_processed": 9,
      "total_lines": 125
    },
    {
      "file_path": "blog.py",
      "code": "def create():\n    \"\"\"Create a new post for the current user.\"\"\"\n    if request.method == \"POST\":\n        title = request.form[\"title\"]\n        body = request.form[\"body\"]\n        error = None\n\n        if not title:\n            error = \"Title is required.\"\n\n        if error is not None:\n            flash(error)\n        else:\n            db = get_db()\n            db.execute(\n                \"INSERT INTO post (title, body, author_id) VALUES (?, ?, ?)\",\n                (title, body, g.user[\"id\"]),\n            )\n            db.commit()\n",
      "variables": [
        "title",
        "body",
        "error",
        "db"
      ],
      "anonymized_code": "def create():\n    \"\"\"Create a new post for the current user.\"\"\"\n    if request.method == \"POST\":\n        var_1 = request.form[\"var_1\"]\n        var_2 = request.form[\"var_2\"]\n        var_3 = None\n\n        if not var_1:\n            var_3 = \"Title is required.\"\n\n        if var_3 is not None:\n            flash(var_3)\n        else:\n            var_4 = get_db()\n            var_4.execute(\n                \"INSERT INTO post (var_1, var_2, author_id) VALUES (?, ?, ?)\",\n                (var_1, var_2, g.user[\"id\"]),\n            )\n            var_4.commit()\n",
      "lines_processed": 19,
      "total_lines": 125
    },
    {
      "file_path": "views.py",
      "code": "def index(js):\n    return render_template(f\"{js}.html\", js=js)\n",
      "variables": [
        "js"
      ],
      "anonymized_code": "def index(var_1):\n    return render_template(f\"{var_1}.html\", var_1=var_1)\n",
      "lines_processed": 2,
      "total_lines": 18
    },
    {
      "file_path": "views.py",
      "code": "def add():\n    a = request.form.get(\"a\", 0, type=float)\n    b = request.form.get(\"b\", 0, type=float)\n    return jsonify(result=a + b)\n",
      "variables": [
        "a",
        "b"
      ],
      "anonymized_code": "def add():\n    var_1 = request.form.get(\"var_1\", 0, type=float)\n    var_2 = request.form.get(\"var_2\", 0, type=float)\n    return jsonify(result=var_1 + var_2)\n",
      "lines_processed": 4,
      "total_lines": 18
    }
  ],
  "psf_black": [
    {
      "file_path": "conf.py",
      "code": "def replace_pr_numbers_with_links(content: str) -> str:\n    \"\"\"Replaces all PR numbers with the corresponding GitHub link.\"\"\"\n    return re.sub(r\"#(\\d+)\", r\"[#\\1](https://github.com/psf/black/pull/\\1)\", content)\n",
      "variables": [
        "content"
      ],
      "anonymized_code": "def replace_pr_numbers_with_links(var_1: str) -> str:\n    \"\"\"Replaces all PR numbers with the corresponding GitHub link.\"\"\"\n    return re.sub(r\"#(\\d+)\", r\"[#\\1](https://github.com/psf/black/pull/\\1)\", var_1)\n",
      "lines_processed": 3,
      "total_lines": 241
    },
    {
      "file_path": "conf.py",
      "code": "def setup(app: Sphinx) -> None:\n    \"\"\"Sets up a minimal sphinx extension.\"\"\"\n    app.connect(\"include-read\", handle_include_read)\n",
      "variables": [
        "app"
      ],
      "anonymized_code": "def setup(var_1: Sphinx) -> None:\n    \"\"\"Sets up a minimal sphinx extension.\"\"\"\n    var_1.connect(\"include-read\", handle_include_read)\n",
      "lines_processed": 3,
      "total_lines": 241
    },
    {
      "file_path": "conf.py",
      "code": "def handle_include_read(\n    app: Sphinx,\n    relative_path: Path,\n    parent_docname: str,\n    content: list[str],\n) -> None:\n    \"\"\"Handler for the include-read sphinx event.\"\"\"\n    if parent_docname == \"change_log\":\n        content[0] = replace_pr_numbers_with_links(content[0])\n",
      "variables": [
        "app",
        "relative_path",
        "parent_docname",
        "content"
      ],
      "anonymized_code": "def handle_include_read(\n    var_1: Sphinx,\n    var_2: Path,\n    var_3: str,\n    var_4: list[str],\n) -> None:\n    \"\"\"Handler for the include-read sphinx event.\"\"\"\n    if var_3 == \"change_log\":\n        var_4[0] = replace_pr_numbers_with_links(var_4[0])\n",
      "lines_processed": 9,
      "total_lines": 241
    },
    {
      "file_path": "conf.py",
      "code": "def make_pypi_svg(version: str) -> None:\n    template: Path = CURRENT_DIR / \"_static\" / \"pypi_template.svg\"\n    target: Path = CURRENT_DIR / \"_static\" / \"pypi.svg\"\n    with open(str(template), encoding=\"utf8\") as f:\n        svg: str = string.Template(f.read()).substitute(version=version)\n    with open(str(target), \"w\", encoding=\"utf8\") as f:\n        f.write(svg)\n",
      "variables": [
        "version",
        "template",
        "target",
        "f",
        "svg"
      ],
      "anonymized_code": "def make_pypi_svg(var_1: str) -> None:\n    var_2: Path = CURRENT_DIR / \"_static\" / \"pypi_template.var_5\"\n    var_3: Path = CURRENT_DIR / \"_static\" / \"pypi.var_5\"\n    with open(str(var_2), encoding=\"utf8\") as var_4:\n        var_5: str = string.Template(var_4.read()).substitute(var_1=var_1)\n    with open(str(var_3), \"w\", encoding=\"utf8\") as var_4:\n        var_4.write(var_5)\n",
      "lines_processed": 7,
      "total_lines": 241
    },
    {
      "file_path": "gallery.py",
      "code": "def black_runner(version: str, black_repo: Path) -> Path:\n    directory = tempfile.TemporaryDirectory()\n    venv.create(directory.name, with_pip=True)\n\n    python = Path(directory.name) / \"bin\" / \"python\"\n    subprocess.run([python, \"-m\", \"pip\", \"install\", \"-e\", black_repo])\n\n    atexit.register(directory.cleanup)\n    return python\n",
      "variables": [
        "version",
        "black_repo",
        "directory",
        "python"
      ],
      "anonymized_code": "def black_runner(var_1: str, var_2: Path) -> Path:\n    var_3 = tempfile.TemporaryDirectory()\n    venv.create(var_3.name, with_pip=True)\n\n    var_4 = Path(var_3.name) / \"bin\" / \"var_4\"\n    subprocess.run([var_4, \"-m\", \"pip\", \"install\", \"-e\", var_2])\n\n    atexit.register(var_3.cleanup)\n    return var_4\n",
      "lines_processed": 9,
      "total_lines": 295
    },
    {
      "file_path": "gallery.py",
      "code": "def download_and_extract_top_packages(\n    directory: Path,\n    workers: int = 8,\n    limit: slice = DEFAULT_SLICE,\n) -> Generator[Path, None, None]:\n    with ThreadPoolExecutor(max_workers=workers) as executor:\n        bound_downloader = partial(get_package, version=None, directory=directory)\n        for package in executor.map(bound_downloader, get_top_packages()[limit]):\n            if package is not None:\n                yield package\n",
      "variables": [
        "directory",
        "workers",
        "limit",
        "executor",
        "bound_downloader",
        "package"
      ],
      "anonymized_code": "def download_and_extract_top_packages(\n    var_1: Path,\n    var_2: int = 8,\n    var_3: slice = DEFAULT_SLICE,\n) -> Generator[Path, None, None]:\n    with ThreadPoolExecutor(max_workers=var_2) as var_4:\n        var_5 = partial(get_package, version=None, var_1=var_1)\n        for var_6 in var_4.map(var_5, get_top_packages()[var_3]):\n            if var_6 is not None:\n                yield var_6\n",
      "lines_processed": 10,
      "total_lines": 295
    },
    {
      "file_path": "gallery.py",
      "code": "def get_archive_manager(local_file: str) -> ArchiveKind:\n    if tarfile.is_tarfile(local_file):\n        return tarfile.open(local_file)\n    elif zipfile.is_zipfile(local_file):\n        return zipfile.ZipFile(local_file)\n    else:\n        raise ValueError(\"Unknown archive kind.\")\n",
      "variables": [
        "local_file"
      ],
      "anonymized_code": "def get_archive_manager(var_1: str) -> ArchiveKind:\n    if tarfile.is_tarfile(var_1):\n        return tarfile.open(var_1)\n    elif zipfile.is_zipfile(var_1):\n        return zipfile.ZipFile(var_1)\n    else:\n        raise ValueError(\"Unknown archive kind.\")\n",
      "lines_processed": 7,
      "total_lines": 295
    },
    {
      "file_path": "gallery.py",
      "code": "def get_top_packages() -> list[str]:\n    with urlopen(PYPI_TOP_PACKAGES) as page:\n        result = json.load(page)\n\n    return [package[\"project\"] for package in result[\"rows\"]]\n",
      "variables": [
        "page",
        "result",
        "package"
      ],
      "anonymized_code": "def get_top_packages() -> list[str]:\n    with urlopen(PYPI_TOP_PACKAGES) as var_1:\n        var_2 = json.load(var_1)\n\n    return [var_3[\"project\"] for var_3 in var_2[\"rows\"]]\n",
      "lines_processed": 5,
      "total_lines": 295
    },
    {
      "file_path": "gallery.py",
      "code": "def git_add_and_commit(msg: str, repo: Path) -> None:\n    subprocess.run([\"git\", \"add\", \".\"], cwd=repo)\n    subprocess.run([\"git\", \"commit\", \"-m\", msg, \"--allow-empty\"], cwd=repo)\n",
      "variables": [
        "msg",
        "repo"
      ],
      "anonymized_code": "def git_add_and_commit(var_1: str, var_2: Path) -> None:\n    subprocess.run([\"git\", \"add\", \".\"], cwd=var_2)\n    subprocess.run([\"git\", \"commit\", \"-m\", var_1, \"--allow-empty\"], cwd=var_2)\n",
      "lines_processed": 3,
      "total_lines": 295
    },
    {
      "file_path": "gallery.py",
      "code": "def git_switch_branch(\n    branch: str, repo: Path, new: bool = False, from_branch: Optional[str] = None\n) -> None:\n    args = [\"git\", \"checkout\"]\n    if new:\n        args.append(\"-b\")\n    args.append(branch)\n    if from_branch:\n        args.append(from_branch)\n    subprocess.run(args, cwd=repo)\n",
      "variables": [
        "branch",
        "repo",
        "new",
        "from_branch",
        "args"
      ],
      "anonymized_code": "def git_switch_branch(\n    var_1: str, var_2: Path, var_3: bool = False, var_4: Optional[str] = None\n) -> None:\n    var_5 = [\"git\", \"checkout\"]\n    if var_3:\n        var_5.append(\"-b\")\n    var_5.append(var_1)\n    if var_4:\n        var_5.append(var_4)\n    subprocess.run(var_5, cwd=var_2)\n",
      "lines_processed": 10,
      "total_lines": 295
    },
    {
      "file_path": "gallery.py",
      "code": "def get_package_source(package: str, version: Optional[str]) -> str:\n    if package == \"cpython\":\n        if version is None:\n            version = \"main\"\n        return f\"https://github.com/python/cpython/archive/{version}.zip\"\n    elif package == \"pypy\":\n        if version is None:\n            version = \"branch/default\"\n        return (\n            f\"https://foss.heptapod.net/pypy/pypy/repository/{version}/archive.tar.bz2\"\n        )\n    else:\n        return get_pypi_download_url(package, version)\n",
      "variables": [
        "package",
        "version"
      ],
      "anonymized_code": "def get_package_source(var_1: str, var_2: Optional[str]) -> str:\n    if var_1 == \"cpython\":\n        if var_2 is None:\n            var_2 = \"main\"\n        return f\"https://github.com/python/cpython/archive/{var_2}.zip\"\n    elif var_1 == \"pypy\":\n        if var_2 is None:\n            var_2 = \"branch/default\"\n        return (\n            f\"https://foss.heptapod.net/pypy/pypy/repository/{var_2}/archive.tar.bz2\"\n        )\n    else:\n        return get_pypi_download_url(var_1, var_2)\n",
      "lines_processed": 13,
      "total_lines": 295
    },
    {
      "file_path": "gallery.py",
      "code": "def download_and_extract(package: str, version: Optional[str], directory: Path) -> Path:\n    source = get_package_source(package, version)\n\n    local_file, _ = urlretrieve(source, directory / f\"{package}-src\")\n    with get_archive_manager(local_file) as archive:\n        archive.extractall(path=directory)\n        result_dir = get_first_archive_member(archive)\n    return directory / result_dir\n",
      "variables": [
        "package",
        "version",
        "directory",
        "source",
        "local_file",
        "_",
        "archive",
        "result_dir"
      ],
      "anonymized_code": "def download_and_extract(var_1: str, var_2: Optional[str], var_3: Path) -> Path:\n    var_4 = get_package_source(var_1, var_2)\n\n    var_5, var_6 = urlretrieve(var_4, var_3 / f\"{var_1}-src\")\n    with get_archive_manager(var_5) as var_7:\n        var_7.extractall(path=var_3)\n        var_8 = get_first_archive_member(var_7)\n    return var_3 / var_8\n",
      "lines_processed": 8,
      "total_lines": 295
    },
    {
      "file_path": "gallery.py",
      "code": "def git_create_repository(repo: Path) -> None:\n    subprocess.run([\"git\", \"init\"], cwd=repo)\n    git_add_and_commit(msg=\"Initial commit\", repo=repo)\n",
      "variables": [
        "repo"
      ],
      "anonymized_code": "def git_create_repository(var_1: Path) -> None:\n    subprocess.run([\"git\", \"init\"], cwd=var_1)\n    git_add_and_commit(msg=\"Initial commit\", var_1=var_1)\n",
      "lines_processed": 3,
      "total_lines": 295
    },
    {
      "file_path": "fuzz.py",
      "code": "def test_idempotent_any_syntatically_valid_python(\n    src_contents: str, mode: black.FileMode\n) -> None:\n    # Before starting, let's confirm that the input string is valid Python:\n    compile(src_contents, \"<string>\", \"exec\")  # else the bug is in hypothesmith\n\n    # Then format the code...\n    dst_contents = black.format_str(src_contents, mode=mode)\n\n    # And check that we got equivalent and stable output.\n    black.assert_equivalent(src_contents, dst_contents)\n    black.assert_stable(src_contents, dst_contents, mode=mode)\n",
      "variables": [
        "src_contents",
        "mode",
        "dst_contents"
      ],
      "anonymized_code": "def test_idempotent_any_syntatically_valid_python(\n    var_1: str, var_2: black.FileMode\n) -> None:\n    # Before starting, let's confirm that the input string is valid Python:\n    compile(var_1, \"<string>\", \"exec\")  # else the bug is in hypothesmith\n\n    # Then format the code...\n    var_3 = black.format_str(var_1, var_2=var_2)\n\n    # And check that we got equivalent and stable output.\n    black.assert_equivalent(var_1, var_3)\n    black.assert_stable(var_1, var_3, var_2=var_2)\n",
      "lines_processed": 12,
      "total_lines": 73
    },
    {
      "file_path": "diff_shades_gha_helper.py",
      "code": "def get_pypi_version() -> Version:\n    data = http_get(\"https://pypi.org/pypi/black/json\")\n    versions = [Version(v) for v in data[\"releases\"]]\n    sorted_versions = sorted(versions, reverse=True)\n    return sorted_versions[0]\n",
      "variables": [
        "data",
        "versions",
        "v",
        "sorted_versions"
      ],
      "anonymized_code": "def get_pypi_version() -> Version:\n    var_1 = http_get(\"https://pypi.org/pypi/black/json\")\n    var_2 = [Version(var_3) for var_3 in var_1[\"releases\"]]\n    var_4 = sorted(var_2, reverse=True)\n    return var_4[0]\n",
      "lines_processed": 5,
      "total_lines": 231
    },
    {
      "file_path": "diff_shades_gha_helper.py",
      "code": "def get_main_revision() -> str:\n    data = http_get(\n        f\"https://api.github.com/repos/{REPO}/commits\",\n        fields={\"per_page\": \"1\", \"sha\": \"main\"},\n    )\n    assert isinstance(data[0][\"sha\"], str)\n    return data[0][\"sha\"]\n",
      "variables": [
        "data"
      ],
      "anonymized_code": "def get_main_revision() -> str:\n    var_1 = http_get(\n        f\"https://api.github.com/repos/{REPO}/commits\",\n        fields={\"per_page\": \"1\", \"sha\": \"main\"},\n    )\n    assert isinstance(var_1[0][\"sha\"], str)\n    return var_1[0][\"sha\"]\n",
      "lines_processed": 7,
      "total_lines": 231
    },
    {
      "file_path": "diff_shades_gha_helper.py",
      "code": "def comment_details(run_id: str) -> None:\n    data = http_get(f\"https://api.github.com/repos/{REPO}/actions/runs/{run_id}\")\n    if data[\"event\"] != \"pull_request\" or data[\"conclusion\"] == \"cancelled\":\n        set_output(\"needs-comment\", \"false\")\n        return\n\n    set_output(\"needs-comment\", \"true\")\n    jobs = http_get(data[\"jobs_url\"])[\"jobs\"]\n    job = next(j for j in jobs if j[\"name\"] == \"analysis / preview-changes\")\n    diff_step = next(s for s in job[\"steps\"] if s[\"name\"] == DIFF_STEP_NAME)\n    diff_url = job[\"html_url\"] + f\"#step:{diff_step['number']}:1\"\n\n    artifacts = http_get(data[\"artifacts_url\"])[\"artifacts\"]\n    comment_artifact = next(a for a in artifacts if a[\"name\"] == COMMENT_FILE)\n    comment_url = comment_artifact[\"archive_download_url\"]\n    comment_zip = BytesIO(http_get(comment_url, is_json=False))\n    with zipfile.ZipFile(comment_zip) as zfile:\n        with zfile.open(COMMENT_FILE) as rf:\n            comment_data = json.loads(rf.read().decode(\"utf-8\"))\n",
      "variables": [
        "run_id",
        "data",
        "jobs",
        "job",
        "j",
        "diff_step",
        "s",
        "diff_url",
        "artifacts",
        "comment_artifact",
        "a",
        "comment_url",
        "comment_zip",
        "zfile",
        "rf",
        "comment_data"
      ],
      "anonymized_code": "def comment_details(var_1: str) -> None:\n    var_2 = http_get(f\"https://api.github.com/repos/{REPO}/actions/runs/{var_1}\")\n    if var_2[\"event\"] != \"pull_request\" or var_2[\"conclusion\"] == \"cancelled\":\n        set_output(\"needs-comment\", \"false\")\n        return\n\n    set_output(\"needs-comment\", \"true\")\n    var_3 = http_get(var_2[\"jobs_url\"])[\"var_3\"]\n    var_4 = next(var_5 for var_5 in var_3 if var_5[\"name\"] == \"analysis / preview-changes\")\n    var_6 = next(var_7 for var_7 in var_4[\"steps\"] if var_7[\"name\"] == DIFF_STEP_NAME)\n    var_8 = var_4[\"html_url\"] + f\"#step:{var_6['number']}:1\"\n\n    var_9 = http_get(var_2[\"artifacts_url\"])[\"var_9\"]\n    var_10 = next(var_11 for var_11 in var_9 if var_11[\"name\"] == COMMENT_FILE)\n    var_12 = var_10[\"archive_download_url\"]\n    var_13 = BytesIO(http_get(var_12, is_json=False))\n    with zipfile.ZipFile(var_13) as var_14:\n        with var_14.open(COMMENT_FILE) as var_15:\n            var_16 = json.loads(var_15.read().decode(\"utf-8\"))\n",
      "lines_processed": 19,
      "total_lines": 231
    },
    {
      "file_path": "diff_shades_gha_helper.py",
      "code": "def get_pr_revision(pr: int) -> str:\n    data = http_get(f\"https://api.github.com/repos/{REPO}/pulls/{pr}\")\n    assert isinstance(data[\"head\"][\"sha\"], str)\n    return data[\"head\"][\"sha\"]\n",
      "variables": [
        "pr",
        "data"
      ],
      "anonymized_code": "def get_pr_revision(var_1: int) -> str:\n    var_2 = http_get(f\"https://api.github.com/repos/{REPO}/pulls/{var_1}\")\n    assert isinstance(var_2[\"head\"][\"sha\"], str)\n    return var_2[\"head\"][\"sha\"]\n",
      "lines_processed": 4,
      "total_lines": 231
    },
    {
      "file_path": "diff_shades_gha_helper.py",
      "code": "def config(event: Literal[\"push\", \"pull_request\"]) -> None:\n    import diff_shades  # type: ignore[import-not-found]\n\n    if event == \"push\":\n        jobs = [{\"mode\": \"preview-changes\", \"force-flag\": \"--force-preview-style\"}]\n        # Push on main, let's use PyPI Black as the baseline.\n        baseline_name = str(get_pypi_version())\n        baseline_cmd = f\"git checkout {baseline_name}\"\n        target_rev = os.getenv(\"GITHUB_SHA\")\n        assert target_rev is not None\n        target_name = \"main-\" + target_rev[:SHA_LENGTH]\n        target_cmd = f\"git checkout {target_rev}\"\n\n    elif event == \"pull_request\":\n        jobs = [\n            {\"mode\": \"preview-changes\", \"force-flag\": \"--force-preview-style\"},\n            {\"mode\": \"assert-no-changes\", \"force-flag\": \"--force-stable-style\"},\n        ]\n        # PR, let's use main as the baseline.\n",
      "variables": [
        "event",
        "jobs",
        "baseline_name",
        "baseline_cmd",
        "target_rev",
        "target_name",
        "target_cmd"
      ],
      "anonymized_code": "def config(var_1: Literal[\"push\", \"pull_request\"]) -> None:\n    import diff_shades  # type: ignore[import-not-found]\n\n    if var_1 == \"push\":\n        var_2 = [{\"mode\": \"preview-changes\", \"force-flag\": \"--force-preview-style\"}]\n        # Push on main, let's use PyPI Black as the baseline.\n        var_3 = str(get_pypi_version())\n        var_4 = f\"git checkout {var_3}\"\n        var_5 = os.getenv(\"GITHUB_SHA\")\n        assert var_5 is not None\n        var_6 = \"main-\" + var_5[:SHA_LENGTH]\n        var_7 = f\"git checkout {var_5}\"\n\n    elif var_1 == \"pull_request\":\n        var_2 = [\n            {\"mode\": \"preview-changes\", \"force-flag\": \"--force-preview-style\"},\n            {\"mode\": \"assert-no-changes\", \"force-flag\": \"--force-stable-style\"},\n        ]\n        # PR, let's use main as the baseline.\n",
      "lines_processed": 19,
      "total_lines": 231
    },
    {
      "file_path": "diff_shades_gha_helper.py",
      "code": "def http_get(url: str, *, is_json: bool = True, **kwargs: Any) -> Any:\n    headers = kwargs.get(\"headers\") or {}\n    headers[\"User-Agent\"] = USER_AGENT\n    if \"github\" in url:\n        if GH_API_TOKEN:\n            headers[\"Authorization\"] = f\"token {GH_API_TOKEN}\"\n        headers[\"Accept\"] = \"application/vnd.github.v3+json\"\n    kwargs[\"headers\"] = headers\n\n    r = http.request(\"GET\", url, **kwargs)\n    if is_json:\n        data = json.loads(r.data.decode(\"utf-8\"))\n    else:\n        data = r.data\n    print(f\"[INFO]: issued GET request for {r.geturl()}\")\n    if not (200 <= r.status < 300):\n        pprint.pprint(dict(r.info()))\n        pprint.pprint(data)\n        raise RuntimeError(f\"unexpected status code: {r.status}\")\n",
      "variables": [
        "url",
        "is_json",
        "kwargs",
        "headers",
        "r",
        "data"
      ],
      "anonymized_code": "def http_get(var_1: str, *, var_2: bool = True, **var_3: Any) -> Any:\n    var_4 = var_3.get(\"var_4\") or {}\n    var_4[\"User-Agent\"] = USER_AGENT\n    if \"github\" in var_1:\n        if GH_API_TOKEN:\n            var_4[\"Authorization\"] = f\"token {GH_API_TOKEN}\"\n        var_4[\"Accept\"] = \"application/vnd.github.v3+json\"\n    var_3[\"var_4\"] = var_4\n\n    var_5 = http.request(\"GET\", var_1, **var_3)\n    if var_2:\n        var_6 = json.loads(var_5.var_6.decode(\"utf-8\"))\n    else:\n        var_6 = var_5.var_6\n    print(f\"[INFO]: issued GET request for {var_5.geturl()}\")\n    if not (200 <= var_5.status < 300):\n        pprint.pprint(dict(var_5.info()))\n        pprint.pprint(var_6)\n        raise RuntimeError(f\"unexpected status code: {var_5.status}\")\n",
      "lines_processed": 19,
      "total_lines": 231
    },
    {
      "file_path": "diff_shades_gha_helper.py",
      "code": "def set_output(name: str, value: str) -> None:\n    if len(value) < 200:\n        print(f\"[INFO]: setting '{name}' to '{value}'\")\n    else:\n        print(f\"[INFO]: setting '{name}' to [{len(value)} chars]\")\n\n    if \"GITHUB_OUTPUT\" in os.environ:\n        if \"\\n\" in value:\n            # https://docs.github.com/en/actions/using-workflows/workflow-commands-for-github-actions#multiline-strings\n            delimiter = b64encode(os.urandom(16)).decode()\n            value = f\"{delimiter}\\n{value}\\n{delimiter}\"\n            command = f\"{name}<<{value}\"\n        else:\n            command = f\"{name}={value}\"\n        with open(os.environ[\"GITHUB_OUTPUT\"], \"a\") as f:\n            print(command, file=f)\n",
      "variables": [
        "name",
        "value",
        "delimiter",
        "command",
        "f"
      ],
      "anonymized_code": "def set_output(var_1: str, var_2: str) -> None:\n    if len(var_2) < 200:\n        print(var_5\"[INFO]: setting '{var_1}' to '{var_2}'\")\n    else:\n        print(var_5\"[INFO]: setting '{var_1}' to [{len(var_2)} chars]\")\n\n    if \"GITHUB_OUTPUT\" in os.environ:\n        if \"\\n\" in var_2:\n            # https://docs.github.com/en/actions/using-workflows/workflow-commands-for-github-actions#multiline-strings\n            var_3 = b64encode(os.urandom(16)).decode()\n            var_2 = var_5\"{var_3}\\n{var_2}\\n{var_3}\"\n            var_4 = var_5\"{var_1}<<{var_2}\"\n        else:\n            var_4 = var_5\"{var_1}={var_2}\"\n        with open(os.environ[\"GITHUB_OUTPUT\"], \"a\") as var_5:\n            print(var_4, file=var_5)\n",
      "lines_processed": 16,
      "total_lines": 231
    }
  ],
  "django_django": [
    {
      "file_path": "global_settings.py",
      "code": "def gettext_noop(s):\n    return s\n",
      "variables": [
        "s"
      ],
      "anonymized_code": "def gettext_noop(var_1):\n    return var_1\n",
      "lines_processed": 2,
      "total_lines": 669
    }
  ]
}