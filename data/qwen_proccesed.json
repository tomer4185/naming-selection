{
  "open-mmlab_mmdetection": [
    {
      "file_path": "gather_models.py",
      "code": "def get_final_epoch_or_iter(config):\n    cfg = Config.fromfile('./configs/' + config)\n    if cfg.train_cfg.type == 'EpochBasedTrainLoop':\n        return cfg.train_cfg.max_epochs\n    else:\n        return cfg.train_cfg.max_iters\n",
      "variables": [
        "config",
        "cfg"
      ],
      "anonymized_code": "def get_final_epoch_or_iter(var_1):\n    var_2 = Config.fromfile('./configs/' + var_1)\n    if var_2.train_cfg.type == 'EpochBasedTrainLoop':\n        return var_2.train_cfg.max_epochs\n    else:\n        return var_2.train_cfg.max_iters\n",
      "lines_processed": 6,
      "total_lines": 308,
      "llm_code": "def get_final_epoch_or_iter(config_file):\n    config = Config.fromfile(\"./configs/\" + config_file)\n    if config.train_cfg.type == \"EpochBasedTrainLoop\":\n        return config.train_cfg.max_epochs\n    else:\n        return config.train_cfg.max_iters",
      "llm_variables": [
        "config_file",
        "config"
      ]
    },
    {
      "file_path": "gather_models.py",
      "code": "def get_real_epoch_or_iter(config):\n    cfg = Config.fromfile('./configs/' + config)\n    if cfg.train_cfg.type == 'EpochBasedTrainLoop':\n        epoch = cfg.train_cfg.max_epochs\n        return epoch\n    else:\n        return cfg.train_cfg.max_iters\n",
      "variables": [
        "config",
        "cfg",
        "epoch"
      ],
      "anonymized_code": "def get_real_epoch_or_iter(var_1):\n    var_2 = Config.fromfile('./configs/' + var_1)\n    if var_2.train_cfg.type == 'EpochBasedTrainLoop':\n        var_3 = var_2.train_cfg.max_epochs\n        return var_3\n    else:\n        return var_2.train_cfg.max_iters\n",
      "lines_processed": 7,
      "total_lines": 308,
      "llm_code": "def get_real_epoch_or_iter(config_file):\n    config = Config.fromfile(\"./configs/\" + config_file)\n    if config.train_cfg.type == \"EpochBasedTrainLoop\":\n        epochs = config.train_cfg.max_epochs\n        return epochs\n    else:\n        return config.train_cfg.max_iters",
      "llm_variables": [
        "config_file",
        "config",
        "epochs"
      ]
    },
    {
      "file_path": "gather_models.py",
      "code": "def convert_model_info_to_pwc(model_infos):\n    pwc_files = {}\n    for model in model_infos:\n        cfg_folder_name = osp.split(model['config'])[-2]\n        pwc_model_info = OrderedDict()\n        pwc_model_info['Name'] = osp.split(model['config'])[-1].split('.')[0]\n        pwc_model_info['In Collection'] = 'Please fill in Collection name'\n        pwc_model_info['Config'] = osp.join('configs', model['config'])\n\n        # get metadata\n        meta_data = OrderedDict()\n        if 'epochs' in model:\n            meta_data['Epochs'] = get_real_epoch_or_iter(model['config'])\n        else:\n            meta_data['Iterations'] = get_real_epoch_or_iter(model['config'])\n        pwc_model_info['Metadata'] = meta_data\n\n        # get dataset name\n        dataset_name = get_dataset_name(model['config'])\n",
      "variables": [
        "model_infos",
        "pwc_files",
        "model",
        "cfg_folder_name",
        "pwc_model_info",
        "meta_data",
        "dataset_name"
      ],
      "anonymized_code": "def convert_model_info_to_pwc(var_1):\n    var_2 = {}\n    for var_3 in var_1:\n        var_4 = osp.split(var_3['config'])[-2]\n        var_5 = OrderedDict()\n        var_5['Name'] = osp.split(var_3['config'])[-1].split('.')[0]\n        var_5['In Collection'] = 'Please fill in Collection name'\n        var_5['Config'] = osp.join('configs', var_3['config'])\n\n        # get metadata\n        var_6 = OrderedDict()\n        if 'epochs' in var_3:\n            var_6['Epochs'] = get_real_epoch_or_iter(var_3['config'])\n        else:\n            var_6['Iterations'] = get_real_epoch_or_iter(var_3['config'])\n        var_5['Metadata'] = var_6\n\n        # get dataset name\n        var_7 = get_dataset_name(var_3['config'])\n",
      "lines_processed": 19,
      "total_lines": 308,
      "llm_code": "def convert_model_info_to_pwc(model_info):\n    result = {}\n    for item in model_info:\n        dir_name = osp.split(item[\"config\"])[-2]\n        info = OrderedDict()\n        info[\"Name\"] = osp.split(item[\"config\"])[-1].split(\".\")[0]\n        info[\"In Collection\"] = \"Please fill in Collection name\"\n        info[\"Config\"] = osp.join(\"configs\", item[\"config\"])\n\n        # get metadata\n        metadata = OrderedDict()\n        if \"epochs\" in item:\n            metadata[\"Epochs\"] = get_real_epoch_or_iter(item[\"config\"])\n        else:\n            metadata[\"Iterations\"] = get_real_epoch_or_iter(item[\"config\"])\n        info[\"Metadata\"] = metadata\n\n        # get dataset name\n        dataset_name = get_dataset_name(item[\"config\"])",
      "llm_variables": [
        "model_info",
        "result",
        "item",
        "dir_name",
        "info",
        "metadata",
        "dataset_name"
      ]
    },
    {
      "file_path": "gather_models.py",
      "code": "def get_best_epoch_or_iter(exp_dir):\n    best_epoch_iter_full_path = list(\n        sorted(glob.glob(osp.join(exp_dir, 'best_*.pth'))))[-1]\n    best_epoch_or_iter_model_path = best_epoch_iter_full_path.split('/')[-1]\n    best_epoch_or_iter = best_epoch_or_iter_model_path.\\\n        split('_')[-1].split('.')[0]\n    return best_epoch_or_iter_model_path, int(best_epoch_or_iter)\n",
      "variables": [
        "exp_dir",
        "best_epoch_iter_full_path",
        "best_epoch_or_iter_model_path",
        "best_epoch_or_iter"
      ],
      "anonymized_code": "def get_best_epoch_or_iter(var_1):\n    var_2 = list(\n        sorted(glob.glob(osp.join(var_1, 'best_*.pth'))))[-1]\n    var_3 = var_2.split('/')[-1]\n    var_4 = var_3.\\\n        split('_')[-1].split('.')[0]\n    return var_3, int(var_4)\n",
      "lines_processed": 7,
      "total_lines": 308,
      "llm_code": "def get_best_epoch_or_iter(directory):\n    files = list(\n        sorted(glob.glob(osp.join(directory, 'best_*.pth'))))[-1]\n    filename = files.split('/')[-1]\n    epoch = filename.\\\n        split('_')[-1].split('.')[0]\n    return filename, int(epoch)\n",
      "llm_variables": [
        "directory",
        "files",
        "filename",
        "epoch"
      ]
    },
    {
      "file_path": "gather_models.py",
      "code": "def ordered_yaml_dump(data, stream=None, Dumper=yaml.SafeDumper, **kwds):\n\n    class OrderedDumper(Dumper):\n        pass\n\n    def _dict_representer(dumper, data):\n        return dumper.represent_mapping(\n            yaml.resolver.BaseResolver.DEFAULT_MAPPING_TAG, data.items())\n\n    OrderedDumper.add_representer(OrderedDict, _dict_representer)\n    return yaml.dump(data, stream, OrderedDumper, **kwds)\n",
      "variables": [
        "data",
        "stream",
        "Dumper",
        "kwds",
        "dumper"
      ],
      "anonymized_code": "def ordered_yaml_dump(var_1, var_2=None, var_3=yaml.SafeDumper, **var_4):\n\n    class OrderedDumper(var_3):\n        pass\n\n    def _dict_representer(var_5, var_1):\n        return var_5.represent_mapping(\n            yaml.resolver.BaseResolver.DEFAULT_MAPPING_TAG, var_1.items())\n\n    OrderedDumper.add_representer(OrderedDict, _dict_representer)\n    return yaml.dump(var_1, var_2, OrderedDumper, **var_4)\n",
      "lines_processed": 11,
      "total_lines": 308,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "gather_models.py",
      "code": "def find_last_dir(model_dir):\n    dst_times = []\n    for time_stamp in os.scandir(model_dir):\n        if osp.isdir(time_stamp):\n            dst_time = time.mktime(\n                time.strptime(time_stamp.name, '%Y%m%d_%H%M%S'))\n            dst_times.append([dst_time, time_stamp.name])\n    return max(dst_times, key=lambda x: x[0])[1]\n",
      "variables": [
        "model_dir",
        "dst_times",
        "time_stamp",
        "dst_time",
        "x"
      ],
      "anonymized_code": "def find_last_dir(var_1):\n    var_2 = []\n    for var_3 in os.scandir(var_1):\n        if osp.isdir(var_3):\n            var_4 = time.mktime(\n                time.strptime(var_3.name, '%Y%m%d_%H%M%S'))\n            var_2.append([var_4, var_3.name])\n    return max(var_2, key=lambda var_5: var_5[0])[1]\n",
      "lines_processed": 8,
      "total_lines": 308,
      "llm_code": "def find_last_dir(directory):\n    directories = []\n    for entry in os.scandir(directory):\n        if osp.isdir(entry):\n            timestamp = time.mktime(\n                time.strptime(entry.name, '%Y%m%d_%H%M%S'))\n            directories.append([timestamp, entry.name])\n    return max(directories, key=lambda x: x[0])[1]\n",
      "llm_variables": [
        "directory",
        "directories",
        "entry",
        "timestamp",
        "x"
      ]
    },
    {
      "file_path": "gather_models.py",
      "code": "def process_checkpoint(in_file, out_file):\n    checkpoint = torch.load(in_file, map_location='cpu')\n    # remove optimizer for smaller file size\n    if 'optimizer' in checkpoint:\n        del checkpoint['optimizer']\n    if 'ema_state_dict' in checkpoint:\n        del checkpoint['ema_state_dict']\n\n    # remove ema state_dict\n    for key in list(checkpoint['state_dict']):\n        if key.startswith('ema_'):\n            checkpoint['state_dict'].pop(key)\n        elif key.startswith('data_preprocessor'):\n            checkpoint['state_dict'].pop(key)\n\n    # if it is necessary to remove some sensitive data in checkpoint['meta'],\n    # add the code here.\n    if digit_version(torch.__version__) >= digit_version('1.6'):\n        torch.save(checkpoint, out_file, _use_new_zipfile_serialization=False)\n",
      "variables": [
        "in_file",
        "out_file",
        "checkpoint",
        "key"
      ],
      "anonymized_code": "def process_checkpoint(var_1, var_2):\n    var_3 = torch.load(var_1, map_location='cpu')\n    # remove optimizer for smaller file size\n    if 'optimizer' in var_3:\n        del var_3['optimizer']\n    if 'ema_state_dict' in var_3:\n        del var_3['ema_state_dict']\n\n    # remove ema state_dict\n    for var_4 in list(var_3['state_dict']):\n        if var_4.startswith('ema_'):\n            var_3['state_dict'].pop(var_4)\n        elif var_4.startswith('data_preprocessor'):\n            var_3['state_dict'].pop(var_4)\n\n    # if it is necessary to remove some sensitive data in var_3['meta'],\n    # add the code here.\n    if digit_version(torch.__version__) >= digit_version('1.6'):\n        torch.save(var_3, var_2, _use_new_zipfile_serialization=False)\n",
      "lines_processed": 19,
      "total_lines": 308,
      "llm_code": "def process_checkpoint(checkpoint_path, output_path):\n    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n    # remove optimizer for smaller file size\n    if \"optimizer\" in checkpoint:\n        del checkpoint[\"optimizer\"]\n    if \"ema_state_dict\" in checkpoint:\n        del checkpoint[\"ema_state_dict\"]\n\n    # remove ema state_dict\n    for key in list(checkpoint[\"state_dict\"]):\n        if key.startswith(\"ema_\"):\n            checkpoint[\"state_dict\"].pop(key)\n        elif key.startswith(\"data_preprocessor\"):\n            checkpoint[\"state_dict\"].pop(key)\n\n    # if it is necessary to remove some sensitive data in checkpoint[\"meta\"],\n    # add the code here.\n    if digit_version(torch.__version__) >= digit_version(\"1.6\"):\n        torch.save(checkpoint, output_path, _use_new_zipfile_serialization=False)",
      "llm_variables": [
        "checkpoint_path",
        "output_path",
        "checkpoint",
        "key"
      ]
    },
    {
      "file_path": "gather_models.py",
      "code": "def is_by_epoch(config):\n    cfg = Config.fromfile('./configs/' + config)\n    return cfg.train_cfg.type == 'EpochBasedTrainLoop'\n",
      "variables": [
        "config",
        "cfg"
      ],
      "anonymized_code": "def is_by_epoch(var_1):\n    var_2 = Config.fromfile('./configs/' + var_1)\n    return var_2.train_cfg.type == 'EpochBasedTrainLoop'\n",
      "lines_processed": 3,
      "total_lines": 308,
      "llm_code": "def is_by_epoch(config):\n    config = Config.fromfile(\"./configs/\" + config)\n    return config.train_cfg.type == \"EpochBasedTrainLoop\"",
      "llm_variables": [
        "config",
        "config"
      ]
    },
    {
      "file_path": "gather_models.py",
      "code": "def parse_args():\n    parser = argparse.ArgumentParser(description='Gather benchmarked models')\n    parser.add_argument(\n        'root',\n        type=str,\n        default='work_dirs',\n        help='root path of benchmarked models to be gathered')\n    parser.add_argument(\n        '--out',\n        type=str,\n        default='gather',\n        help='output path of gathered models to be stored')\n    parser.add_argument(\n        '--best',\n        action='store_true',\n        help='whether to gather the best model.')\n\n    args = parser.parse_args()\n    return args\n",
      "variables": [
        "parser",
        "args"
      ],
      "anonymized_code": "def parse_args():\n    var_1 = argparse.ArgumentParser(description='Gather benchmarked models')\n    var_1.add_argument(\n        'root',\n        type=str,\n        default='work_dirs',\n        help='root path of benchmarked models to be gathered')\n    var_1.add_argument(\n        '--out',\n        type=str,\n        default='gather',\n        help='output path of gathered models to be stored')\n    var_1.add_argument(\n        '--best',\n        action='store_true',\n        help='whether to gather the best model.')\n\n    var_2 = var_1.parse_args()\n    return var_2\n",
      "lines_processed": 19,
      "total_lines": 308,
      "llm_code": "def parse_args():\n    parser = argparse.ArgumentParser(description='Gather benchmarked models')\n    parser.add_argument(\n        'root',\n        type=str,\n        default='work_dirs',\n        help='root path of benchmarked models to be gathered')\n    parser.add_argument(\n        '--out',\n        type=str,\n        default='gather',\n        help='output path of gathered models to be stored')\n    parser.add_argument(\n        '--best',\n        action='store_true',\n        help='whether to gather the best model.')\n\n    args = parser.parse_args()\n    return args\n",
      "llm_variables": [
        "parser",
        "args"
      ]
    },
    {
      "file_path": "benchmark_inference_fps.py",
      "code": "def parse_args():\n    parser = argparse.ArgumentParser(\n        description='MMDet benchmark a model of FPS')\n    parser.add_argument('config', help='test config file path')\n    parser.add_argument('checkpoint_root', help='Checkpoint file root path')\n    parser.add_argument(\n        '--round-num',\n        type=int,\n        default=1,\n        help='round a number to a given precision in decimal digits')\n    parser.add_argument(\n        '--repeat-num',\n        type=int,\n        default=1,\n        help='number of repeat times of measurement for averaging the results')\n    parser.add_argument(\n        '--out', type=str, help='output path of gathered fps to be stored')\n    parser.add_argument(\n        '--max-iter', type=int, default=2000, help='num of max iter')\n",
      "variables": [
        "parser"
      ],
      "anonymized_code": "def parse_args():\n    var_1 = argparse.ArgumentParser(\n        description='MMDet benchmark a model of FPS')\n    var_1.add_argument('config', help='test config file path')\n    var_1.add_argument('checkpoint_root', help='Checkpoint file root path')\n    var_1.add_argument(\n        '--round-num',\n        type=int,\n        default=1,\n        help='round a number to a given precision in decimal digits')\n    var_1.add_argument(\n        '--repeat-num',\n        type=int,\n        default=1,\n        help='number of repeat times of measurement for averaging the results')\n    var_1.add_argument(\n        '--out', type=str, help='output path of gathered fps to be stored')\n    var_1.add_argument(\n        '--max-iter', type=int, default=2000, help='num of max iter')\n",
      "lines_processed": 19,
      "total_lines": 171,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "convert_test_benchmark_script.py",
      "code": "def process_model_info(model_info, work_dir):\n    config = model_info['config'].strip()\n    fname, _ = osp.splitext(osp.basename(config))\n    job_name = fname\n    work_dir = '$WORK_DIR/' + fname\n    checkpoint = model_info['checkpoint'].strip()\n    return dict(\n        config=config,\n        job_name=job_name,\n        work_dir=work_dir,\n        checkpoint=checkpoint)\n",
      "variables": [
        "model_info",
        "work_dir",
        "config",
        "fname",
        "_",
        "job_name",
        "checkpoint"
      ],
      "anonymized_code": "def process_model_info(var_1, var_2):\n    var_3 = var_1['var_3'].strip()\n    var_4, var_5 = osp.splitext(osp.basename(var_3))\n    var_6 = var_4\n    var_2 = '$WORK_DIR/' + var_4\n    var_7 = var_1['var_7'].strip()\n    return dict(\n        var_3=var_3,\n        var_6=var_6,\n        var_2=var_2,\n        var_7=var_7)\n",
      "lines_processed": 11,
      "total_lines": 114,
      "llm_code": "def process_model_info(model_info, work_dir):\n    file_name = model_info[\"file_name\"].strip()\n    dir_name, file_ext = osp.splitext(osp.basename(file_name))\n    model_name = dir_name\n    work_path = \"$WORK_DIR/\" + dir_name\n    model_version = model_info[\"model_version\"].strip()\n    return dict(\n        file_name=file_name,\n        model_name=model_name,\n        work_path=work_path,\n        model_version=model_version\n    )",
      "llm_variables": [
        "model_info",
        "work_dir",
        "file_name",
        "file_ext",
        "model_name",
        "work_path",
        "model_version"
      ]
    },
    {
      "file_path": "convert_test_benchmark_script.py",
      "code": "def parse_args():\n    parser = argparse.ArgumentParser(\n        description='Convert benchmark model list to script')\n    parser.add_argument('config', help='test config file path')\n    parser.add_argument('--port', type=int, default=29666, help='dist port')\n    parser.add_argument(\n        '--run', action='store_true', help='run script directly')\n    parser.add_argument(\n        '--out', type=str, help='path to save model benchmark script')\n\n    args = parser.parse_args()\n    return args\n",
      "variables": [
        "parser",
        "args"
      ],
      "anonymized_code": "def parse_args():\n    var_1 = argparse.ArgumentParser(\n        description='Convert benchmark model list to script')\n    var_1.add_argument('config', help='test config file path')\n    var_1.add_argument('--port', type=int, default=29666, help='dist port')\n    var_1.add_argument(\n        '--run', action='store_true', help='run script directly')\n    var_1.add_argument(\n        '--out', type=str, help='path to save model benchmark script')\n\n    var_2 = var_1.parse_args()\n    return var_2\n",
      "lines_processed": 12,
      "total_lines": 114,
      "llm_code": "def parse_args():\n    parser = argparse.ArgumentParser(\n        description='Convert benchmark model list to script')\n    parser.add_argument('config', help='test config file path')\n    parser.add_argument('--port', type=int, default=29666, help='dist port')\n    parser.add_argument(\n        '--run', action='store_true', help='run script directly')\n    parser.add_argument(\n        '--out', type=str, help='path to save model benchmark script')\n\n    args = parser.parse_args()\n    return args\n",
      "llm_variables": [
        "parser",
        "args"
      ]
    },
    {
      "file_path": "convert_test_benchmark_script.py",
      "code": "def create_test_bash_info(commands, model_test_dict, port, script_name,\n                          partition):\n    config = model_test_dict['config']\n    job_name = model_test_dict['job_name']\n    checkpoint = model_test_dict['checkpoint']\n    work_dir = model_test_dict['work_dir']\n\n    echo_info = f' \\necho \\'{config}\\' &'\n    commands.append(echo_info)\n    commands.append('\\n')\n\n    command_info = f'GPUS=8  GPUS_PER_NODE=8  ' \\\n                   f'CPUS_PER_TASK=$CPUS_PRE_TASK {script_name} '\n\n    command_info += f'{partition} '\n    command_info += f'{job_name} '\n    command_info += f'{config} '\n    command_info += f'$CHECKPOINT_DIR/{checkpoint} '\n    command_info += f'--work-dir {work_dir} '\n",
      "variables": [
        "commands",
        "model_test_dict",
        "port",
        "script_name",
        "partition",
        "config",
        "job_name",
        "checkpoint",
        "work_dir",
        "echo_info",
        "command_info"
      ],
      "anonymized_code": "def create_test_bash_info(var_1, var_2, var_3, var_4,\n                          var_5):\n    var_6 = var_2['var_6']\n    var_7 = var_2['var_7']\n    var_8 = var_2['var_8']\n    var_9 = var_2['var_9']\n\n    var_10 = f' \\necho \\'{var_6}\\' &'\n    var_1.append(var_10)\n    var_1.append('\\n')\n\n    var_11 = f'GPUS=8  GPUS_PER_NODE=8  ' \\\n                   f'CPUS_PER_TASK=$CPUS_PRE_TASK {var_4} '\n\n    var_11 += f'{var_5} '\n    var_11 += f'{var_7} '\n    var_11 += f'{var_6} '\n    var_11 += f'$CHECKPOINT_DIR/{var_8} '\n    var_11 += f'--work-dir {var_9} '\n",
      "lines_processed": 19,
      "total_lines": 114,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "convert_test_benchmark_script.py",
      "code": "def main():\n    args = parse_args()\n    if args.out:\n        out_suffix = args.out.split('.')[-1]\n        assert args.out.endswith('.sh'), \\\n            f'Expected out file path suffix is .sh, but get .{out_suffix}'\n    assert args.out or args.run, \\\n        ('Please specify at least one operation (save/run/ the '\n         'script) with the argument \"--out\" or \"--run\"')\n\n    commands = []\n    partition_name = 'PARTITION=$1 '\n    commands.append(partition_name)\n    commands.append('\\n')\n\n    checkpoint_root = 'CHECKPOINT_DIR=$2 '\n    commands.append(checkpoint_root)\n    commands.append('\\n')\n\n",
      "variables": [
        "args",
        "out_suffix",
        "commands",
        "partition_name",
        "checkpoint_root"
      ],
      "anonymized_code": "def main():\n    var_1 = parse_args()\n    if var_1.out:\n        var_2 = var_1.out.split('.')[-1]\n        assert var_1.out.endswith('.sh'), \\\n            f'Expected out file path suffix is .sh, but get .{var_2}'\n    assert var_1.out or var_1.run, \\\n        ('Please specify at least one operation (save/run/ the '\n         'script) with the argument \"--out\" or \"--run\"')\n\n    var_3 = []\n    var_4 = 'PARTITION=$1 '\n    var_3.append(var_4)\n    var_3.append('\\n')\n\n    var_5 = 'CHECKPOINT_DIR=$2 '\n    var_3.append(var_5)\n    var_3.append('\\n')\n\n",
      "lines_processed": 19,
      "total_lines": 114,
      "llm_code": "def main():\n    args = parse_args()\n    if args.out:\n        var_2 = args.out.split('.')[-1]\n        assert args.out.endswith('.sh'), \\\n            f'Expected out file path suffix is .sh, but get .{var_2}'\n    assert args.out or args.run, \\\n        ('Please specify at least one operation (save/run/ the '\n         'script) with the argument \"--out\" or \"--run\"')\n\n    var_3 = []\n    var_4 = 'PARTITION=$1 '\n    var_3.append(var_4)\n    var_3.append('\\n')\n\n    var_5 = 'CHECKPOINT_DIR=$2 '\n    var_3.append(var_5)\n    var_3.append('\\n')\n\n",
      "llm_variables": [
        "args",
        "var_2",
        "var_3",
        "var_4",
        "var_5"
      ]
    },
    {
      "file_path": "gather_train_benchmark_metric.py",
      "code": "def parse_args():\n    parser = argparse.ArgumentParser(\n        description='Gather benchmarked models metric')\n    parser.add_argument(\n        'root',\n        type=str,\n        help='root path of benchmarked models to be gathered')\n    parser.add_argument(\n        'txt_path', type=str, help='txt path output by benchmark_filter')\n    parser.add_argument(\n        '--out', type=str, help='output path of gathered metrics to be stored')\n    parser.add_argument(\n        '--not-show', action='store_true', help='not show metrics')\n    parser.add_argument(\n        '--excel', type=str, help='input path of excel to be recorded')\n    parser.add_argument(\n        '--ncol', type=int, help='Number of column to be modified or appended')\n\n    args = parser.parse_args()\n",
      "variables": [
        "parser",
        "args"
      ],
      "anonymized_code": "def parse_args():\n    var_1 = argparse.ArgumentParser(\n        description='Gather benchmarked models metric')\n    var_1.add_argument(\n        'root',\n        type=str,\n        help='root path of benchmarked models to be gathered')\n    var_1.add_argument(\n        'txt_path', type=str, help='txt path output by benchmark_filter')\n    var_1.add_argument(\n        '--out', type=str, help='output path of gathered metrics to be stored')\n    var_1.add_argument(\n        '--not-show', action='store_true', help='not show metrics')\n    var_1.add_argument(\n        '--excel', type=str, help='input path of excel to be recorded')\n    var_1.add_argument(\n        '--ncol', type=int, help='Number of column to be modified or appended')\n\n    var_2 = var_1.parse_args()\n",
      "lines_processed": 19,
      "total_lines": 151,
      "llm_code": "def parse_args():\n    parser = argparse.ArgumentParser(\n        description='Gather benchmarked models metric')\n    parser.add_argument(\n        'root',\n        type=str,\n        help='root path of benchmarked models to be gathered')\n    parser.add_argument(\n        'txt_path', type=str, help='txt path output by benchmark_filter')\n    parser.add_argument(\n        '--out', type=str, help='output path of gathered metrics to be stored')\n    parser.add_argument(\n        '--not-show', action='store_true', help='not show metrics')\n    parser.add_argument(\n        '--excel', type=str, help='input path of excel to be recorded')\n    parser.add_argument(\n        '--ncol', type=int, help='Number of column to be modified or appended')\n\n    args = parser.parse_args()\n",
      "llm_variables": [
        "parser",
        "args"
      ]
    },
    {
      "file_path": "check_links.py",
      "code": "def parse_args():\n    parser = argparse.ArgumentParser(\n        description='Goes through all the inline-links '\n        'in markdown files and reports the breakages')\n    parser.add_argument(\n        '--num-threads',\n        type=int,\n        default=100,\n        help='Number of processes to confirm the link')\n    parser.add_argument('--https-proxy', type=str, help='https proxy')\n    parser.add_argument(\n        '--out',\n        type=str,\n        default='link_reports.txt',\n        help='output path of reports')\n    args = parser.parse_args()\n    return args\n",
      "variables": [
        "parser",
        "args"
      ],
      "anonymized_code": "def parse_args():\n    var_1 = argparse.ArgumentParser(\n        description='Goes through all the inline-links '\n        'in markdown files and reports the breakages')\n    var_1.add_argument(\n        '--num-threads',\n        type=int,\n        default=100,\n        help='Number of processes to confirm the link')\n    var_1.add_argument('--https-proxy', type=str, help='https proxy')\n    var_1.add_argument(\n        '--out',\n        type=str,\n        default='link_reports.txt',\n        help='output path of reports')\n    var_2 = var_1.parse_args()\n    return var_2\n",
      "lines_processed": 17,
      "total_lines": 157,
      "llm_code": "def parse_args():\n    parser = argparse.ArgumentParser(\n        description='Goes through all the inline-links '\n        'in markdown files and reports the breakages')\n    parser.add_argument(\n        '--num-threads',\n        type=int,\n        default=100,\n        help='Number of processes to confirm the link')\n    parser.add_argument('--https-proxy', type=str, help='https proxy')\n    parser.add_argument(\n        '--out',\n        type=str,\n        default='link_reports.txt',\n        help='output path of reports')\n    args = parser.parse_args()\n    return args\n",
      "llm_variables": [
        "parser",
        "args"
      ]
    },
    {
      "file_path": "check_links.py",
      "code": "def check_link(\n        match_tuple: MatchTuple,\n        http_session: requests.Session,\n        logger: logging = None) -> Tuple[MatchTuple, bool, Optional[str]]:\n    reason: Optional[str] = None\n    if match_tuple.link.startswith('http'):\n        result_ok, reason = check_url(match_tuple, http_session)\n    else:\n        result_ok = check_path(match_tuple)\n    if logger is None:\n        print(f\"  {'\u2713' if result_ok else '\u2717'} {match_tuple.link}\")\n    else:\n        logger.info(f\"  {'\u2713' if result_ok else '\u2717'} {match_tuple.link}\")\n    return match_tuple, result_ok, reason\n",
      "variables": [
        "match_tuple",
        "http_session",
        "logger",
        "reason",
        "result_ok"
      ],
      "anonymized_code": "def check_link(\n        var_1: MatchTuple,\n        var_2: requests.Session,\n        var_3: logging = None) -> Tuple[MatchTuple, bool, Optional[str]]:\n    var_4: Optional[str] = None\n    if var_1.link.startswith('http'):\n        var_5, var_4 = check_url(var_1, var_2)\n    else:\n        var_5 = check_path(var_1)\n    if var_3 is None:\n        print(f\"  {'\u2713' if var_5 else '\u2717'} {var_1.link}\")\n    else:\n        var_3.info(f\"  {'\u2713' if var_5 else '\u2717'} {var_1.link}\")\n    return var_1, var_5, var_4\n",
      "lines_processed": 14,
      "total_lines": 157,
      "llm_code": "def check_link(\n        link_match: MatchTuple,\n        session: requests.Session,\n        logger: logging = None) -> Tuple[MatchTuple, bool, Optional[str]]:\n    result: Optional[str] = None\n    if link_match.link.startswith('http'):\n        is_valid, result = check_url(link_match, session)\n    else:\n        is_valid = check_path(link_match)\n    if logger is None:\n        print(f\"  {'\u2713' if is_valid else '\u2717'} {link_match.link}\")\n    else:\n        logger.info(f\"  {'\u2713' if is_valid else '\u2717'} {link_match.link}\")\n    return link_match, is_valid, result\n",
      "llm_variables": [
        "link_match",
        "session",
        "logger",
        "result",
        "is_valid"
      ]
    },
    {
      "file_path": "check_links.py",
      "code": "def check_path(match_tuple: MatchTuple) -> bool:\n    \"\"\"Check if a file in this repository exists.\"\"\"\n    relative_path = match_tuple.link.split('#')[0]\n    full_path = os.path.join(\n        os.path.dirname(str(match_tuple.source)), relative_path)\n    return os.path.exists(full_path)\n",
      "variables": [
        "match_tuple",
        "relative_path",
        "full_path"
      ],
      "anonymized_code": "def check_path(var_1: MatchTuple) -> bool:\n    \"\"\"Check if a file in this repository exists.\"\"\"\n    var_2 = var_1.link.split('#')[0]\n    var_3 = os.path.join(\n        os.path.dirname(str(var_1.source)), var_2)\n    return os.path.exists(var_3)\n",
      "lines_processed": 6,
      "total_lines": 157,
      "llm_code": "def check_path(match: MatchTuple) -> bool:\n    \"\"\"Check if a file in this repository exists.\"\"\"\n    url = match.link.split('#')[0]\n    path = os.path.join(\n        os.path.dirname(str(match.source)), url)\n    return os.path.exists(path)\n",
      "llm_variables": [
        "match",
        "url",
        "path"
      ]
    },
    {
      "file_path": "check_links.py",
      "code": "def check_url(match_tuple: MatchTuple,\n              http_session: requests.Session) -> Tuple[bool, str]:\n    \"\"\"Check if a URL is reachable.\"\"\"\n    try:\n        result = http_session.head(\n            match_tuple.link, timeout=5, allow_redirects=True)\n        return (\n            result.ok or result.status_code in OK_STATUS_CODES,\n            f'status code = {result.status_code}',\n        )\n    except (requests.ConnectionError, requests.Timeout):\n        return False, 'connection error'\n",
      "variables": [
        "match_tuple",
        "http_session",
        "result"
      ],
      "anonymized_code": "def check_url(var_1: MatchTuple,\n              var_2: requests.Session) -> Tuple[bool, str]:\n    \"\"\"Check if a URL is reachable.\"\"\"\n    try:\n        var_3 = var_2.head(\n            var_1.link, timeout=5, allow_redirects=True)\n        return (\n            var_3.ok or var_3.status_code in OK_STATUS_CODES,\n            f'status code = {var_3.status_code}',\n        )\n    except (requests.ConnectionError, requests.Timeout):\n        return False, 'connection error'\n",
      "lines_processed": 12,
      "total_lines": 157,
      "llm_code": "def check_url(match_tuple: MatchTuple,\n              session: requests.Session) -> Tuple[bool, str]:\n    \"\"\"Check if a URL is reachable.\"\"\"\n    try:\n        response = session.head(\n            match_tuple.link, timeout=5, allow_redirects=True)\n        return (\n            response.ok or response.status_code in OK_STATUS_CODES,\n            f'status code = {response.status_code}',\n        )\n    except (requests.ConnectionError, requests.Timeout):\n        return False, 'connection error'\n",
      "llm_variables": [
        "match_tuple",
        "session",
        "response"
      ]
    },
    {
      "file_path": "benchmark_valid_flops.py",
      "code": "def show_summary(summary_data, args):\n    table = Table(title='Validation Benchmark Regression Summary')\n    table.add_column('Model')\n    table.add_column('Validation')\n    table.add_column('Resolution (c, h, w)')\n    if args.flops:\n        table.add_column('Flops', justify='right', width=11)\n        table.add_column('Params', justify='right')\n\n    for model_name, summary in summary_data.items():\n        row = [model_name]\n        valid = summary['valid']\n        color = 'green' if valid == 'PASS' else 'red'\n        row.append(f'[{color}]{valid}[/{color}]')\n        if valid == 'PASS':\n            row.append(str(summary['resolution']))\n            if args.flops:\n                row.append(str(summary['flops']))\n                row.append(str(summary['params']))\n",
      "variables": [
        "summary_data",
        "args",
        "table",
        "model_name",
        "summary",
        "row",
        "valid",
        "color"
      ],
      "anonymized_code": "def show_summary(var_1, var_2):\n    var_3 = Table(title='Validation Benchmark Regression Summary')\n    var_3.add_column('Model')\n    var_3.add_column('Validation')\n    var_3.add_column('Resolution (c, h, w)')\n    if var_2.flops:\n        var_3.add_column('Flops', justify='right', width=11)\n        var_3.add_column('Params', justify='right')\n\n    for var_4, var_5 in var_1.items():\n        var_6 = [var_4]\n        var_7 = var_5['var_7']\n        var_8 = 'green' if var_7 == 'PASS' else 'red'\n        var_6.append(f'[{var_8}]{var_7}[/{var_8}]')\n        if var_7 == 'PASS':\n            var_6.append(str(var_5['resolution']))\n            if var_2.flops:\n                var_6.append(str(var_5['flops']))\n                var_6.append(str(var_5['params']))\n",
      "lines_processed": 19,
      "total_lines": 295,
      "llm_code": "def show_summary(models, config):\n    table = Table(title='Validation Benchmark Regression Summary')\n    table.add_column('Model')\n    table.add_column('Validation')\n    table.add_column('Resolution (c, h, w)')\n    if config.flops:\n        table.add_column('Flops', justify='right', width=11)\n        table.add_column('Params', justify='right')\n\n    for model_name, model_data in models.items():\n        row = [model_name]\n        status = model_data['status']\n        color = 'green' if status == 'PASS' else 'red'\n        row.append(f'[{color}]{status}[/{color}]')\n        if status == 'PASS':\n            row.append(str(model_data['resolution']))\n            if config.flops:\n                row.append(str(model_data['flops']))\n                row.append(str(model_data['params']))\n",
      "llm_variables": [
        "models",
        "config",
        "table",
        "model_name",
        "model_data",
        "row",
        "status",
        "color"
      ]
    },
    {
      "file_path": "benchmark_filter.py",
      "code": "def main():\n    args = parse_args()\n\n    benchmark_type = []\n    if args.basic_arch:\n        benchmark_type += basic_arch_root\n    if args.datasets:\n        benchmark_type += datasets_root\n    if args.data_pipeline:\n        benchmark_type += data_pipeline_root\n    if args.nn_module:\n        benchmark_type += nn_module_root\n\n    special_model = args.model_options\n    if special_model is not None:\n        benchmark_type += special_model\n\n    config_dpath = 'configs/'\n    benchmark_configs = []\n",
      "variables": [
        "args",
        "benchmark_type",
        "special_model",
        "config_dpath",
        "benchmark_configs"
      ],
      "anonymized_code": "def main():\n    var_1 = parse_args()\n\n    var_2 = []\n    if var_1.basic_arch:\n        var_2 += basic_arch_root\n    if var_1.datasets:\n        var_2 += datasets_root\n    if var_1.data_pipeline:\n        var_2 += data_pipeline_root\n    if var_1.nn_module:\n        var_2 += nn_module_root\n\n    var_3 = var_1.model_options\n    if var_3 is not None:\n        var_2 += var_3\n\n    var_4 = 'configs/'\n    var_5 = []\n",
      "lines_processed": 19,
      "total_lines": 167,
      "llm_code": "def main():\n    args = parse_args()\n\n    paths = []\n    if args.basic_arch:\n        paths += basic_arch_root\n    if args.datasets:\n        paths += datasets_root\n    if args.data_pipeline:\n        paths += data_pipeline_root\n    if args.nn_module:\n        paths += nn_module_root\n\n    model_options = args.model_options\n    if model_options is not None:\n        paths += model_options\n\n    config_dir = 'configs/'\n    output_paths = []\n",
      "llm_variables": [
        "args",
        "paths",
        "model_options",
        "config_dir",
        "output_paths"
      ]
    },
    {
      "file_path": "convert_train_benchmark_script.py",
      "code": "def parse_args():\n    parser = argparse.ArgumentParser(\n        description='Convert benchmark model json to script')\n    parser.add_argument(\n        'txt_path', type=str, help='txt path output by benchmark_filter')\n    parser.add_argument(\n        '--run', action='store_true', help='run script directly')\n    parser.add_argument(\n        '--out', type=str, help='path to save model benchmark script')\n\n    args = parser.parse_args()\n    return args\n",
      "variables": [
        "parser",
        "args"
      ],
      "anonymized_code": "def parse_args():\n    var_1 = argparse.ArgumentParser(\n        description='Convert benchmark model json to script')\n    var_1.add_argument(\n        'txt_path', type=str, help='txt path output by benchmark_filter')\n    var_1.add_argument(\n        '--run', action='store_true', help='run script directly')\n    var_1.add_argument(\n        '--out', type=str, help='path to save model benchmark script')\n\n    var_2 = var_1.parse_args()\n    return var_2\n",
      "lines_processed": 12,
      "total_lines": 104,
      "llm_code": "def parse_args():\n    parser = argparse.ArgumentParser(\n        description='Convert benchmark model json to script')\n    parser.add_argument(\n        'txt_path', type=str, help='txt path output by benchmark_filter')\n    parser.add_argument(\n        '--run', action='store_true', help='run script directly')\n    parser.add_argument(\n        '--out', type=str, help='path to save model benchmark script')\n\n    args = parser.parse_args()\n    return args\n",
      "llm_variables": [
        "parser",
        "args"
      ]
    },
    {
      "file_path": "convert_train_benchmark_script.py",
      "code": "def main():\n    args = parse_args()\n    if args.out:\n        out_suffix = args.out.split('.')[-1]\n        assert args.out.endswith('.sh'), \\\n            f'Expected out file path suffix is .sh, but get .{out_suffix}'\n    assert args.out or args.run, \\\n        ('Please specify at least one operation (save/run/ the '\n         'script) with the argument \"--out\" or \"--run\"')\n\n    root_name = './tools'\n    train_script_name = osp.join(root_name, 'slurm_train.sh')\n\n    commands = []\n    partition_name = 'PARTITION=$1 '\n    commands.append(partition_name)\n    commands.append('\\n')\n\n    work_dir = 'WORK_DIR=$2 '\n",
      "variables": [
        "args",
        "out_suffix",
        "root_name",
        "train_script_name",
        "commands",
        "partition_name",
        "work_dir"
      ],
      "anonymized_code": "def main():\n    var_1 = parse_args()\n    if var_1.out:\n        var_2 = var_1.out.split('.')[-1]\n        assert var_1.out.endswith('.sh'), \\\n            f'Expected out file path suffix is .sh, but get .{var_2}'\n    assert var_1.out or var_1.run, \\\n        ('Please specify at least one operation (save/run/ the '\n         'script) with the argument \"--out\" or \"--run\"')\n\n    var_3 = './tools'\n    var_4 = osp.join(var_3, 'slurm_train.sh')\n\n    var_5 = []\n    var_6 = 'PARTITION=$1 '\n    var_5.append(var_6)\n    var_5.append('\\n')\n\n    var_7 = 'WORK_DIR=$2 '\n",
      "lines_processed": 19,
      "total_lines": 104,
      "llm_code": "def main():\n    args = parse_args()\n    if args.out:\n        var_2 = args.out.split('.')[-1]\n        assert args.out.endswith('.sh'), \\\n            f'Expected out file path suffix is .sh, but get .{var_2}'\n    assert args.out or args.run, \\\n        ('Please specify at least one operation (save/run/ the '\n         'script) with the argument \"--out\" or \"--run\"')\n\n    var_3 = './tools'\n    var_4 = osp.join(var_3, 'slurm_train.sh')\n\n    var_5 = []\n    var_6 = 'PARTITION=$1 '\n    var_5.append(var_6)\n    var_5.append('\\n')\n\n    var_7 = 'WORK_DIR=$2 '\n",
      "llm_variables": [
        "args",
        "var_2",
        "var_3",
        "var_4",
        "var_5",
        "var_6",
        "var_7"
      ]
    },
    {
      "file_path": "convert_train_benchmark_script.py",
      "code": "def determine_gpus(cfg_name):\n    gpus = 8\n    gpus_pre_node = 8\n\n    if cfg_name.find('16x') >= 0:\n        gpus = 16\n    elif cfg_name.find('4xb4') >= 0:\n        gpus = 4\n        gpus_pre_node = 4\n    elif 'lad' in cfg_name:\n        gpus = 2\n        gpus_pre_node = 2\n\n    return gpus, gpus_pre_node\n",
      "variables": [
        "cfg_name",
        "gpus",
        "gpus_pre_node"
      ],
      "anonymized_code": "def determine_gpus(var_1):\n    var_2 = 8\n    var_3 = 8\n\n    if var_1.find('16x') >= 0:\n        var_2 = 16\n    elif var_1.find('4xb4') >= 0:\n        var_2 = 4\n        var_3 = 4\n    elif 'lad' in var_1:\n        var_2 = 2\n        var_3 = 2\n\n    return var_2, var_3\n",
      "lines_processed": 14,
      "total_lines": 104,
      "llm_code": "def determine_gpus(gpu_config):\n    max_gpus = 8\n    min_gpus = 8\n\n    if gpu_config.find('16x') >= 0:\n        max_gpus = 16\n    elif gpu_config.find('4xb4') >= 0:\n        max_gpus = 4\n        min_gpus = 4\n    elif 'lad' in gpu_config:\n        max_gpus = 2\n        min_gpus = 2\n\n    return max_gpus, min_gpus\n",
      "llm_variables": [
        "gpu_config",
        "max_gpus",
        "min_gpus"
      ]
    },
    {
      "file_path": "download_checkpoints.py",
      "code": "def parse_args():\n    parser = argparse.ArgumentParser(description='Download checkpoints')\n    parser.add_argument('config', help='test config file path')\n    parser.add_argument(\n        'out', type=str, help='output dir of checkpoints to be stored')\n    parser.add_argument(\n        '--nproc', type=int, default=16, help='num of Processes')\n    parser.add_argument(\n        '--intranet',\n        action='store_true',\n        help='switch to internal network url')\n    args = parser.parse_args()\n    return args\n",
      "variables": [
        "parser",
        "args"
      ],
      "anonymized_code": "def parse_args():\n    var_1 = argparse.ArgumentParser(description='Download checkpoints')\n    var_1.add_argument('config', help='test config file path')\n    var_1.add_argument(\n        'out', type=str, help='output dir of checkpoints to be stored')\n    var_1.add_argument(\n        '--nproc', type=int, default=16, help='num of Processes')\n    var_1.add_argument(\n        '--intranet',\n        action='store_true',\n        help='switch to internal network url')\n    var_2 = var_1.parse_args()\n    return var_2\n",
      "lines_processed": 13,
      "total_lines": 83,
      "llm_code": "def parse_args():\n    parser = argparse.ArgumentParser(description='Download checkpoints')\n    parser.add_argument('config', help='test config file path')\n    parser.add_argument(\n        'out', type=str, help='output dir of checkpoints to be stored')\n    parser.add_argument(\n        '--nproc', type=int, default=16, help='num of Processes')\n    parser.add_argument(\n        '--intranet',\n        action='store_true',\n        help='switch to internal network url')\n    args = parser.parse_args()\n    return args\n",
      "llm_variables": [
        "parser",
        "args"
      ]
    },
    {
      "file_path": "download_checkpoints.py",
      "code": "def download(url, out_file, min_bytes=math.pow(1024, 2), progress=True):\n    # math.pow(1024, 2) is mean 1 MB\n    assert_msg = f\"Downloaded url '{url}' does not exist \" \\\n                 f'or size is < min_bytes={min_bytes}'\n    try:\n        print(f'Downloading {url} to {out_file}...')\n        torch.hub.download_url_to_file(url, str(out_file), progress=progress)\n        assert osp.exists(\n            out_file) and osp.getsize(out_file) > min_bytes, assert_msg\n    except Exception as e:\n        if osp.exists(out_file):\n            os.remove(out_file)\n        print(f'ERROR: {e}\\nRe-attempting {url} to {out_file} ...')\n        os.system(f\"curl -L '{url}' -o '{out_file}' --retry 3 -C -\"\n                  )  # curl download, retry and resume on fail\n    finally:\n        if osp.exists(out_file) and osp.getsize(out_file) < min_bytes:\n            os.remove(out_file)  # remove partial downloads\n\n",
      "variables": [
        "url",
        "out_file",
        "min_bytes",
        "progress",
        "assert_msg"
      ],
      "anonymized_code": "def download(var_1, var_2, var_3=math.pow(1024, 2), var_4=True):\n    # math.pow(1024, 2) is mean 1 MB\n    var_5 = f\"Downloaded var_1 '{var_1}' does not exist \" \\\n                 f'or size is < var_3={var_3}'\n    try:\n        print(f'Downloading {var_1} to {var_2}...')\n        torch.hub.download_url_to_file(var_1, str(var_2), var_4=var_4)\n        assert osp.exists(\n            var_2) and osp.getsize(var_2) > var_3, var_5\n    except Exception as e:\n        if osp.exists(var_2):\n            os.remove(var_2)\n        print(f'ERROR: {e}\\nRe-attempting {var_1} to {var_2} ...')\n        os.system(f\"curl -L '{var_1}' -o '{var_2}' --retry 3 -C -\"\n                  )  # curl download, retry and resume on fail\n    finally:\n        if osp.exists(var_2) and osp.getsize(var_2) < var_3:\n            os.remove(var_2)  # remove partial downloads\n\n",
      "lines_processed": 19,
      "total_lines": 83,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "test_init_backbone.py",
      "code": "def _get_config_module(fname):\n    \"\"\"Load a configuration as a python module.\"\"\"\n    config_dpath = _get_config_directory()\n    config_fpath = join(config_dpath, fname)\n    config_mod = Config.fromfile(config_fpath)\n    return config_mod\n",
      "variables": [
        "fname",
        "config_dpath",
        "config_fpath",
        "config_mod"
      ],
      "anonymized_code": "def _get_config_module(var_1):\n    \"\"\"Load a configuration as a python module.\"\"\"\n    var_2 = _get_config_directory()\n    var_3 = join(var_2, var_1)\n    var_4 = Config.fromfile(var_3)\n    return var_4\n",
      "lines_processed": 6,
      "total_lines": 178,
      "llm_code": "def _get_config_module(config_file):\n    \"\"\"Load a configuration as a python module.\"\"\"\n    config_dir = _get_config_directory()\n    config_path = join(config_dir, config_file)\n    config = Config.fromfile(config_path)\n    return config\n",
      "llm_variables": [
        "config_file",
        "config_dir",
        "config_path",
        "config"
      ]
    },
    {
      "file_path": "test_init_backbone.py",
      "code": "def _traversed_config_file():\n    \"\"\"We traversed all potential config files under the `config` file. If you\n    need to print details or debug code, you can use this function.\n\n    If the `backbone.init_cfg` is None (do not use `Pretrained` init way), you\n    need add the folder name in `ignores_folder` (if the config files in this\n    folder all set backbone.init_cfg is None) or add config name in\n    `ignores_file` (if the config file set backbone.init_cfg is None)\n    \"\"\"\n    config_path = _get_config_directory()\n    check_cfg_names = []\n\n    # `base`, `legacy_1.x` and `common` ignored by default.\n    ignores_folder = ['_base_', 'legacy_1.x', 'common']\n    # 'ld' need load teacher model, if want to check 'ld',\n    # please check teacher_config path first.\n    ignores_folder += ['ld']\n    # `selfsup_pretrain` need convert model, if want to check this model,\n    # need to convert the model first.\n",
      "variables": [
        "config_path",
        "check_cfg_names",
        "ignores_folder"
      ],
      "anonymized_code": "def _traversed_config_file():\n    \"\"\"We traversed all potential config files under the `config` file. If you\n    need to print details or debug code, you can use this function.\n\n    If the `backbone.init_cfg` is None (do not use `Pretrained` init way), you\n    need add the folder name in `var_3` (if the config files in this\n    folder all set backbone.init_cfg is None) or add config name in\n    `ignores_file` (if the config file set backbone.init_cfg is None)\n    \"\"\"\n    var_1 = _get_config_directory()\n    var_2 = []\n\n    # `base`, `legacy_1.x` and `common` ignored by default.\n    var_3 = ['_base_', 'legacy_1.x', 'common']\n    # 'ld' need load teacher model, if want to check 'ld',\n    # please check teacher_config path first.\n    var_3 += ['ld']\n    # `selfsup_pretrain` need convert model, if want to check this model,\n    # need to convert the model first.\n",
      "lines_processed": 19,
      "total_lines": 178,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "test_init_backbone.py",
      "code": "def _test_load_pretrained():\n    \"\"\"We traversed all potential config files under the `config` file. If you\n    need to print details or debug code, you can use this function.\n\n    Returns:\n        check_cfg_names (list[str]): Config files that backbone initialized\n        from pretrained checkpoint might be problematic. Need to recheck\n        the config file. The output including the config files that the\n        backbone.init_cfg is None\n    \"\"\"\n    check_cfg_names = _traversed_config_file()\n    need_check_cfg = []\n\n    prog_bar = ProgressBar(len(check_cfg_names))\n    for config in check_cfg_names:\n        init_cfg_name = _check_backbone(config)\n        if init_cfg_name is not None:\n            need_check_cfg.append(init_cfg_name)\n        prog_bar.update()\n",
      "variables": [
        "check_cfg_names",
        "need_check_cfg",
        "prog_bar",
        "config",
        "init_cfg_name"
      ],
      "anonymized_code": "def _test_load_pretrained():\n    \"\"\"We traversed all potential var_4 files under the `var_4` file. If you\n    need to print details or debug code, you can use this function.\n\n    Returns:\n        var_1 (list[str]): Config files that backbone initialized\n        from pretrained checkpoint might be problematic. Need to recheck\n        the var_4 file. The output including the var_4 files that the\n        backbone.init_cfg is None\n    \"\"\"\n    var_1 = _traversed_config_file()\n    var_2 = []\n\n    var_3 = ProgressBar(len(var_1))\n    for var_4 in var_1:\n        var_5 = _check_backbone(var_4)\n        if var_5 is not None:\n            var_2.append(var_5)\n        var_3.update()\n",
      "lines_processed": 19,
      "total_lines": 178,
      "llm_code": "def _test_load_pretrained():\n    \"\"\"We traversed all potential config_file files under the `config_file` file. If you\n    need to print details or debug code, you can use this function.\n\n    Returns:\n        bad_config_list (list[str]): Config files that backbone initialized\n        from pretrained checkpoint might be problematic. Need to recheck\n        the config_file file. The output including the config_file files that the\n        backbone.init_cfg is None\n    \"\"\"\n    bad_config_list = _traversed_config_file()\n    problematic_configs = []\n\n    progress_bar = ProgressBar(len(bad_config_list))\n    for config_file in bad_config_list:\n        is_problematic = _check_backbone(config_file)\n        if is_problematic is not None:\n            problematic_configs.append(is_problematic)\n        progress_bar.update()\n",
      "llm_variables": [
        "bad_config_list",
        "problematic_configs",
        "progress_bar",
        "config_file",
        "is_problematic"
      ]
    },
    {
      "file_path": "test_init_backbone.py",
      "code": "def _get_detector_cfg(fname):\n    \"\"\"Grab configs necessary to create a detector.\n\n    These are deep copied to allow for safe modification of parameters without\n    influencing other tests.\n    \"\"\"\n    config = _get_config_module(fname)\n    model = copy.deepcopy(config.model)\n    return model\n",
      "variables": [
        "fname",
        "config",
        "model"
      ],
      "anonymized_code": "def _get_detector_cfg(var_1):\n    \"\"\"Grab configs necessary to create a detector.\n\n    These are deep copied to allow for safe modification of parameters without\n    influencing other tests.\n    \"\"\"\n    var_2 = _get_config_module(var_1)\n    var_3 = copy.deepcopy(var_2.var_3)\n    return var_3\n",
      "lines_processed": 9,
      "total_lines": 178,
      "llm_code": "def _get_detector_cfg(cfg):\n    \"\"\"Grab configs necessary to create a detector.\n\n    These are deep copied to allow for safe modification of parameters without\n    influencing other tests.\n    \"\"\"\n    module = _get_config_module(cfg)\n    detector_cfg = copy.deepcopy(module.detector_cfg)\n    return detector_cfg\n",
      "llm_variables": [
        "cfg",
        "module",
        "detector_cfg"
      ]
    },
    {
      "file_path": "test_init_backbone.py",
      "code": "def test_load_pretrained(config):\n    \"\"\"Check out backbone whether successfully load pretrained model by using\n    `backbone.init_cfg`.\n\n    Details please refer to `_check_backbone`\n    \"\"\"\n    _check_backbone(config, print_cfg=False)\n",
      "variables": [
        "config"
      ],
      "anonymized_code": "def test_load_pretrained(var_1):\n    \"\"\"Check out backbone whether successfully load pretrained model by using\n    `backbone.init_cfg`.\n\n    Details please refer to `_check_backbone`\n    \"\"\"\n    _check_backbone(var_1, print_cfg=False)\n",
      "lines_processed": 7,
      "total_lines": 178,
      "llm_code": "def test_load_pretrained(backbone):\n    \"\"\"Check out backbone whether successfully load pretrained model by using\n    `backbone.init_cfg`.\n\n    Details please refer to `_check_backbone`\n    \"\"\"\n    _check_backbone(backbone, print_cfg=False)\n",
      "llm_variables": [
        "backbone"
      ]
    },
    {
      "file_path": "test_init_backbone.py",
      "code": "def _get_config_directory():\n    \"\"\"Find the predefined detector config directory.\"\"\"\n    try:\n        # Assume we are running in the source mmdetection repo\n        repo_dpath = dirname(dirname(__file__))\n    except NameError:\n        # For IPython development when this __file__ is not defined\n        import mmdet\n        repo_dpath = dirname(dirname(mmdet.__file__))\n    config_dpath = join(repo_dpath, 'configs')\n    if not exists(config_dpath):\n        raise Exception('Cannot find config path')\n    return config_dpath\n",
      "variables": [
        "repo_dpath",
        "config_dpath"
      ],
      "anonymized_code": "def _get_config_directory():\n    \"\"\"Find the predefined detector config directory.\"\"\"\n    try:\n        # Assume we are running in the source mmdetection repo\n        var_1 = dirname(dirname(__file__))\n    except NameError:\n        # For IPython development when this __file__ is not defined\n        import mmdet\n        var_1 = dirname(dirname(mmdet.__file__))\n    var_2 = join(var_1, 'configs')\n    if not exists(var_2):\n        raise Exception('Cannot find config path')\n    return var_2\n",
      "lines_processed": 13,
      "total_lines": 178,
      "llm_code": "def _get_config_directory():\n    \"\"\"Find the predefined detector config directory.\"\"\"\n    try:\n        # Assume we are running in the source mmdetection repo\n        root = dirname(dirname(__file__))\n    except NameError:\n        # For IPython development when this __file__ is not defined\n        import mmdet\n        root = dirname(dirname(mmdet.__file__))\n    config_dir = join(root, 'configs')\n    if not exists(config_dir):\n        raise Exception('Cannot find config path')\n    return config_dir\n",
      "llm_variables": [
        "root",
        "config_dir"
      ]
    },
    {
      "file_path": "gather_test_benchmark_metric.py",
      "code": "def parse_args():\n    parser = argparse.ArgumentParser(\n        description='Gather benchmarked models metric')\n    parser.add_argument('config', help='test config file path')\n    parser.add_argument(\n        'root',\n        type=str,\n        help='root path of benchmarked models to be gathered')\n    parser.add_argument(\n        '--out', type=str, help='output path of gathered metrics to be stored')\n    parser.add_argument(\n        '--not-show', action='store_true', help='not show metrics')\n    parser.add_argument(\n        '--show-all', action='store_true', help='show all model metrics')\n\n    args = parser.parse_args()\n    return args\n",
      "variables": [
        "parser",
        "args"
      ],
      "anonymized_code": "def parse_args():\n    var_1 = argparse.ArgumentParser(\n        description='Gather benchmarked models metric')\n    var_1.add_argument('config', help='test config file path')\n    var_1.add_argument(\n        'root',\n        type=str,\n        help='root path of benchmarked models to be gathered')\n    var_1.add_argument(\n        '--out', type=str, help='output path of gathered metrics to be stored')\n    var_1.add_argument(\n        '--not-show', action='store_true', help='not show metrics')\n    var_1.add_argument(\n        '--show-all', action='store_true', help='show all model metrics')\n\n    var_2 = var_1.parse_args()\n    return var_2\n",
      "lines_processed": 17,
      "total_lines": 96,
      "llm_code": "def parse_args():\n    parser = argparse.ArgumentParser(\n        description='Gather benchmarked models metric')\n    parser.add_argument('config', help='test config file path')\n    parser.add_argument(\n        'root',\n        type=str,\n        help='root path of benchmarked models to be gathered')\n    parser.add_argument(\n        '--out', type=str, help='output path of gathered metrics to be stored')\n    parser.add_argument(\n        '--not-show', action='store_true', help='not show metrics')\n    parser.add_argument(\n        '--show-all', action='store_true', help='show all model metrics')\n\n    args = parser.parse_args()\n    return args\n",
      "llm_variables": [
        "parser",
        "args"
      ]
    },
    {
      "file_path": "benchmark_test_image.py",
      "code": "def main(args):\n    # register all modules in mmdet into the registries\n    register_all_modules()\n\n    config = Config.fromfile(args.config)\n\n    # init visualizer\n    visualizer_cfg = dict(type='DetLocalVisualizer', name='visualizer')\n    visualizer = VISUALIZERS.build(visualizer_cfg)\n\n    # test single model\n    if args.model_name:\n        if args.model_name in config:\n            model_infos = config[args.model_name]\n            if not isinstance(model_infos, list):\n                model_infos = [model_infos]\n            model_info = model_infos[0]\n            config_name = model_info['config'].strip()\n            print(f'processing: {config_name}', flush=True)\n",
      "variables": [
        "args",
        "config",
        "visualizer_cfg",
        "visualizer",
        "model_infos",
        "model_info",
        "config_name"
      ],
      "anonymized_code": "def main(var_1):\n    # register all modules in mmdet into the registries\n    register_all_modules()\n\n    var_2 = Config.fromfile(var_1.var_2)\n\n    # init var_4\n    var_3 = dict(type='DetLocalVisualizer', name='var_4')\n    var_4 = VISUALIZERS.build(var_3)\n\n    # test single model\n    if var_1.model_name:\n        if var_1.model_name in var_2:\n            var_5 = var_2[var_1.model_name]\n            if not isinstance(var_5, list):\n                var_5 = [var_5]\n            var_6 = var_5[0]\n            var_7 = var_6['var_2'].strip()\n            print(f'processing: {var_7}', flush=True)\n",
      "lines_processed": 19,
      "total_lines": 134,
      "llm_code": "def main(config):\n    # register all modules in mmdet into the registries\n    register_all_modules()\n\n    config = Config.fromfile(config.config_file)\n\n    # init visualizer\n    visualizer_config = dict(type=\"DetLocalVisualizer\", name=\"visualizer\")\n    visualizer = VISUALIZERS.build(visualizer_config)\n\n    # test single model\n    if config.model_name:\n        if config.model_name in config:\n            model_config = config[config.model_name]\n            if not isinstance(model_config, list):\n                model_config = [model_config]\n            first_model = model_config[0]\n            model_type = first_model[\"type\"].strip()\n            print(f\"processing: {model_type}\", flush=True)",
      "llm_variables": [
        "config",
        "config",
        "visualizer_config",
        "visualizer",
        "model_config",
        "first_model",
        "model_type"
      ]
    },
    {
      "file_path": "benchmark_test_image.py",
      "code": "def inference_model(config_name, checkpoint, visualizer, args, logger=None):\n    cfg = Config.fromfile(config_name)\n    if args.aug:\n        raise NotImplementedError()\n\n    model = init_detector(\n        cfg, checkpoint, palette=args.palette, device=args.device)\n    visualizer.dataset_meta = model.dataset_meta\n\n    # test a single image\n    result = inference_detector(model, args.img)\n\n    # show the results\n    if args.show or args.out_dir is not None:\n        img = mmcv.imread(args.img)\n        img = mmcv.imconvert(img, 'bgr', 'rgb')\n        out_file = None\n        if args.out_dir is not None:\n            out_dir = args.out_dir\n",
      "variables": [
        "config_name",
        "checkpoint",
        "visualizer",
        "args",
        "logger",
        "cfg",
        "model",
        "result",
        "img",
        "out_file",
        "out_dir"
      ],
      "anonymized_code": "def inference_model(var_1, var_2, var_3, var_4, var_5=None):\n    var_6 = Config.fromfile(var_1)\n    if var_4.aug:\n        raise NotImplementedError()\n\n    var_7 = init_detector(\n        var_6, var_2, palette=var_4.palette, device=var_4.device)\n    var_3.dataset_meta = var_7.dataset_meta\n\n    # test a single image\n    var_8 = inference_detector(var_7, var_4.var_9)\n\n    # show the results\n    if var_4.show or var_4.var_11 is not None:\n        var_9 = mmcv.imread(var_4.var_9)\n        var_9 = mmcv.imconvert(var_9, 'bgr', 'rgb')\n        var_10 = None\n        if var_4.var_11 is not None:\n            var_11 = var_4.var_11\n",
      "lines_processed": 19,
      "total_lines": 134,
      "llm_code": "def inference_model(config, model_cfg, dataset, device, show=False):\n    config = Config.fromfile(config)\n    if model_cfg.aug:\n        raise NotImplementedError()\n\n    model = init_detector(\n        config, model_cfg, palette=model_cfg.palette, device=model_cfg.device)\n    dataset.dataset_meta = model.dataset_meta\n\n    # test a single image\n    result = inference_detector(model, model_cfg.image_path)\n\n    # show the results\n    if model_cfg.show or model_cfg.output_path is not None:\n        image = mmcv.imread(model_cfg.image_path)\n        image = mmcv.imconvert(image, \"bgr\", \"rgb\")\n        output_path = None\n        if model_cfg.output_path is not None:\n            output_path = model_cfg.output_path",
      "llm_variables": [
        "config",
        "model_cfg",
        "dataset",
        "device",
        "show",
        "config",
        "model",
        "dataset",
        "result",
        "image",
        "output_path"
      ]
    },
    {
      "file_path": "benchmark_test.py",
      "code": "def fast_test_model(config_name, checkpoint, args, logger=None):\n    cfg = Config.fromfile(config_name)\n    cfg = replace_cfg_vals(cfg)\n    cfg.launcher = args.launcher\n    if args.cfg_options is not None:\n        cfg.merge_from_dict(args.cfg_options)\n\n    # work_dir is determined in this priority: CLI > segment in file > filename\n    if args.work_dir is not None:\n        # update configs according to CLI args if args.work_dir is not None\n        cfg.work_dir = osp.join(args.work_dir,\n                                osp.splitext(osp.basename(config_name))[0])\n    elif cfg.get('work_dir', None) is None:\n        # use config filename as default work_dir if cfg.work_dir is None\n        cfg.work_dir = osp.join('./work_dirs',\n                                osp.splitext(osp.basename(config_name))[0])\n\n    if args.ceph:\n        replace_to_ceph(cfg)\n",
      "variables": [
        "config_name",
        "checkpoint",
        "args",
        "logger",
        "cfg"
      ],
      "anonymized_code": "def fast_test_model(var_1, var_2, var_3, var_4=None):\n    var_5 = Config.fromfile(var_1)\n    var_5 = replace_cfg_vals(var_5)\n    var_5.launcher = var_3.launcher\n    if var_3.cfg_options is not None:\n        var_5.merge_from_dict(var_3.cfg_options)\n\n    # work_dir is determined in this priority: CLI > segment in file > filename\n    if var_3.work_dir is not None:\n        # update configs according to CLI var_3 if var_3.work_dir is not None\n        var_5.work_dir = osp.join(var_3.work_dir,\n                                osp.splitext(osp.basename(var_1))[0])\n    elif var_5.get('work_dir', None) is None:\n        # use config filename as default work_dir if var_5.work_dir is None\n        var_5.work_dir = osp.join('./work_dirs',\n                                osp.splitext(osp.basename(var_1))[0])\n\n    if var_3.ceph:\n        replace_to_ceph(var_5)\n",
      "lines_processed": 19,
      "total_lines": 115,
      "llm_code": "def fast_test_model(config_file, launcher, cfg_options, work_dir=None):\n    config = Config.fromfile(config_file)\n    config = replace_cfg_vals(config)\n    config.launcher = cfg_options.launcher\n    if cfg_options.cfg_options is not None:\n        config.merge_from_dict(cfg_options.cfg_options)\n\n    # work_dir is determined in this priority: CLI > segment in file > filename\n    if cfg_options.work_dir is not None:\n        # update configs according to CLI cfg_options if cfg_options.work_dir is not None\n        config.work_dir = osp.join(cfg_options.work_dir,\n                                osp.splitext(osp.basename(config_file))[0])\n    elif config.get('work_dir', None) is None:\n        # use config filename as default work_dir if config.work_dir is None\n        config.work_dir = osp.join('./work_dirs',\n                                osp.splitext(osp.basename(config_file))[0])\n\n    if cfg_options.ceph:\n        replace_to_ceph(config)\n",
      "llm_variables": [
        "config_file",
        "launcher",
        "cfg_options",
        "work_dir",
        "config"
      ]
    },
    {
      "file_path": "benchmark_test.py",
      "code": "def main(args):\n    # register all modules in mmdet into the registries\n    register_all_modules(init_default_scope=False)\n\n    config = Config.fromfile(args.config)\n\n    # test all model\n    logger = MMLogger.get_instance(\n        name='MMLogger',\n        log_file='benchmark_test.log',\n        log_level=logging.ERROR)\n\n    for model_key in config:\n        model_infos = config[model_key]\n        if not isinstance(model_infos, list):\n            model_infos = [model_infos]\n        for model_info in model_infos:\n            print('processing: ', model_info['config'], flush=True)\n            config_name = model_info['config'].strip()\n",
      "variables": [
        "args",
        "config",
        "logger",
        "model_key",
        "model_infos",
        "model_info",
        "config_name"
      ],
      "anonymized_code": "def main(var_1):\n    # register all modules in mmdet into the registries\n    register_all_modules(init_default_scope=False)\n\n    var_2 = Config.fromfile(var_1.var_2)\n\n    # test all model\n    var_3 = MMLogger.get_instance(\n        name='MMLogger',\n        log_file='benchmark_test.log',\n        log_level=logging.ERROR)\n\n    for var_4 in var_2:\n        var_5 = var_2[var_4]\n        if not isinstance(var_5, list):\n            var_5 = [var_5]\n        for var_6 in var_5:\n            print('processing: ', var_6['var_2'], flush=True)\n            var_7 = var_6['var_2'].strip()\n",
      "lines_processed": 19,
      "total_lines": 115,
      "llm_code": "def main(config):\n    # register all modules in mmdet into the registries\n    register_all_modules(init_default_scope=False)\n\n    config = Config.fromfile(config.config)\n\n    # test all model\n    logger = MMLogger.get_instance(\n        name='MMLogger',\n        log_file='benchmark_test.log',\n        log_level=logging.ERROR)\n\n    for model_config in config:\n        model_config = config[model_config]\n        if not isinstance(model_config, list):\n            model_config = [model_config]\n        for model in model_config:\n            print('processing: ', model['config'], flush=True)\n            config_name = model['config'].strip()\n",
      "llm_variables": [
        "config",
        "config",
        "logger",
        "model_config",
        "model_config",
        "model",
        "config_name"
      ]
    },
    {
      "file_path": "benchmark_train.py",
      "code": "def parse_args():\n    parser = ArgumentParser()\n    parser.add_argument('config', help='test config file path')\n    parser.add_argument('--work-dir', help='the dir to save logs and models')\n    parser.add_argument('--ceph', action='store_true')\n    parser.add_argument('--save-ckpt', action='store_true')\n    parser.add_argument(\n        '--amp',\n        action='store_true',\n        default=False,\n        help='enable automatic-mixed-precision training')\n    parser.add_argument(\n        '--auto-scale-lr',\n        action='store_true',\n        help='enable automatically scaling LR.')\n    parser.add_argument(\n        '--resume',\n        action='store_true',\n        help='resume from the latest checkpoint in the work_dir automatically')\n",
      "variables": [
        "parser"
      ],
      "anonymized_code": "def parse_args():\n    var_1 = ArgumentParser()\n    var_1.add_argument('config', help='test config file path')\n    var_1.add_argument('--work-dir', help='the dir to save logs and models')\n    var_1.add_argument('--ceph', action='store_true')\n    var_1.add_argument('--save-ckpt', action='store_true')\n    var_1.add_argument(\n        '--amp',\n        action='store_true',\n        default=False,\n        help='enable automatic-mixed-precision training')\n    var_1.add_argument(\n        '--auto-scale-lr',\n        action='store_true',\n        help='enable automatically scaling LR.')\n    var_1.add_argument(\n        '--resume',\n        action='store_true',\n        help='resume from the latest checkpoint in the work_dir automatically')\n",
      "lines_processed": 19,
      "total_lines": 178,
      "llm_code": "def parse_args():\n    parser = ArgumentParser()\n    parser.add_argument('config', help='test config file path')\n    parser.add_argument('--work-dir', help='the dir to save logs and models')\n    parser.add_argument('--ceph', action='store_true')\n    parser.add_argument('--save-ckpt', action='store_true')\n    parser.add_argument(\n        '--amp',\n        action='store_true',\n        default=False,\n        help='enable automatic-mixed-precision training')\n    parser.add_argument(\n        '--auto-scale-lr',\n        action='store_true',\n        help='enable automatically scaling LR.')\n    parser.add_argument(\n        '--resume',\n        action='store_true',\n        help='resume from the latest checkpoint in the work_dir automatically')\n",
      "llm_variables": [
        "parser"
      ]
    },
    {
      "file_path": "benchmark_train.py",
      "code": "def main(args):\n    # register all modules in mmdet into the registries\n    register_all_modules(init_default_scope=False)\n\n    config = Config.fromfile(args.config)\n\n    # test all model\n    logger = MMLogger.get_instance(\n        name='MMLogger',\n        log_file='benchmark_train.log',\n        log_level=logging.ERROR)\n\n    for model_key in config:\n        model_infos = config[model_key]\n        if not isinstance(model_infos, list):\n            model_infos = [model_infos]\n        for model_info in model_infos:\n            print('processing: ', model_info['config'], flush=True)\n            config_name = model_info['config'].strip()\n",
      "variables": [
        "args",
        "config",
        "logger",
        "model_key",
        "model_infos",
        "model_info",
        "config_name"
      ],
      "anonymized_code": "def main(var_1):\n    # register all modules in mmdet into the registries\n    register_all_modules(init_default_scope=False)\n\n    var_2 = Config.fromfile(var_1.var_2)\n\n    # test all model\n    var_3 = MMLogger.get_instance(\n        name='MMLogger',\n        log_file='benchmark_train.log',\n        log_level=logging.ERROR)\n\n    for var_4 in var_2:\n        var_5 = var_2[var_4]\n        if not isinstance(var_5, list):\n            var_5 = [var_5]\n        for var_6 in var_5:\n            print('processing: ', var_6['var_2'], flush=True)\n            var_7 = var_6['var_2'].strip()\n",
      "lines_processed": 19,
      "total_lines": 178,
      "llm_code": "def main(config):\n    # register all modules in mmdet into the registries\n    register_all_modules(init_default_scope=False)\n\n    config_ = Config.fromfile(config.var_2)\n\n    # test all model\n    logger = MMLogger.get_instance(\n        name=\"MMLogger\",\n        log_file=\"benchmark_train.log\",\n        log_level=logging.ERROR)\n\n    for model_config in config_:\n        model_config_ = config_[model_config]\n        if not isinstance(model_config_, list):\n            model_config_ = [model_config_]\n        for model in model_config_:\n            print(\"processing: \", model[\"var_2\"], flush=True)\n            var_7 = model[\"var_2\"].strip()",
      "llm_variables": [
        "config",
        "config_",
        "logger",
        "model_config",
        "model_config_",
        "model",
        "var_7"
      ]
    },
    {
      "file_path": "benchmark_train.py",
      "code": "def fast_train_model(config_name, args, logger=None):\n    cfg = Config.fromfile(config_name)\n    cfg = replace_cfg_vals(cfg)\n    cfg.launcher = args.launcher\n    if args.cfg_options is not None:\n        cfg.merge_from_dict(args.cfg_options)\n\n    # work_dir is determined in this priority: CLI > segment in file > filename\n    if args.work_dir is not None:\n        # update configs according to CLI args if args.work_dir is not None\n        cfg.work_dir = osp.join(args.work_dir,\n                                osp.splitext(osp.basename(config_name))[0])\n    elif cfg.get('work_dir', None) is None:\n        # use config filename as default work_dir if cfg.work_dir is None\n        cfg.work_dir = osp.join('./work_dirs',\n                                osp.splitext(osp.basename(config_name))[0])\n\n    ckpt_hook = cfg.default_hooks.checkpoint\n    by_epoch = ckpt_hook.get('by_epoch', True)\n",
      "variables": [
        "config_name",
        "args",
        "logger",
        "cfg",
        "ckpt_hook",
        "by_epoch"
      ],
      "anonymized_code": "def fast_train_model(var_1, var_2, var_3=None):\n    var_4 = Config.fromfile(var_1)\n    var_4 = replace_cfg_vals(var_4)\n    var_4.launcher = var_2.launcher\n    if var_2.cfg_options is not None:\n        var_4.merge_from_dict(var_2.cfg_options)\n\n    # work_dir is determined in this priority: CLI > segment in file > filename\n    if var_2.work_dir is not None:\n        # update configs according to CLI var_2 if var_2.work_dir is not None\n        var_4.work_dir = osp.join(var_2.work_dir,\n                                osp.splitext(osp.basename(var_1))[0])\n    elif var_4.get('work_dir', None) is None:\n        # use config filename as default work_dir if var_4.work_dir is None\n        var_4.work_dir = osp.join('./work_dirs',\n                                osp.splitext(osp.basename(var_1))[0])\n\n    var_5 = var_4.default_hooks.checkpoint\n    var_6 = var_5.get('var_6', True)\n",
      "lines_processed": 19,
      "total_lines": 178,
      "llm_code": "def fast_train_model(cfg_file, launcher, cfg_options=None):\n    config = Config.fromfile(cfg_file)\n    config = replace_cfg_vals(config)\n    config.launcher = launcher.launcher\n    if launcher.cfg_options is not None:\n        config.merge_from_dict(launcher.cfg_options)\n\n    # work_dir is determined in this priority: CLI > segment in file > filename\n    if launcher.work_dir is not None:\n        # update configs according to CLI launcher if launcher.work_dir is not None\n        config.work_dir = osp.join(launcher.work_dir,\n                                osp.splitext(osp.basename(cfg_file))[0])\n    elif config.get('work_dir', None) is None:\n        # use config filename as default work_dir if config.work_dir is None\n        config.work_dir = osp.join('./work_dirs',\n                                osp.splitext(osp.basename(cfg_file))[0])\n\n    checkpoint_hook = config.default_hooks.checkpoint\n    var_6 = checkpoint_hook.get('var_6', True)\n",
      "llm_variables": [
        "cfg_file",
        "launcher",
        "cfg_options",
        "config",
        "checkpoint_hook",
        "var_6"
      ]
    }
  ],
  "facebookresearch_fairseq": [
    {
      "file_path": "fairseqmmmodel.py",
      "code": "def mmarch(args):\n    pass\n",
      "variables": [
        "args"
      ],
      "anonymized_code": "def mmarch(var_1):\n    pass\n",
      "lines_processed": 2,
      "total_lines": 51,
      "llm_code": "def mmarch(name):\n    pass\n",
      "llm_variables": [
        "name"
      ]
    },
    {
      "file_path": "locallaunch.py",
      "code": "def main(args):\n    job_type = args.jobtype if args.jobtype else None\n    # parse multiple pipelines.\n    pipelines = [Pipeline(fn) for fn in args.yamls.split(\",\")]\n\n    for pipe_id, pipeline in enumerate(pipelines):\n        if not hasattr(pipeline, \"project_dir\"):\n            for job in pipeline[0]:\n                job(job_type=job_type, dryrun=args.dryrun)\n",
      "variables": [
        "args",
        "job_type",
        "pipelines",
        "fn",
        "pipe_id",
        "pipeline",
        "job"
      ],
      "anonymized_code": "def main(var_1):\n    var_2 = var_1.jobtype if var_1.jobtype else None\n    # parse multiple var_3.\n    var_3 = [Pipeline(var_4) for var_4 in var_1.yamls.split(\",\")]\n\n    for var_5, var_6 in enumerate(var_3):\n        if not hasattr(var_6, \"project_dir\"):\n            for var_7 in var_6[0]:\n                var_7(var_2=var_2, dryrun=var_1.dryrun)\n",
      "lines_processed": 9,
      "total_lines": 148,
      "llm_code": "def main(job):\n    var_2 = job.jobtype if job.jobtype else None\n    # parse multiple var_3.\n    var_3 = [Pipeline(var_4) for var_4 in job.yamls.split(\",\")]\n\n    for var_5, var_6 in enumerate(var_3):\n        if not hasattr(var_6, \"project_dir\"):\n            for var_7 in var_6[0]:\n                var_7(var_2=var_2, dryrun=job.dryrun)\n",
      "llm_variables": [
        "job",
        "var_2",
        "var_3",
        "var_4",
        "var_5",
        "var_6",
        "var_7"
      ]
    }
  ],
  "pytorch_pytorch": [
    {
      "file_path": "manylinux1-check.py",
      "code": "def have_compatible_glibc(major, minimum_minor):\n    import ctypes\n\n    process_namespace = ctypes.CDLL(None)\n    try:\n        gnu_get_libc_version = process_namespace.gnu_get_libc_version\n    except AttributeError:\n        # Symbol doesn't exist -> therefore, we are not linked to\n        # glibc.\n        return False\n\n    # Call gnu_get_libc_version, which returns a string like \"2.5\".\n    gnu_get_libc_version.restype = ctypes.c_char_p\n    version_str = gnu_get_libc_version()\n    # py2 / py3 compatibility:\n    if not isinstance(version_str, str):\n        version_str = version_str.decode(\"ascii\")\n\n    # Parse string and check against requested version.\n",
      "variables": [
        "major",
        "minimum_minor",
        "process_namespace",
        "gnu_get_libc_version",
        "version_str"
      ],
      "anonymized_code": "def have_compatible_glibc(var_1, var_2):\n    import ctypes\n\n    var_3 = ctypes.CDLL(None)\n    try:\n        var_4 = var_3.var_4\n    except AttributeError:\n        # Symbol doesn't exist -> therefore, we are not linked to\n        # glibc.\n        return False\n\n    # Call var_4, which returns a string like \"2.5\".\n    var_4.restype = ctypes.c_char_p\n    var_5 = var_4()\n    # py2 / py3 compatibility:\n    if not isinstance(var_5, str):\n        var_5 = var_5.decode(\"ascii\")\n\n    # Parse string and check against requested version.\n",
      "lines_processed": 19,
      "total_lines": 60,
      "llm_code": "def have_compatible_glibc(glibc_lib, glibc_version):\n    import ctypes\n\n    glibc_handle = ctypes.CDLL(None)\n    try:\n        glibc_version_symbol = glibc_handle.glibc_version_symbol\n    except AttributeError:\n        # Symbol doesn't exist -> therefore, we are not linked to\n        # glibc.\n        return False\n\n    # Call glibc_version_symbol, which returns a string like \"2.5\".\n    glibc_version_symbol.restype = ctypes.c_char_p\n    glibc_version_string = glibc_version_symbol()\n    # py2 / py3 compatibility:\n    if not isinstance(glibc_version_string, str):\n        glibc_version_string = glibc_version_string.decode(\"ascii\")\n\n    # Parse string and check against requested version.\n",
      "llm_variables": [
        "glibc_lib",
        "glibc_version",
        "glibc_handle",
        "glibc_version_symbol",
        "glibc_version_string"
      ]
    },
    {
      "file_path": "build_aarch64_wheel.py",
      "code": "def wait_for_connection(addr, port, timeout=15, attempt_cnt=5):\n    import socket\n\n    for i in range(attempt_cnt):\n        try:\n            with socket.create_connection((addr, port), timeout=timeout):\n                return\n        except (ConnectionRefusedError, socket.timeout):  # noqa: PERF203\n            if i == attempt_cnt - 1:\n                raise\n            time.sleep(timeout)\n",
      "variables": [
        "addr",
        "port",
        "timeout",
        "attempt_cnt",
        "i"
      ],
      "anonymized_code": "def wait_for_connection(var_1, var_2, var_3=15, var_4=5):\n    import socket\n\n    for var_5 in range(var_4):\n        try:\n            with socket.create_connection((var_1, var_2), var_3=var_3):\n                return\n        except (ConnectionRefusedError, socket.var_3):  # noqa: PERF203\n            if var_5 == var_4 - 1:\n                raise\n            time.sleep(var_3)\n",
      "lines_processed": 11,
      "total_lines": 1037,
      "llm_code": "def wait_for_connection(host, port, timeout=15, retry_count=5):\n    import socket\n\n    for _ in range(retry_count):\n        try:\n            with socket.create_connection((host, port), var_3=timeout):\n                return\n        except (ConnectionRefusedError, socket.timeout):  # noqa: PERF203\n            if _ == retry_count - 1:\n                raise\n            time.sleep(timeout)",
      "llm_variables": [
        "host",
        "port",
        "timeout",
        "retry_count",
        "_"
      ]
    },
    {
      "file_path": "build_aarch64_wheel.py",
      "code": "def ec2_instances_by_id(instance_id):\n    rc = list(ec2_get_instances(\"instance-id\", instance_id))\n    return rc[0] if len(rc) > 0 else None\n",
      "variables": [
        "instance_id",
        "rc"
      ],
      "anonymized_code": "def ec2_instances_by_id(var_1):\n    var_2 = list(ec2_get_instances(\"instance-id\", var_1))\n    return var_2[0] if len(var_2) > 0 else None\n",
      "lines_processed": 3,
      "total_lines": 1037,
      "llm_code": "def ec2_instances_by_id(instance_ids):\n    instances = list(ec2_get_instances(\"instance-id\", instance_ids))\n    return instances[0] if len(instances) > 0 else None\n",
      "llm_variables": [
        "instance_ids",
        "instances"
      ]
    },
    {
      "file_path": "build_aarch64_wheel.py",
      "code": "def build_ArmComputeLibrary(host: RemoteHost, git_clone_flags: str = \"\") -> None:\n    print(\"Building Arm Compute Library\")\n    acl_build_flags = \" \".join(\n        [\n            \"debug=0\",\n            \"neon=1\",\n            \"opencl=0\",\n            \"os=linux\",\n            \"openmp=1\",\n            \"cppthreads=0\",\n            \"arch=armv8a\",\n            \"multi_isa=1\",\n            \"fixed_format_kernels=1\",\n            \"build=native\",\n        ]\n    )\n    host.run_cmd(\n        f\"git clone https://github.com/ARM-software/ComputeLibrary.git -b v25.02 {git_clone_flags}\"\n    )\n",
      "variables": [
        "host",
        "git_clone_flags",
        "acl_build_flags"
      ],
      "anonymized_code": "def build_ArmComputeLibrary(var_1: RemoteHost, var_2: str = \"\") -> None:\n    print(\"Building Arm Compute Library\")\n    var_3 = \" \".join(\n        [\n            \"debug=0\",\n            \"neon=1\",\n            \"opencl=0\",\n            \"os=linux\",\n            \"openmp=1\",\n            \"cppthreads=0\",\n            \"arch=armv8a\",\n            \"multi_isa=1\",\n            \"fixed_format_kernels=1\",\n            \"build=native\",\n        ]\n    )\n    var_1.run_cmd(\n        f\"git clone https://github.com/ARM-software/ComputeLibrary.git -b v25.02 {var_2}\"\n    )\n",
      "lines_processed": 19,
      "total_lines": 1037,
      "llm_code": "def build_ArmComputeLibrary(host: RemoteHost, build_dir: str = \"\") -> None:\n    print(\"Building Arm Compute Library\")\n    config = \" \".join(\n        [\n            \"debug=0\",\n            \"neon=1\",\n            \"opencl=0\",\n            \"os=linux\",\n            \"openmp=1\",\n            \"cppthreads=0\",\n            \"arch=armv8a\",\n            \"multi_isa=1\",\n            \"fixed_format_kernels=1\",\n            \"build=native\",\n        ]\n    )\n    host.run_cmd(\n        f\"git clone https://github.com/ARM-software/ComputeLibrary.git -b v25.02 {build_dir}\"\n    )\n",
      "llm_variables": [
        "host",
        "build_dir",
        "config"
      ]
    },
    {
      "file_path": "build_aarch64_wheel.py",
      "code": "def update_apt_repo(host: RemoteHost) -> None:\n    time.sleep(5)\n    host.run_cmd(\"sudo systemctl stop apt-daily.service || true\")\n    host.run_cmd(\"sudo systemctl stop unattended-upgrades.service || true\")\n    host.run_cmd(\n        \"while systemctl is-active --quiet apt-daily.service; do sleep 1; done\"\n    )\n    host.run_cmd(\n        \"while systemctl is-active --quiet unattended-upgrades.service; do sleep 1; done\"\n    )\n    host.run_cmd(\"sudo apt-get update\")\n    time.sleep(3)\n    host.run_cmd(\"sudo apt-get update\")\n",
      "variables": [
        "host"
      ],
      "anonymized_code": "def update_apt_repo(var_1: RemoteHost) -> None:\n    time.sleep(5)\n    var_1.run_cmd(\"sudo systemctl stop apt-daily.service || true\")\n    var_1.run_cmd(\"sudo systemctl stop unattended-upgrades.service || true\")\n    var_1.run_cmd(\n        \"while systemctl is-active --quiet apt-daily.service; do sleep 1; done\"\n    )\n    var_1.run_cmd(\n        \"while systemctl is-active --quiet unattended-upgrades.service; do sleep 1; done\"\n    )\n    var_1.run_cmd(\"sudo apt-get update\")\n    time.sleep(3)\n    var_1.run_cmd(\"sudo apt-get update\")\n",
      "lines_processed": 13,
      "total_lines": 1037,
      "llm_code": "def update_apt_repo(host: RemoteHost) -> None:\n    time.sleep(5)\n    host.run_cmd(\"sudo systemctl stop apt-daily.service || true\")\n    host.run_cmd(\"sudo systemctl stop unattended-upgrades.service || true\")\n    host.run_cmd(\n        \"while systemctl is-active --quiet apt-daily.service; do sleep 1; done\"\n    )\n    host.run_cmd(\n        \"while systemctl is-active --quiet unattended-upgrades.service; do sleep 1; done\"\n    )\n    host.run_cmd(\"sudo apt-get update\")\n    time.sleep(3)\n    host.run_cmd(\"sudo apt-get update\")\n",
      "llm_variables": [
        "host"
      ]
    },
    {
      "file_path": "smoke_test.py",
      "code": "def load_json_from_basedir(filename: str):\n    try:\n        with open(BASE_DIR / filename) as fptr:\n            return json.load(fptr)\n    except FileNotFoundError as exc:\n        raise ImportError(f\"File {filename} not found error: {exc.strerror}\") from exc\n    except json.JSONDecodeError as exc:\n        raise ImportError(f\"Invalid JSON {filename}\") from exc\n",
      "variables": [
        "filename",
        "fptr"
      ],
      "anonymized_code": "def load_json_from_basedir(var_1: str):\n    try:\n        with open(BASE_DIR / var_1) as var_2:\n            return json.load(var_2)\n    except FileNotFoundError as exc:\n        raise ImportError(f\"File {var_1} not found error: {exc.strerror}\") from exc\n    except json.JSONDecodeError as exc:\n        raise ImportError(f\"Invalid JSON {var_1}\") from exc\n",
      "lines_processed": 8,
      "total_lines": 484,
      "llm_code": "def load_json_from_basedir(file_path: str):\n    try:\n        with open(BASE_DIR / file_path) as file:\n            return json.load(file)\n    except FileNotFoundError as exc:\n        raise ImportError(f\"File {file_path} not found error: {exc.strerror}\") from exc\n    except json.JSONDecodeError as exc:\n        raise ImportError(f\"Invalid JSON {file_path}\") from exc\n",
      "llm_variables": [
        "file_path",
        "file"
      ]
    },
    {
      "file_path": "smoke_test.py",
      "code": "def smoke_test_conv2d() -> None:\n    import torch.nn as nn\n\n    print(\"Testing smoke_test_conv2d\")\n    # With square kernels and equal stride\n    m = nn.Conv2d(16, 33, 3, stride=2)\n    # non-square kernels and unequal stride and with padding\n    m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n    assert m is not None\n    # non-square kernels and unequal stride and with padding and dilation\n    basic_conv = nn.Conv2d(\n        16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1)\n    )\n    input = torch.randn(20, 16, 50, 100)\n    output = basic_conv(input)\n\n    if is_cuda_system:\n        print(\"Testing smoke_test_conv2d with cuda\")\n        conv = nn.Conv2d(3, 3, 3).cuda()\n",
      "variables": [
        "m",
        "basic_conv",
        "input",
        "output",
        "conv"
      ],
      "anonymized_code": "def smoke_test_conv2d() -> None:\n    import torch.nn as nn\n\n    print(\"Testing smoke_test_conv2d\")\n    # With square kernels and equal stride\n    var_1 = nn.Conv2d(16, 33, 3, stride=2)\n    # non-square kernels and unequal stride and with padding\n    var_1 = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n    assert var_1 is not None\n    # non-square kernels and unequal stride and with padding and dilation\n    var_2 = nn.Conv2d(\n        16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1)\n    )\n    var_3 = torch.randn(20, 16, 50, 100)\n    var_4 = var_2(var_3)\n\n    if is_cuda_system:\n        print(\"Testing smoke_test_conv2d with cuda\")\n        var_5 = nn.Conv2d(3, 3, 3).cuda()\n",
      "lines_processed": 19,
      "total_lines": 484,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "smoke_test.py",
      "code": "def find_pypi_package_version(package: str) -> Optional[str]:\n    from importlib import metadata\n\n    dists = metadata.distributions()\n    for dist in dists:\n        if dist.metadata[\"Name\"].startswith(package):\n            return dist.version\n    return None\n",
      "variables": [
        "package",
        "dists",
        "dist"
      ],
      "anonymized_code": "def find_pypi_package_version(var_1: str) -> Optional[str]:\n    from importlib import metadata\n\n    var_2 = metadata.distributions()\n    for var_3 in var_2:\n        if var_3.metadata[\"Name\"].startswith(var_1):\n            return var_3.version\n    return None\n",
      "lines_processed": 8,
      "total_lines": 484,
      "llm_code": "def find_pypi_package_version(package_name: str) -> Optional[str]:\n    from importlib import metadata\n\n    distributions = metadata.distributions()\n    for distribution in distributions:\n        if distribution.metadata[\"Name\"].startswith(package_name):\n            return distribution.version\n    return None\n",
      "llm_variables": [
        "package_name",
        "distributions",
        "distribution"
      ]
    },
    {
      "file_path": "smoke_test.py",
      "code": "def test_cuda_runtime_errors_captured() -> None:\n    cuda_exception_missed = True\n    try:\n        print(\"Testing test_cuda_runtime_errors_captured\")\n        torch._assert_async(torch.tensor(0, device=\"cuda\"))\n        torch._assert_async(torch.tensor(0 + 0j, device=\"cuda\"))\n    except RuntimeError as e:\n        if re.search(\"CUDA\", f\"{e}\"):\n            print(f\"Caught CUDA exception with success: {e}\")\n            cuda_exception_missed = False\n        else:\n            raise e\n    if cuda_exception_missed:\n        raise RuntimeError(\"Expected CUDA RuntimeError but have not received!\")\n",
      "variables": [
        "cuda_exception_missed"
      ],
      "anonymized_code": "def test_cuda_runtime_errors_captured() -> None:\n    var_1 = True\n    try:\n        print(\"Testing test_cuda_runtime_errors_captured\")\n        torch._assert_async(torch.tensor(0, device=\"cuda\"))\n        torch._assert_async(torch.tensor(0 + 0j, device=\"cuda\"))\n    except RuntimeError as e:\n        if re.search(\"CUDA\", f\"{e}\"):\n            print(f\"Caught CUDA exception with success: {e}\")\n            var_1 = False\n        else:\n            raise e\n    if var_1:\n        raise RuntimeError(\"Expected CUDA RuntimeError but have not received!\")\n",
      "lines_processed": 14,
      "total_lines": 484,
      "llm_code": "def test_cuda_runtime_errors_captured() -> None:\n    expected = True\n    try:\n        print(\"Testing test_cuda_runtime_errors_captured\")\n        torch._assert_async(torch.tensor(0, device=\"cuda\"))\n        torch._assert_async(torch.tensor(0 + 0j, device=\"cuda\"))\n    except RuntimeError as e:\n        if re.search(\"CUDA\", f\"{e}\"):\n            print(f\"Caught CUDA exception with success: {e}\")\n            expected = False\n        else:\n            raise e\n    if expected:\n        raise RuntimeError(\"Expected CUDA RuntimeError but have not received!\")\n",
      "llm_variables": [
        "expected"
      ]
    },
    {
      "file_path": "smoke_test.py",
      "code": "def cudnn_to_version_str(cudnn_version: int) -> str:\n    patch = int(cudnn_version % 10)\n    minor = int((cudnn_version / 100) % 100)\n    major = int((cudnn_version / 10000) % 10000)\n    return f\"{major}.{minor}.{patch}\"\n",
      "variables": [
        "cudnn_version",
        "patch",
        "minor",
        "major"
      ],
      "anonymized_code": "def cudnn_to_version_str(var_1: int) -> str:\n    var_2 = int(var_1 % 10)\n    var_3 = int((var_1 / 100) % 100)\n    var_4 = int((var_1 / 10000) % 10000)\n    return f\"{var_4}.{var_3}.{var_2}\"\n",
      "lines_processed": 5,
      "total_lines": 484,
      "llm_code": "def cudnn_to_version_str(version_number: int) -> str:\n    major = int(version_number % 10)\n    minor = int((version_number / 100) % 100)\n    patch = int((version_number / 10000) % 10000)\n    return f\"{patch}.{minor}.{major}\"",
      "llm_variables": [
        "version_number",
        "major",
        "minor",
        "patch"
      ]
    },
    {
      "file_path": "reuse_old_whl.py",
      "code": "def check_labels_for_pr() -> bool:\n    # Check if the current commit is part of a PR and if it has the\n    # FORCE_REBUILD_LABEL\n    head_sha = get_head_sha()\n    url = f\"https://api.github.com/repos/pytorch/pytorch/commits/{head_sha}/pulls\"\n    response = query_github_api(url)\n\n    print(\n        f\"Found {len(response)} PRs for commit {head_sha}: {[pr['number'] for pr in response]}\"\n    )\n    for pr in response:\n        labels = pr.get(\"labels\", [])\n        for label in labels:\n            if label[\"name\"] == FORCE_REBUILD_LABEL:\n                print(f\"Found label {FORCE_REBUILD_LABEL} in PR {pr['number']}.\")\n                return True\n    return False\n",
      "variables": [
        "head_sha",
        "url",
        "response",
        "pr",
        "labels",
        "label"
      ],
      "anonymized_code": "def check_labels_for_pr() -> bool:\n    # Check if the current commit is part of a PR and if it has the\n    # FORCE_REBUILD_LABEL\n    var_1 = get_head_sha()\n    var_2 = f\"https://api.github.com/repos/pytorch/pytorch/commits/{var_1}/pulls\"\n    var_3 = query_github_api(var_2)\n\n    print(\n        f\"Found {len(var_3)} PRs for commit {var_1}: {[var_4['number'] for var_4 in var_3]}\"\n    )\n    for var_4 in var_3:\n        var_5 = var_4.get(\"var_5\", [])\n        for var_6 in var_5:\n            if var_6[\"name\"] == FORCE_REBUILD_LABEL:\n                print(f\"Found var_6 {FORCE_REBUILD_LABEL} in PR {var_4['number']}.\")\n                return True\n    return False\n",
      "lines_processed": 17,
      "total_lines": 351,
      "llm_code": "def check_labels_for_pr() -> bool:\n    # Check if the current commit is part of a PR and if it has the\n    # FORCE_REBUILD_LABEL\n    commit_sha = get_head_sha()\n    prs_url = f\"https://api.github.com/repos/pytorch/pytorch/commits/{commit_sha}/pulls\"\n    prs = query_github_api(prs_url)\n\n    print(\n        f\"Found {len(prs)} PRs for commit {commit_sha}: {[pr[\"number\"] for pr in prs]}\")\n    for pr in prs:\n        labels = pr.get(\"labels\", [])\n        for label in labels:\n            if label[\"name\"] == FORCE_REBUILD_LABEL:\n                print(f\"Found label {FORCE_REBUILD_LABEL} in PR {pr[\"number\"]}.\")\n                return True\n    return False",
      "llm_variables": [
        "commit_sha",
        "prs_url",
        "prs",
        "pr",
        "labels",
        "label"
      ]
    },
    {
      "file_path": "reuse_old_whl.py",
      "code": "def unzip_artifact_and_replace_files() -> None:\n    # Unzip the artifact and replace files\n    subprocess.check_output(\n        [\"unzip\", \"-o\", \"artifacts.zip\", \"-d\", \"artifacts\"],\n    )\n    os.remove(\"artifacts.zip\")\n\n    head_sha = get_head_sha()\n\n    # Rename wheel into zip\n    wheel_path = Path(\"artifacts/dist\").glob(\"*.whl\")\n    for path in wheel_path:\n        # Should be of the form torch-2.0.0+git1234567-cp37-etc.whl\n        # Should usually be the merge base sha but for the ones that didn't do\n        # the replacement, it won't be.  Can probably change it to just be merge\n        # base later\n        old_version = f\"+git{path.stem.split('+')[1].split('-')[0][3:]}\"\n        new_version = f\"+git{head_sha[:7]}\"\n\n",
      "variables": [
        "head_sha",
        "wheel_path",
        "path",
        "old_version",
        "new_version"
      ],
      "anonymized_code": "def unzip_artifact_and_replace_files() -> None:\n    # Unzip the artifact and replace files\n    subprocess.check_output(\n        [\"unzip\", \"-o\", \"artifacts.zip\", \"-d\", \"artifacts\"],\n    )\n    os.remove(\"artifacts.zip\")\n\n    var_1 = get_head_sha()\n\n    # Rename wheel into zip\n    var_2 = Path(\"artifacts/dist\").glob(\"*.whl\")\n    for var_3 in var_2:\n        # Should be of the form torch-2.0.0+git1234567-cp37-etc.whl\n        # Should usually be the merge base sha but for the ones that didn't do\n        # the replacement, it won't be.  Can probably change it to just be merge\n        # base later\n        var_4 = f\"+git{var_3.stem.split('+')[1].split('-')[0][3:]}\"\n        var_5 = f\"+git{var_1[:7]}\"\n\n",
      "lines_processed": 19,
      "total_lines": 351,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "reuse_old_whl.py",
      "code": "def get_head_sha() -> str:\n    sha = subprocess.check_output(\n        [\"git\", \"rev-parse\", \"HEAD\"],\n        text=True,\n        stderr=subprocess.DEVNULL,\n    ).strip()\n    return sha\n",
      "variables": [
        "sha"
      ],
      "anonymized_code": "def get_head_sha() -> str:\n    var_1 = subprocess.check_output(\n        [\"git\", \"rev-parse\", \"HEAD\"],\n        text=True,\n        stderr=subprocess.DEVNULL,\n    ).strip()\n    return var_1\n",
      "lines_processed": 7,
      "total_lines": 351,
      "llm_code": "def get_head_sha() -> str:\n    commit_hash = subprocess.check_output(\n        [\"git\", \"rev-parse\", \"HEAD\"],\n        text=True,\n        stderr=subprocess.DEVNULL,\n    ).strip()\n    return commit_hash\n",
      "llm_variables": [
        "commit_hash"
      ]
    },
    {
      "file_path": "reuse_old_whl.py",
      "code": "def get_merge_base() -> str:\n    merge_base = subprocess.check_output(\n        [\"git\", \"merge-base\", \"HEAD\", \"origin/main\"],\n        text=True,\n        stderr=subprocess.DEVNULL,\n    ).strip()\n    # Remove this when we turn this off for the main branch\n    if merge_base == get_head_sha():\n        print(\"Merge base is the same as HEAD, using HEAD^\")\n        merge_base = subprocess.check_output(\n            [\"git\", \"rev-parse\", \"HEAD^\"],\n            text=True,\n            stderr=subprocess.DEVNULL,\n        ).strip()\n    print(f\"Merge base: {merge_base}\")\n    return merge_base\n",
      "variables": [
        "merge_base"
      ],
      "anonymized_code": "def get_merge_base() -> str:\n    var_1 = subprocess.check_output(\n        [\"git\", \"merge-base\", \"HEAD\", \"origin/main\"],\n        text=True,\n        stderr=subprocess.DEVNULL,\n    ).strip()\n    # Remove this when we turn this off for the main branch\n    if var_1 == get_head_sha():\n        print(\"Merge base is the same as HEAD, using HEAD^\")\n        var_1 = subprocess.check_output(\n            [\"git\", \"rev-parse\", \"HEAD^\"],\n            text=True,\n            stderr=subprocess.DEVNULL,\n        ).strip()\n    print(f\"Merge base: {var_1}\")\n    return var_1\n",
      "lines_processed": 16,
      "total_lines": 351,
      "llm_code": "def get_merge_base() -> str:\n    commit_hash = subprocess.check_output(\n        [\"git\", \"merge-base\", \"HEAD\", \"origin/main\"],\n        text=True,\n        stderr=subprocess.DEVNULL,\n    ).strip()\n    # Remove this when we turn this off for the main branch\n    if commit_hash == get_head_sha():\n        print(\"Merge base is the same as HEAD, using HEAD^\")\n        commit_hash = subprocess.check_output(\n            [\"git\", \"rev-parse\", \"HEAD^\"],\n            text=True,\n            stderr=subprocess.DEVNULL,\n        ).strip()\n    print(f\"Merge base: {commit_hash}\")\n    return commit_hash\n",
      "llm_variables": [
        "commit_hash"
      ]
    },
    {
      "file_path": "reuse_old_whl.py",
      "code": "def set_output() -> None:\n    # Disable for now so we can monitor first\n    # pass\n    if os.getenv(\"GITHUB_OUTPUT\"):\n        with open(str(os.getenv(\"GITHUB_OUTPUT\")), \"a\") as env:\n            print(\"reuse=true\", file=env)\n    else:\n        print(\"::set-output name=reuse::true\")\n",
      "variables": [
        "env"
      ],
      "anonymized_code": "def set_output() -> None:\n    # Disable for now so we can monitor first\n    # pass\n    if os.getenv(\"GITHUB_OUTPUT\"):\n        with open(str(os.getenv(\"GITHUB_OUTPUT\")), \"a\") as var_1:\n            print(\"reuse=true\", file=var_1)\n    else:\n        print(\"::set-output name=reuse::true\")\n",
      "lines_processed": 8,
      "total_lines": 351,
      "llm_code": "def set_output() -> None:\n    # Disable for now so we can monitor first\n    # pass\n    if os.getenv(\"GITHUB_OUTPUT\"):\n        with open(str(os.getenv(\"GITHUB_OUTPUT\")), \"a\") as output_file:\n            print(\"reuse=true\", file=output_file)\n    else:\n        print(\"::set-output name=reuse::true\")\n",
      "llm_variables": [
        "output_file"
      ]
    },
    {
      "file_path": "reuse_old_whl.py",
      "code": "def check_issue_open() -> bool:\n    # Check if issue #153759 is open.  This is the config issue for quickly\n    # forcing everyone to build\n    url = \"https://api.github.com/repos/pytorch/pytorch/issues/153759\"\n    response = query_github_api(url)\n    if response.get(\"state\") == \"open\":\n        print(\"Issue #153759 is open.\")\n        return True\n    else:\n        print(\"Issue #153759 is not open.\")\n        return False\n",
      "variables": [
        "url",
        "response"
      ],
      "anonymized_code": "def check_issue_open() -> bool:\n    # Check if issue #153759 is open.  This is the config issue for quickly\n    # forcing everyone to build\n    var_1 = \"https://api.github.com/repos/pytorch/pytorch/issues/153759\"\n    var_2 = query_github_api(var_1)\n    if var_2.get(\"state\") == \"open\":\n        print(\"Issue #153759 is open.\")\n        return True\n    else:\n        print(\"Issue #153759 is not open.\")\n        return False\n",
      "lines_processed": 11,
      "total_lines": 351,
      "llm_code": "def check_issue_open() -> bool:\n    # Check if issue #153759 is open.  This is the config issue for quickly\n    # forcing everyone to build\n    url = \"https://api.github.com/repos/pytorch/pytorch/issues/153759\"\n    response = query_github_api(url)\n    if response.get(\"state\") == \"open\":\n        print(\"Issue #153759 is open.\")\n        return True\n    else:\n        print(\"Issue #153759 is not open.\")\n        return False\n",
      "llm_variables": [
        "url",
        "response"
      ]
    },
    {
      "file_path": "reuse_old_whl.py",
      "code": "def ok_changed_file(file: str) -> bool:\n    # Return true if the file is in the list of allowed files to be changed to\n    # reuse the old whl\n    if (\n        file.startswith(\"torch/\")\n        and file.endswith(\".py\")\n        and not file.startswith(\"torch/csrc/\")\n    ):\n        return True\n    if file.startswith(\"test/\") and file.endswith(\".py\"):\n        return True\n    return False\n",
      "variables": [
        "file"
      ],
      "anonymized_code": "def ok_changed_file(var_1: str) -> bool:\n    # Return true if the var_1 is in the list of allowed files to be changed to\n    # reuse the old whl\n    if (\n        var_1.startswith(\"torch/\")\n        and var_1.endswith(\".py\")\n        and not var_1.startswith(\"torch/csrc/\")\n    ):\n        return True\n    if var_1.startswith(\"test/\") and var_1.endswith(\".py\"):\n        return True\n    return False\n",
      "lines_processed": 12,
      "total_lines": 351,
      "llm_code": "def ok_changed_file(file_path: str) -> bool:\n    # Return true if the file_path is in the list of allowed files to be changed to\n    # reuse the old whl\n    if (\n        file_path.startswith(\"torch/\")\n        and file_path.endswith(\".py\")\n        and not file_path.startswith(\"torch/csrc/\")\n    ):\n        return True\n    if file_path.startswith(\"test/\") and file_path.endswith(\".py\"):\n        return True\n    return False\n",
      "llm_variables": [
        "file_path"
      ]
    },
    {
      "file_path": "reuse_old_whl.py",
      "code": "def get_workflow_id(run_id: str) -> Optional[str]:\n    # Get the workflow ID that corresponds to the file for the run ID\n    url = f\"https://api.github.com/repos/pytorch/pytorch/actions/runs/{run_id}\"\n    response = query_github_api(url)\n    if \"workflow_id\" in response:\n        print(f\"Found workflow ID for run ID {run_id}: {response['workflow_id']}\")\n        return cast(str, response[\"workflow_id\"])\n    else:\n        print(\"No workflow ID found.\")\n        return None\n",
      "variables": [
        "run_id",
        "url",
        "response"
      ],
      "anonymized_code": "def get_workflow_id(var_1: str) -> Optional[str]:\n    # Get the workflow ID that corresponds to the file for the run ID\n    var_2 = f\"https://api.github.com/repos/pytorch/pytorch/actions/runs/{var_1}\"\n    var_3 = query_github_api(var_2)\n    if \"workflow_id\" in var_3:\n        print(f\"Found workflow ID for run ID {var_1}: {var_3['workflow_id']}\")\n        return cast(str, var_3[\"workflow_id\"])\n    else:\n        print(\"No workflow ID found.\")\n        return None\n",
      "lines_processed": 10,
      "total_lines": 351,
      "llm_code": "def get_workflow_id(run_id: str) -> Optional[str]:\n    # Get the workflow ID that corresponds to the file for the run ID\n    url = f\"https://api.github.com/repos/pytorch/pytorch/actions/runs/{run_id}\"\n    response = query_github_api(url)\n    if \"workflow_id\" in response:\n        print(f\"Found workflow ID for run ID {run_id}: {response['workflow_id']}\")\n        return cast(str, response[\"workflow_id\"])\n    else:\n        print(\"No workflow ID found.\")\n        return None\n",
      "llm_variables": [
        "run_id",
        "url",
        "response"
      ]
    },
    {
      "file_path": "aarch64_wheel_ci_build.py",
      "code": "def parse_arguments():\n    \"\"\"\n    Parse inline arguments\n    \"\"\"\n    from argparse import ArgumentParser\n\n    parser = ArgumentParser(\"AARCH64 wheels python CD\")\n    parser.add_argument(\"--debug\", action=\"store_true\")\n    parser.add_argument(\"--build-only\", action=\"store_true\")\n    parser.add_argument(\"--test-only\", type=str)\n    parser.add_argument(\"--enable-mkldnn\", action=\"store_true\")\n    parser.add_argument(\"--enable-cuda\", action=\"store_true\")\n    return parser.parse_args()\n",
      "variables": [
        "parser"
      ],
      "anonymized_code": "def parse_arguments():\n    \"\"\"\n    Parse inline arguments\n    \"\"\"\n    from argparse import ArgumentParser\n\n    var_1 = ArgumentParser(\"AARCH64 wheels python CD\")\n    var_1.add_argument(\"--debug\", action=\"store_true\")\n    var_1.add_argument(\"--build-only\", action=\"store_true\")\n    var_1.add_argument(\"--test-only\", type=str)\n    var_1.add_argument(\"--enable-mkldnn\", action=\"store_true\")\n    var_1.add_argument(\"--enable-cuda\", action=\"store_true\")\n    return var_1.parse_args()\n",
      "lines_processed": 13,
      "total_lines": 259,
      "llm_code": "def parse_arguments():\n    \"\"\"\n    Parse inline arguments\n    \"\"\"\n    from argparse import ArgumentParser\n\n    parser = ArgumentParser(\"AARCH64 wheels python CD\")\n    parser.add_argument(\"--debug\", action=\"store_true\")\n    parser.add_argument(\"--build-only\", action=\"store_true\")\n    parser.add_argument(\"--test-only\", type=str)\n    parser.add_argument(\"--enable-mkldnn\", action=\"store_true\")\n    parser.add_argument(\"--enable-cuda\", action=\"store_true\")\n    return parser.parse_args()\n",
      "llm_variables": [
        "parser"
      ]
    },
    {
      "file_path": "aarch64_wheel_ci_build.py",
      "code": "def build_ArmComputeLibrary() -> None:\n    \"\"\"\n    Using ArmComputeLibrary for aarch64 PyTorch\n    \"\"\"\n    print(\"Building Arm Compute Library\")\n    acl_build_flags = [\n        \"debug=0\",\n        \"neon=1\",\n        \"opencl=0\",\n        \"os=linux\",\n        \"openmp=1\",\n        \"cppthreads=0\",\n        \"arch=armv8a\",\n        \"multi_isa=1\",\n        \"fixed_format_kernels=1\",\n        \"build=native\",\n    ]\n    acl_install_dir = \"/acl\"\n    acl_checkout_dir = os.getenv(\"ACL_SOURCE_DIR\", \"ComputeLibrary\")\n",
      "variables": [
        "acl_build_flags",
        "acl_install_dir",
        "acl_checkout_dir"
      ],
      "anonymized_code": "def build_ArmComputeLibrary() -> None:\n    \"\"\"\n    Using ArmComputeLibrary for aarch64 PyTorch\n    \"\"\"\n    print(\"Building Arm Compute Library\")\n    var_1 = [\n        \"debug=0\",\n        \"neon=1\",\n        \"opencl=0\",\n        \"os=linux\",\n        \"openmp=1\",\n        \"cppthreads=0\",\n        \"arch=armv8a\",\n        \"multi_isa=1\",\n        \"fixed_format_kernels=1\",\n        \"build=native\",\n    ]\n    var_2 = \"/acl\"\n    var_3 = os.getenv(\"ACL_SOURCE_DIR\", \"ComputeLibrary\")\n",
      "lines_processed": 19,
      "total_lines": 259,
      "llm_code": "def build_ArmComputeLibrary() -> None:\n    \"\"\"\n    Using ArmComputeLibrary for aarch64 PyTorch\n    \"\"\"\n    print(\"Building Arm Compute Library\")\n    build_flags = [\n        \"debug=0\",\n        \"neon=1\",\n        \"opencl=0\",\n        \"os=linux\",\n        \"openmp=1\",\n        \"cppthreads=0\",\n        \"arch=armv8a\",\n        \"multi_isa=1\",\n        \"fixed_format_kernels=1\",\n        \"build=native\",\n    ]\n    output_dir = \"/acl\"\n    source_dir = os.getenv(\"ACL_SOURCE_DIR\", \"ComputeLibrary\")\n",
      "llm_variables": [
        "build_flags",
        "output_dir",
        "source_dir"
      ]
    },
    {
      "file_path": "aarch64_wheel_ci_build.py",
      "code": "def list_dir(path: str) -> list[str]:\n    \"\"\"'\n    Helper for getting paths for Python\n    \"\"\"\n    return check_output([\"ls\", \"-1\", path]).decode().split(\"\\n\")\n",
      "variables": [
        "path"
      ],
      "anonymized_code": "def list_dir(var_1: str) -> list[str]:\n    \"\"\"'\n    Helper for getting paths for Python\n    \"\"\"\n    return check_output([\"ls\", \"-1\", var_1]).decode().split(\"\\n\")\n",
      "lines_processed": 5,
      "total_lines": 259,
      "llm_code": "def list_dir(path: str) -> list[str]:\n    \"\"\"'\n    Helper for getting paths for Python\n    \"\"\"\n    return check_output([\"ls\", \"-1\", path]).decode().split(\"\\n\")\n",
      "llm_variables": [
        "path"
      ]
    },
    {
      "file_path": "aarch64_wheel_ci_build.py",
      "code": "def complete_wheel(folder: str) -> str:\n    \"\"\"\n    Complete wheel build and put in artifact location\n    \"\"\"\n    wheel_name = list_dir(f\"/{folder}/dist\")[0]\n\n    # Please note for cuda we don't run auditwheel since we use custom script to package\n    # the cuda dependencies to the wheel file using update_wheel() method.\n    # However we need to make sure filename reflects the correct Manylinux platform.\n    if \"pytorch\" in folder and not enable_cuda:\n        print(\"Repairing Wheel with AuditWheel\")\n        check_call([\"auditwheel\", \"repair\", f\"dist/{wheel_name}\"], cwd=folder)\n        repaired_wheel_name = list_dir(f\"/{folder}/wheelhouse\")[0]\n\n        print(f\"Moving {repaired_wheel_name} wheel to /{folder}/dist\")\n        os.rename(\n            f\"/{folder}/wheelhouse/{repaired_wheel_name}\",\n            f\"/{folder}/dist/{repaired_wheel_name}\",\n        )\n",
      "variables": [
        "folder",
        "wheel_name",
        "repaired_wheel_name"
      ],
      "anonymized_code": "def complete_wheel(var_1: str) -> str:\n    \"\"\"\n    Complete wheel build and put in artifact location\n    \"\"\"\n    var_2 = list_dir(f\"/{var_1}/dist\")[0]\n\n    # Please note for cuda we don't run auditwheel since we use custom script to package\n    # the cuda dependencies to the wheel file using update_wheel() method.\n    # However we need to make sure filename reflects the correct Manylinux platform.\n    if \"pytorch\" in var_1 and not enable_cuda:\n        print(\"Repairing Wheel with AuditWheel\")\n        check_call([\"auditwheel\", \"repair\", f\"dist/{var_2}\"], cwd=var_1)\n        var_3 = list_dir(f\"/{var_1}/wheelhouse\")[0]\n\n        print(f\"Moving {var_3} wheel to /{var_1}/dist\")\n        os.rename(\n            f\"/{var_1}/wheelhouse/{var_3}\",\n            f\"/{var_1}/dist/{var_3}\",\n        )\n",
      "lines_processed": 19,
      "total_lines": 259,
      "llm_code": "def complete_wheel(package_name: str) -> str:\n    \"\"\"\n    Complete wheel build and put in artifact location\n    \"\"\"\n    dist_wheel = list_dir(f\"/{package_name}/dist\")[0]\n\n    # Please note for cuda we don't run auditwheel since we use custom script to package\n    # the cuda dependencies to the wheel file using update_wheel() method.\n    # However we need to make sure filename reflects the correct Manylinux platform.\n    if \"pytorch\" in package_name and not enable_cuda:\n        print(\"Repairing Wheel with AuditWheel\")\n        check_call([\"auditwheel\", \"repair\", f\"dist/{dist_wheel}\"], cwd=package_name)\n        wheelhouse_wheel = list_dir(f\"/{package_name}/wheelhouse\")[0]\n\n        print(f\"Moving {wheelhouse_wheel} wheel to /{package_name}/dist\")\n        os.rename(\n            f\"/{package_name}/wheelhouse/{wheelhouse_wheel}\",\n            f\"/{package_name}/dist/{wheelhouse_wheel}\",\n        )\n",
      "llm_variables": [
        "package_name",
        "dist_wheel",
        "wheelhouse_wheel"
      ]
    },
    {
      "file_path": "aarch64_wheel_ci_build.py",
      "code": "def replace_tag(filename) -> None:\n    with open(filename) as f:\n        lines = f.readlines()\n    for i, line in enumerate(lines):\n        if line.startswith(\"Tag:\"):\n            lines[i] = line.replace(\"-linux_\", \"-manylinux_2_28_\")\n            print(f\"Updated tag from {line} to {lines[i]}\")\n            break\n\n    with open(filename, \"w\") as f:\n        f.writelines(lines)\n",
      "variables": [
        "filename",
        "f",
        "lines",
        "i",
        "line"
      ],
      "anonymized_code": "def replace_tag(var_1) -> None:\n    with open(var_1) as var_2:\n        var_3 = var_2.readlines()\n    for var_4, var_5 in enumerate(var_3):\n        if var_5.startswith(\"Tag:\"):\n            var_3[var_4] = var_5.replace(\"-linux_\", \"-manylinux_2_28_\")\n            print(var_2\"Updated tag from {var_5} to {var_3[var_4]}\")\n            break\n\n    with open(var_1, \"w\") as var_2:\n        var_2.writelines(var_3)\n",
      "lines_processed": 11,
      "total_lines": 259,
      "llm_code": "def replace_tag(filename) -> None:\n    with open(filename) as file:\n        lines = file.readlines()\n    for index, line in enumerate(lines):\n        if line.startswith(\"Tag:\"):\n            lines[index] = line.replace(\"-linux_\", \"-manylinux_2_28_\")\n            print(file\"Updated tag from {line} to {lines[index]}\")\n            break\n\n    with open(filename, \"w\") as file:\n        file.writelines(lines)\n",
      "llm_variables": [
        "filename",
        "file",
        "lines",
        "index",
        "line"
      ]
    },
    {
      "file_path": "normalize_yaml_fragment.py",
      "code": "def regurgitate(depth, use_pyyaml_formatter=False):\n    data = yaml.safe_load(sys.stdin)\n\n    if use_pyyaml_formatter:\n        output = yaml.dump(data, sort_keys=True)\n        sys.stdout.write(output)\n    else:\n        miniyaml.render(sys.stdout, data, depth)\n",
      "variables": [
        "depth",
        "use_pyyaml_formatter",
        "data",
        "output"
      ],
      "anonymized_code": "def regurgitate(var_1, var_2=False):\n    var_3 = yaml.safe_load(sys.stdin)\n\n    if var_2:\n        var_4 = yaml.dump(var_3, sort_keys=True)\n        sys.stdout.write(var_4)\n    else:\n        miniyaml.render(sys.stdout, var_3, var_1)\n",
      "lines_processed": 8,
      "total_lines": 26,
      "llm_code": "def regurgitate(data, is_sorted=False):\n    config = yaml.safe_load(sys.stdin)\n\n    if is_sorted:\n        rendered = yaml.dump(config, sort_keys=True)\n        sys.stdout.write(rendered)\n    else:\n        miniyaml.render(sys.stdout, config, data)\n",
      "llm_variables": [
        "data",
        "is_sorted",
        "config",
        "rendered"
      ]
    },
    {
      "file_path": "embed_library.py",
      "code": "def embed_library(whl_path, lib_soname, update_tag=False):\n    patcher = AlignedPatchelf()\n    out_dir = TemporaryDirectory()\n    whl_name = os.path.basename(whl_path)\n    tmp_whl_name = os.path.join(out_dir.name, whl_name)\n    with InWheelCtx(whl_path) as ctx:\n        torchlib_path = os.path.join(ctx._tmpdir.name, \"torch\", \"lib\")\n        ctx.out_wheel = tmp_whl_name\n        new_lib_path, new_lib_soname = None, None\n        for filename, _ in elf_file_filter(ctx.iter_files()):\n            if not filename.startswith(\"torch/lib\"):\n                continue\n            libtree = lddtree(filename)\n            if lib_soname not in libtree[\"needed\"]:\n                continue\n            lib_path = libtree[\"libs\"][lib_soname][\"path\"]\n            if lib_path is None:\n                print(f\"Can't embed {lib_soname} as it could not be found\")\n                break\n",
      "variables": [
        "whl_path",
        "lib_soname",
        "update_tag",
        "patcher",
        "out_dir",
        "whl_name",
        "tmp_whl_name",
        "ctx",
        "torchlib_path",
        "new_lib_path",
        "new_lib_soname",
        "filename",
        "_",
        "libtree",
        "lib_path"
      ],
      "anonymized_code": "def embed_library(var_1, var_2, var_3=False):\n    var_4 = AlignedPatchelf()\n    var_5 = TemporaryDirectory()\n    var_6 = os.path.basename(var_1)\n    var_7 = os.path.join(var_5.name, var_6)\n    with InWheelCtx(var_1) as var_8:\n        var_9 = os.path.join(var_8._tmpdir.name, \"torch\", \"lib\")\n        var_8.out_wheel = var_7\n        var_10, var_11 = None, None\n        for var_12, var_13 in elf_file_filter(var_8.iter_files()):\n            if not var_12.startswith(\"torch/lib\"):\n                continue\n            var_14 = lddtree(var_12)\n            if var_2 not in var_14[\"needed\"]:\n                continue\n            var_15 = var_14[\"libs\"][var_2][\"path\"]\n            if var_15 is None:\n                print(f\"Can't embed {var_2} as it could not be found\")\n                break\n",
      "lines_processed": 19,
      "total_lines": 87,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "embed_library.py",
      "code": "def replace_tag(filename):\n    with open(filename) as f:\n        lines = f.read().split(\"\\\\n\")\n    for i, line in enumerate(lines):\n        if not line.startswith(\"Tag: \"):\n            continue\n        lines[i] = line.replace(\"-linux_\", \"-manylinux2014_\")\n        print(f\"Updated tag from {line} to {lines[i]}\")\n\n    with open(filename, \"w\") as f:\n        f.write(\"\\\\n\".join(lines))\n",
      "variables": [
        "filename",
        "f",
        "lines",
        "i",
        "line"
      ],
      "anonymized_code": "def replace_tag(var_1):\n    with open(var_1) as var_2:\n        var_3 = var_2.read().split(\"\\\\n\")\n    for var_4, var_5 in enumerate(var_3):\n        if not var_5.startswith(\"Tag: \"):\n            continue\n        var_3[var_4] = var_5.replace(\"-linux_\", \"-manylinux2014_\")\n        print(var_2\"Updated tag from {var_5} to {var_3[var_4]}\")\n\n    with open(var_1, \"w\") as var_2:\n        var_2.write(\"\\\\n\".join(var_3))\n",
      "lines_processed": 11,
      "total_lines": 87,
      "llm_code": "def replace_tag(file_path):\n    with open(file_path) as file:\n        lines = file.read().split(\"\\\\n\")\n    for index, line in enumerate(lines):\n        if not line.startswith(\"Tag: \"):\n            continue\n        lines[index] = line.replace(\"-linux_\", \"-manylinux2014_\")\n        print(file\"Updated tag from {line} to {lines[index]}\")\n\n    with open(file_path, \"w\") as file:\n        file.write(\"\\\\n\".join(lines))\n",
      "llm_variables": [
        "file_path",
        "file",
        "lines",
        "index",
        "line"
      ]
    },
    {
      "file_path": "check_gomp.py",
      "code": "def main():\n    omp_max_threads = get_gomp_thread()\n    print(\n        f\"omp_max_threads after loading libgomp.so and libtorch_cpu.so: {omp_max_threads}\"\n    )\n    if omp_max_threads == 1:\n        raise RuntimeError(\n            \"omp_max_threads is 1. Check whether libgomp.so is loaded twice.\"\n        )\n",
      "variables": [
        "omp_max_threads"
      ],
      "anonymized_code": "def main():\n    var_1 = get_gomp_thread()\n    print(\n        f\"var_1 after loading libgomp.so and libtorch_cpu.so: {var_1}\"\n    )\n    if var_1 == 1:\n        raise RuntimeError(\n            \"var_1 is 1. Check whether libgomp.so is loaded twice.\"\n        )\n",
      "lines_processed": 9,
      "total_lines": 77,
      "llm_code": "def main():\n    gomp_thread = get_gomp_thread()\n    print(\n        f\"gomp_thread after loading libgomp.so and libtorch_cpu.so: {gomp_thread}\"\n    )\n    if gomp_thread == 1:\n        raise RuntimeError(\n            \"gomp_thread is 1. Check whether libgomp.so is loaded twice.\"\n        )\n",
      "llm_variables": [
        "gomp_thread"
      ]
    },
    {
      "file_path": "check_binary_symbols.py",
      "code": "def check_lib_symbols_for_abi_correctness(lib: str) -> None:\n    print(f\"lib: {lib}\")\n    cxx11_symbols = grep_symbols(lib, LIBTORCH_CXX11_PATTERNS)\n    pre_cxx11_symbols = grep_symbols(lib, LIBTORCH_PRE_CXX11_PATTERNS)\n    num_cxx11_symbols = len(cxx11_symbols)\n    num_pre_cxx11_symbols = len(pre_cxx11_symbols)\n    print(f\"num_cxx11_symbols: {num_cxx11_symbols}\")\n    print(f\"num_pre_cxx11_symbols: {num_pre_cxx11_symbols}\")\n    if num_pre_cxx11_symbols > 0:\n        raise RuntimeError(\n            f\"Found pre-cxx11 symbols, but there shouldn't be any, see: {pre_cxx11_symbols[:100]}\"\n        )\n    if num_cxx11_symbols < 100:\n        raise RuntimeError(\"Didn't find enought cxx11 symbols\")\n",
      "variables": [
        "lib",
        "cxx11_symbols",
        "pre_cxx11_symbols",
        "num_cxx11_symbols",
        "num_pre_cxx11_symbols"
      ],
      "anonymized_code": "def check_lib_symbols_for_abi_correctness(var_1: str) -> None:\n    print(f\"var_1: {var_1}\")\n    var_2 = grep_symbols(var_1, LIBTORCH_CXX11_PATTERNS)\n    var_3 = grep_symbols(var_1, LIBTORCH_PRE_CXX11_PATTERNS)\n    var_4 = len(var_2)\n    var_5 = len(var_3)\n    print(f\"var_4: {var_4}\")\n    print(f\"var_5: {var_5}\")\n    if var_5 > 0:\n        raise RuntimeError(\n            f\"Found pre-cxx11 symbols, but there shouldn't be any, see: {var_3[:100]}\"\n        )\n    if var_4 < 100:\n        raise RuntimeError(\"Didn't find enought cxx11 symbols\")\n",
      "lines_processed": 14,
      "total_lines": 113,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "check_binary_symbols.py",
      "code": "def main() -> None:\n    if \"install_root\" in os.environ:\n        install_root = Path(os.getenv(\"install_root\"))  # noqa: SIM112\n    else:\n        if os.getenv(\"PACKAGE_TYPE\") == \"libtorch\":\n            install_root = Path(os.getcwd())\n        else:\n            install_root = Path(distutils.sysconfig.get_python_lib()) / \"torch\"\n\n    libtorch_cpu_path = str(install_root / \"lib\" / \"libtorch_cpu.so\")\n    check_lib_symbols_for_abi_correctness(libtorch_cpu_path)\n",
      "variables": [
        "install_root",
        "libtorch_cpu_path"
      ],
      "anonymized_code": "def main() -> None:\n    if \"var_1\" in os.environ:\n        var_1 = Path(os.getenv(\"var_1\"))  # noqa: SIM112\n    else:\n        if os.getenv(\"PACKAGE_TYPE\") == \"libtorch\":\n            var_1 = Path(os.getcwd())\n        else:\n            var_1 = Path(distutils.sysconfig.get_python_lib()) / \"torch\"\n\n    var_2 = str(var_1 / \"lib\" / \"libtorch_cpu.so\")\n    check_lib_symbols_for_abi_correctness(var_2)\n",
      "lines_processed": 11,
      "total_lines": 113,
      "llm_code": "def main() -> None:\n    if \"install_path\" in os.environ:\n        install_path = Path(os.getenv(\"install_path\"))  # noqa: SIM112\n    else:\n        if os.getenv(\"PACKAGE_TYPE\") == \"libtorch\":\n            install_path = Path(os.getcwd())\n        else:\n            install_path = Path(distutils.sysconfig.get_python_lib()) / \"torch\"\n\n    lib_path = str(install_path / \"lib\" / \"libtorch_cpu.so\")\n    check_lib_symbols_for_abi_correctness(lib_path)\n",
      "llm_variables": [
        "install_path",
        "lib_path"
      ]
    },
    {
      "file_path": "check_binary_symbols.py",
      "code": "def get_symbols(lib: str) -> list[tuple[str, str, str]]:\n    from subprocess import check_output\n\n    lines = check_output(f'nm \"{lib}\"|c++filt', shell=True)\n    return [x.split(\" \", 2) for x in lines.decode(\"latin1\").split(\"\\n\")[:-1]]\n",
      "variables": [
        "lib",
        "lines",
        "x"
      ],
      "anonymized_code": "def get_symbols(var_1: str) -> list[tuple[str, str, str]]:\n    from subprocess import check_output\n\n    var_2 = check_output(f'nm \"{var_1}\"|c++filt', shell=True)\n    return [var_3.split(\" \", 2) for var_3 in var_2.decode(\"latin1\").split(\"\\n\")[:-1]]\n",
      "lines_processed": 5,
      "total_lines": 113,
      "llm_code": "def get_symbols(filename: str) -> list[tuple[str, str, str]]:\n    from subprocess import check_output\n\n    output = check_output(f'nm \"{filename}\"|c++filt', shell=True)\n    return [line.split(\" \", 2) for line in output.decode(\"latin1\").split(\"\\n\")[:-1]]\n",
      "llm_variables": [
        "filename",
        "output",
        "line"
      ]
    },
    {
      "file_path": "check_binary_symbols.py",
      "code": "def grep_symbols(lib: str, patterns: list[Any]) -> list[str]:\n    def _grep_symbols(\n        symbols: list[tuple[str, str, str]], patterns: list[Any]\n    ) -> list[str]:\n        rc = []\n        for _s_addr, _s_type, s_name in symbols:\n            for pattern in patterns:\n                if pattern.match(s_name):\n                    rc.append(s_name)\n                    continue\n        return rc\n\n    all_symbols = get_symbols(lib)\n    num_workers = 32\n    chunk_size = (len(all_symbols) + num_workers - 1) // num_workers\n\n    def _get_symbols_chunk(i):\n        return all_symbols[i * chunk_size : (i + 1) * chunk_size]\n\n",
      "variables": [
        "lib",
        "patterns",
        "symbols",
        "rc",
        "_s_addr",
        "_s_type",
        "s_name",
        "pattern",
        "all_symbols",
        "num_workers",
        "chunk_size",
        "i"
      ],
      "anonymized_code": "def grep_symbols(var_1: str, var_2: list[Any]) -> list[str]:\n    def _grep_symbols(\n        var_3: list[tuple[str, str, str]], var_2: list[Any]\n    ) -> list[str]:\n        var_4 = []\n        for var_5, var_6, var_7 in var_3:\n            for var_8 in var_2:\n                if var_8.match(var_7):\n                    var_4.append(var_7)\n                    continue\n        return var_4\n\n    var_9 = get_symbols(var_1)\n    var_10 = 32\n    var_11 = (len(var_9) + var_10 - 1) // var_10\n\n    def _get_symbols_chunk(var_12):\n        return var_9[var_12 * var_11 : (var_12 + 1) * var_11]\n\n",
      "lines_processed": 19,
      "total_lines": 113,
      "llm_code": "def grep_symbols(pattern: str, data: list[Any]) -> list[str]:\n    def _grep_symbols(\n        regex_list: list[tuple[str, str, str]], data: list[Any]\n    ) -> list[str]:\n        result = []\n        for symbol, _, name in regex_list:\n            for item in data:\n                if item.match(name):\n                    result.append(name)\n                    continue\n        return result\n\n    symbols = get_symbols(pattern)\n    chunk_size = 32\n    num_chunks = (len(symbols) + chunk_size - 1) // chunk_size\n\n    def _get_symbols_chunk(index):\n        return symbols[index * num_chunks : (index + 1) * num_chunks]",
      "llm_variables": [
        "pattern",
        "data",
        "regex_list",
        "result",
        "symbol",
        "_",
        "name",
        "item",
        "symbols",
        "chunk_size",
        "num_chunks",
        "index"
      ]
    },
    {
      "file_path": "check_binary_symbols.py",
      "code": "def _apply_libtorch_symbols(symbols):\n    return [\n        re.compile(f\"{x}.*{y}\")\n        for (x, y) in itertools.product(LIBTORCH_NAMESPACE_LIST, symbols)\n    ]\n",
      "variables": [
        "symbols",
        "x",
        "y"
      ],
      "anonymized_code": "def _apply_libtorch_symbols(var_1):\n    return [\n        re.compile(f\"{var_2}.*{var_3}\")\n        for (var_2, var_3) in itertools.product(LIBTORCH_NAMESPACE_LIST, var_1)\n    ]\n",
      "lines_processed": 5,
      "total_lines": 113,
      "llm_code": "def _apply_libtorch_symbols(names):\n    return [\n        re.compile(f\"{pattern}.*{suffix}\")\n        for (pattern, suffix) in itertools.product(LIBTORCH_NAMESPACE_LIST, names)\n    ]\n",
      "llm_variables": [
        "names",
        "pattern",
        "suffix"
      ]
    },
    {
      "file_path": "create_test_cert.py",
      "code": "def sign_certificate_request(path, csr_cert, ca_cert, private_ca_key):\n    cert = (\n        x509.CertificateBuilder()\n        .subject_name(csr_cert.subject)\n        .issuer_name(ca_cert.subject)\n        .public_key(csr_cert.public_key())\n        .serial_number(x509.random_serial_number())\n        .not_valid_before(datetime.now(timezone.utc))\n        .not_valid_after(\n            # Our certificate will be valid for 10 days\n            datetime.now(timezone.utc) + timedelta(days=10)\n            # Sign our certificate with our private key\n        )\n        .sign(private_ca_key, hashes.SHA256())\n    )\n    with open(path, \"wb\") as f:\n        f.write(cert.public_bytes(serialization.Encoding.PEM))\n    return cert\n",
      "variables": [
        "path",
        "csr_cert",
        "ca_cert",
        "private_ca_key",
        "cert",
        "f"
      ],
      "anonymized_code": "def sign_certificate_request(var_1, var_2, var_3, var_4):\n    var_5 = (\n        x509.CertificateBuilder()\n        .subject_name(var_2.subject)\n        .issuer_name(var_3.subject)\n        .public_key(var_2.public_key())\n        .serial_number(x509.random_serial_number())\n        .not_valid_before(datetime.now(timezone.utc))\n        .not_valid_after(\n            # Our certificate will be valid for 10 days\n            datetime.now(timezone.utc) + timedelta(days=10)\n            # Sign our certificate with our private key\n        )\n        .sign(var_4, hashes.SHA256())\n    )\n    with open(var_1, \"wb\") as var_6:\n        var_6.write(var_5.public_bytes(serialization.Encoding.PEM))\n    return var_5\n",
      "lines_processed": 18,
      "total_lines": 123,
      "llm_code": "def sign_certificate_request(certificate_request, csr, ca_certificate, private_key):\n    certificate = (\n        x509.CertificateBuilder()\n        .subject_name(csr.subject)\n        .issuer_name(ca_certificate.subject)\n        .public_key(csr.public_key())\n        .serial_number(x509.random_serial_number())\n        .not_valid_before(datetime.now(timezone.utc))\n        .not_valid_after(\n            # Our certificate will be valid for 10 days\n            datetime.now(timezone.utc) + timedelta(days=10)\n            # Sign our certificate with our private key\n        )\n        .sign(private_key, hashes.SHA256())\n    )\n    with open(certificate_request, \"wb\") as output_file:\n        output_file.write(certificate.public_bytes(serialization.Encoding.PEM))\n    return certificate\n",
      "llm_variables": [
        "certificate_request",
        "csr",
        "ca_certificate",
        "private_key",
        "certificate",
        "output_file"
      ]
    },
    {
      "file_path": "create_test_cert.py",
      "code": "def create_req(path, C, ST, L, O, key):\n    csr = (\n        x509.CertificateSigningRequestBuilder()\n        .subject_name(\n            x509.Name(\n                [\n                    # Provide various details about who we are.\n                    x509.NameAttribute(NameOID.COUNTRY_NAME, C),\n                    x509.NameAttribute(NameOID.STATE_OR_PROVINCE_NAME, ST),\n                    x509.NameAttribute(NameOID.LOCALITY_NAME, L),\n                    x509.NameAttribute(NameOID.ORGANIZATION_NAME, O),\n                ]\n            )\n        )\n        .sign(key, hashes.SHA256())\n    )\n    with open(path, \"wb\") as f:\n        f.write(csr.public_bytes(serialization.Encoding.PEM))\n    return csr\n",
      "variables": [
        "path",
        "C",
        "ST",
        "L",
        "O",
        "key",
        "csr",
        "f"
      ],
      "anonymized_code": "def create_req(var_1, var_2, var_3, var_4, var_5, var_6):\n    var_7 = (\n        x509.CertificateSigningRequestBuilder()\n        .subject_name(\n            x509.Name(\n                [\n                    # Provide various details about who we are.\n                    x509.NameAttribute(NameOID.COUNTRY_NAME, var_2),\n                    x509.NameAttribute(NameOID.STATE_OR_PROVINCE_NAME, var_3),\n                    x509.NameAttribute(NameOID.LOCALITY_NAME, var_4),\n                    x509.NameAttribute(NameOID.ORGANIZATION_NAME, var_5),\n                ]\n            )\n        )\n        .sign(var_6, hashes.SHA256())\n    )\n    with open(var_1, \"wb\") as var_8:\n        var_8.write(var_7.public_bytes(serialization.Encoding.PEM))\n    return var_7\n",
      "lines_processed": 19,
      "total_lines": 123,
      "llm_code": "def create_req(country, state, locality, org, private_key, password):\n    csr = (\n        x509.CertificateSigningRequestBuilder()\n       .subject_name(\n            x509.Name(\n                [\n                    # Provide various details about who we are.\n                    x509.NameAttribute(NameOID.COUNTRY_NAME, country),\n                    x509.NameAttribute(NameOID.STATE_OR_PROVINCE_NAME, state),\n                    x509.NameAttribute(NameOID.LOCALITY_NAME, locality),\n                    x509.NameAttribute(NameOID.ORGANIZATION_NAME, org),\n                ]\n            )\n        )\n       .sign(private_key, hashes.SHA256())\n    )\n    with open(password, \"wb\") as file:\n        file.write(csr.public_bytes(serialization.Encoding.PEM))\n    return csr",
      "llm_variables": [
        "country",
        "state",
        "locality",
        "org",
        "private_key",
        "password",
        "csr",
        "file"
      ]
    },
    {
      "file_path": "create_test_cert.py",
      "code": "def genrsa(path):\n    key = rsa.generate_private_key(\n        public_exponent=65537,\n        key_size=2048,\n    )\n    with open(path, \"wb\") as f:\n        f.write(\n            key.private_bytes(\n                encoding=serialization.Encoding.PEM,\n                format=serialization.PrivateFormat.TraditionalOpenSSL,\n                encryption_algorithm=serialization.NoEncryption(),\n            )\n        )\n    return key\n",
      "variables": [
        "path",
        "key",
        "f"
      ],
      "anonymized_code": "def genrsa(var_1):\n    var_2 = rsa.generate_private_key(\n        public_exponent=65537,\n        key_size=2048,\n    )\n    with open(var_1, \"wb\") as var_3:\n        var_3.write(\n            var_2.private_bytes(\n                encoding=serialization.Encoding.PEM,\n                format=serialization.PrivateFormat.TraditionalOpenSSL,\n                encryption_algorithm=serialization.NoEncryption(),\n            )\n        )\n    return var_2\n",
      "lines_processed": 14,
      "total_lines": 123,
      "llm_code": "def genrsa(private_key_file):\n    private_key = rsa.generate_private_key(\n        public_exponent=65537,\n        key_size=2048,\n    )\n    with open(private_key_file, \"wb\") as file:\n        file.write(\n            private_key.private_bytes(\n                encoding=serialization.Encoding.PEM,\n                format=serialization.PrivateFormat.TraditionalOpenSSL,\n                encryption_algorithm=serialization.NoEncryption(),\n            )\n        )\n    return private_key\n",
      "llm_variables": [
        "private_key_file",
        "private_key",
        "file"
      ]
    },
    {
      "file_path": "trigger_azure_pipeline.py",
      "code": "def get_log_content(url):\n    resp = s.get(url)\n    return resp.text\n",
      "variables": [
        "url",
        "resp"
      ],
      "anonymized_code": "def get_log_content(var_1):\n    var_2 = s.get(var_1)\n    return var_2.text\n",
      "lines_processed": 3,
      "total_lines": 157,
      "llm_code": "def get_log_content(log_id):\n    response = s.get(log_id)\n    return response.text\n",
      "llm_variables": [
        "log_id",
        "response"
      ]
    },
    {
      "file_path": "trigger_azure_pipeline.py",
      "code": "def get_build_logs(_id):\n    get_build_logs_url = (\n        AZURE_PIPELINE_BASE_URL + f\"/_apis/build/builds/{_id}/logs?api-version=6.0\"\n    )\n    get_build_logs_raw = s.get(get_build_logs_url)\n    return get_build_logs_raw.json()\n",
      "variables": [
        "_id",
        "get_build_logs_url",
        "get_build_logs_raw"
      ],
      "anonymized_code": "def get_build_logs(var_1):\n    var_2 = (\n        AZURE_PIPELINE_BASE_URL + f\"/_apis/build/builds/{var_1}/logs?api-version=6.0\"\n    )\n    var_3 = s.get(var_2)\n    return var_3.json()\n",
      "lines_processed": 6,
      "total_lines": 157,
      "llm_code": "def get_build_logs(build_id):\n    url = (\n        AZURE_PIPELINE_BASE_URL + f\"/_apis/build/builds/{build_id}/logs?api-version=6.0\"\n    )\n    response = s.get(url)\n    return response.json()\n",
      "llm_variables": [
        "build_id",
        "url",
        "response"
      ]
    },
    {
      "file_path": "trigger_azure_pipeline.py",
      "code": "def get_build(_id):\n    get_build_url = (\n        AZURE_PIPELINE_BASE_URL + f\"/_apis/build/builds/{_id}?api-version=6.0\"\n    )\n    get_build_raw = s.get(get_build_url)\n    return get_build_raw.json()\n",
      "variables": [
        "_id",
        "get_build_url",
        "get_build_raw"
      ],
      "anonymized_code": "def get_build(var_1):\n    var_2 = (\n        AZURE_PIPELINE_BASE_URL + f\"/_apis/build/builds/{var_1}?api-version=6.0\"\n    )\n    var_3 = s.get(var_2)\n    return var_3.json()\n",
      "lines_processed": 6,
      "total_lines": 157,
      "llm_code": "def get_build(build_id):\n    url = (\n        AZURE_PIPELINE_BASE_URL + f\"/_apis/build/builds/{build_id}?api-version=6.0\"\n    )\n    response = s.get(url)\n    return response.json()\n",
      "llm_variables": [
        "build_id",
        "url",
        "response"
      ]
    },
    {
      "file_path": "trigger_azure_pipeline.py",
      "code": "def wait_for_build(_id):\n    build_detail = get_build(_id)\n    build_status = build_detail[\"status\"]\n\n    while build_status == \"notStarted\":\n        print(\"Waiting for run to start: \" + str(_id))\n        sys.stdout.flush()\n        try:\n            build_detail = get_build(_id)\n            build_status = build_detail[\"status\"]\n        except Exception as e:\n            print(\"Error getting build\")\n            print(e)\n\n        time.sleep(30)\n\n    print(\"Bulid started: \", str(_id))\n\n    handled_logs = set()\n",
      "variables": [
        "_id",
        "build_detail",
        "build_status",
        "handled_logs"
      ],
      "anonymized_code": "def wait_for_build(var_1):\n    var_2 = get_build(var_1)\n    var_3 = var_2[\"status\"]\n\n    while var_3 == \"notStarted\":\n        print(\"Waiting for run to start: \" + str(var_1))\n        sys.stdout.flush()\n        try:\n            var_2 = get_build(var_1)\n            var_3 = var_2[\"status\"]\n        except Exception as e:\n            print(\"Error getting build\")\n            print(e)\n\n        time.sleep(30)\n\n    print(\"Bulid started: \", str(var_1))\n\n    var_4 = set()\n",
      "lines_processed": 19,
      "total_lines": 157,
      "llm_code": "def wait_for_build(build_id):\n    build = get_build(build_id)\n    status = build[\"status\"]\n\n    while status == \"notStarted\":\n        print(\"Waiting for run to start: \" + str(build_id))\n        sys.stdout.flush()\n        try:\n            build = get_build(build_id)\n            status = build[\"status\"]\n        except Exception as e:\n            print(\"Error getting build\")\n            print(e)\n\n        time.sleep(30)\n\n    print(\"Bulid started: \", str(build_id))\n\n    error_set = set()\n",
      "llm_variables": [
        "build_id",
        "build",
        "status",
        "error_set"
      ]
    },
    {
      "file_path": "max_autotune.py",
      "code": "def train(args, model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % args.log_interval == 0:\n            print(\n                f\"Train Epoch: {epoch} \"\n                f\"[{batch_idx * len(data)}/{len(train_loader.dataset)} \"\n                f\"({100.0 * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\"\n            )\n            if args.dry_run:\n                break\n",
      "variables": [
        "args",
        "model",
        "device",
        "train_loader",
        "optimizer",
        "epoch",
        "batch_idx",
        "data",
        "target",
        "output",
        "loss"
      ],
      "anonymized_code": "def train(var_1, var_2, var_3, var_4, var_5, var_6):\n    var_2.train()\n    for var_7, (var_8, var_9) in enumerate(var_4):\n        var_8, var_9 = var_8.to(var_3), var_9.to(var_3)\n        var_5.zero_grad()\n        var_10 = var_2(var_8)\n        var_11 = F.nll_loss(var_10, var_9)\n        var_11.backward()\n        var_5.step()\n        if var_7 % var_1.log_interval == 0:\n            print(\n                f\"Train Epoch: {var_6} \"\n                f\"[{var_7 * len(var_8)}/{len(var_4.dataset)} \"\n                f\"({100.0 * var_7 / len(var_4):.0f}%)]\\tLoss: {var_11.item():.6f}\"\n            )\n            if var_1.dry_run:\n                break\n",
      "lines_processed": 17,
      "total_lines": 209,
      "llm_code": "def train(model, optimizer, device, train_loader, epoch, log_interval):\n    optimizer.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        epoch.zero_grad()\n        output = optimizer(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        epoch.step()\n        if batch_idx % model.log_interval == 0:\n            print(\n                f\"Train Epoch: {log_interval} \"\n                f\"[{batch_idx * len(data)}/{len(train_loader.dataset)} \"\n                f\"({100.0 * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\"\n            )\n            if model.dry_run:\n                break\n",
      "llm_variables": [
        "model",
        "optimizer",
        "device",
        "train_loader",
        "epoch",
        "log_interval",
        "batch_idx",
        "data",
        "target",
        "output",
        "loss"
      ]
    },
    {
      "file_path": "max_autotune.py",
      "code": "def timed(fn):\n    start = torch.cuda.Event(enable_timing=True)\n    end = torch.cuda.Event(enable_timing=True)\n    start.record()\n    result = fn()\n    end.record()\n    torch.cuda.synchronize()\n    return result, start.elapsed_time(end) / 1000\n",
      "variables": [
        "fn",
        "start",
        "end",
        "result"
      ],
      "anonymized_code": "def timed(var_1):\n    var_2 = torch.cuda.Event(enable_timing=True)\n    var_3 = torch.cuda.Event(enable_timing=True)\n    var_2.record()\n    var_4 = var_1()\n    var_3.record()\n    torch.cuda.synchronize()\n    return var_4, var_2.elapsed_time(var_3) / 1000\n",
      "lines_processed": 8,
      "total_lines": 209,
      "llm_code": "def timed(func):\n    start_event = torch.cuda.Event(enable_timing=True)\n    end_event = torch.cuda.Event(enable_timing=True)\n    start_event.record()\n    result = func()\n    end_event.record()\n    torch.cuda.synchronize()\n    return result, start_event.elapsed_time(end_event) / 1000",
      "llm_variables": [
        "func",
        "start_event",
        "end_event",
        "result"
      ]
    }
  ],
  "pydantic_pydantic": [
    {
      "file_path": "_docs_extraction.py",
      "code": "def _dedent_source_lines(source: list[str]) -> str:\n    # Required for nested class definitions, e.g. in a function block\n    dedent_source = textwrap.dedent(''.join(source))\n    if dedent_source.startswith((' ', '\\t')):\n        # We are in the case where there's a dedented (usually multiline) string\n        # at a lower indentation level than the class itself. We wrap our class\n        # in a function as a workaround.\n        dedent_source = f'def dedent_workaround():\\n{dedent_source}'\n    return dedent_source\n",
      "variables": [
        "source",
        "dedent_source"
      ],
      "anonymized_code": "def _dedent_source_lines(var_1: list[str]) -> str:\n    # Required for nested class definitions, e.g. in a function block\n    var_2 = textwrap.dedent(''.join(var_1))\n    if var_2.startswith((' ', '\\t')):\n        # We are in the case where there's a dedented (usually multiline) string\n        # at a lower indentation level than the class itself. We wrap our class\n        # in a function as a workaround.\n        var_2 = f'def dedent_workaround():\\n{var_2}'\n    return var_2\n",
      "lines_processed": 9,
      "total_lines": 113,
      "llm_code": "def _dedent_source_lines(source_lines: list[str]) -> str:\n    # Required for nested class definitions, e.g. in a function block\n    dedented = textwrap.dedent(\"\".join(source_lines))\n    if dedented.startswith((\" \", \"\t\")):\n        # We are in the case where there\"s a dedented (usually multiline) string\n        # at a lower indentation level than the class itself. We wrap our class\n        # in a function as a workaround.\n        dedented = f\"def dedent_workaround():\\n{dedented}\"\n    return dedented",
      "llm_variables": [
        "source_lines",
        "dedented"
      ]
    },
    {
      "file_path": "_docs_extraction.py",
      "code": "def _extract_source_from_frame(cls: type[Any]) -> list[str] | None:\n    frame = inspect.currentframe()\n\n    while frame:\n        if inspect.getmodule(frame) is inspect.getmodule(cls):\n            lnum = frame.f_lineno\n            try:\n                lines, _ = inspect.findsource(frame)\n            except OSError:  # pragma: no cover\n                # Source can't be retrieved (maybe because running in an interactive terminal),\n                # we don't want to error here.\n                pass\n            else:\n                block_lines = inspect.getblock(lines[lnum - 1 :])\n                dedent_source = _dedent_source_lines(block_lines)\n                try:\n                    block_tree = ast.parse(dedent_source)\n                except SyntaxError:\n                    pass\n",
      "variables": [
        "cls",
        "frame",
        "lnum",
        "lines",
        "_",
        "block_lines",
        "dedent_source",
        "block_tree"
      ],
      "anonymized_code": "def _extract_source_from_frame(var_1: type[Any]) -> list[str] | None:\n    var_2 = inspect.currentframe()\n\n    while var_2:\n        if inspect.getmodule(var_2) is inspect.getmodule(var_1):\n            var_3 = var_2.f_lineno\n            try:\n                var_4, var_5 = inspect.findsource(var_2)\n            except OSError:  # pragma: no cover\n                # Source can't be retrieved (maybe because running in an interactive terminal),\n                # we don't want to error here.\n                pass\n            else:\n                var_6 = inspect.getblock(var_4[var_3 - 1 :])\n                var_7 = _dedent_source_lines(var_6)\n                try:\n                    var_8 = ast.parse(var_7)\n                except SyntaxError:\n                    pass\n",
      "lines_processed": 19,
      "total_lines": 113,
      "llm_code": "def _extract_source_from_frame(func: type[Any]) -> list[str] | None:\n    frame = inspect.currentframe()\n\n    while frame:\n        if inspect.getmodule(frame) is inspect.getmodule(func):\n            line_number = frame.f_lineno\n            try:\n                source, _ = inspect.findsource(frame)\n            except OSError:  # pragma: no cover\n                # Source can't be retrieved (maybe because running in an interactive terminal),\n                # we don't want to error here.\n                pass\n            else:\n                block = inspect.getblock(source[line_number - 1 :])\n                dedented_source = _dedent_source_lines(block)\n                try:\n                    ast = ast.parse(dedented_source)\n                except SyntaxError:\n                    pass\n",
      "llm_variables": [
        "func",
        "frame",
        "line_number",
        "source",
        "_",
        "block",
        "dedented_source",
        "ast"
      ]
    },
    {
      "file_path": "_core_utils.py",
      "code": "def is_core_schema_field(\n    schema: CoreSchemaOrField,\n) -> TypeGuard[CoreSchemaField]:\n    return schema['type'] in _CORE_SCHEMA_FIELD_TYPES\n",
      "variables": [
        "schema"
      ],
      "anonymized_code": "def is_core_schema_field(\n    var_1: CoreSchemaOrField,\n) -> TypeGuard[CoreSchemaField]:\n    return var_1['type'] in _CORE_SCHEMA_FIELD_TYPES\n",
      "lines_processed": 4,
      "total_lines": 182,
      "llm_code": "def is_core_schema_field(\n    field: CoreSchemaOrField,\n) -> TypeGuard[CoreSchemaField]:\n    return field['type'] in _CORE_SCHEMA_FIELD_TYPES\n",
      "llm_variables": [
        "field"
      ]
    },
    {
      "file_path": "_core_utils.py",
      "code": "def pretty_print_core_schema(\n    val: Any,\n    *,\n    console: Console | None = None,\n    max_depth: int | None = None,\n    strip_metadata: bool = True,\n) -> None:  # pragma: no cover\n    \"\"\"Pretty-print a core schema using the `rich` library.\n\n    Args:\n        val: The core schema to print, or a Pydantic model/dataclass/type adapter\n            (in which case the cached core schema is fetched and printed).\n        console: A rich console to use when printing. Defaults to the global rich console instance.\n        max_depth: The number of nesting levels which may be printed.\n        strip_metadata: Whether to strip metadata in the output. If `True` any known core metadata\n            attributes will be stripped (but custom attributes are kept). Defaults to `True`.\n    \"\"\"\n    # lazy import:\n    from rich.pretty import pprint\n",
      "variables": [
        "val",
        "console",
        "max_depth",
        "strip_metadata"
      ],
      "anonymized_code": "def pretty_print_core_schema(\n    var_1: Any,\n    *,\n    var_2: Console | None = None,\n    var_3: int | None = None,\n    var_4: bool = True,\n) -> None:  # pragma: no cover\n    \"\"\"Pretty-print a core schema using the `rich` library.\n\n    Args:\n        var_1: The core schema to print, or a Pydantic model/dataclass/type adapter\n            (in which case the cached core schema is fetched and printed).\n        var_2: A rich var_2 to use when printing. Defaults to the global rich var_2 instance.\n        var_3: The number of nesting levels which may be printed.\n        var_4: Whether to strip metadata in the output. If `True` any known core metadata\n            attributes will be stripped (but custom attributes are kept). Defaults to `True`.\n    \"\"\"\n    # lazy import:\n    from rich.pretty import pprint\n",
      "lines_processed": 19,
      "total_lines": 182,
      "llm_code": "def pretty_print_core_schema(\n    schema: Any,\n    *,\n    console: Console | None = None,\n    max_depth: int | None = None,\n    strip_metadata: bool = True,\n) -> None:  # pragma: no cover\n    \"\"\"Pretty-print a core schema using the `rich` library.\n\n    Args:\n        schema: The core schema to print, or a Pydantic model/dataclass/type adapter\n            (in which case the cached core schema is fetched and printed).\n        console: A rich console to use when printing. Defaults to the global rich console instance.\n        max_depth: The number of nesting levels which may be printed.\n        strip_metadata: Whether to strip metadata in the output. If `True` any known core metadata\n            attributes will be stripped (but custom attributes are kept). Defaults to `True`.\n    \"\"\"\n    # lazy import:\n    from rich.pretty import pprint\n",
      "llm_variables": [
        "schema",
        "console",
        "max_depth",
        "strip_metadata"
      ]
    },
    {
      "file_path": "_core_utils.py",
      "code": "def is_function_with_inner_schema(\n    schema: CoreSchemaOrField,\n) -> TypeGuard[FunctionSchemaWithInnerSchema]:\n    return schema['type'] in _FUNCTION_WITH_INNER_SCHEMA_TYPES\n",
      "variables": [
        "schema"
      ],
      "anonymized_code": "def is_function_with_inner_schema(\n    var_1: CoreSchemaOrField,\n) -> TypeGuard[FunctionSchemaWithInnerSchema]:\n    return var_1['type'] in _FUNCTION_WITH_INNER_SCHEMA_TYPES\n",
      "lines_processed": 4,
      "total_lines": 182,
      "llm_code": "def is_function_with_inner_schema(\n    schema: CoreSchemaOrField,\n) -> TypeGuard[FunctionSchemaWithInnerSchema]:\n    return schema['type'] in _FUNCTION_WITH_INNER_SCHEMA_TYPES\n",
      "llm_variables": [
        "schema"
      ]
    },
    {
      "file_path": "_core_utils.py",
      "code": "def is_list_like_schema_with_items_schema(\n    schema: CoreSchema,\n) -> TypeGuard[core_schema.ListSchema | core_schema.SetSchema | core_schema.FrozenSetSchema]:\n    return schema['type'] in _LIST_LIKE_SCHEMA_WITH_ITEMS_TYPES\n",
      "variables": [
        "schema"
      ],
      "anonymized_code": "def is_list_like_schema_with_items_schema(\n    var_1: CoreSchema,\n) -> TypeGuard[core_schema.ListSchema | core_schema.SetSchema | core_schema.FrozenSetSchema]:\n    return var_1['type'] in _LIST_LIKE_SCHEMA_WITH_ITEMS_TYPES\n",
      "lines_processed": 4,
      "total_lines": 182,
      "llm_code": "def is_list_like_schema_with_items_schema(\n    schema: CoreSchema,\n) -> TypeGuard[core_schema.ListSchema | core_schema.SetSchema | core_schema.FrozenSetSchema]:\n    return schema[\"type\"] in _LIST_LIKE_SCHEMA_WITH_ITEMS_TYPES",
      "llm_variables": [
        "schema"
      ]
    },
    {
      "file_path": "_core_utils.py",
      "code": "def is_core_schema(\n    schema: CoreSchemaOrField,\n) -> TypeGuard[CoreSchema]:\n    return schema['type'] not in _CORE_SCHEMA_FIELD_TYPES\n",
      "variables": [
        "schema"
      ],
      "anonymized_code": "def is_core_schema(\n    var_1: CoreSchemaOrField,\n) -> TypeGuard[CoreSchema]:\n    return var_1['type'] not in _CORE_SCHEMA_FIELD_TYPES\n",
      "lines_processed": 4,
      "total_lines": 182,
      "llm_code": "def is_core_schema(\n    schema: CoreSchemaOrField,\n) -> TypeGuard[CoreSchema]:\n    return schema['type'] not in _CORE_SCHEMA_FIELD_TYPES\n",
      "llm_variables": [
        "schema"
      ]
    },
    {
      "file_path": "_core_utils.py",
      "code": "def _clean_schema_for_pretty_print(obj: Any, strip_metadata: bool = True) -> Any:  # pragma: no cover\n    \"\"\"A utility function to remove irrelevant information from a core schema.\"\"\"\n    if isinstance(obj, Mapping):\n        new_dct = {}\n        for k, v in obj.items():\n            if k == 'metadata' and strip_metadata:\n                new_metadata = {}\n\n                for meta_k, meta_v in v.items():\n                    if meta_k in ('pydantic_js_functions', 'pydantic_js_annotation_functions'):\n                        new_metadata['js_metadata'] = '<stripped>'\n                    else:\n                        new_metadata[meta_k] = _clean_schema_for_pretty_print(meta_v, strip_metadata=strip_metadata)\n\n                if list(new_metadata.keys()) == ['js_metadata']:\n                    new_metadata = {'<stripped>'}\n\n                new_dct[k] = new_metadata\n            # Remove some defaults:\n",
      "variables": [
        "obj",
        "strip_metadata",
        "new_dct",
        "k",
        "v",
        "new_metadata",
        "meta_k",
        "meta_v"
      ],
      "anonymized_code": "def _clean_schema_for_pretty_print(var_1: Any, var_2: bool = True) -> Any:  # pragma: no cover\n    \"\"\"A utility function to remove irrelevant information from a core schema.\"\"\"\n    if isinstance(var_1, Mapping):\n        var_3 = {}\n        for var_4, var_5 in var_1.items():\n            if var_4 == 'metadata' and var_2:\n                var_6 = {}\n\n                for var_7, var_8 in var_5.items():\n                    if var_7 in ('pydantic_js_functions', 'pydantic_js_annotation_functions'):\n                        var_6['js_metadata'] = '<stripped>'\n                    else:\n                        var_6[var_7] = _clean_schema_for_pretty_print(var_8, var_2=var_2)\n\n                if list(var_6.keys()) == ['js_metadata']:\n                    var_6 = {'<stripped>'}\n\n                var_3[var_4] = var_6\n            # Remove some defaults:\n",
      "lines_processed": 19,
      "total_lines": 182,
      "llm_code": "def _clean_schema_for_pretty_print(schema: Any, remove_metadata: bool = True) -> Any:  # pragma: no cover\n    \"\"\"A utility function to remove irrelevant information from a core schema.\"\"\"\n    if isinstance(schema, Mapping):\n        cleaned = {}\n        for key, value in schema.items():\n            if key == 'metadata' and remove_metadata:\n                metadata = {}\n\n                for sub_key, sub_value in value.items():\n                    if sub_key in ('pydantic_js_functions', 'pydantic_js_annotation_functions'):\n                        metadata['js_metadata'] = '<stripped>'\n                    else:\n                        metadata[sub_key] = _clean_schema_for_pretty_print(sub_value, remove_metadata=remove_metadata)\n\n                if list(metadata.keys()) == ['js_metadata']:\n                    metadata = {'<stripped>'}\n\n                cleaned[key] = metadata\n            # Remove some defaults:\n",
      "llm_variables": [
        "schema",
        "remove_metadata",
        "cleaned",
        "key",
        "value",
        "metadata",
        "sub_key",
        "sub_value"
      ]
    },
    {
      "file_path": "_core_utils.py",
      "code": "def validate_core_schema(schema: CoreSchema) -> CoreSchema:\n    if os.getenv('PYDANTIC_VALIDATE_CORE_SCHEMAS'):\n        return _validate_core_schema(schema)\n    return schema\n",
      "variables": [
        "schema"
      ],
      "anonymized_code": "def validate_core_schema(var_1: CoreSchema) -> CoreSchema:\n    if os.getenv('PYDANTIC_VALIDATE_CORE_SCHEMAS'):\n        return _validate_core_schema(var_1)\n    return var_1\n",
      "lines_processed": 4,
      "total_lines": 182,
      "llm_code": "def validate_core_schema(schema: CoreSchema) -> CoreSchema:\n    if os.getenv('PYDANTIC_VALIDATE_CORE_SCHEMAS'):\n        return _validate_core_schema(schema)\n    return schema\n",
      "llm_variables": [
        "schema"
      ]
    },
    {
      "file_path": "_core_utils.py",
      "code": "def get_ref(s: core_schema.CoreSchema) -> None | str:\n    \"\"\"Get the ref from the schema if it has one.\n    This exists just for type checking to work correctly.\n    \"\"\"\n    return s.get('ref', None)\n",
      "variables": [
        "s"
      ],
      "anonymized_code": "def get_ref(var_1: core_schema.CoreSchema) -> None | str:\n    \"\"\"Get the ref from the schema if it has one.\n    This exists just for type checking to work correctly.\n    \"\"\"\n    return var_1.get('ref', None)\n",
      "lines_processed": 5,
      "total_lines": 182,
      "llm_code": "def get_ref(schema: core_schema.CoreSchema) -> None | str:\n    \"\"\"Get the ref from the schema if it has one.\n    This exists just for type checking to work correctly.\n    \"\"\"\n    return schema.get('ref', None)\n",
      "llm_variables": [
        "schema"
      ]
    },
    {
      "file_path": "people.py",
      "code": "def get_graphql_response(\n    *,\n    settings: Settings,\n    query: str,\n    after: str | None = None,\n) -> dict[str, Any]:\n    \"\"\"Make a GraphQL request to GitHub API.\n\n    Args:\n        settings: Configuration settings including API token\n        query: GraphQL query string\n        after: Cursor for pagination, if any\n\n    Returns:\n        Response data from GitHub API in JSON format\n\n    Raises:\n        RuntimeError: If the API request fails or returns errors\n    \"\"\"\n",
      "variables": [
        "settings",
        "query",
        "after"
      ],
      "anonymized_code": "def get_graphql_response(\n    *,\n    var_1: Settings,\n    var_2: str,\n    var_3: str | None = None,\n) -> dict[str, Any]:\n    \"\"\"Make a GraphQL request to GitHub API.\n\n    Args:\n        var_1: Configuration var_1 including API token\n        var_2: GraphQL var_2 string\n        var_3: Cursor for pagination, if any\n\n    Returns:\n        Response data from GitHub API in JSON format\n\n    Raises:\n        RuntimeError: If the API request fails or returns errors\n    \"\"\"\n",
      "lines_processed": 19,
      "total_lines": 781,
      "llm_code": "def get_graphql_response(\n    *,\n    settings: Settings,\n    query: str,\n    cursor: str | None = None,\n) -> dict[str, Any]:\n    \"\"\"Make a GraphQL request to GitHub API.\n\n    Args:\n        settings: Configuration settings including API token\n        query: GraphQL query string\n        cursor: Cursor for pagination, if any\n\n    Returns:\n        Response data from GitHub API in JSON format\n\n    Raises:\n        RuntimeError: If the API request fails or returns errors\n    \"\"\"\n",
      "llm_variables": [
        "settings",
        "query",
        "cursor"
      ]
    },
    {
      "file_path": "people.py",
      "code": "def get_graphql_issue_edges(*, settings: Settings, after: str | None = None) -> list[IssuesEdge]:\n    \"\"\"Fetch issue edges from GitHub GraphQL API.\n\n    Args:\n        settings: Configuration settings\n        after: Cursor for pagination, if any\n\n    Returns:\n        List of issue edges from the GraphQL response\n    \"\"\"\n    data = get_graphql_response(settings=settings, query=issues_query, after=after)\n    graphql_response = IssuesResponse.model_validate(data)\n    return graphql_response.data.repository.issues.edges\n",
      "variables": [
        "settings",
        "after",
        "data",
        "graphql_response"
      ],
      "anonymized_code": "def get_graphql_issue_edges(*, var_1: Settings, var_2: str | None = None) -> list[IssuesEdge]:\n    \"\"\"Fetch issue edges from GitHub GraphQL API.\n\n    Args:\n        var_1: Configuration var_1\n        var_2: Cursor for pagination, if any\n\n    Returns:\n        List of issue edges from the GraphQL response\n    \"\"\"\n    var_3 = get_graphql_response(var_1=var_1, query=issues_query, var_2=var_2)\n    var_4 = IssuesResponse.model_validate(var_3)\n    return var_4.var_3.repository.issues.edges\n",
      "lines_processed": 13,
      "total_lines": 781,
      "llm_code": "def get_graphql_issue_edges(*, settings: Settings, cursor: str | None = None) -> list[IssuesEdge]:\n    \"\"\"Fetch issue edges from GitHub GraphQL API.\n\n    Args:\n        settings: Configuration settings\n        cursor: Cursor for pagination, if any\n\n    Returns:\n        List of issue edges from the GraphQL response\n    \"\"\"\n    response = get_graphql_response(settings=settings, query=issues_query, cursor=cursor)\n    data = IssuesResponse.model_validate(response)\n    return data.response.repository.issues.edges\n",
      "llm_variables": [
        "settings",
        "cursor",
        "response",
        "data"
      ]
    },
    {
      "file_path": "people.py",
      "code": "def get_issues_experts(settings: Settings) -> tuple[Counter, Counter, dict[str, Author]]:\n    \"\"\"Analyze issues to identify expert contributors.\n\n    Args:\n        settings: Configuration settings\n\n    Returns:\n        A tuple containing:\n            - Counter of all commentors\n            - Counter of commentors from the last month\n            - Dictionary mapping usernames to Author objects\n    \"\"\"\n    issue_nodes: list[IssuesNode] = []\n    issue_edges = get_graphql_issue_edges(settings=settings)\n\n    while issue_edges:\n        for edge in issue_edges:\n            issue_nodes.append(edge.node)\n        last_edge = issue_edges[-1]\n",
      "variables": [
        "settings",
        "issue_nodes",
        "issue_edges",
        "edge",
        "last_edge"
      ],
      "anonymized_code": "def get_issues_experts(var_1: Settings) -> tuple[Counter, Counter, dict[str, Author]]:\n    \"\"\"Analyze issues to identify expert contributors.\n\n    Args:\n        var_1: Configuration var_1\n\n    Returns:\n        A tuple containing:\n            - Counter of all commentors\n            - Counter of commentors from the last month\n            - Dictionary mapping usernames to Author objects\n    \"\"\"\n    var_2: list[IssuesNode] = []\n    var_3 = get_graphql_issue_edges(var_1=var_1)\n\n    while var_3:\n        for var_4 in var_3:\n            var_2.append(var_4.node)\n        var_5 = var_3[-1]\n",
      "lines_processed": 19,
      "total_lines": 781,
      "llm_code": "def get_issues_experts(settings: Settings) -> tuple[Counter, Counter, dict[str, Author]]:\n    \"\"\"Analyze issues to identify expert contributors.\n\n    Args:\n        settings: Configuration settings\n\n    Returns:\n        A tuple containing:\n            - Counter of all commentors\n            - Counter of commentors from the last month\n            - Dictionary mapping usernames to Author objects\n    \"\"\"\n    issues: list[IssuesNode] = []\n    issue_edges = get_graphql_issue_edges(settings=settings)\n\n    while issue_edges:\n        for issue in issue_edges:\n            issues.append(issue.node)\n        last_issue = issue_edges[-1]\n",
      "llm_variables": [
        "settings",
        "issues",
        "issue_edges",
        "issue",
        "last_issue"
      ]
    },
    {
      "file_path": "people.py",
      "code": "def get_graphql_pr_edges(*, settings: Settings, after: str | None = None) -> list[PullRequestEdge]:\n    \"\"\"Fetch pull request edges from GitHub GraphQL API.\n\n    Args:\n        settings: Configuration settings\n        after: Cursor for pagination, if any\n\n    Returns:\n        List of pull request edges from the GraphQL response\n    \"\"\"\n    data = get_graphql_response(settings=settings, query=prs_query, after=after)\n    graphql_response = PRsResponse.model_validate(data)\n    return graphql_response.data.repository.pullRequests.edges\n",
      "variables": [
        "settings",
        "after",
        "data",
        "graphql_response"
      ],
      "anonymized_code": "def get_graphql_pr_edges(*, var_1: Settings, var_2: str | None = None) -> list[PullRequestEdge]:\n    \"\"\"Fetch pull request edges from GitHub GraphQL API.\n\n    Args:\n        var_1: Configuration var_1\n        var_2: Cursor for pagination, if any\n\n    Returns:\n        List of pull request edges from the GraphQL response\n    \"\"\"\n    var_3 = get_graphql_response(var_1=var_1, query=prs_query, var_2=var_2)\n    var_4 = PRsResponse.model_validate(var_3)\n    return var_4.var_3.repository.pullRequests.edges\n",
      "lines_processed": 13,
      "total_lines": 781,
      "llm_code": "def get_graphql_pr_edges(*, settings: Settings, cursor: str | None = None) -> list[PullRequestEdge]:\n    \"\"\"Fetch pull request edges from GitHub GraphQL API.\n\n    Args:\n        settings: Configuration settings\n        cursor: Cursor for pagination, if any\n\n    Returns:\n        List of pull request edges from the GraphQL response\n    \"\"\"\n    response = get_graphql_response(settings=settings, query=prs_query, cursor=cursor)\n    data = PRsResponse.model_validate(response)\n    return data.response.repository.pullRequests.edges\n",
      "llm_variables": [
        "settings",
        "cursor",
        "response",
        "data"
      ]
    },
    {
      "file_path": "people.py",
      "code": "def get_graphql_question_discussion_edges(\n    *,\n    settings: Settings,\n    after: str | None = None,\n) -> list[DiscussionsEdge]:\n    \"\"\"Fetch discussion edges from GitHub GraphQL API.\n\n    Args:\n        settings: Configuration settings\n        after: Cursor for pagination, if any\n\n    Returns:\n        List of discussion edges from the GraphQL response\n    \"\"\"\n    data = get_graphql_response(\n        settings=settings,\n        query=discussions_query,\n        after=after,\n    )\n",
      "variables": [
        "settings",
        "after",
        "data"
      ],
      "anonymized_code": "def get_graphql_question_discussion_edges(\n    *,\n    var_1: Settings,\n    var_2: str | None = None,\n) -> list[DiscussionsEdge]:\n    \"\"\"Fetch discussion edges from GitHub GraphQL API.\n\n    Args:\n        var_1: Configuration var_1\n        var_2: Cursor for pagination, if any\n\n    Returns:\n        List of discussion edges from the GraphQL response\n    \"\"\"\n    var_3 = get_graphql_response(\n        var_1=var_1,\n        query=discussions_query,\n        var_2=var_2,\n    )\n",
      "lines_processed": 19,
      "total_lines": 781,
      "llm_code": "def get_graphql_question_discussion_edges(\n    *,\n    settings: Settings,\n    cursor: str | None = None,\n) -> list[DiscussionsEdge]:\n    \"\"\"Fetch discussion edges from GitHub GraphQL API.\n\n    Args:\n        settings: Configuration settings\n        cursor: Cursor for pagination, if any\n\n    Returns:\n        List of discussion edges from the GraphQL response\n    \"\"\"\n    response = get_graphql_response(\n        settings=settings,\n        query=discussions_query,\n        cursor=cursor,\n    )\n",
      "llm_variables": [
        "settings",
        "cursor",
        "response"
      ]
    },
    {
      "file_path": "people.py",
      "code": "def get_contributors(settings: Settings) -> tuple[Counter, Counter, Counter, dict[str, Author]]:\n    \"\"\"Analyze pull requests to identify contributors, commentors, and reviewers.\n\n    Args:\n        settings: Configuration settings\n\n    Returns:\n        A tuple containing:\n            - Counter of contributors (merged PRs)\n            - Counter of commentors\n            - Counter of reviewers\n            - Dictionary mapping usernames to Author objects\n    \"\"\"\n    pr_nodes: list[PullRequestNode] = []\n    pr_edges = get_graphql_pr_edges(settings=settings)\n\n    while pr_edges:\n        for edge in pr_edges:\n            pr_nodes.append(edge.node)\n",
      "variables": [
        "settings",
        "pr_nodes",
        "pr_edges",
        "edge"
      ],
      "anonymized_code": "def get_contributors(var_1: Settings) -> tuple[Counter, Counter, Counter, dict[str, Author]]:\n    \"\"\"Analyze pull requests to identify contributors, commentors, and reviewers.\n\n    Args:\n        var_1: Configuration var_1\n\n    Returns:\n        A tuple containing:\n            - Counter of contributors (merged PRs)\n            - Counter of commentors\n            - Counter of reviewers\n            - Dictionary mapping usernames to Author objects\n    \"\"\"\n    var_2: list[PullRequestNode] = []\n    var_3 = get_graphql_pr_edges(var_1=var_1)\n\n    while var_3:\n        for var_4 in var_3:\n            var_2.append(var_4.node)\n",
      "lines_processed": 19,
      "total_lines": 781,
      "llm_code": "def get_contributors(settings: Settings) -> tuple[Counter, Counter, Counter, dict[str, Author]]:\n    \"\"\"Analyze pull requests to identify contributors, commentors, and reviewers.\n\n    Args:\n        settings: Configuration settings\n\n    Returns:\n        A tuple containing:\n            - Counter of contributors (merged PRs)\n            - Counter of commentors\n            - Counter of reviewers\n            - Dictionary mapping usernames to Author objects\n    \"\"\"\n    pr_edges: list[PullRequestNode] = []\n    graph_data = get_graphql_pr_edges(settings=settings)\n\n    while graph_data:\n        for edge in graph_data:\n            pr_edges.append(edge.node)\n",
      "llm_variables": [
        "settings",
        "pr_edges",
        "graph_data",
        "edge"
      ]
    },
    {
      "file_path": "people.py",
      "code": "def get_discussions_experts(settings: Settings) -> tuple[Counter, Counter, dict[str, Author]]:\n    \"\"\"Analyze discussions to identify expert contributors.\n\n    Args:\n        settings: Configuration settings\n\n    Returns:\n        A tuple containing:\n            - Counter of all commentors\n            - Counter of commentors from the last month\n            - Dictionary mapping usernames to Author objects\n    \"\"\"\n    discussion_nodes: list[DiscussionsNode] = []\n    discussion_edges = get_graphql_question_discussion_edges(settings=settings)\n\n    while discussion_edges:\n        for discussion_edge in discussion_edges:\n            discussion_nodes.append(discussion_edge.node)\n        last_edge = discussion_edges[-1]\n",
      "variables": [
        "settings",
        "discussion_nodes",
        "discussion_edges",
        "discussion_edge",
        "last_edge"
      ],
      "anonymized_code": "def get_discussions_experts(var_1: Settings) -> tuple[Counter, Counter, dict[str, Author]]:\n    \"\"\"Analyze discussions to identify expert contributors.\n\n    Args:\n        var_1: Configuration var_1\n\n    Returns:\n        A tuple containing:\n            - Counter of all commentors\n            - Counter of commentors from the last month\n            - Dictionary mapping usernames to Author objects\n    \"\"\"\n    var_2: list[DiscussionsNode] = []\n    var_3 = get_graphql_question_discussion_edges(var_1=var_1)\n\n    while var_3:\n        for var_4 in var_3:\n            var_2.append(var_4.node)\n        var_5 = var_3[-1]\n",
      "lines_processed": 19,
      "total_lines": 781,
      "llm_code": "def get_discussions_experts(settings: Settings) -> tuple[Counter, Counter, dict[str, Author]]:\n    \"\"\"Analyze discussions to identify expert contributors.\n\n    Args:\n        settings: Configuration settings\n\n    Returns:\n        A tuple containing:\n            - Counter of all commentors\n            - Counter of commentors from the last month\n            - Dictionary mapping usernames to Author objects\n    \"\"\"\n    discussions: list[DiscussionsNode] = []\n    all_edges = get_graphql_question_discussion_edges(settings=settings)\n\n    while all_edges:\n        for discussion in all_edges:\n            discussions.append(discussion.node)\n        last_edge = all_edges[-1]\n",
      "llm_variables": [
        "settings",
        "discussions",
        "all_edges",
        "discussion",
        "last_edge"
      ]
    },
    {
      "file_path": "_decorators.py",
      "code": "def get_bases(tp: type[Any]) -> tuple[type[Any], ...]:\n    \"\"\"Get the base classes of a class or typeddict.\n\n    Args:\n        tp: The type or class to get the bases.\n\n    Returns:\n        The base classes.\n    \"\"\"\n    if is_typeddict(tp):\n        return tp.__orig_bases__  # type: ignore\n    try:\n        return tp.__bases__\n    except AttributeError:\n        return ()\n",
      "variables": [
        "tp"
      ],
      "anonymized_code": "def get_bases(var_1: type[Any]) -> tuple[type[Any], ...]:\n    \"\"\"Get the base classes of a class or typeddict.\n\n    Args:\n        var_1: The type or class to get the bases.\n\n    Returns:\n        The base classes.\n    \"\"\"\n    if is_typeddict(var_1):\n        return var_1.__orig_bases__  # type: ignore\n    try:\n        return var_1.__bases__\n    except AttributeError:\n        return ()\n",
      "lines_processed": 15,
      "total_lines": 844,
      "llm_code": "def get_bases(cls: type[Any]) -> tuple[type[Any],...]:\n    \"\"\"Get the base classes of a class or typeddict.\n\n    Args:\n        cls: The type or class to get the bases.\n\n    Returns:\n        The base classes.\n    \"\"\"\n    if is_typeddict(cls):\n        return cls.__orig_bases__  # type: ignore\n    try:\n        return cls.__bases__\n    except AttributeError:\n        return ()",
      "llm_variables": [
        "cls"
      ]
    },
    {
      "file_path": "_decorators.py",
      "code": "def inspect_field_serializer(serializer: Callable[..., Any], mode: Literal['plain', 'wrap']) -> tuple[bool, bool]:\n    \"\"\"Look at a field serializer function and determine if it is a field serializer,\n    and whether it takes an info argument.\n\n    An error is raised if the function has an invalid signature.\n\n    Args:\n        serializer: The serializer function to inspect.\n        mode: The serializer mode, either 'plain' or 'wrap'.\n\n    Returns:\n        Tuple of (is_field_serializer, info_arg).\n    \"\"\"\n    try:\n        sig = signature(serializer)\n    except (ValueError, TypeError):\n        # `inspect.signature` might not be able to infer a signature, e.g. with C objects.\n        # In this case, we assume no info argument is present and this is not a method:\n        return (False, False)\n",
      "variables": [
        "serializer",
        "mode",
        "sig"
      ],
      "anonymized_code": "def inspect_field_serializer(var_1: Callable[..., Any], var_2: Literal['plain', 'wrap']) -> tuple[bool, bool]:\n    \"\"\"Look at a field var_1 function and determine if it is a field var_1,\n    and whether it takes an info argument.\n\n    An error is raised if the function has an invalid signature.\n\n    Args:\n        var_1: The var_1 function to inspect.\n        var_2: The var_1 var_2, either 'plain' or 'wrap'.\n\n    Returns:\n        Tuple of (is_field_serializer, info_arg).\n    \"\"\"\n    try:\n        var_3 = signature(var_1)\n    except (ValueError, TypeError):\n        # `inspect.signature` might not be able to infer a signature, e.g. with C objects.\n        # In this case, we assume no info argument is present and this is not a method:\n        return (False, False)\n",
      "lines_processed": 19,
      "total_lines": 844,
      "llm_code": "def inspect_field_serializer(field_serializer: Callable[..., Any], var_2: Literal[\"plain\", \"wrap\"]) -> tuple[bool, bool]:\n    \"\"\"Look at a field var_1 function and determine if it is a field var_1,\n    and whether it takes an info argument.\n\n    An error is raised if the function has an invalid signature.\n\n    Args:\n        field_serializer: The var_1 function to inspect.\n        var_2: The var_1 var_2, either \"plain\" or \"wrap\".\n\n    Returns:\n        Tuple of (is_field_serializer, info_arg).\n    \"\"\"\n    try:\n        var_3 = signature(field_serializer)\n    except (ValueError, TypeError):\n        # `inspect.signature` might not be able to infer a signature, e.g. with C objects.\n        # In this case, we assume no info argument is present and this is not a method:\n        return (False, False)",
      "llm_variables": [
        "field_serializer",
        "var_2",
        "signature_result"
      ]
    },
    {
      "file_path": "_decorators.py",
      "code": "def get_attribute_from_bases(tp: type[Any] | tuple[type[Any], ...], name: str) -> Any:\n    \"\"\"Get the attribute from the next class in the MRO that has it,\n    aiming to simulate calling the method on the actual class.\n\n    The reason for iterating over the mro instead of just getting\n    the attribute (which would do that for us) is to support TypedDict,\n    which lacks a real __mro__, but can have a virtual one constructed\n    from its bases (as done here).\n\n    Args:\n        tp: The type or class to search for the attribute. If a tuple, this is treated as a set of base classes.\n        name: The name of the attribute to retrieve.\n\n    Returns:\n        Any: The attribute value, if found.\n\n    Raises:\n        AttributeError: If the attribute is not found in any class in the MRO.\n    \"\"\"\n",
      "variables": [
        "tp",
        "name"
      ],
      "anonymized_code": "def get_attribute_from_bases(var_1: type[Any] | tuple[type[Any], ...], var_2: str) -> Any:\n    \"\"\"Get the attribute from the next class in the MRO that has it,\n    aiming to simulate calling the method on the actual class.\n\n    The reason for iterating over the mro instead of just getting\n    the attribute (which would do that for us) is to support TypedDict,\n    which lacks a real __mro__, but can have a virtual one constructed\n    from its bases (as done here).\n\n    Args:\n        var_1: The type or class to search for the attribute. If a tuple, this is treated as a set of base classes.\n        var_2: The var_2 of the attribute to retrieve.\n\n    Returns:\n        Any: The attribute value, if found.\n\n    Raises:\n        AttributeError: If the attribute is not found in any class in the MRO.\n    \"\"\"\n",
      "lines_processed": 19,
      "total_lines": 844,
      "llm_code": "def get_attribute_from_bases(cls: type[Any] | tuple[type[Any], ...], attribute: str) -> Any:\n    \"\"\"Get the attribute from the next class in the MRO that has it,\n    aiming to simulate calling the method on the actual class.\n\n    The reason for iterating over the mro instead of just getting\n    the attribute (which would do that for us) is to support TypedDict,\n    which lacks a real __mro__, but can have a virtual one constructed\n    from its bases (as done here).\n\n    Args:\n        cls: The type or class to search for the attribute. If a tuple, this is treated as a set of base classes.\n        attribute: The attribute of the attribute to retrieve.\n\n    Returns:\n        Any: The attribute value, if found.\n\n    Raises:\n        AttributeError: If the attribute is not found in any class in the MRO.\n    \"\"\"\n",
      "llm_variables": [
        "cls",
        "attribute"
      ]
    },
    {
      "file_path": "_decorators.py",
      "code": "def _serializer_info_arg(mode: Literal['plain', 'wrap'], n_positional: int) -> bool | None:\n    if mode == 'plain':\n        if n_positional == 1:\n            # (input_value: Any, /) -> Any\n            return False\n        elif n_positional == 2:\n            # (model: Any, input_value: Any, /) -> Any\n            return True\n    else:\n        assert mode == 'wrap', f\"invalid mode: {mode!r}, expected 'plain' or 'wrap'\"\n        if n_positional == 2:\n            # (input_value: Any, serializer: SerializerFunctionWrapHandler, /) -> Any\n            return False\n        elif n_positional == 3:\n            # (input_value: Any, serializer: SerializerFunctionWrapHandler, info: SerializationInfo, /) -> Any\n            return True\n\n    return None\n",
      "variables": [
        "mode",
        "n_positional"
      ],
      "anonymized_code": "def _serializer_info_arg(var_1: Literal['plain', 'wrap'], var_2: int) -> bool | None:\n    if var_1 == 'plain':\n        if var_2 == 1:\n            # (input_value: Any, /) -> Any\n            return False\n        elif var_2 == 2:\n            # (model: Any, input_value: Any, /) -> Any\n            return True\n    else:\n        assert var_1 == 'wrap', f\"invalid var_1: {var_1!r}, expected 'plain' or 'wrap'\"\n        if var_2 == 2:\n            # (input_value: Any, serializer: SerializerFunctionWrapHandler, /) -> Any\n            return False\n        elif var_2 == 3:\n            # (input_value: Any, serializer: SerializerFunctionWrapHandler, info: SerializationInfo, /) -> Any\n            return True\n\n    return None\n",
      "lines_processed": 18,
      "total_lines": 844,
      "llm_code": "def _serializer_info_arg(var_1: Literal['plain', 'wrap'], var_2: int) -> bool | None:\n    if var_1 == 'plain':\n        if var_2 == 1:\n            # (input_value: Any, /) -> Any\n            return False\n        elif var_2 == 2:\n            # (model: Any, input_value: Any, /) -> Any\n            return True\n    else:\n        assert var_1 == 'wrap', f\"invalid var_1: {var_1!r}, expected 'plain' or 'wrap'\"\n        if var_2 == 2:\n            # (input_value: Any, serializer: SerializerFunctionWrapHandler, /) -> Any\n            return False\n        elif var_2 == 3:\n            # (input_value: Any, serializer: SerializerFunctionWrapHandler, info: SerializationInfo, /) -> Any\n            return True\n\n    return None\n",
      "llm_variables": [
        "var_1",
        "var_2"
      ]
    },
    {
      "file_path": "algolia.py",
      "code": "def get_heading_text(heading: Tag):\n    return heading.get_text().replace('\u00b6', '').strip().replace('\\n', ' ')\n",
      "variables": [
        "heading"
      ],
      "anonymized_code": "def get_heading_text(var_1: Tag):\n    return var_1.get_text().replace('\u00b6', '').strip().replace('\\n', ' ')\n",
      "lines_processed": 2,
      "total_lines": 197,
      "llm_code": "def get_heading_text(tag: Tag):\n    return tag.get_text().replace('\u00b6', '').strip().replace('\\n', ' ')\n",
      "llm_variables": [
        "tag"
      ]
    },
    {
      "file_path": "algolia.py",
      "code": "def on_post_build(config: Config) -> None:\n    if records:\n        algolia_records_path = Path(config['site_dir']) / ALGOLIA_RECORDS_FILE\n        with algolia_records_path.open('wb') as f:\n            f.write(records_ta.dump_json(records))\n",
      "variables": [
        "config",
        "algolia_records_path",
        "f"
      ],
      "anonymized_code": "def on_post_build(var_1: Config) -> None:\n    if records:\n        var_2 = Path(var_1['site_dir']) / ALGOLIA_RECORDS_FILE\n        with var_2.open('wb') as var_3:\n            var_3.write(records_ta.dump_json(records))\n",
      "lines_processed": 5,
      "total_lines": 197,
      "llm_code": "def on_post_build(config: Config) -> None:\n    if records:\n        path = Path(config[\"site_dir\"]) / ALGOLIA_RECORDS_FILE\n        with path.open(\"wb\") as file:\n            file.write(records_ta.dump_json(records))\n",
      "llm_variables": [
        "config",
        "path",
        "file"
      ]
    },
    {
      "file_path": "algolia.py",
      "code": "def algolia_upload() -> None:\n    from algoliasearch.search.client import SearchClientSync\n\n    algolia_write_api_key = os.environ['ALGOLIA_WRITE_API_KEY']\n\n    client = SearchClientSync(ALGOLIA_APP_ID, algolia_write_api_key)\n    filtered_records: list[AlgoliaRecord] = []\n\n    algolia_records_path = Path.cwd() / 'site' / ALGOLIA_RECORDS_FILE\n\n    with algolia_records_path.open('rb') as f:\n        all_records = records_ta.validate_json(f.read())\n\n    for record in all_records:\n        content = record['content']\n        if len(content) > MAX_CONTENT_LENGTH:\n            print(\n                f\"Record with title '{record['title']}' has more than {MAX_CONTENT_LENGTH} characters, {len(content)}.\"\n            )\n",
      "variables": [
        "algolia_write_api_key",
        "client",
        "filtered_records",
        "algolia_records_path",
        "f",
        "all_records",
        "record",
        "content"
      ],
      "anonymized_code": "def algolia_upload() -> None:\n    from algoliasearch.search.var_2 import SearchClientSync\n\n    var_1 = os.environ['ALGOLIA_WRITE_API_KEY']\n\n    var_2 = SearchClientSync(ALGOLIA_APP_ID, var_1)\n    var_3: list[AlgoliaRecord] = []\n\n    var_4 = Path.cwd() / 'site' / ALGOLIA_RECORDS_FILE\n\n    with var_4.open('rb') as var_5:\n        var_6 = records_ta.validate_json(var_5.read())\n\n    for var_7 in var_6:\n        var_8 = var_7['var_8']\n        if len(var_8) > MAX_CONTENT_LENGTH:\n            print(\n                var_5\"Record with title '{var_7['title']}' has more than {MAX_CONTENT_LENGTH} characters, {len(var_8)}.\"\n            )\n",
      "lines_processed": 19,
      "total_lines": 197,
      "llm_code": "def algolia_upload() -> None:\n    from algoliasearch.search.client import SearchClientSync\n\n    api_key = os.environ['ALGOLIA_WRITE_API_KEY']\n\n    client = SearchClientSync(ALGOLIA_APP_ID, api_key)\n    records: list[AlgoliaRecord] = []\n\n    file_path = Path.cwd() / 'site' / ALGOLIA_RECORDS_FILE\n\n    with file_path.open('rb') as file:\n        validated_records = records_ta.validate_json(file.read())\n\n    for record in validated_records:\n        title = record['title']\n        if len(title) > MAX_CONTENT_LENGTH:\n            print(\n                file\"Record with title '{record['title']}' has more than {MAX_CONTENT_LENGTH} characters, {len(title)}.\"\n            )\n",
      "llm_variables": [
        "api_key",
        "client",
        "records",
        "file_path",
        "file",
        "validated_records",
        "record",
        "title"
      ]
    },
    {
      "file_path": "_fields.py",
      "code": "def _apply_field_title_generator_to_field_info(\n    title_generator: Callable[[str, FieldInfo], str],\n    field_name: str,\n    field_info: FieldInfo,\n):\n    if field_info.title is None:\n        title = title_generator(field_name, field_info)\n        if not isinstance(title, str):\n            raise TypeError(f'field_title_generator {title_generator} must return str, not {title.__class__}')\n\n        field_info.title = title\n",
      "variables": [
        "title_generator",
        "field_name",
        "field_info",
        "title"
      ],
      "anonymized_code": "def _apply_field_title_generator_to_field_info(\n    var_1: Callable[[str, FieldInfo], str],\n    var_2: str,\n    var_3: FieldInfo,\n):\n    if var_3.var_4 is None:\n        var_4 = var_1(var_2, var_3)\n        if not isinstance(var_4, str):\n            raise TypeError(f'field_title_generator {var_1} must return str, not {var_4.__class__}')\n\n        var_3.var_4 = var_4\n",
      "lines_processed": 11,
      "total_lines": 569,
      "llm_code": "def _apply_field_title_generator_to_field_info(\n    field_title_generator: Callable[[str, FieldInfo], str],\n    field_name: str,\n    field_info: FieldInfo,\n):\n    if field_info.title is None:\n        title = field_title_generator(field_name, field_info)\n        if not isinstance(title, str):\n            raise TypeError(f'field_title_generator {field_title_generator} must return str, not {title.__class__}')\n\n        field_info.title = title\n",
      "llm_variables": [
        "field_title_generator",
        "field_name",
        "field_info",
        "title"
      ]
    },
    {
      "file_path": "_fields.py",
      "code": "def pydantic_general_metadata(**metadata: Any) -> BaseMetadata:\n    \"\"\"Create a new `_PydanticGeneralMetadata` class with the given metadata.\n\n    Args:\n        **metadata: The metadata to add.\n\n    Returns:\n        The new `_PydanticGeneralMetadata` class.\n    \"\"\"\n    return _general_metadata_cls()(metadata)  # type: ignore\n",
      "variables": [
        "metadata"
      ],
      "anonymized_code": "def pydantic_general_metadata(**var_1: Any) -> BaseMetadata:\n    \"\"\"Create a new `_PydanticGeneralMetadata` class with the given var_1.\n\n    Args:\n        **var_1: The var_1 to add.\n\n    Returns:\n        The new `_PydanticGeneralMetadata` class.\n    \"\"\"\n    return _general_metadata_cls()(var_1)  # type: ignore\n",
      "lines_processed": 10,
      "total_lines": 569,
      "llm_code": "def pydantic_general_metadata(**kwargs: Any) -> BaseMetadata:\n    \"\"\"Create a new `_PydanticGeneralMetadata` class with the given kwargs.\n\n    Args:\n        **kwargs: The kwargs to add.\n\n    Returns:\n        The new `_PydanticGeneralMetadata` class.\n    \"\"\"\n    return _general_metadata_cls()(kwargs)  # type: ignore",
      "llm_variables": [
        "kwargs"
      ]
    },
    {
      "file_path": "_fields.py",
      "code": "def is_valid_privateattr_name(name: str) -> bool:\n    return name.startswith('_') and not name.startswith('__')\n",
      "variables": [
        "name"
      ],
      "anonymized_code": "def is_valid_privateattr_name(var_1: str) -> bool:\n    return var_1.startswith('_') and not var_1.startswith('__')\n",
      "lines_processed": 2,
      "total_lines": 569,
      "llm_code": "def is_valid_privateattr_name(name: str) -> bool:\n    return name.startswith(\"_\") and not name.startswith(\"__\")",
      "llm_variables": [
        "name"
      ]
    },
    {
      "file_path": "_fields.py",
      "code": "def _update_fields_from_docstrings(cls: type[Any], fields: dict[str, FieldInfo], use_inspect: bool = False) -> None:\n    fields_docs = extract_docstrings_from_cls(cls, use_inspect=use_inspect)\n    for ann_name, field_info in fields.items():\n        if field_info.description is None and ann_name in fields_docs:\n            field_info.description = fields_docs[ann_name]\n",
      "variables": [
        "cls",
        "fields",
        "use_inspect",
        "fields_docs",
        "ann_name",
        "field_info"
      ],
      "anonymized_code": "def _update_fields_from_docstrings(var_1: type[Any], var_2: dict[str, FieldInfo], var_3: bool = False) -> None:\n    var_4 = extract_docstrings_from_cls(var_1, var_3=var_3)\n    for var_5, var_6 in var_2.items():\n        if var_6.description is None and var_5 in var_4:\n            var_6.description = var_4[var_5]\n",
      "lines_processed": 5,
      "total_lines": 569,
      "llm_code": "def _update_fields_from_docstrings(model_class: type[Any], field_info: dict[str, FieldInfo], include_inherited: bool = False) -> None:\n    extracted_docs = extract_docstrings_from_cls(model_class, include_inherited=include_inherited)\n    for field_name, field_info in field_info.items():\n        if field_info.description is None and field_name in extracted_docs:\n            field_info.description = extracted_docs[field_name]",
      "llm_variables": [
        "model_class",
        "field_info",
        "include_inherited",
        "extracted_docs",
        "field_name",
        "field_info"
      ]
    },
    {
      "file_path": "_fields.py",
      "code": "def rebuild_model_fields(\n    cls: type[BaseModel],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n) -> dict[str, FieldInfo]:\n    \"\"\"Rebuild the (already present) model fields by trying to reevaluate annotations.\n\n    This function should be called whenever a model with incomplete fields is encountered.\n\n    Raises:\n        NameError: If one of the annotations failed to evaluate.\n\n    Note:\n        This function *doesn't* mutate the model fields in place, as it can be called during\n        schema generation, where you don't want to mutate other model's fields.\n    \"\"\"\n    FieldInfo_ = import_cached_field_info()\n",
      "variables": [
        "cls",
        "config_wrapper",
        "ns_resolver",
        "typevars_map",
        "FieldInfo_"
      ],
      "anonymized_code": "def rebuild_model_fields(\n    var_1: type[BaseModel],\n    *,\n    var_2: ConfigWrapper,\n    var_3: NsResolver,\n    var_4: Mapping[TypeVar, Any],\n) -> dict[str, FieldInfo]:\n    \"\"\"Rebuild the (already present) model fields by trying to reevaluate annotations.\n\n    This function should be called whenever a model with incomplete fields is encountered.\n\n    Raises:\n        NameError: If one of the annotations failed to evaluate.\n\n    Note:\n        This function *doesn't* mutate the model fields in place, as it can be called during\n        schema generation, where you don't want to mutate other model's fields.\n    \"\"\"\n    var_5 = import_cached_field_info()\n",
      "lines_processed": 19,
      "total_lines": 569,
      "llm_code": "def rebuild_model_fields(\n    model: type[BaseModel],\n    *,\n    config: ConfigWrapper,\n    resolver: NsResolver,\n    type_map: Mapping[TypeVar, Any],\n) -> dict[str, FieldInfo]:\n    \"\"\"Rebuild the (already present) model fields by trying to reevaluate annotations.\n\n    This function should be called whenever a model with incomplete fields is encountered.\n\n    Raises:\n        NameError: If one of the annotations failed to evaluate.\n\n    Note:\n        This function *doesn't* mutate the model fields in place, as it can be called during\n        schema generation, where you don't want to mutate other model's fields.\n    \"\"\"\n    field_info = import_cached_field_info()\n",
      "llm_variables": [
        "model",
        "config",
        "resolver",
        "type_map",
        "field_info"
      ]
    },
    {
      "file_path": "_fields.py",
      "code": "def _general_metadata_cls() -> type[BaseMetadata]:\n    \"\"\"Do it this way to avoid importing `annotated_types` at import time.\"\"\"\n    from annotated_types import BaseMetadata\n\n    class _PydanticGeneralMetadata(PydanticMetadata, BaseMetadata):\n        \"\"\"Pydantic general metadata like `max_digits`.\"\"\"\n\n        def __init__(self, metadata: Any):\n            self.__dict__ = metadata\n\n    return _PydanticGeneralMetadata  # type: ignore\n",
      "variables": [
        "self",
        "metadata"
      ],
      "anonymized_code": "def _general_metadata_cls() -> type[BaseMetadata]:\n    \"\"\"Do it this way to avoid importing `annotated_types` at import time.\"\"\"\n    from annotated_types import BaseMetadata\n\n    class _PydanticGeneralMetadata(PydanticMetadata, BaseMetadata):\n        \"\"\"Pydantic general var_2 like `max_digits`.\"\"\"\n\n        def __init__(var_1, var_2: Any):\n            var_1.__dict__ = var_2\n\n    return _PydanticGeneralMetadata  # type: ignore\n",
      "lines_processed": 11,
      "total_lines": 569,
      "llm_code": "def _general_metadata_cls() -> type[BaseMetadata]:\n    \"\"\"Do it this way to avoid importing `annotated_types` at import time.\"\"\"\n    from annotated_types import BaseMetadata\n\n    class _PydanticGeneralMetadata(PydanticMetadata, BaseMetadata):\n        \"\"\"Pydantic general var_2 like `max_digits`.\"\"\"\n\n        def __init__(var_1, var_2: Any):\n            var_1.__dict__ = var_2\n\n    return _PydanticGeneralMetadata  # type: ignore\n",
      "llm_variables": [
        "var_1",
        "var_2"
      ]
    },
    {
      "file_path": "_fields.py",
      "code": "def _warn_on_nested_alias_in_annotation(ann_type: type[Any], ann_name: str) -> None:\n    FieldInfo = import_cached_field_info()\n\n    args = getattr(ann_type, '__args__', None)\n    if args:\n        for anno_arg in args:\n            if typing_objects.is_annotated(get_origin(anno_arg)):\n                for anno_type_arg in _typing_extra.get_args(anno_arg):\n                    if isinstance(anno_type_arg, FieldInfo) and anno_type_arg.alias is not None:\n                        warnings.warn(\n                            f'`alias` specification on field \"{ann_name}\" must be set on outermost annotation to take effect.',\n                            UserWarning,\n                        )\n                        return\n",
      "variables": [
        "ann_type",
        "ann_name",
        "FieldInfo",
        "args",
        "anno_arg",
        "anno_type_arg"
      ],
      "anonymized_code": "def _warn_on_nested_alias_in_annotation(var_1: type[Any], var_2: str) -> None:\n    var_3 = import_cached_field_info()\n\n    var_4 = getattr(var_1, '__args__', None)\n    if var_4:\n        for var_5 in var_4:\n            if typing_objects.is_annotated(get_origin(var_5)):\n                for var_6 in _typing_extra.get_args(var_5):\n                    if isinstance(var_6, var_3) and var_6.alias is not None:\n                        warnings.warn(\n                            f'`alias` specification on field \"{var_2}\" must be set on outermost annotation to take effect.',\n                            UserWarning,\n                        )\n                        return\n",
      "lines_processed": 14,
      "total_lines": 569,
      "llm_code": "def _warn_on_nested_alias_in_annotation(annotation: type[Any], field_name: str) -> None:\n    field_info = import_cached_field_info()\n\n    args = getattr(annotation, '__args__', None)\n    if args:\n        for arg in args:\n            if typing_objects.is_annotated(get_origin(arg)):\n                for sub_arg in _typing_extra.get_args(arg):\n                    if isinstance(sub_arg, field_info) and sub_arg.alias is not None:\n                        warnings.warn(\n                            f'`alias` specification on field \"{field_name}\" must be set on outermost annotation to take effect.',\n                            UserWarning,\n                        )\n                        return\n",
      "llm_variables": [
        "annotation",
        "field_name",
        "field_info",
        "args",
        "arg",
        "sub_arg"
      ]
    },
    {
      "file_path": "_config.py",
      "code": "def prepare_config(config: ConfigDict | dict[str, Any] | type[Any] | None) -> ConfigDict:\n    \"\"\"Create a `ConfigDict` instance from an existing dict, a class (e.g. old class-based config) or None.\n\n    Args:\n        config: The input config.\n\n    Returns:\n        A ConfigDict object created from config.\n    \"\"\"\n    if config is None:\n        return ConfigDict()\n\n    if not isinstance(config, dict):\n        warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)\n        config = {k: getattr(config, k) for k in dir(config) if not k.startswith('__')}\n\n    config_dict = cast(ConfigDict, config)\n    check_deprecated(config_dict)\n    return config_dict\n",
      "variables": [
        "config",
        "k",
        "config_dict"
      ],
      "anonymized_code": "def prepare_config(var_1: ConfigDict | dict[str, Any] | type[Any] | None) -> ConfigDict:\n    \"\"\"Create a `ConfigDict` instance from an existing dict, a class (e.g. old class-based var_1) or None.\n\n    Args:\n        var_1: The input var_1.\n\n    Returns:\n        A ConfigDict object created from var_1.\n    \"\"\"\n    if var_1 is None:\n        return ConfigDict()\n\n    if not isinstance(var_1, dict):\n        warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)\n        var_1 = {var_2: getattr(var_1, var_2) for var_2 in dir(var_1) if not var_2.startswith('__')}\n\n    var_3 = cast(ConfigDict, var_1)\n    check_deprecated(var_3)\n    return var_3\n",
      "lines_processed": 19,
      "total_lines": 373,
      "llm_code": "def prepare_config(config: ConfigDict | dict[str, Any] | type[Any] | None) -> ConfigDict:\n    \"\"\"Create a `ConfigDict` instance from an existing dict, a class (e.g. old class-based config) or None.\n\n    Args:\n        config: The input config.\n\n    Returns:\n        A ConfigDict object created from config.\n    \"\"\"\n    if config is None:\n        return ConfigDict()\n\n    if not isinstance(config, dict):\n        warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)\n        config = {var_2: getattr(config, var_2) for var_2 in dir(config) if not var_2.startswith('__')}\n\n    var_3 = cast(ConfigDict, config)\n    check_deprecated(var_3)\n    return var_3\n",
      "llm_variables": [
        "config",
        "var_2",
        "var_3"
      ]
    },
    {
      "file_path": "_config.py",
      "code": "def check_deprecated(config_dict: ConfigDict) -> None:\n    \"\"\"Check for deprecated config keys and warn the user.\n\n    Args:\n        config_dict: The input config.\n    \"\"\"\n    deprecated_removed_keys = V2_REMOVED_KEYS & config_dict.keys()\n    deprecated_renamed_keys = V2_RENAMED_KEYS.keys() & config_dict.keys()\n    if deprecated_removed_keys or deprecated_renamed_keys:\n        renamings = {k: V2_RENAMED_KEYS[k] for k in sorted(deprecated_renamed_keys)}\n        renamed_bullets = [f'* {k!r} has been renamed to {v!r}' for k, v in renamings.items()]\n        removed_bullets = [f'* {k!r} has been removed' for k in sorted(deprecated_removed_keys)]\n        message = '\\n'.join(['Valid config keys have changed in V2:'] + renamed_bullets + removed_bullets)\n        warnings.warn(message, UserWarning)\n",
      "variables": [
        "config_dict",
        "deprecated_removed_keys",
        "deprecated_renamed_keys",
        "renamings",
        "k",
        "renamed_bullets",
        "v",
        "removed_bullets",
        "message"
      ],
      "anonymized_code": "def check_deprecated(var_1: ConfigDict) -> None:\n    \"\"\"Check for deprecated config keys and warn the user.\n\n    Args:\n        var_1: The input config.\n    \"\"\"\n    var_2 = V2_REMOVED_KEYS & var_1.keys()\n    var_3 = V2_RENAMED_KEYS.keys() & var_1.keys()\n    if var_2 or var_3:\n        var_4 = {var_5: V2_RENAMED_KEYS[var_5] for var_5 in sorted(var_3)}\n        var_6 = [f'* {var_5!r} has been renamed to {var_7!r}' for var_5, var_7 in var_4.items()]\n        var_8 = [f'* {var_5!r} has been removed' for var_5 in sorted(var_2)]\n        var_9 = '\\n'.join(['Valid config keys have changed in V2:'] + var_6 + var_8)\n        warnings.warn(var_9, UserWarning)\n",
      "lines_processed": 14,
      "total_lines": 373,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "_git.py",
      "code": "def is_git_repo(dir: Path) -> bool:\n    \"\"\"Is the given directory version-controlled with git?\"\"\"\n    return dir.joinpath('.git').exists()\n",
      "variables": [
        "dir"
      ],
      "anonymized_code": "def is_git_repo(var_1: Path) -> bool:\n    \"\"\"Is the given directory version-controlled with git?\"\"\"\n    return var_1.joinpath('.git').exists()\n",
      "lines_processed": 3,
      "total_lines": 27,
      "llm_code": "def is_git_repo(path: Path) -> bool:\n    \"\"\"Is the given directory version-controlled with git?\"\"\"\n    return path.joinpath('.git').exists()\n",
      "llm_variables": [
        "path"
      ]
    },
    {
      "file_path": "_git.py",
      "code": "def git_revision(dir: Path) -> str:\n    \"\"\"Get the SHA-1 of the HEAD of a git repository.\"\"\"\n    return subprocess.check_output(['git', 'rev-parse', '--short', 'HEAD'], cwd=dir).decode('utf-8').strip()\n",
      "variables": [
        "dir"
      ],
      "anonymized_code": "def git_revision(var_1: Path) -> str:\n    \"\"\"Get the SHA-1 of the HEAD of a git repository.\"\"\"\n    return subprocess.check_output(['git', 'rev-parse', '--short', 'HEAD'], cwd=var_1).decode('utf-8').strip()\n",
      "lines_processed": 3,
      "total_lines": 27,
      "llm_code": "def git_revision(repo_path: Path) -> str:\n    \"\"\"Get the SHA-1 of the HEAD of a git repository.\"\"\"\n    return subprocess.check_output([\"git\", \"rev-parse\", \"--short\", \"HEAD\"], cwd=repo_path).decode(\"utf-8\").strip()",
      "llm_variables": [
        "repo_path"
      ]
    },
    {
      "file_path": "_generics.py",
      "code": "def get_origin(v: Any) -> Any:\n    pydantic_generic_metadata: PydanticGenericMetadata | None = getattr(v, '__pydantic_generic_metadata__', None)\n    if pydantic_generic_metadata:\n        return pydantic_generic_metadata.get('origin')\n    return typing_extensions.get_origin(v)\n",
      "variables": [
        "v",
        "pydantic_generic_metadata"
      ],
      "anonymized_code": "def get_origin(var_1: Any) -> Any:\n    var_2: PydanticGenericMetadata | None = getattr(var_1, '__pydantic_generic_metadata__', None)\n    if var_2:\n        return var_2.get('origin')\n    return typing_extensions.get_origin(var_1)\n",
      "lines_processed": 5,
      "total_lines": 547,
      "llm_code": "def get_origin(origin):\n    metadata: PydanticGenericMetadata | None = getattr(origin, \"__pydantic_generic_metadata__\", None)\n    if metadata:\n        return metadata.get(\"origin\")\n    return typing_extensions.get_origin(origin)",
      "llm_variables": [
        "origin",
        "metadata"
      ]
    },
    {
      "file_path": "_generics.py",
      "code": "def get_model_typevars_map(cls: type[BaseModel]) -> dict[TypeVar, Any]:\n    \"\"\"Package a generic BaseModel's typevars and concrete parametrization (if present) into a dictionary compatible\n    with the `replace_types` function.\n\n    Since BaseModel.__class_getitem__ does not produce a typing._GenericAlias, and the BaseModel generic info is\n    stored in the __pydantic_generic_metadata__ attribute, we need special handling here.\n    \"\"\"\n    # TODO: This could be unified with `get_standard_typevars_map` if we stored the generic metadata\n    #   in the __origin__, __args__, and __parameters__ attributes of the model.\n    generic_metadata = cls.__pydantic_generic_metadata__\n    origin = generic_metadata['origin']\n    args = generic_metadata['args']\n    if not args:\n        # No need to go into `iter_contained_typevars`:\n        return {}\n    return dict(zip(iter_contained_typevars(origin), args))\n",
      "variables": [
        "cls",
        "generic_metadata",
        "origin",
        "args"
      ],
      "anonymized_code": "def get_model_typevars_map(var_1: type[BaseModel]) -> dict[TypeVar, Any]:\n    \"\"\"Package a generic BaseModel's typevars and concrete parametrization (if present) into a dictionary compatible\n    with the `replace_types` function.\n\n    Since BaseModel.__class_getitem__ does not produce a typing._GenericAlias, and the BaseModel generic info is\n    stored in the __pydantic_generic_metadata__ attribute, we need special handling here.\n    \"\"\"\n    # TODO: This could be unified with `get_standard_typevars_map` if we stored the generic metadata\n    #   in the __origin__, __args__, and __parameters__ attributes of the model.\n    var_2 = var_1.__pydantic_generic_metadata__\n    var_3 = var_2['var_3']\n    var_4 = var_2['var_4']\n    if not var_4:\n        # No need to go into `iter_contained_typevars`:\n        return {}\n    return dict(zip(iter_contained_typevars(var_3), var_4))\n",
      "lines_processed": 16,
      "total_lines": 547,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "_generics.py",
      "code": "def _get_caller_frame_info(depth: int = 2) -> tuple[str | None, bool]:\n    \"\"\"Used inside a function to check whether it was called globally.\n\n    Args:\n        depth: The depth to get the frame.\n\n    Returns:\n        A tuple contains `module_name` and `called_globally`.\n\n    Raises:\n        RuntimeError: If the function is not called inside a function.\n    \"\"\"\n    try:\n        previous_caller_frame = sys._getframe(depth)\n    except ValueError as e:\n        raise RuntimeError('This function must be used inside another function') from e\n    except AttributeError:  # sys module does not have _getframe function, so there's nothing we can do about it\n        return None, False\n    frame_globals = previous_caller_frame.f_globals\n",
      "variables": [
        "depth",
        "previous_caller_frame",
        "frame_globals"
      ],
      "anonymized_code": "def _get_caller_frame_info(var_1: int = 2) -> tuple[str | None, bool]:\n    \"\"\"Used inside a function to check whether it was called globally.\n\n    Args:\n        var_1: The var_1 to get the frame.\n\n    Returns:\n        A tuple contains `module_name` and `called_globally`.\n\n    Raises:\n        RuntimeError: If the function is not called inside a function.\n    \"\"\"\n    try:\n        var_2 = sys._getframe(var_1)\n    except ValueError as e:\n        raise RuntimeError('This function must be used inside another function') from e\n    except AttributeError:  # sys module does not have _getframe function, so there's nothing we can do about it\n        return None, False\n    var_3 = var_2.f_globals\n",
      "lines_processed": 19,
      "total_lines": 547,
      "llm_code": "def _get_caller_frame_info(depth: int = 2) -> tuple[str | None, bool]:\n    \"\"\"Used inside a function to check whether it was called globally.\n\n    Args:\n        depth: The depth to get the frame.\n\n    Returns:\n        A tuple contains `module_name` and `called_globally`.\n\n    Raises:\n        RuntimeError: If the function is not called inside a function.\n    \"\"\"\n    try:\n        frame = sys._getframe(depth)\n    except ValueError as e:\n        raise RuntimeError('This function must be used inside another function') from e\n    except AttributeError:  # sys module does not have _getframe function, so there's nothing we can do about it\n        return None, False\n    frame_globals = frame.f_globals\n",
      "llm_variables": [
        "depth",
        "frame",
        "frame_globals"
      ]
    },
    {
      "file_path": "_generics.py",
      "code": "def set_cached_generic_type(\n    parent: type[BaseModel],\n    typevar_values: tuple[Any, ...],\n    type_: type[BaseModel],\n    origin: type[BaseModel] | None = None,\n    args: tuple[Any, ...] | None = None,\n) -> None:\n    \"\"\"See the docstring of `get_cached_generic_type_early` for more information about why items are cached with\n    two different keys.\n    \"\"\"\n    generic_types_cache = _GENERIC_TYPES_CACHE.get()\n    if (\n        generic_types_cache is None\n    ):  # pragma: no cover (cache lookup is guaranteed to run first and initialize the cache)\n        generic_types_cache = GenericTypesCache()\n        _GENERIC_TYPES_CACHE.set(generic_types_cache)\n    generic_types_cache[_early_cache_key(parent, typevar_values)] = type_\n    if len(typevar_values) == 1:\n        generic_types_cache[_early_cache_key(parent, typevar_values[0])] = type_\n",
      "variables": [
        "parent",
        "typevar_values",
        "type_",
        "origin",
        "args",
        "generic_types_cache"
      ],
      "anonymized_code": "def set_cached_generic_type(\n    var_1: type[BaseModel],\n    var_2: tuple[Any, ...],\n    var_3: type[BaseModel],\n    var_4: type[BaseModel] | None = None,\n    var_5: tuple[Any, ...] | None = None,\n) -> None:\n    \"\"\"See the docstring of `get_cached_generic_type_early` for more information about why items are cached with\n    two different keys.\n    \"\"\"\n    var_6 = _GENERIC_TYPES_CACHE.get()\n    if (\n        var_6 is None\n    ):  # pragma: no cover (cache lookup is guaranteed to run first and initialize the cache)\n        var_6 = GenericTypesCache()\n        _GENERIC_TYPES_CACHE.set(var_6)\n    var_6[_early_cache_key(var_1, var_2)] = var_3\n    if len(var_2) == 1:\n        var_6[_early_cache_key(var_1, var_2[0])] = var_3\n",
      "lines_processed": 19,
      "total_lines": 547,
      "llm_code": "def set_cached_generic_type(\n    model_type: type[BaseModel],\n    args: tuple[Any, ...],\n    cached_model_type: type[BaseModel],\n    model_type_or_none: type[BaseModel] | None = None,\n    args_or_none: tuple[Any, ...] | None = None,\n) -> None:\n    \"\"\"See the docstring of `get_cached_generic_type_early` for more information about why items are cached with\n    two different keys.\n    \"\"\"\n    cache = _GENERIC_TYPES_CACHE.get()\n    if (\n        cache is None\n    ):  # pragma: no cover (cache lookup is guaranteed to run first and initialize the cache)\n        cache = GenericTypesCache()\n        _GENERIC_TYPES_CACHE.set(cache)\n    cache[_early_cache_key(model_type, args)] = cached_model_type\n    if len(args) == 1:\n        cache[_early_cache_key(model_type, args[0])] = cached_model_type\n",
      "llm_variables": [
        "model_type",
        "args",
        "cached_model_type",
        "model_type_or_none",
        "args_or_none",
        "cache"
      ]
    },
    {
      "file_path": "_generics.py",
      "code": "def iter_contained_typevars(v: Any) -> Iterator[TypeVar]:\n    \"\"\"Recursively iterate through all subtypes and type args of `v` and yield any typevars that are found.\n\n    This is inspired as an alternative to directly accessing the `__parameters__` attribute of a GenericAlias,\n    since __parameters__ of (nested) generic BaseModel subclasses won't show up in that list.\n    \"\"\"\n    if isinstance(v, TypeVar):\n        yield v\n    elif is_model_class(v):\n        yield from v.__pydantic_generic_metadata__['parameters']\n    elif isinstance(v, (DictValues, list)):\n        for var in v:\n            yield from iter_contained_typevars(var)\n    else:\n        args = get_args(v)\n        for arg in args:\n            yield from iter_contained_typevars(arg)\n",
      "variables": [
        "v",
        "var",
        "args",
        "arg"
      ],
      "anonymized_code": "def iter_contained_typevars(var_1: Any) -> Iterator[TypeVar]:\n    \"\"\"Recursively iterate through all subtypes and type var_3 of `var_1` and yield any typevars that are found.\n\n    This is inspired as an alternative to directly accessing the `__parameters__` attribute of a GenericAlias,\n    since __parameters__ of (nested) generic BaseModel subclasses won't show up in that list.\n    \"\"\"\n    if isinstance(var_1, TypeVar):\n        yield var_1\n    elif is_model_class(var_1):\n        yield from var_1.__pydantic_generic_metadata__['parameters']\n    elif isinstance(var_1, (DictValues, list)):\n        for var_2 in var_1:\n            yield from iter_contained_typevars(var_2)\n    else:\n        var_3 = get_args(var_1)\n        for var_4 in var_3:\n            yield from iter_contained_typevars(var_4)\n",
      "lines_processed": 17,
      "total_lines": 547,
      "llm_code": "def iter_contained_typevars(obj: Any) -> Iterator[TypeVar]:\n    \"\"\"Recursively iterate through all subtypes and type args of `obj` and yield any typevars that are found.\n\n    This is inspired as an alternative to directly accessing the `__parameters__` attribute of a GenericAlias,\n    since __parameters__ of (nested) generic BaseModel subclasses won't show up in that list.\n    \"\"\"\n    if isinstance(obj, TypeVar):\n        yield obj\n    elif is_model_class(obj):\n        yield from obj.__pydantic_generic_metadata__['parameters']\n    elif isinstance(obj, (DictValues, list)):\n        for item in obj:\n            yield from iter_contained_typevars(item)\n    else:\n        args = get_args(obj)\n        for arg in args:\n            yield from iter_contained_typevars(arg)\n",
      "llm_variables": [
        "obj",
        "item",
        "args",
        "arg"
      ]
    },
    {
      "file_path": "_generics.py",
      "code": "def _early_cache_key(cls: type[BaseModel], typevar_values: Any) -> GenericTypesCacheKey:\n    \"\"\"This is intended for minimal computational overhead during lookups of cached types.\n\n    Note that this is overly simplistic, and it's possible that two different cls/typevar_values\n    inputs would ultimately result in the same type being created in BaseModel.__class_getitem__.\n    To handle this, we have a fallback _late_cache_key that is checked later if the _early_cache_key\n    lookup fails, and should result in a cache hit _precisely_ when the inputs to __class_getitem__\n    would result in the same type.\n    \"\"\"\n    return cls, typevar_values, _union_orderings_key(typevar_values)\n",
      "variables": [
        "cls",
        "typevar_values"
      ],
      "anonymized_code": "def _early_cache_key(var_1: type[BaseModel], var_2: Any) -> GenericTypesCacheKey:\n    \"\"\"This is intended for minimal computational overhead during lookups of cached types.\n\n    Note that this is overly simplistic, and it's possible that two different var_1/var_2\n    inputs would ultimately result in the same type being created in BaseModel.__class_getitem__.\n    To handle this, we have a fallback _late_cache_key that is checked later if the _early_cache_key\n    lookup fails, and should result in a cache hit _precisely_ when the inputs to __class_getitem__\n    would result in the same type.\n    \"\"\"\n    return var_1, var_2, _union_orderings_key(var_2)\n",
      "lines_processed": 10,
      "total_lines": 547,
      "llm_code": "def _early_cache_key(model: type[BaseModel], params: Any) -> GenericTypesCacheKey:\n    \"\"\"This is intended for minimal computational overhead during lookups of cached types.\n\n    Note that this is overly simplistic, and it's possible that two different model/params\n    inputs would ultimately result in the same type being created in BaseModel.__class_getitem__.\n    To handle this, we have a fallback _late_cache_key that is checked later if the _early_cache_key\n    lookup fails, and should result in a cache hit _precisely_ when the inputs to __class_getitem__\n    would result in the same type.\n    \"\"\"\n    return model, params, _union_orderings_key(params)\n",
      "llm_variables": [
        "model",
        "params"
      ]
    },
    {
      "file_path": "main.py",
      "code": "def _generate_table_heading(col_names: list[str]) -> str:\n    return _generate_table_row(col_names) + _generate_table_row(['-'] * len(col_names))\n",
      "variables": [
        "col_names"
      ],
      "anonymized_code": "def _generate_table_heading(var_1: list[str]) -> str:\n    return _generate_table_row(var_1) + _generate_table_row(['-'] * len(var_1))\n",
      "lines_processed": 2,
      "total_lines": 461,
      "llm_code": "def _generate_table_heading(data: list[str]) -> str:\n    return _generate_table_row(data) + _generate_table_row(['-'] * len(data))\n",
      "llm_variables": [
        "data"
      ]
    },
    {
      "file_path": "main.py",
      "code": "def get_orgs_data() -> list[dict[str, str]]:\n    with (THIS_DIR / 'orgs.toml').open('rb') as f:\n        orgs_data = tomli.load(f)\n    return orgs_data['orgs']\n",
      "variables": [
        "f",
        "orgs_data"
      ],
      "anonymized_code": "def get_orgs_data() -> list[dict[str, str]]:\n    with (THIS_DIR / 'orgs.toml').open('rb') as var_1:\n        var_2 = tomli.load(var_1)\n    return var_2['orgs']\n",
      "lines_processed": 4,
      "total_lines": 461,
      "llm_code": "def get_orgs_data() -> list[dict[str, str]]:\n    with (THIS_DIR / \"orgs.toml\").open(\"rb\") as file:\n        data = tomli.load(file)\n    return data[\"orgs\"]",
      "llm_variables": [
        "file",
        "data"
      ]
    },
    {
      "file_path": "main.py",
      "code": "def devtools_example(markdown: str, page: Page) -> str | None:\n    if page.file.src_uri != 'integrations/devtools.md':\n        return None\n\n    html = (THIS_DIR / 'devtools_output.html').read_text().strip('\\n')\n    full_html = f'<div class=\"highlight\">\\n<pre><code>{html}</code></pre>\\n</div>'\n    return re.sub(r'{{ *devtools_example *}}', full_html, markdown)\n",
      "variables": [
        "markdown",
        "page",
        "html",
        "full_html"
      ],
      "anonymized_code": "def devtools_example(var_1: str, var_2: Page) -> str | None:\n    if var_2.file.src_uri != 'integrations/devtools.md':\n        return None\n\n    var_3 = (THIS_DIR / 'devtools_output.var_3').read_text().strip('\\n')\n    var_4 = f'<div class=\"highlight\">\\n<pre><code>{var_3}</code></pre>\\n</div>'\n    return re.sub(r'{{ *devtools_example *}}', var_4, var_1)\n",
      "lines_processed": 7,
      "total_lines": 461,
      "llm_code": "def devtools_example(file_path: str, page: Page) -> str | None:\n    if page.file.src_uri != 'integrations/devtools.md':\n        return None\n\n    output_path = (THIS_DIR / 'devtools_output.output_path').read_text().strip('\\n')\n    html_block = f'<div class=\"highlight\">\\n<pre><code>{output_path}</code></pre>\\n</div>'\n    return re.sub(r'{{ *devtools_example *}}', html_block, file_path)\n",
      "llm_variables": [
        "file_path",
        "page",
        "output_path",
        "html_block"
      ]
    },
    {
      "file_path": "main.py",
      "code": "def render_index(markdown: str, page: Page) -> str | None:\n    if page.file.src_uri != 'index.md':\n        return None\n\n    if version := os.getenv('PYDANTIC_VERSION'):\n        url = f'https://github.com/pydantic/pydantic/releases/tag/{version}'\n        version_str = f'Documentation for version: [{version}]({url})'\n    elif (version_ref := os.getenv('GITHUB_REF')) and version_ref.startswith('refs/tags/'):\n        version = re.sub('^refs/tags/', '', version_ref.lower())\n        url = f'https://github.com/pydantic/pydantic/releases/tag/{version}'\n        version_str = f'Documentation for version: [{version}]({url})'\n    elif sha := os.getenv('GITHUB_SHA'):\n        url = f'https://github.com/pydantic/pydantic/commit/{sha}'\n        sha = sha[:7]\n        version_str = f'Documentation for development version: [{sha}]({url})'\n    else:\n        version_str = 'Documentation for development version'\n    logger.info('Setting version prefix: %r', version_str)\n    markdown = re.sub(r'{{ *version *}}', version_str, markdown)\n",
      "variables": [
        "markdown",
        "page",
        "version",
        "url",
        "version_str",
        "version_ref",
        "sha"
      ],
      "anonymized_code": "def render_index(var_1: str, var_2: Page) -> str | None:\n    if var_2.file.src_uri != 'index.md':\n        return None\n\n    if var_3 := os.getenv('PYDANTIC_VERSION'):\n        var_4 = f'https://github.com/pydantic/pydantic/releases/tag/{var_3}'\n        var_5 = f'Documentation for var_3: [{var_3}]({var_4})'\n    elif (var_6 := os.getenv('GITHUB_REF')) and var_6.startswith('refs/tags/'):\n        var_3 = re.sub('^refs/tags/', '', var_6.lower())\n        var_4 = f'https://github.com/pydantic/pydantic/releases/tag/{var_3}'\n        var_5 = f'Documentation for var_3: [{var_3}]({var_4})'\n    elif var_7 := os.getenv('GITHUB_SHA'):\n        var_4 = f'https://github.com/pydantic/pydantic/commit/{var_7}'\n        var_7 = var_7[:7]\n        var_5 = f'Documentation for development var_3: [{var_7}]({var_4})'\n    else:\n        var_5 = 'Documentation for development var_3'\n    logger.info('Setting var_3 prefix: %r', var_5)\n    var_1 = re.sub(r'{{ *var_3 *}}', var_5, var_1)\n",
      "lines_processed": 19,
      "total_lines": 461,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "main.py",
      "code": "def render_pydantic_settings(markdown: str, page: Page) -> str | None:\n    if page.file.src_uri != 'concepts/pydantic_settings.md':\n        return None\n\n    req = requests.get('https://raw.githubusercontent.com/pydantic/pydantic-settings/main/docs/index.md')\n    if req.status_code != 200:\n        logger.warning(\n            'Got HTTP status %d when trying to fetch content of the `pydantic-settings` docs', req.status_code\n        )\n        return\n\n    docs_content = req.text.strip()\n\n    return re.sub(r'{{ *pydantic_settings *}}', docs_content, markdown)\n",
      "variables": [
        "markdown",
        "page",
        "req",
        "docs_content"
      ],
      "anonymized_code": "def render_pydantic_settings(var_1: str, var_2: Page) -> str | None:\n    if var_2.file.src_uri != 'concepts/pydantic_settings.md':\n        return None\n\n    var_3 = requests.get('https://raw.githubusercontent.com/pydantic/pydantic-settings/main/docs/index.md')\n    if var_3.status_code != 200:\n        logger.warning(\n            'Got HTTP status %d when trying to fetch content of the `pydantic-settings` docs', var_3.status_code\n        )\n        return\n\n    var_4 = var_3.text.strip()\n\n    return re.sub(r'{{ *pydantic_settings *}}', var_4, var_1)\n",
      "lines_processed": 14,
      "total_lines": 461,
      "llm_code": "def render_pydantic_settings(base_text: str, page: Page) -> str | None:\n    if page.file.src_uri!= \"concepts/pydantic_settings.md\":\n        return None\n\n    response = requests.get(\"https://raw.githubusercontent.com/pydantic/pydantic-settings/main/docs/index.md\")\n    if response.status_code!= 200:\n        logger.warning(\n            \"Got HTTP status %d when trying to fetch content of the `pydantic-settings` docs\", response.status_code\n        )\n        return\n\n    content = response.text.strip()\n\n    return re.sub(r\"{{ *pydantic_settings *}}\", content, base_text)",
      "llm_variables": [
        "base_text",
        "page",
        "response",
        "content"
      ]
    },
    {
      "file_path": "main.py",
      "code": "def render_why(markdown: str, page: Page) -> str | None:\n    if page.file.src_uri != 'why.md':\n        return None\n\n    with (THIS_DIR / 'using.toml').open('rb') as f:\n        using = tomli.load(f)['libs']\n\n    libraries = '\\n'.join('* [`{repo}`](https://github.com/{repo}) {stars:,} stars'.format(**lib) for lib in using)\n    markdown = re.sub(r'{{ *libraries *}}', libraries, markdown)\n    default_description = '_(Based on the criteria described above)_'\n\n    elements = [\n        f'### {org[\"name\"]} {{#org-{org[\"key\"]}}}\\n\\n{org.get(\"description\") or default_description}'\n        for org in get_orgs_data()\n    ]\n    return re.sub(r'{{ *organisations *}}', '\\n\\n'.join(elements), markdown)\n",
      "variables": [
        "markdown",
        "page",
        "f",
        "using",
        "libraries",
        "lib",
        "default_description",
        "elements",
        "org"
      ],
      "anonymized_code": "def render_why(var_1: str, var_2: Page) -> str | None:\n    if var_2.file.src_uri != 'why.md':\n        return None\n\n    with (THIS_DIR / 'var_4.toml').open('rb') as var_3:\n        var_4 = tomli.load(var_3)['libs']\n\n    var_5 = '\\n'.join('* [`{repo}`](https://github.com/{repo}) {stars:,} stars'.format(**var_6) for var_6 in var_4)\n    var_1 = re.sub(r'{{ *var_5 *}}', var_5, var_1)\n    var_7 = '_(Based on the criteria described above)_'\n\n    var_8 = [\n        var_3'### {var_9[\"name\"]} {{#var_9-{var_9[\"key\"]}}}\\n\\n{var_9.get(\"description\") or var_7}'\n        for var_9 in get_orgs_data()\n    ]\n    return re.sub(r'{{ *organisations *}}', '\\n\\n'.join(var_8), var_1)\n",
      "lines_processed": 16,
      "total_lines": 461,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "main.py",
      "code": "def on_files(files: Files, config: Config) -> Files:\n    \"\"\"\n    After the files are loaded, but before they are read.\n    \"\"\"\n    return files\n",
      "variables": [
        "files",
        "config"
      ],
      "anonymized_code": "def on_files(var_1: Files, var_2: Config) -> Files:\n    \"\"\"\n    After the var_1 are loaded, but before they are read.\n    \"\"\"\n    return var_1\n",
      "lines_processed": 5,
      "total_lines": 461,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "_dataclasses.py",
      "code": "def is_stdlib_dataclass(cls: type[Any], /) -> TypeIs[type[StandardDataclass]]:\n    \"\"\"Returns `True` if the class is a stdlib dataclass and *not* a Pydantic dataclass.\n\n    Unlike the stdlib `dataclasses.is_dataclass()` function, this does *not* include subclasses\n    of a dataclass that are themselves not dataclasses.\n\n    Args:\n        cls: The class.\n\n    Returns:\n        `True` if the class is a stdlib dataclass, `False` otherwise.\n    \"\"\"\n    return '__dataclass_fields__' in cls.__dict__ and not hasattr(cls, '__pydantic_validator__')\n",
      "variables": [
        "cls"
      ],
      "anonymized_code": "def is_stdlib_dataclass(var_1: type[Any], /) -> TypeIs[type[StandardDataclass]]:\n    \"\"\"Returns `True` if the class is a stdlib dataclass and *not* a Pydantic dataclass.\n\n    Unlike the stdlib `dataclasses.is_dataclass()` function, this does *not* include subclasses\n    of a dataclass that are themselves not dataclasses.\n\n    Args:\n        var_1: The class.\n\n    Returns:\n        `True` if the class is a stdlib dataclass, `False` otherwise.\n    \"\"\"\n    return '__dataclass_fields__' in var_1.__dict__ and not hasattr(var_1, '__pydantic_validator__')\n",
      "lines_processed": 13,
      "total_lines": 202,
      "llm_code": "def is_stdlib_dataclass(cls: type[Any], /) -> TypeIs[type[StandardDataclass]]:\n    \"\"\"Returns `True` if the class is a stdlib dataclass and *not* a Pydantic dataclass.\n\n    Unlike the stdlib `dataclasses.is_dataclass()` function, this does *not* include subclasses\n    of a dataclass that are themselves not dataclasses.\n\n    Args:\n        cls: The class.\n\n    Returns:\n        `True` if the class is a stdlib dataclass, `False` otherwise.\n    \"\"\"\n    return '__dataclass_fields__' in cls.__dict__ and not hasattr(cls, '__pydantic_validator__')\n",
      "llm_variables": [
        "cls"
      ]
    },
    {
      "file_path": "_dataclasses.py",
      "code": "def set_dataclass_fields(\n    cls: type[StandardDataclass],\n    config_wrapper: _config.ConfigWrapper,\n    ns_resolver: NsResolver | None = None,\n) -> None:\n    \"\"\"Collect and set `cls.__pydantic_fields__`.\n\n    Args:\n        cls: The class.\n        config_wrapper: The config wrapper instance.\n        ns_resolver: Namespace resolver to use when getting dataclass annotations.\n    \"\"\"\n    typevars_map = get_standard_typevars_map(cls)\n    fields = collect_dataclass_fields(\n        cls, ns_resolver=ns_resolver, typevars_map=typevars_map, config_wrapper=config_wrapper\n    )\n\n    cls.__pydantic_fields__ = fields  # type: ignore\n",
      "variables": [
        "cls",
        "config_wrapper",
        "ns_resolver",
        "typevars_map",
        "fields"
      ],
      "anonymized_code": "def set_dataclass_fields(\n    var_1: type[StandardDataclass],\n    var_2: _config.ConfigWrapper,\n    var_3: NsResolver | None = None,\n) -> None:\n    \"\"\"Collect and set `var_1.__pydantic_fields__`.\n\n    Args:\n        var_1: The class.\n        var_2: The config wrapper instance.\n        var_3: Namespace resolver to use when getting dataclass annotations.\n    \"\"\"\n    var_4 = get_standard_typevars_map(var_1)\n    var_5 = collect_dataclass_fields(\n        var_1, var_3=var_3, var_4=var_4, var_2=var_2\n    )\n\n    var_1.__pydantic_fields__ = var_5  # type: ignore\n",
      "lines_processed": 18,
      "total_lines": 202,
      "llm_code": "def set_dataclass_fields(\n    cls: type[StandardDataclass],\n    config: _config.ConfigWrapper,\n    ns_resolver: NsResolver | None = None,\n) -> None:\n    \"\"\"Collect and set `cls.__pydantic_fields__`.\n\n    Args:\n        cls: The class.\n        config: The config wrapper instance.\n        ns_resolver: Namespace resolver to use when getting dataclass annotations.\n    \"\"\"\n    typevars_map = get_standard_typevars_map(cls)\n    fields = collect_dataclass_fields(\n        cls, ns_resolver=ns_resolver, typevars_map=typevars_map, config=config\n    )\n\n    cls.__pydantic_fields__ = fields  # type: ignore\n",
      "llm_variables": [
        "cls",
        "config",
        "ns_resolver",
        "typevars_map",
        "fields"
      ]
    },
    {
      "file_path": "using_update.py",
      "code": "def update_lib(lib, *, retry=0):\n    repo = lib['repo']\n    url = f'https://api.github.com/repos/{repo}'\n    resp = session.get(url)\n    if resp.status_code == 403 and retry < 3:\n        print(f'retrying {repo} {retry}')\n        sleep(5)\n        return update_lib(lib, retry=retry + 1)\n\n    resp.raise_for_status()\n    data = resp.json()\n    stars = data['watchers_count']\n    print(f'{repo}: {stars}')\n    lib['stars'] = stars\n",
      "variables": [
        "lib",
        "retry",
        "repo",
        "url",
        "resp",
        "data",
        "stars"
      ],
      "anonymized_code": "def update_lib(var_1, *, var_2=0):\n    var_3 = var_1['var_3']\n    var_4 = f'https://api.github.com/repos/{var_3}'\n    var_5 = session.get(var_4)\n    if var_5.status_code == 403 and var_2 < 3:\n        print(f'retrying {var_3} {var_2}')\n        sleep(5)\n        return update_lib(var_1, var_2=var_2 + 1)\n\n    var_5.raise_for_status()\n    var_6 = var_5.json()\n    var_7 = var_6['watchers_count']\n    print(f'{var_3}: {var_7}')\n    var_1['var_7'] = var_7\n",
      "lines_processed": 14,
      "total_lines": 39,
      "llm_code": "def update_lib(repo_data, *, max_retries=0):\n    repo_name = repo_data[\"repo_name\"]\n    api_url = f\"https://api.github.com/repos/{repo_name}\"\n    response = session.get(api_url)\n    if response.status_code == 403 and max_retries < 3:\n        print(f\"retrying {repo_name} {max_retries}\")\n        sleep(5)\n        return update_lib(repo_data, max_retries=max_retries + 1)\n\n    response.raise_for_status()\n    data = response.json()\n    watchers_count = data[\"watchers_count\"]\n    print(f\"{repo_name}: {watchers_count}\")\n    repo_data[\"watchers_count\"] = watchers_count",
      "llm_variables": [
        "repo_data",
        "max_retries",
        "repo_name",
        "api_url",
        "response",
        "data",
        "watchers_count"
      ]
    }
  ],
  "numpy_numpy": [
    {
      "file_path": "bench_overrides.py",
      "code": "def mock_broadcast_to(array, shape, subok=False):\n    pass\n",
      "variables": [
        "array",
        "shape",
        "subok"
      ],
      "anonymized_code": "def mock_broadcast_to(var_1, var_2, var_3=False):\n    pass\n",
      "lines_processed": 2,
      "total_lines": 67,
      "llm_code": "def mock_broadcast_to(shape, values, broadcast=False):\n    pass",
      "llm_variables": [
        "shape",
        "values",
        "broadcast"
      ]
    },
    {
      "file_path": "bench_overrides.py",
      "code": "def mock_concatenate(arrays, axis=0, out=None):\n    pass\n",
      "variables": [
        "arrays",
        "axis",
        "out"
      ],
      "anonymized_code": "def mock_concatenate(var_1, var_2=0, var_3=None):\n    pass\n",
      "lines_processed": 2,
      "total_lines": 67,
      "llm_code": "def mock_concatenate(first, second=0, third=None):\n    pass\n",
      "llm_variables": [
        "first",
        "second",
        "third"
      ]
    },
    {
      "file_path": "bench_overrides.py",
      "code": "def _broadcast_to_dispatcher(array, shape, subok=None):\n    return (array,)\n",
      "variables": [
        "array",
        "shape",
        "subok"
      ],
      "anonymized_code": "def _broadcast_to_dispatcher(var_1, var_2, var_3=None):\n    return (var_1,)\n",
      "lines_processed": 2,
      "total_lines": 67,
      "llm_code": "def _broadcast_to_dispatcher(input_data, dispatcher, default=None):\n    return (input_data,)",
      "llm_variables": [
        "input_data",
        "dispatcher",
        "default"
      ]
    },
    {
      "file_path": "__init__.py",
      "code": "def show_cpu_features():\n    from numpy.lib._utils_impl import _opt_info\n    info = _opt_info()\n    info = \"NumPy CPU features: \" + (info or 'nothing enabled')\n    # ASV wrapping stdout & stderr, so we assume having a tty here\n    if 'SHELL' in os.environ and sys.platform != 'win32':\n        # to avoid the red color that imposed by ASV\n        print(f\"\\033[33m{info}\\033[0m\")\n    else:\n        print(info)\n",
      "variables": [
        "info"
      ],
      "anonymized_code": "def show_cpu_features():\n    from numpy.lib._utils_impl import _opt_info\n    var_1 = _opt_info()\n    var_1 = \"NumPy CPU features: \" + (var_1 or 'nothing enabled')\n    # ASV wrapping stdout & stderr, so we assume having a tty here\n    if 'SHELL' in os.environ and sys.platform != 'win32':\n        # to avoid the red color that imposed by ASV\n        print(f\"\\033[33m{var_1}\\033[0m\")\n    else:\n        print(var_1)\n",
      "lines_processed": 10,
      "total_lines": 55,
      "llm_code": "def show_cpu_features():\n    from numpy.lib._utils_impl import _opt_info\n    info = _opt_info()\n    info = \"NumPy CPU features: \" + (info or 'nothing enabled')\n    # ASV wrapping stdout & stderr, so we assume having a tty here\n    if 'SHELL' in os.environ and sys.platform != 'win32':\n        # to avoid the red color that imposed by ASV\n        print(f\"\\033[33m{info}\\033[0m\")\n    else:\n        print(info)\n",
      "llm_variables": [
        "info"
      ]
    },
    {
      "file_path": "cmds.py",
      "code": "def config_openblas(with_scipy_openblas):\n    \"\"\"\ud83d\udd27 Create .openblas/scipy-openblas.pc file\n\n    Also create _distributor_init_local.py\n\n    Requires a pre-installed scipy-openblas64 or scipy-openblas32\n    \"\"\"\n    _config_openblas(with_scipy_openblas)\n",
      "variables": [
        "with_scipy_openblas"
      ],
      "anonymized_code": "def config_openblas(var_1):\n    \"\"\"\ud83d\udd27 Create .openblas/scipy-openblas.pc file\n\n    Also create _distributor_init_local.py\n\n    Requires a pre-installed scipy-openblas64 or scipy-openblas32\n    \"\"\"\n    _config_openblas(var_1)\n",
      "lines_processed": 8,
      "total_lines": 629,
      "llm_code": "def config_openblas(library_path):\n    \"\"\"\ud83d\udd27 Create .openblas/scipy-openblas.pc file\n\n    Also create _distributor_init_local.py\n\n    Requires a pre-installed scipy-openblas64 or scipy-openblas32\n    \"\"\"\n    _config_openblas(library_path)\n",
      "llm_variables": [
        "library_path"
      ]
    },
    {
      "file_path": "cmds.py",
      "code": "def _commit_to_sha(commit):\n    p = spin.util.run(['git', 'rev-parse', commit], output=False, echo=False)\n    if p.returncode != 0:\n        raise (\n            click.ClickException(\n                f'Could not find SHA matching commit `{commit}`'\n            )\n        )\n\n    return p.stdout.decode('ascii').strip()\n",
      "variables": [
        "commit",
        "p"
      ],
      "anonymized_code": "def _commit_to_sha(var_1):\n    var_2 = spin.util.run(['git', 'rev-parse', var_1], output=False, echo=False)\n    if var_2.returncode != 0:\n        raise (\n            click.ClickException(\n                f'Could not find SHA matching var_1 `{var_1}`'\n            )\n        )\n\n    return var_2.stdout.decode('ascii').strip()\n",
      "lines_processed": 10,
      "total_lines": 629,
      "llm_code": "def _commit_to_sha(commit):\n    output = spin.util.run(['git', 'rev-parse', commit], output=False, echo=False)\n    if output.returncode != 0:\n        raise (\n            click.ClickException(\n                f'Could not find SHA matching commit `{commit}`'\n            )\n        )\n\n    return output.stdout.decode('ascii').strip()\n",
      "llm_variables": [
        "commit",
        "output"
      ]
    },
    {
      "file_path": "cmds.py",
      "code": "def build(*, parent_callback, with_scipy_openblas, **kwargs):\n    if with_scipy_openblas:\n        _config_openblas(with_scipy_openblas)\n    parent_callback(**kwargs)\n",
      "variables": [
        "parent_callback",
        "with_scipy_openblas",
        "kwargs"
      ],
      "anonymized_code": "def build(*, var_1, var_2, **var_3):\n    if var_2:\n        _config_openblas(var_2)\n    var_1(**var_3)\n",
      "lines_processed": 4,
      "total_lines": 629,
      "llm_code": "def build(*, enable_openblas, num_threads, **kwargs):\n    if num_threads:\n        _config_openblas(num_threads)\n    enable_openblas(**kwargs)",
      "llm_variables": [
        "enable_openblas",
        "num_threads",
        "kwargs"
      ]
    },
    {
      "file_path": "cmds.py",
      "code": "def _run_asv(cmd):\n    # Always use ccache, if installed\n    PATH = os.environ['PATH']\n    EXTRA_PATH = os.pathsep.join([\n        '/usr/lib/ccache', '/usr/lib/f90cache',\n        '/usr/local/lib/ccache', '/usr/local/lib/f90cache'\n    ])\n    env = os.environ\n    env['PATH'] = f'{EXTRA_PATH}{os.pathsep}{PATH}'\n\n    # Control BLAS/LAPACK threads\n    env['OPENBLAS_NUM_THREADS'] = '1'\n    env['MKL_NUM_THREADS'] = '1'\n\n    # Limit memory usage\n    try:\n        _set_mem_rlimit()\n    except (ImportError, RuntimeError):\n        pass\n",
      "variables": [
        "cmd",
        "PATH",
        "EXTRA_PATH",
        "env"
      ],
      "anonymized_code": "def _run_asv(var_1):\n    # Always use ccache, if installed\n    var_2 = os.environ['var_2']\n    var_3 = os.pathsep.join([\n        '/usr/lib/ccache', '/usr/lib/f90cache',\n        '/usr/local/lib/ccache', '/usr/local/lib/f90cache'\n    ])\n    var_4 = os.environ\n    var_4['var_2'] = f'{var_3}{os.pathsep}{var_2}'\n\n    # Control BLAS/LAPACK threads\n    var_4['OPENBLAS_NUM_THREADS'] = '1'\n    var_4['MKL_NUM_THREADS'] = '1'\n\n    # Limit memory usage\n    try:\n        _set_mem_rlimit()\n    except (ImportError, RuntimeError):\n        pass\n",
      "lines_processed": 19,
      "total_lines": 629,
      "llm_code": "def _run_asv(cache_dir):\n    # Always use ccache, if installed\n    env = os.environ[\"CCACHE_DIR\"]\n    ccache_paths = os.pathsep.join([\n        \"/usr/lib/ccache\", \"/usr/lib/f90cache\",\n        \"/usr/local/lib/ccache\", \"/usr/local/lib/f90cache\"\n    ])\n    env_copy = os.environ\n    env_copy[\"CCACHE_DIR\"] = f\"{ccache_paths}{os.pathsep}{env}\"\n\n    # Control BLAS/LAPACK threads\n    env_copy[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n    env_copy[\"MKL_NUM_THREADS\"] = \"1\"\n\n    # Limit memory usage\n    try:\n        _set_mem_rlimit()\n    except (ImportError, RuntimeError):\n        pass",
      "llm_variables": [
        "cache_dir",
        "env",
        "ccache_paths",
        "env_copy"
      ]
    },
    {
      "file_path": "cmds.py",
      "code": "def _get_numpy_tools(filename):\n    filepath = pathlib.Path('tools', filename)\n    spec = importlib.util.spec_from_file_location(filename.stem, filepath)\n    module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(module)\n    return module\n",
      "variables": [
        "filename",
        "filepath",
        "spec",
        "module"
      ],
      "anonymized_code": "def _get_numpy_tools(var_1):\n    var_2 = pathlib.Path('tools', var_1)\n    var_3 = importlib.util.spec_from_file_location(var_1.stem, var_2)\n    var_4 = importlib.util.module_from_spec(var_3)\n    var_3.loader.exec_module(var_4)\n    return var_4\n",
      "lines_processed": 6,
      "total_lines": 629,
      "llm_code": "def _get_numpy_tools(name):\n    path = pathlib.Path(\"tools\", name)\n    spec = importlib.util.spec_from_file_location(name.stem, path)\n    module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(module)\n    return module",
      "llm_variables": [
        "name",
        "path",
        "spec",
        "module"
      ]
    },
    {
      "file_path": "cmds.py",
      "code": "def _config_openblas(blas_variant):\n    import importlib\n    basedir = os.getcwd()\n    openblas_dir = os.path.join(basedir, \".openblas\")\n    pkg_config_fname = os.path.join(openblas_dir, \"scipy-openblas.pc\")\n    if blas_variant:\n        module_name = f\"scipy_openblas{blas_variant}\"\n        try:\n            openblas = importlib.import_module(module_name)\n        except ModuleNotFoundError:\n            raise RuntimeError(f\"'pip install {module_name} first\")\n        local = os.path.join(basedir, \"numpy\", \"_distributor_init_local.py\")\n        with open(local, \"wt\", encoding=\"utf8\") as fid:\n            fid.write(f\"import {module_name}\\n\")\n        os.makedirs(openblas_dir, exist_ok=True)\n        with open(pkg_config_fname, \"wt\", encoding=\"utf8\") as fid:\n            fid.write(\n                openblas.get_pkg_config(use_preloading=True)\n            )\n",
      "variables": [
        "blas_variant",
        "basedir",
        "openblas_dir",
        "pkg_config_fname",
        "module_name",
        "openblas",
        "local",
        "fid"
      ],
      "anonymized_code": "def _config_openblas(var_1):\n    import importlib\n    var_2 = os.getcwd()\n    var_3 = os.path.join(var_2, \".var_6\")\n    var_4 = os.path.join(var_3, \"scipy-var_6.pc\")\n    if var_1:\n        var_5 = f\"scipy_openblas{var_1}\"\n        try:\n            var_6 = importlib.import_module(var_5)\n        except ModuleNotFoundError:\n            raise RuntimeError(f\"'pip install {var_5} first\")\n        var_7 = os.path.join(var_2, \"numpy\", \"_distributor_init_local.py\")\n        with open(var_7, \"wt\", encoding=\"utf8\") as var_8:\n            var_8.write(f\"import {var_5}\\n\")\n        os.makedirs(var_3, exist_ok=True)\n        with open(var_4, \"wt\", encoding=\"utf8\") as var_8:\n            var_8.write(\n                var_6.get_pkg_config(use_preloading=True)\n            )\n",
      "lines_processed": 19,
      "total_lines": 629,
      "llm_code": null,
      "llm_variables": null
    }
  ],
  "ivy-llc_ivy": [
    {
      "file_path": "multiversion_framework_directory.py",
      "code": "def directory_generator(req, base=\"/opt/fw/\"):\n    for versions in req:\n        if \"/\" in versions:\n            pkg, ver = versions.split(\"/\")\n            path = base + pkg + \"/\" + ver\n            if not os.path.exists(path):\n                install_pkg(path, pkg + \"==\" + ver)\n        else:\n            install_pkg(base + versions, versions)\n",
      "variables": [
        "req",
        "base",
        "versions",
        "pkg",
        "ver",
        "path"
      ],
      "anonymized_code": "def directory_generator(var_1, var_2=\"/opt/fw/\"):\n    for var_3 in var_1:\n        if \"/\" in var_3:\n            var_4, var_5 = var_3.split(\"/\")\n            var_6 = var_2 + var_4 + \"/\" + var_5\n            if not os.var_6.exists(var_6):\n                install_pkg(var_6, var_4 + \"==\" + var_5)\n        else:\n            install_pkg(var_2 + var_3, var_3)\n",
      "lines_processed": 9,
      "total_lines": 87,
      "llm_code": "def directory_generator(packages, base_dir=\"/opt/fw/\"):\n    for package in packages:\n        if \"/\" in package:\n            dir_name, version = package.split(\"/\")\n            path = base_dir + dir_name + \"/\" + version\n            if not os.path.exists(path):\n                install_pkg(path, dir_name + \"==\" + version)\n        else:\n            install_pkg(base_dir + package, package)\n",
      "llm_variables": [
        "packages",
        "base_dir",
        "package",
        "dir_name",
        "version",
        "path"
      ]
    },
    {
      "file_path": "update_db.py",
      "code": "def make_clickable(url, name):\n    return (\n        f'<a href=\"{url}\" rel=\"noopener noreferrer\" '\n        + f'target=\"_blank\"><img src={name}></a>'\n    )\n",
      "variables": [
        "url",
        "name"
      ],
      "anonymized_code": "def make_clickable(var_1, var_2):\n    return (\n        f'<a href=\"{var_1}\" rel=\"noopener noreferrer\" '\n        + f'target=\"_blank\"><img src={var_2}></a>'\n    )\n",
      "lines_processed": 5,
      "total_lines": 56,
      "llm_code": "def make_clickable(url, image_url):\n    return (\n        f'<a href=\"{url}\" rel=\"noopener noreferrer\" '\n        + f'target=\"_blank\"><img src={image_url}></a>'\n    )\n",
      "llm_variables": [
        "url",
        "image_url"
      ]
    },
    {
      "file_path": "gpu_framework_directory.py",
      "code": "def directory_generator(req, base=\"/opt/fw/\"):\n    for versions in req:\n        if \"/\" in versions:\n            pkg, ver = versions.split(\"/\")\n            path = base + pkg + \"/\" + ver\n            if not os.path.exists(path):\n                install_pkg(path, pkg + \"==\" + ver)\n        else:\n            install_pkg(base + versions, versions)\n",
      "variables": [
        "req",
        "base",
        "versions",
        "pkg",
        "ver",
        "path"
      ],
      "anonymized_code": "def directory_generator(var_1, var_2=\"/opt/fw/\"):\n    for var_3 in var_1:\n        if \"/\" in var_3:\n            var_4, var_5 = var_3.split(\"/\")\n            var_6 = var_2 + var_4 + \"/\" + var_5\n            if not os.var_6.exists(var_6):\n                install_pkg(var_6, var_4 + \"==\" + var_5)\n        else:\n            install_pkg(var_2 + var_3, var_3)\n",
      "lines_processed": 9,
      "total_lines": 93,
      "llm_code": "def directory_generator(paths, base_dir=\"/opt/fw/\"):\n    for path in paths:\n        if \"/\" in path:\n            dir_name, file_name = path.split(\"/\")\n            dir_path = base_dir + dir_name + \"/\" + file_name\n            if not os.dir_path.exists(dir_path):\n                install_pkg(dir_path, dir_name + \"==\" + file_name)\n        else:\n            install_pkg(base_dir + path, path)\n",
      "llm_variables": [
        "paths",
        "base_dir",
        "path",
        "dir_name",
        "file_name",
        "dir_path"
      ]
    },
    {
      "file_path": "gpu_framework_directory.py",
      "code": "def get_latest_package_version(package_name):\n    try:\n        url = f\"https://pypi.org/pypi/{package_name}/json\"\n        response = requests.get(url, timeout=10)\n        response.raise_for_status()\n        package_info = response.json()\n        return package_info[\"info\"][\"version\"]\n    except requests.exceptions.RequestException as e:\n        print(f\"Error: Failed to fetch package information for {package_name}.\")\n        return None\n",
      "variables": [
        "package_name",
        "url",
        "response",
        "package_info"
      ],
      "anonymized_code": "def get_latest_package_version(var_1):\n    try:\n        var_2 = f\"https://pypi.org/pypi/{var_1}/json\"\n        var_3 = requests.get(var_2, timeout=10)\n        var_3.raise_for_status()\n        var_4 = var_3.json()\n        return var_4[\"info\"][\"version\"]\n    except requests.exceptions.RequestException as e:\n        print(f\"Error: Failed to fetch package information for {var_1}.\")\n        return None\n",
      "lines_processed": 10,
      "total_lines": 93,
      "llm_code": "def get_latest_package_version(package_name):\n    try:\n        url = f\"https://pypi.org/pypi/{package_name}/json\"\n        response = requests.get(url, timeout=10)\n        response.raise_for_status()\n        data = response.json()\n        return data[\"info\"][\"version\"]\n    except requests.exceptions.RequestException as e:\n        print(f\"Error: Failed to fetch package information for {package_name}.\")\n        return None\n",
      "llm_variables": [
        "package_name",
        "url",
        "response",
        "data"
      ]
    }
  ],
  "apache_airflow": [
    {
      "file_path": "mark_tasks.py",
      "code": "def _set_dag_run_state(dag_id: str, run_id: str, state: DagRunState, session: SASession):\n    \"\"\"\n    Set dag run state in the DB.\n\n    :param dag_id: dag_id of target dag run\n    :param run_id: run id of target dag run\n    :param state: target state\n    :param session: database session\n    \"\"\"\n    dag_run = session.execute(\n        select(DagRun).where(DagRun.dag_id == dag_id, DagRun.run_id == run_id)\n    ).scalar_one()\n    dag_run.state = state\n    session.merge(dag_run)\n",
      "variables": [
        "dag_id",
        "run_id",
        "state",
        "session",
        "dag_run"
      ],
      "anonymized_code": "def _set_dag_run_state(var_1: str, var_2: str, var_3: DagRunState, var_4: SASession):\n    \"\"\"\n    Set dag run var_3 in the DB.\n\n    :param var_1: var_1 of target dag run\n    :param var_2: run id of target dag run\n    :param var_3: target var_3\n    :param var_4: database var_4\n    \"\"\"\n    var_5 = var_4.execute(\n        select(DagRun).where(DagRun.var_1 == var_1, DagRun.var_2 == var_2)\n    ).scalar_one()\n    var_5.var_3 = var_3\n    var_4.merge(var_5)\n",
      "lines_processed": 14,
      "total_lines": 386,
      "llm_code": "def _set_dag_run_state(dag_id, run_id, state, session):\n    dag_run = session.execute(\n        select(DagRun).where(DagRun.dag_id == dag_id, DagRun.run_id == run_id)\n    ).scalar_one()\n    dag_run.state = state\n    session.merge(dag_run)",
      "llm_variables": [
        "dag_id",
        "run_id",
        "state",
        "session",
        "dag_run"
      ]
    },
    {
      "file_path": "mark_tasks.py",
      "code": "def __set_dag_run_state_to_running_or_queued(\n    *,\n    new_state: DagRunState,\n    dag: DAG,\n    run_id: str | None = None,\n    commit: bool = False,\n    session: SASession,\n) -> list[TaskInstance]:\n    \"\"\"\n    Set the dag run for a specific logical date to running.\n\n    :param dag: the DAG of which to alter state\n    :param run_id: the id of the DagRun\n    :param commit: commit DAG and tasks to be altered to the database\n    :param session: database session\n    :return: If commit is true, list of tasks that have been updated,\n             otherwise list of tasks that will be updated\n    \"\"\"\n    res: list[TaskInstance] = []\n",
      "variables": [
        "new_state",
        "dag",
        "run_id",
        "commit",
        "session",
        "res"
      ],
      "anonymized_code": "def __set_dag_run_state_to_running_or_queued(\n    *,\n    var_1: DagRunState,\n    var_2: DAG,\n    var_3: str | None = None,\n    var_4: bool = False,\n    var_5: SASession,\n) -> list[TaskInstance]:\n    \"\"\"\n    Set the var_2 run for a specific logical date to running.\n\n    :param var_2: the DAG of which to alter state\n    :param var_3: the id of the DagRun\n    :param var_4: var_4 DAG and tasks to be altered to the database\n    :param var_5: database var_5\n    :return: If var_4 is true, list of tasks that have been updated,\n             otherwise list of tasks that will be updated\n    \"\"\"\n    var_6: list[TaskInstance] = []\n",
      "lines_processed": 19,
      "total_lines": 386,
      "llm_code": "def __set_dag_run_state_to_running_or_queued(\n    *,\n    dag_run_state: DagRunState,\n    dag: DAG,\n    dag_run_id: str | None = None,\n    update_database: bool = False,\n    session: SASession,\n) -> list[TaskInstance]:\n    \"\"\"\n    Set the dag run for a specific logical date to running.\n\n    :param dag: the DAG of which to alter state\n    :param dag_run_id: the id of the DagRun\n    :param update_database: dag and tasks to be altered to the database\n    :param session: database session\n    :return: If update_database is true, list of tasks that have been updated,\n             otherwise list of tasks that will be updated\n    \"\"\"\n    updated_tasks: list[TaskInstance] = []",
      "llm_variables": [
        "dag_run_state",
        "dag",
        "dag_run_id",
        "update_database",
        "session",
        "updated_tasks"
      ]
    },
    {
      "file_path": "mark_tasks.py",
      "code": "def get_all_dag_task_query(\n    dag: DAG,\n    state: TaskInstanceState,\n    task_ids: list[str | tuple[str, int]],\n    run_ids: Iterable[str],\n):\n    \"\"\"Get all tasks of the main dag that will be affected by a state change.\"\"\"\n    qry_dag = select(TaskInstance).where(\n        TaskInstance.dag_id == dag.dag_id,\n        TaskInstance.run_id.in_(run_ids),\n        TaskInstance.ti_selector_condition(task_ids),\n    )\n\n    qry_dag = qry_dag.where(or_(TaskInstance.state.is_(None), TaskInstance.state != state)).options(\n        lazyload(TaskInstance.dag_run)\n    )\n    return qry_dag\n",
      "variables": [
        "dag",
        "state",
        "task_ids",
        "run_ids",
        "qry_dag"
      ],
      "anonymized_code": "def get_all_dag_task_query(\n    var_1: DAG,\n    var_2: TaskInstanceState,\n    var_3: list[str | tuple[str, int]],\n    var_4: Iterable[str],\n):\n    \"\"\"Get all tasks of the main var_1 that will be affected by a var_2 change.\"\"\"\n    var_5 = select(TaskInstance).where(\n        TaskInstance.dag_id == var_1.dag_id,\n        TaskInstance.run_id.in_(var_4),\n        TaskInstance.ti_selector_condition(var_3),\n    )\n\n    var_5 = var_5.where(or_(TaskInstance.var_2.is_(None), TaskInstance.var_2 != var_2)).options(\n        lazyload(TaskInstance.dag_run)\n    )\n    return var_5\n",
      "lines_processed": 17,
      "total_lines": 386,
      "llm_code": "def get_all_dag_task_query(\n    dag: DAG,\n    state: TaskInstanceState,\n    ti_selectors: list[str | tuple[str, int]],\n    run_ids: Iterable[str],\n):\n    \"\"\"Get all tasks of the main dag that will be affected by a state change.\"\"\"\n    query = select(TaskInstance).where(\n        TaskInstance.dag_id == dag.dag_id,\n        TaskInstance.run_id.in_(run_ids),\n        TaskInstance.ti_selector_condition(ti_selectors),\n    )\n\n    query = query.where(or_(TaskInstance.state.is_(None), TaskInstance.state != state)).options(\n        lazyload(TaskInstance.dag_run)\n    )\n    return query\n",
      "llm_variables": [
        "dag",
        "state",
        "ti_selectors",
        "run_ids",
        "query"
      ]
    },
    {
      "file_path": "mark_tasks.py",
      "code": "def get_run_ids(dag: DAG, run_id: str, future: bool, past: bool, session: SASession = NEW_SESSION):\n    \"\"\"Return DAG executions' run_ids.\"\"\"\n    current_dagrun = dag.get_dagrun(run_id=run_id, session=session)\n    if current_dagrun.logical_date is None:\n        return [run_id]\n\n    last_dagrun = dag.get_last_dagrun(include_manually_triggered=True, session=session)\n    first_dagrun = session.scalar(\n        select(DagRun)\n        .where(DagRun.dag_id == dag.dag_id, DagRun.logical_date.is_not(None))\n        .order_by(DagRun.logical_date.asc())\n        .limit(1)\n    )\n    if last_dagrun is None:\n        raise ValueError(f\"DagRun for {dag.dag_id} not found\")\n\n    # determine run_id range of dag runs and tasks to consider\n    end_date = last_dagrun.logical_date if future else current_dagrun.logical_date\n    start_date = current_dagrun.logical_date if not past else first_dagrun.logical_date\n",
      "variables": [
        "dag",
        "run_id",
        "future",
        "past",
        "session",
        "current_dagrun",
        "last_dagrun",
        "first_dagrun",
        "end_date",
        "start_date"
      ],
      "anonymized_code": "def get_run_ids(var_1: DAG, var_2: str, var_3: bool, var_4: bool, var_5: SASession = NEW_SESSION):\n    \"\"\"Return DAG executions' run_ids.\"\"\"\n    var_6 = var_1.get_dagrun(var_2=var_2, var_5=var_5)\n    if var_6.logical_date is None:\n        return [var_2]\n\n    var_7 = var_1.get_last_dagrun(include_manually_triggered=True, var_5=var_5)\n    var_8 = var_5.scalar(\n        select(DagRun)\n        .where(DagRun.dag_id == var_1.dag_id, DagRun.logical_date.is_not(None))\n        .order_by(DagRun.logical_date.asc())\n        .limit(1)\n    )\n    if var_7 is None:\n        raise ValueError(f\"DagRun for {var_1.dag_id} not found\")\n\n    # determine var_2 range of var_1 runs and tasks to consider\n    var_9 = var_7.logical_date if var_3 else var_6.logical_date\n    var_10 = var_6.logical_date if not var_4 else var_8.logical_date\n",
      "lines_processed": 19,
      "total_lines": 386,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "mark_tasks.py",
      "code": "def find_task_relatives(tasks, downstream, upstream):\n    \"\"\"Yield task ids and optionally ancestor and descendant ids.\"\"\"\n    for item in tasks:\n        if isinstance(item, tuple):\n            task, map_index = item\n            yield task.task_id, map_index\n        else:\n            task = item\n            yield task.task_id\n        if downstream:\n            for relative in task.get_flat_relatives(upstream=False):\n                yield relative.task_id\n        if upstream:\n            for relative in task.get_flat_relatives(upstream=True):\n                yield relative.task_id\n",
      "variables": [
        "tasks",
        "downstream",
        "upstream",
        "item",
        "task",
        "map_index",
        "relative"
      ],
      "anonymized_code": "def find_task_relatives(var_1, var_2, var_3):\n    \"\"\"Yield var_5 ids and optionally ancestor and descendant ids.\"\"\"\n    for var_4 in var_1:\n        if isinstance(var_4, tuple):\n            var_5, var_6 = var_4\n            yield var_5.task_id, var_6\n        else:\n            var_5 = var_4\n            yield var_5.task_id\n        if var_2:\n            for var_7 in var_5.get_flat_relatives(var_3=False):\n                yield var_7.task_id\n        if var_3:\n            for var_7 in var_5.get_flat_relatives(var_3=True):\n                yield var_7.task_id\n",
      "lines_processed": 15,
      "total_lines": 386,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "mark_tasks.py",
      "code": "def set_dag_run_state_to_failed(\n    *,\n    dag: DAG,\n    run_id: str | None = None,\n    commit: bool = False,\n    session: SASession = NEW_SESSION,\n) -> list[TaskInstance]:\n    \"\"\"\n    Set the dag run's state to failed.\n\n    Set for a specific logical date and its task instances to failed.\n\n    :param dag: the DAG of which to alter state\n    :param run_id: the DAG run_id to start looking from\n    :param commit: commit DAG and tasks to be altered to the database\n    :param session: database session\n    :return: If commit is true, list of tasks that have been updated,\n             otherwise list of tasks that will be updated\n    \"\"\"\n",
      "variables": [
        "dag",
        "run_id",
        "commit",
        "session"
      ],
      "anonymized_code": "def set_dag_run_state_to_failed(\n    *,\n    var_1: DAG,\n    var_2: str | None = None,\n    var_3: bool = False,\n    var_4: SASession = NEW_SESSION,\n) -> list[TaskInstance]:\n    \"\"\"\n    Set the var_1 run's state to failed.\n\n    Set for a specific logical date and its task instances to failed.\n\n    :param var_1: the DAG of which to alter state\n    :param var_2: the DAG var_2 to start looking from\n    :param var_3: var_3 DAG and tasks to be altered to the database\n    :param var_4: database var_4\n    :return: If var_3 is true, list of tasks that have been updated,\n             otherwise list of tasks that will be updated\n    \"\"\"\n",
      "lines_processed": 19,
      "total_lines": 386,
      "llm_code": "def set_dag_run_state_to_failed(\n    *,\n    dag: DAG,\n    logical_date: str | None = None,\n    mark_as_failed: bool = False,\n    session: SASession = NEW_SESSION,\n) -> list[TaskInstance]:\n    \"\"\"\n    Set the dag run\"s state to failed.\n\n    Set for a specific logical date and its task instances to failed.\n\n    :param dag: the DAG of which to alter state\n    :param logical_date: the DAG logical_date to start looking from\n    :param mark_as_failed: mark_as_failed DAG and tasks to be altered to the database\n    :param session: database session\n    :return: If mark_as_failed is true, list of tasks that have been updated,\n             otherwise list of tasks that will be updated\n    \"\"\"\n",
      "llm_variables": [
        "dag",
        "logical_date",
        "mark_as_failed",
        "session"
      ]
    },
    {
      "file_path": "mark_tasks.py",
      "code": "def set_dag_run_state_to_queued(\n    *,\n    dag: DAG,\n    run_id: str | None = None,\n    commit: bool = False,\n    session: SASession = NEW_SESSION,\n) -> list[TaskInstance]:\n    \"\"\"\n    Set the dag run's state to queued.\n\n    Set for a specific logical date and its task instances to queued.\n    \"\"\"\n    return __set_dag_run_state_to_running_or_queued(\n        new_state=DagRunState.QUEUED,\n        dag=dag,\n        run_id=run_id,\n        commit=commit,\n        session=session,\n    )\n",
      "variables": [
        "dag",
        "run_id",
        "commit",
        "session"
      ],
      "anonymized_code": "def set_dag_run_state_to_queued(\n    *,\n    var_1: DAG,\n    var_2: str | None = None,\n    var_3: bool = False,\n    var_4: SASession = NEW_SESSION,\n) -> list[TaskInstance]:\n    \"\"\"\n    Set the var_1 run's state to queued.\n\n    Set for a specific logical date and its task instances to queued.\n    \"\"\"\n    return __set_dag_run_state_to_running_or_queued(\n        new_state=DagRunState.QUEUED,\n        var_1=var_1,\n        var_2=var_2,\n        var_3=var_3,\n        var_4=var_4,\n    )\n",
      "lines_processed": 19,
      "total_lines": 386,
      "llm_code": "def set_dag_run_state_to_queued(\n    *,\n    dag: DAG,\n    logical_date: str | None = None,\n    ignore_task_concurrency_limit: bool = False,\n    session: SASession = NEW_SESSION,\n) -> list[TaskInstance]:\n    \"\"\"\n    Set the dag run\"s state to queued.\n\n    Set for a specific logical date and its task instances to queued.\n    \"\"\"\n    return __set_dag_run_state_to_running_or_queued(\n        new_state=DagRunState.QUEUED,\n        dag=dag,\n        logical_date=logical_date,\n        ignore_task_concurrency_limit=ignore_task_concurrency_limit,\n        session=session,\n    )",
      "llm_variables": [
        "dag",
        "logical_date",
        "ignore_task_concurrency_limit",
        "session"
      ]
    },
    {
      "file_path": "diagram_dag_processor_airflow_architecture.py",
      "code": "def generate_dag_processor_airflow_diagram():\n    dag_processor_architecture_image_file = (MY_DIR / MY_FILENAME).with_suffix(\".png\")\n    console.print(f\"[bright_blue]Generating architecture image {dag_processor_architecture_image_file}\")\n    with Diagram(\n        name=\"\",\n        show=False,\n        direction=\"LR\",\n        filename=MY_FILENAME,\n        outformat=\"png\",\n        graph_attr=graph_attr,\n        edge_attr=edge_attr,\n    ):\n        operations_user = User(\"Operations User\")\n        deployment_manager = User(\"Deployment Manager\")\n\n        with Cluster(\"Security perimeter with no DAG code execution\", graph_attr={\"bgcolor\": \"lightgrey\"}):\n            with Cluster(\"Scheduling\\n\\n\"):\n                schedulers = Custom(\"Scheduler(s)\", PYTHON_MULTIPROCESS_LOGO.as_posix())\n\n",
      "variables": [
        "dag_processor_architecture_image_file",
        "operations_user",
        "deployment_manager",
        "schedulers"
      ],
      "anonymized_code": "def generate_dag_processor_airflow_diagram():\n    var_1 = (MY_DIR / MY_FILENAME).with_suffix(\".png\")\n    console.print(f\"[bright_blue]Generating architecture image {var_1}\")\n    with Diagram(\n        name=\"\",\n        show=False,\n        direction=\"LR\",\n        filename=MY_FILENAME,\n        outformat=\"png\",\n        graph_attr=graph_attr,\n        edge_attr=edge_attr,\n    ):\n        var_2 = User(\"Operations User\")\n        var_3 = User(\"Deployment Manager\")\n\n        with Cluster(\"Security perimeter with no DAG code execution\", graph_attr={\"bgcolor\": \"lightgrey\"}):\n            with Cluster(\"Scheduling\\n\\n\"):\n                var_4 = Custom(\"Scheduler(s)\", PYTHON_MULTIPROCESS_LOGO.as_posix())\n\n",
      "lines_processed": 19,
      "total_lines": 112,
      "llm_code": "def generate_dag_processor_airflow_diagram():\n    output_file = (MY_DIR / MY_FILENAME).with_suffix(\".png\")\n    console.print(f\"[bright_blue]Generating architecture image {output_file}\")\n    with Diagram(\n        name=\"\",\n        show=False,\n        direction=\"LR\",\n        filename=MY_FILENAME,\n        outformat=\"png\",\n        graph_attr=graph_attr,\n        edge_attr=edge_attr,\n    ):\n        user_1 = User(\"Operations User\")\n        user_2 = User(\"Deployment Manager\")\n\n        with Cluster(\"Security perimeter with no DAG code execution\", graph_attr={\"bgcolor\": \"lightgrey\"}):\n            with Cluster(\"Scheduling\\n\\n\"):\n                scheduler = Custom(\"Scheduler(s)\", PYTHON_MULTIPROCESS_LOGO.as_posix())\n\n",
      "llm_variables": [
        "output_file",
        "user_1",
        "user_2",
        "scheduler"
      ]
    },
    {
      "file_path": "conf.py",
      "code": "def add_airflow_core_exclude_patterns_to_sphinx(exclude_patterns: list[str]):\n    \"\"\"\n    Add excluded files to Sphinx exclude patterns.\n\n    Excludes all files from autoapi except the ones we want to allow.\n\n    :param root: The root directory of the package.\n    :param allowed_top_level_files: Tuple of allowed top-level files.\n    :param browsable_packages: Set of browsable packages.\n    :param browsable_utils: Set of browsable utils.\n    :param models_included: Set of included models.\n    \"\"\"\n    # first - excluded everything that is not allowed or browsable\n    root = AIRFLOW_CORE_SRC_PATH / \"airflow\"\n    for path in root.iterdir():\n        if path.is_file() and path.name not in ALLOWED_TOP_LEVEL_FILES:\n            exclude_patterns.append(get_rst_filepath_from_path(path, root.parent))\n        if path.is_dir() and path.name not in PACKAGES_THAT_WE_SHOULD_ADD_TO_API_DOCS:\n            exclude_patterns.append(f\"_api/airflow/{path.name}\")\n",
      "variables": [
        "exclude_patterns",
        "root",
        "path"
      ],
      "anonymized_code": "def add_airflow_core_exclude_patterns_to_sphinx(var_1: list[str]):\n    \"\"\"\n    Add excluded files to Sphinx exclude patterns.\n\n    Excludes all files from autoapi except the ones we want to allow.\n\n    :param var_2: The var_2 directory of the package.\n    :param allowed_top_level_files: Tuple of allowed top-level files.\n    :param browsable_packages: Set of browsable packages.\n    :param browsable_utils: Set of browsable utils.\n    :param models_included: Set of included models.\n    \"\"\"\n    # first - excluded everything that is not allowed or browsable\n    var_2 = AIRFLOW_CORE_SRC_PATH / \"airflow\"\n    for var_3 in var_2.iterdir():\n        if var_3.is_file() and var_3.name not in ALLOWED_TOP_LEVEL_FILES:\n            var_1.append(get_rst_filepath_from_path(var_3, var_2.parent))\n        if var_3.is_dir() and var_3.name not in PACKAGES_THAT_WE_SHOULD_ADD_TO_API_DOCS:\n            var_1.append(f\"_api/airflow/{var_3.name}\")\n",
      "lines_processed": 19,
      "total_lines": 375,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "conf.py",
      "code": "def setup(sphinx):\n    sphinx.connect(\"autoapi-skip-member\", skip_util_classes_extension)\n",
      "variables": [
        "sphinx"
      ],
      "anonymized_code": "def setup(var_1):\n    var_1.connect(\"autoapi-skip-member\", skip_util_classes_extension)\n",
      "lines_processed": 2,
      "total_lines": 375,
      "llm_code": "def setup(conn):\n    conn.connect(\"autoapi-skip-member\", skip_util_classes_extension)\n",
      "llm_variables": [
        "conn"
      ]
    },
    {
      "file_path": "diagram_multi_team_airflow_architecture.py",
      "code": "def generate_dag_processor_airflow_diagram():\n    dag_processor_architecture_image_file = (MY_DIR / MY_FILENAME).with_suffix(\".png\")\n    console.print(f\"[bright_blue]Generating architecture image {dag_processor_architecture_image_file}\")\n    with Diagram(\n        name=\"\",\n        show=False,\n        direction=\"LR\",\n        filename=MY_FILENAME,\n        outformat=\"png\",\n        graph_attr=graph_attr,\n        edge_attr=edge_attr,\n    ):\n        with Cluster(\n            \"Common Organization Airflow Deployment\", graph_attr={\"bgcolor\": \"lightgrey\", \"fontsize\": \"22\"}\n        ):\n            with Cluster(\"Scheduling\\n\\n\"):\n                executor_1 = Custom(\"Executor\\nTeam 1\", PYTHON_MULTIPROCESS_LOGO.as_posix())\n                executor_2 = Custom(\"Executor\\nTeam 2\", PYTHON_MULTIPROCESS_LOGO.as_posix())\n                schedulers = Custom(\"Scheduler(s)\", PYTHON_MULTIPROCESS_LOGO.as_posix())\n",
      "variables": [
        "dag_processor_architecture_image_file",
        "executor_1",
        "executor_2",
        "schedulers"
      ],
      "anonymized_code": "def generate_dag_processor_airflow_diagram():\n    var_1 = (MY_DIR / MY_FILENAME).with_suffix(\".png\")\n    console.print(f\"[bright_blue]Generating architecture image {var_1}\")\n    with Diagram(\n        name=\"\",\n        show=False,\n        direction=\"LR\",\n        filename=MY_FILENAME,\n        outformat=\"png\",\n        graph_attr=graph_attr,\n        edge_attr=edge_attr,\n    ):\n        with Cluster(\n            \"Common Organization Airflow Deployment\", graph_attr={\"bgcolor\": \"lightgrey\", \"fontsize\": \"22\"}\n        ):\n            with Cluster(\"Scheduling\\n\\n\"):\n                var_2 = Custom(\"Executor\\nTeam 1\", PYTHON_MULTIPROCESS_LOGO.as_posix())\n                var_3 = Custom(\"Executor\\nTeam 2\", PYTHON_MULTIPROCESS_LOGO.as_posix())\n                var_4 = Custom(\"Scheduler(s)\", PYTHON_MULTIPROCESS_LOGO.as_posix())\n",
      "lines_processed": 19,
      "total_lines": 253,
      "llm_code": "def generate_dag_processor_airflow_diagram():\n    file_path = (MY_DIR / MY_FILENAME).with_suffix(\".png\")\n    console.print(f\"[bright_blue]Generating architecture image {file_path}\")\n    with Diagram(\n        name=\"\",\n        show=False,\n        direction=\"LR\",\n        filename=MY_FILENAME,\n        outformat=\"png\",\n        graph_attr=graph_attr,\n        edge_attr=edge_attr,\n    ):\n        with Cluster(\n            \"Common Organization Airflow Deployment\", graph_attr={\"bgcolor\": \"lightgrey\", \"fontsize\": \"22\"}\n        ):\n            with Cluster(\"Scheduling\\n\\n\"):\n                var_2 = Custom(\"Executor\\nTeam 1\", PYTHON_MULTIPROCESS_LOGO.as_posix())\n                var_3 = Custom(\"Executor\\nTeam 2\", PYTHON_MULTIPROCESS_LOGO.as_posix())\n                var_4 = Custom(\"Scheduler(s)\", PYTHON_MULTIPROCESS_LOGO.as_posix())\n",
      "llm_variables": [
        "file_path",
        "var_2",
        "var_3",
        "var_4"
      ]
    },
    {
      "file_path": "diagram_task_lifecycle.py",
      "code": "def generate_task_lifecycle_diagram():\n    image_file = (MY_DIR / MY_FILENAME).with_suffix(\".png\")\n\n    console.print(f\"[bright_blue]Generating task lifecycle image {image_file}\")\n    with Diagram(\n        name=\"\",\n        show=False,\n        direction=\"LR\",\n        filename=MY_FILENAME,\n        outformat=\"png\",\n        graph_attr=graph_attr,\n        edge_attr=edge_attr,\n        node_attr=node_attr,\n    ):\n        state_none = Custom(\"none\", SHARED_STATE_IMG, **STATE_NODE_ATTRS)\n        state_removed = Custom(\"removed\", TERMINAL_STATE_IMG, **STATE_NODE_ATTRS)\n        state_upstream_failed = Custom(\"upstream_failed\", TERMINAL_STATE_IMG, **STATE_NODE_ATTRS)\n        state_skipped = Custom(\"skipped\", TERMINAL_STATE_IMG, **STATE_NODE_ATTRS)\n        state_scheduled = Custom(\"scheduled\", SHARED_STATE_IMG, **STATE_NODE_ATTRS)\n",
      "variables": [
        "image_file",
        "state_none",
        "state_removed",
        "state_upstream_failed",
        "state_skipped",
        "state_scheduled"
      ],
      "anonymized_code": "def generate_task_lifecycle_diagram():\n    var_1 = (MY_DIR / MY_FILENAME).with_suffix(\".png\")\n\n    console.print(f\"[bright_blue]Generating task lifecycle image {var_1}\")\n    with Diagram(\n        name=\"\",\n        show=False,\n        direction=\"LR\",\n        filename=MY_FILENAME,\n        outformat=\"png\",\n        graph_attr=graph_attr,\n        edge_attr=edge_attr,\n        node_attr=node_attr,\n    ):\n        var_2 = Custom(\"none\", SHARED_STATE_IMG, **STATE_NODE_ATTRS)\n        var_3 = Custom(\"removed\", TERMINAL_STATE_IMG, **STATE_NODE_ATTRS)\n        var_4 = Custom(\"upstream_failed\", TERMINAL_STATE_IMG, **STATE_NODE_ATTRS)\n        var_5 = Custom(\"skipped\", TERMINAL_STATE_IMG, **STATE_NODE_ATTRS)\n        var_6 = Custom(\"scheduled\", SHARED_STATE_IMG, **STATE_NODE_ATTRS)\n",
      "lines_processed": 19,
      "total_lines": 213,
      "llm_code": "def generate_task_lifecycle_diagram():\n    output_file = (MY_DIR / MY_FILENAME).with_suffix(\".png\")\n\n    console.print(f\"[bright_blue]Generating task lifecycle image {output_file}\")\n    with Diagram(\n        name=\"\",\n        show=False,\n        direction=\"LR\",\n        filename=MY_FILENAME,\n        outformat=\"png\",\n        graph_attr=graph_attr,\n        edge_attr=edge_attr,\n        node_attr=node_attr,\n    ):\n        none_node = Custom(\"none\", SHARED_STATE_IMG, **STATE_NODE_ATTRS)\n        removed_node = Custom(\"removed\", TERMINAL_STATE_IMG, **STATE_NODE_ATTRS)\n        upstream_failed_node = Custom(\"upstream_failed\", TERMINAL_STATE_IMG, **STATE_NODE_ATTRS)\n        skipped_node = Custom(\"skipped\", TERMINAL_STATE_IMG, **STATE_NODE_ATTRS)\n        scheduled_node = Custom(\"scheduled\", SHARED_STATE_IMG, **STATE_NODE_ATTRS)\n",
      "llm_variables": [
        "output_file",
        "none_node",
        "removed_node",
        "upstream_failed_node",
        "skipped_node",
        "scheduled_node"
      ]
    },
    {
      "file_path": "diagram_distributed_airflow_architecture.py",
      "code": "def generate_distributed_airflow_diagram():\n    image_file = (MY_DIR / MY_FILENAME).with_suffix(\".png\")\n\n    console.print(f\"[bright_blue]Generating architecture image {image_file}\")\n    with Diagram(\n        name=\"\",\n        show=False,\n        direction=\"LR\",\n        filename=MY_FILENAME,\n        outformat=\"png\",\n        graph_attr=graph_attr,\n        edge_attr=edge_attr,\n    ):\n        dag_author = User(\"DAG Author\")\n        deployment_manager = User(\"Deployment Manager\")\n\n        dag_files = Custom(\"DAG files\", MULTIPLE_FILES_IMAGE.as_posix(), height=\"1.8\")\n        dag_author >> Edge(color=\"brown\", style=\"solid\", reverse=False, label=\"author\\n\\n\") >> dag_files\n\n",
      "variables": [
        "image_file",
        "dag_author",
        "deployment_manager",
        "dag_files"
      ],
      "anonymized_code": "def generate_distributed_airflow_diagram():\n    var_1 = (MY_DIR / MY_FILENAME).with_suffix(\".png\")\n\n    console.print(f\"[bright_blue]Generating architecture image {var_1}\")\n    with Diagram(\n        name=\"\",\n        show=False,\n        direction=\"LR\",\n        filename=MY_FILENAME,\n        outformat=\"png\",\n        graph_attr=graph_attr,\n        edge_attr=edge_attr,\n    ):\n        var_2 = User(\"DAG Author\")\n        var_3 = User(\"Deployment Manager\")\n\n        var_4 = Custom(\"DAG files\", MULTIPLE_FILES_IMAGE.as_posix(), height=\"1.8\")\n        var_2 >> Edge(color=\"brown\", style=\"solid\", reverse=False, label=\"author\\n\\n\") >> var_4\n\n",
      "lines_processed": 19,
      "total_lines": 112,
      "llm_code": "def generate_distributed_airflow_diagram():\n    output_path = (MY_DIR / MY_FILENAME).with_suffix(\".png\")\n\n    console.print(f\"[bright_blue]Generating architecture image {output_path}\")\n    with Diagram(\n        name=\"\",\n        show=False,\n        direction=\"LR\",\n        filename=MY_FILENAME,\n        outformat=\"png\",\n        graph_attr=graph_attr,\n        edge_attr=edge_attr,\n    ):\n        user_author = User(\"DAG Author\")\n        user_deployment_manager = User(\"Deployment Manager\")\n\n        dag_files = Custom(\"DAG files\", MULTIPLE_FILES_IMAGE.as_posix(), height=\"1.8\")\n        user_author >> Edge(color=\"brown\", style=\"solid\", reverse=False, label=\"author\\n\\n\") >> dag_files\n\n",
      "llm_variables": [
        "output_path",
        "user_author",
        "user_deployment_manager",
        "dag_files"
      ]
    },
    {
      "file_path": "__main__.py",
      "code": "def main():\n    conf = configuration.conf\n    if conf.get(\"core\", \"security\") == \"kerberos\":\n        os.environ[\"KRB5CCNAME\"] = conf.get(\"kerberos\", \"ccache\")\n        os.environ[\"KRB5_KTNAME\"] = conf.get(\"kerberos\", \"keytab\")\n    parser = cli_parser.get_parser()\n    argcomplete.autocomplete(parser)\n    args = parser.parse_args()\n    if args.subcommand not in [\"lazy_loaded\", \"version\"]:\n        # Here we ensure that the default configuration is written if needed before running any command\n        # that might need it. This used to be done during configuration initialization but having it\n        # in main ensures that it is not done during tests and other ways airflow imports are used\n        from airflow.configuration import write_default_airflow_configuration_if_needed\n\n        conf = write_default_airflow_configuration_if_needed()\n    args.func(args)\n",
      "variables": [
        "conf",
        "parser",
        "args"
      ],
      "anonymized_code": "def main():\n    var_1 = configuration.var_1\n    if var_1.get(\"core\", \"security\") == \"kerberos\":\n        os.environ[\"KRB5CCNAME\"] = var_1.get(\"kerberos\", \"ccache\")\n        os.environ[\"KRB5_KTNAME\"] = var_1.get(\"kerberos\", \"keytab\")\n    var_2 = cli_parser.get_parser()\n    argcomplete.autocomplete(var_2)\n    var_3 = var_2.parse_args()\n    if var_3.subcommand not in [\"lazy_loaded\", \"version\"]:\n        # Here we ensure that the default configuration is written if needed before running any command\n        # that might need it. This used to be done during configuration initialization but having it\n        # in main ensures that it is not done during tests and other ways airflow imports are used\n        from airflow.configuration import write_default_airflow_configuration_if_needed\n\n        var_1 = write_default_airflow_configuration_if_needed()\n    var_3.func(var_3)\n",
      "lines_processed": 16,
      "total_lines": 59,
      "llm_code": "def main():\n    config = configuration.config\n    if config.get(\"core\", \"security\") == \"kerberos\":\n        os.environ[\"KRB5CCNAME\"] = config.get(\"kerberos\", \"ccache\")\n        os.environ[\"KRB5_KTNAME\"] = config.get(\"kerberos\", \"keytab\")\n    parser = cli_parser.get_parser()\n    argcomplete.autocomplete(parser)\n    args = parser.parse_args()\n    if args.subcommand not in [\"lazy_loaded\", \"version\"]:\n        # Here we ensure that the default configuration is written if needed before running any command\n        # that might need it. This used to be done during configuration initialization but having it\n        # in main ensures that it is not done during tests and other ways airflow imports are used\n        from airflow.configuration import write_default_airflow_configuration_if_needed\n\n        config = write_default_airflow_configuration_if_needed()\n    args.func(args)\n",
      "llm_variables": [
        "config",
        "parser",
        "args"
      ]
    }
  ],
  "iterative_dvc": [
    {
      "file_path": "show.py",
      "code": "def _postprocess(results):\n    processed: dict[str, dict] = {}\n    for rev, rev_data in results.items():\n        if not rev_data:\n            continue\n\n        processed[rev] = {}\n\n        counts: typing.Counter[str] = Counter()\n        for file_data in rev_data[\"data\"].values():\n            for k in file_data[\"data\"]:\n                counts[k] += 1\n        for file_name, file_data in rev_data[\"data\"].items():\n            to_merge = {\n                (k if counts[k] == 1 else f\"{file_name}:{k}\"): v\n                for k, v in file_data[\"data\"].items()\n            }\n            processed[rev] = processed[rev] | to_merge\n\n",
      "variables": [
        "results",
        "processed",
        "rev",
        "rev_data",
        "counts",
        "file_data",
        "k",
        "file_name",
        "to_merge",
        "v"
      ],
      "anonymized_code": "def _postprocess(var_1):\n    var_2: dict[str, dict] = {}\n    for var_3, var_4 in var_1.items():\n        if not var_4:\n            continue\n\n        var_2[var_3] = {}\n\n        var_5: typing.Counter[str] = Counter()\n        for var_6 in var_4[\"data\"].values():\n            for var_7 in var_6[\"data\"]:\n                var_5[var_7] += 1\n        for var_8, var_6 in var_4[\"data\"].items():\n            var_9 = {\n                (var_7 if var_5[var_7] == 1 else f\"{var_8}:{var_7}\"): var_10\n                for var_7, var_10 in var_6[\"data\"].items()\n            }\n            var_2[var_3] = var_2[var_3] | var_9\n\n",
      "lines_processed": 19,
      "total_lines": 404,
      "llm_code": "def _postprocess(data):\n    result: dict[str, dict] = {}\n    for key, item in data.items():\n        if not item:\n            continue\n\n        result[key] = {}\n\n        counter: typing.Counter[str] = Counter()\n        for entry in item[\"data\"].values():\n            for value in entry[\"data\"]:\n                counter[value] += 1\n        for key2, entry in item[\"data\"].items():\n            value2 = {\n                (value if counter[value] == 1 else f\"{key2}:{value}\"): temp\n                for value, temp in entry[\"data\"].items()\n            }\n            result[key] = result[key] | value2\n\n",
      "llm_variables": [
        "data",
        "result",
        "key",
        "item",
        "counter",
        "entry",
        "value",
        "key2",
        "value2",
        "temp"
      ]
    },
    {
      "file_path": "dataset.py",
      "code": "def get(name: str) -> Union[DatachainDataset, DVCDataset, URLDataset]:\n    from difflib import get_close_matches\n\n    from dvc.fs import get_cloud_fs\n    from dvc.repo import Repo, datasets\n\n    repo = Repo()\n    try:\n        dataset = repo.datasets[name]\n    except datasets.DatasetNotFoundError as e:\n        add_note = getattr(e, \"add_note\", lambda _: None)\n        if matches := get_close_matches(name, repo.datasets):\n            add_note(f\"Did you mean: {matches[0]!r}?\")\n        raise\n\n    if dataset._invalidated:\n        raise ValueError(f\"dataset not in sync. Sync with 'dvc ds update {name}'.\")\n    if not dataset.lock:\n        raise ValueError(\"missing lock information\")\n",
      "variables": [
        "name",
        "repo",
        "dataset",
        "add_note",
        "_",
        "matches"
      ],
      "anonymized_code": "def get(var_1: str) -> Union[DatachainDataset, DVCDataset, URLDataset]:\n    from difflib import get_close_matches\n\n    from dvc.fs import get_cloud_fs\n    from dvc.var_2 import Repo, datasets\n\n    var_2 = Repo()\n    try:\n        var_3 = var_2.datasets[var_1]\n    except datasets.DatasetNotFoundError as e:\n        var_4 = getattr(e, \"var_4\", lambda var_5: None)\n        if var_6 := get_close_matches(var_1, var_2.datasets):\n            var_4(f\"Did you mean: {var_6[0]!r}?\")\n        raise\n\n    if var_3._invalidated:\n        raise ValueError(f\"var_3 not in sync. Sync with 'dvc ds update {var_1}'.\")\n    if not var_3.lock:\n        raise ValueError(\"missing lock information\")\n",
      "lines_processed": 19,
      "total_lines": 66,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "experiments.py",
      "code": "def _postprocess(exp_rows):\n    for exp_row in exp_rows:\n        for k, v in exp_row.items():\n            if isinstance(v, Text):\n                v_str = str(v)\n                try:\n                    exp_row[k] = float(v_str)\n                except ValueError:\n                    exp_row[k] = v_str\n\n            elif not exp_row[k]:\n                exp_row[k] = None\n\n    return exp_rows\n",
      "variables": [
        "exp_rows",
        "exp_row",
        "k",
        "v",
        "v_str"
      ],
      "anonymized_code": "def _postprocess(var_1):\n    for var_2 in var_1:\n        for var_3, var_4 in var_2.items():\n            if isinstance(var_4, Text):\n                var_5 = str(var_4)\n                try:\n                    var_2[var_3] = float(var_5)\n                except ValueError:\n                    var_2[var_3] = var_5\n\n            elif not var_2[var_3]:\n                var_2[var_3] = None\n\n    return var_1\n",
      "lines_processed": 14,
      "total_lines": 120,
      "llm_code": "def _postprocess(data):\n    for item in data:\n        for key, value in item.items():\n            if isinstance(value, Text):\n                converted = str(value)\n                try:\n                    item[key] = float(converted)\n                except ValueError:\n                    item[key] = converted\n\n            elif not item[key]:\n                item[key] = None\n\n    return data",
      "llm_variables": [
        "data",
        "item",
        "key",
        "value",
        "converted"
      ]
    },
    {
      "file_path": "analytics.py",
      "code": "def _scm_in_use():\n    from dvc.exceptions import NotDvcRepoError\n    from dvc.repo import Repo\n    from dvc.scm import NoSCM\n\n    from .scm import SCM, SCMError\n\n    try:\n        scm = SCM(root_dir=Repo.find_root())\n        return type(scm).__name__\n    except SCMError:\n        return NoSCM.__name__\n    except NotDvcRepoError:\n        pass\n",
      "variables": [
        "scm"
      ],
      "anonymized_code": "def _scm_in_use():\n    from dvc.exceptions import NotDvcRepoError\n    from dvc.repo import Repo\n    from dvc.var_1 import NoSCM\n\n    from .var_1 import SCM, SCMError\n\n    try:\n        var_1 = SCM(root_dir=Repo.find_root())\n        return type(var_1).__name__\n    except SCMError:\n        return NoSCM.__name__\n    except NotDvcRepoError:\n        pass\n",
      "lines_processed": 14,
      "total_lines": 164,
      "llm_code": "def _scm_in_use():\n    from dvc.exceptions import NotDvcRepoError\n    from dvc.repo import Repo\n    from dvc.scm import NoSCM\n\n    from .scm import SCM, SCMError\n\n    try:\n        scm = SCM(root_dir=Repo.find_root())\n        return type(scm).__name__\n    except SCMError:\n        return NoSCM.__name__\n    except NotDvcRepoError:\n        pass\n",
      "llm_variables": [
        "scm"
      ]
    },
    {
      "file_path": "analytics.py",
      "code": "def collect_and_send_report(args=None, return_code=None):\n    \"\"\"\n    Collect information from the runtime/environment and the command\n    being executed into a report and send it over the network.\n\n    To prevent analytics from blocking the execution of the main thread,\n    sending the report is done in a separate process.\n\n    The inter-process communication happens through a file containing the\n    report as a JSON, where the _collector_ generates it and the _sender_\n    removes it after sending it.\n    \"\"\"\n    import tempfile\n\n    from dvc.daemon import daemon\n\n    report = {}\n\n    # Include command execution information on the report only when available.\n",
      "variables": [
        "args",
        "return_code",
        "report"
      ],
      "anonymized_code": "def collect_and_send_report(var_1=None, var_2=None):\n    \"\"\"\n    Collect information from the runtime/environment and the command\n    being executed into a var_3 and send it over the network.\n\n    To prevent analytics from blocking the execution of the main thread,\n    sending the var_3 is done in a separate process.\n\n    The inter-process communication happens through a file containing the\n    var_3 as a JSON, where the _collector_ generates it and the _sender_\n    removes it after sending it.\n    \"\"\"\n    import tempfile\n\n    from dvc.daemon import daemon\n\n    var_3 = {}\n\n    # Include command execution information on the var_3 only when available.\n",
      "lines_processed": 19,
      "total_lines": 164,
      "llm_code": "def collect_and_send_report(command=None, env=None):\n    \"\"\"\n    Collect information from the runtime/environment and the command\n    being executed into a report and send it over the network.\n\n    To prevent analytics from blocking the execution of the main thread,\n    sending the report is done in a separate process.\n\n    The inter-process communication happens through a file containing the\n    report as a JSON, where the _collector_ generates it and the _sender_\n    removes it after sending it.\n    \"\"\"\n    import tempfile\n\n    from dvc.daemon import daemon\n\n    report = {}\n\n    # Include command execution information on the report only when available.\n    ",
      "llm_variables": [
        "command",
        "env",
        "report"
      ]
    },
    {
      "file_path": "analytics.py",
      "code": "def _system_info():\n    import platform\n    import sys\n\n    import distro\n\n    system = platform.system()\n\n    if system == \"Windows\":\n        version = sys.getwindowsversion()  # type: ignore[attr-defined]\n\n        return {\n            \"os\": \"windows\",\n            \"windows_version_build\": version.build,\n            \"windows_version_major\": version.major,\n            \"windows_version_minor\": version.minor,\n            \"windows_version_service_pack\": version.service_pack,\n        }\n\n",
      "variables": [
        "system",
        "version"
      ],
      "anonymized_code": "def _system_info():\n    import platform\n    import sys\n\n    import distro\n\n    var_1 = platform.var_1()\n\n    if var_1 == \"Windows\":\n        var_2 = sys.getwindowsversion()  # type: ignore[attr-defined]\n\n        return {\n            \"os\": \"windows\",\n            \"windows_version_build\": var_2.build,\n            \"windows_version_major\": var_2.major,\n            \"windows_version_minor\": var_2.minor,\n            \"windows_version_service_pack\": var_2.service_pack,\n        }\n\n",
      "lines_processed": 19,
      "total_lines": 164,
      "llm_code": "def _system_info():\n    import platform\n    import sys\n\n    import distro\n\n    os_name = platform.os_name()\n\n    if os_name == \"Windows\":\n        windows_version = sys.getwindowsversion()  # type: ignore[attr-defined]\n\n        return {\n            \"os\": \"windows\",\n            \"windows_version_build\": windows_version.build,\n            \"windows_version_major\": windows_version.major,\n            \"windows_version_minor\": windows_version.minor,\n            \"windows_version_service_pack\": windows_version.service_pack,\n        }\n\n",
      "llm_variables": [
        "os_name",
        "windows_version"
      ]
    },
    {
      "file_path": "analytics.py",
      "code": "def is_enabled():\n    from dvc.config import Config, to_bool\n    from dvc.utils import env2bool\n\n    if env2bool(\"DVC_TEST\"):\n        return False\n\n    enabled = not os.getenv(DVC_NO_ANALYTICS)\n    if enabled:\n        enabled = to_bool(\n            Config.from_cwd(validate=False).get(\"core\", {}).get(\"analytics\", \"true\")\n        )\n\n    logger.debug(\"Analytics is %sabled.\", \"en\" if enabled else \"dis\")\n\n    return enabled\n",
      "variables": [
        "enabled"
      ],
      "anonymized_code": "def is_enabled():\n    from dvc.config import Config, to_bool\n    from dvc.utils import env2bool\n\n    if env2bool(\"DVC_TEST\"):\n        return False\n\n    var_1 = not os.getenv(DVC_NO_ANALYTICS)\n    if var_1:\n        var_1 = to_bool(\n            Config.from_cwd(validate=False).get(\"core\", {}).get(\"analytics\", \"true\")\n        )\n\n    logger.debug(\"Analytics is %sabled.\", \"en\" if var_1 else \"dis\")\n\n    return var_1\n",
      "lines_processed": 16,
      "total_lines": 164,
      "llm_code": "def is_enabled():\n    from dvc.config import Config, to_bool\n    from dvc.utils import env2bool\n\n    if env2bool(\"DVC_TEST\"):\n        return False\n\n    enabled = not os.getenv(DVC_NO_ANALYTICS)\n    if enabled:\n        enabled = to_bool(\n            Config.from_cwd(validate=False).get(\"core\", {}).get(\"analytics\", \"true\")\n        )\n\n    logger.debug(\"Analytics is %sabled.\", \"en\" if enabled else \"dis\")\n\n    return enabled\n",
      "llm_variables": [
        "enabled"
      ]
    },
    {
      "file_path": "analytics.py",
      "code": "def send(path):\n    \"\"\"\n    Side effect: Removes the report after sending it.\n\n    The report is generated and stored in a temporary file, see:\n    `collect_and_send_report`. Sending happens on another process,\n    thus, the need of removing such file afterwards.\n    \"\"\"\n    import requests\n\n    url = os.environ.get(DVC_ANALYTICS_ENDPOINT, \"https://analytics.dvc.org\")\n    headers = {\"content-type\": \"application/json\"}\n\n    with open(path, encoding=\"utf-8\") as fobj:\n        report = json.load(fobj)\n\n    report.update(_runtime_info())\n\n    logger.debug(\"uploading report to %s\", url)\n",
      "variables": [
        "path",
        "url",
        "headers",
        "fobj",
        "report"
      ],
      "anonymized_code": "def send(var_1):\n    \"\"\"\n    Side effect: Removes the var_5 after sending it.\n\n    The var_5 is generated and stored in a temporary file, see:\n    `collect_and_send_report`. Sending happens on another process,\n    thus, the need of removing such file afterwards.\n    \"\"\"\n    import requests\n\n    var_2 = os.environ.get(DVC_ANALYTICS_ENDPOINT, \"https://analytics.dvc.org\")\n    var_3 = {\"content-type\": \"application/json\"}\n\n    with open(var_1, encoding=\"utf-8\") as var_4:\n        var_5 = json.load(var_4)\n\n    var_5.update(_runtime_info())\n\n    logger.debug(\"uploading var_5 to %s\", var_2)\n",
      "lines_processed": 19,
      "total_lines": 164,
      "llm_code": "def send(report):\n    \"\"\"\n    Side effect: Removes the report_data after sending it.\n\n    The report_data is generated and stored in a temporary file, see:\n    `collect_and_send_report`. Sending happens on another process,\n    thus, the need of removing such file afterwards.\n    \"\"\"\n    import requests\n\n    endpoint = os.environ.get(DVC_ANALYTICS_ENDPOINT, \"https://analytics.dvc.org\")\n    headers = {\"content-type\": \"application/json\"}\n\n    with open(report, encoding=\"utf-8\") as file:\n        report_data = json.load(file)\n\n    report_data.update(_runtime_info())\n\n    logger.debug(\"uploading report_data to %s\", endpoint)\n",
      "llm_variables": [
        "report",
        "endpoint",
        "headers",
        "file",
        "report_data"
      ]
    },
    {
      "file_path": "scm.py",
      "code": "def all_commits(repo: Optional[str] = None) -> list[str]:\n    \"\"\"Get all Git commits in a DVC repository.\n\n    Args:\n        repo (str, optional): location of the DVC repository.\n            Defaults to the current project (found by walking up from the\n            current working directory tree).\n            It can be a URL or a file system path.\n            Both HTTP and SSH protocols are supported for online Git repos\n            (e.g. [user@]server:project.git).\n    Returns:\n        List[str]: SHAs of the Git commits.\n    \"\"\"\n    with Repo.open(repo) as _repo:\n        return _repo.scm.list_all_commits()\n",
      "variables": [
        "repo",
        "_repo"
      ],
      "anonymized_code": "def all_commits(var_1: Optional[str] = None) -> list[str]:\n    \"\"\"Get all Git commits in a DVC repository.\n\n    Args:\n        var_1 (str, optional): location of the DVC repository.\n            Defaults to the current project (found by walking up from the\n            current working directory tree).\n            It can be a URL or a file system path.\n            Both HTTP and SSH protocols are supported for online Git repos\n            (e.g. [user@]server:project.git).\n    Returns:\n        List[str]: SHAs of the Git commits.\n    \"\"\"\n    with Repo.open(var_1) as var_2:\n        return var_2.scm.list_all_commits()\n",
      "lines_processed": 15,
      "total_lines": 54,
      "llm_code": "def all_commits(repo_path: Optional[str] = None) -> list[str]:\n    \"\"\"Get all Git commits in a DVC repository.\n\n    Args:\n        repo_path (str, optional): location of the DVC repository.\n            Defaults to the current project (found by walking up from the\n            current working directory tree).\n            It can be a URL or a file system path.\n            Both HTTP and SSH protocols are supported for online Git repos\n            (e.g. [user@]server:project.git).\n    Returns:\n        List[str]: SHAs of the Git commits.\n    \"\"\"\n    with Repo.open(repo_path) as repo:\n        return repo.scm.list_all_commits()",
      "llm_variables": [
        "repo_path",
        "repo"
      ]
    },
    {
      "file_path": "scm.py",
      "code": "def all_branches(repo: Optional[str] = None) -> list[str]:\n    \"\"\"Get all Git branches in a DVC repository.\n\n    Args:\n        repo (str, optional): location of the DVC repository.\n            Defaults to the current project (found by walking up from the\n            current working directory tree).\n            It can be a URL or a file system path.\n            Both HTTP and SSH protocols are supported for online Git repos\n            (e.g. [user@]server:project.git).\n    Returns:\n        List[str]: Names of the Git branches.\n    \"\"\"\n    with Repo.open(repo) as _repo:\n        return _repo.scm.list_branches()\n",
      "variables": [
        "repo",
        "_repo"
      ],
      "anonymized_code": "def all_branches(var_1: Optional[str] = None) -> list[str]:\n    \"\"\"Get all Git branches in a DVC repository.\n\n    Args:\n        var_1 (str, optional): location of the DVC repository.\n            Defaults to the current project (found by walking up from the\n            current working directory tree).\n            It can be a URL or a file system path.\n            Both HTTP and SSH protocols are supported for online Git repos\n            (e.g. [user@]server:project.git).\n    Returns:\n        List[str]: Names of the Git branches.\n    \"\"\"\n    with Repo.open(var_1) as var_2:\n        return var_2.scm.list_branches()\n",
      "lines_processed": 15,
      "total_lines": 54,
      "llm_code": "def all_branches(repo_location: Optional[str] = None) -> list[str]:\n    \"\"\"Get all Git branches in a DVC repository.\n\n    Args:\n        repo_location (str, optional): location of the DVC repository.\n            Defaults to the current project (found by walking up from the\n            current working directory tree).\n            It can be a URL or a file system path.\n            Both HTTP and SSH protocols are supported for online Git repos\n            (e.g. [user@]server:project.git).\n    Returns:\n        List[str]: Names of the Git branches.\n    \"\"\"\n    with Repo.open(repo_location) as repo:\n        return repo.scm.list_branches()\n",
      "llm_variables": [
        "repo_location",
        "repo"
      ]
    },
    {
      "file_path": "scm.py",
      "code": "def all_tags(repo: Optional[str] = None) -> list[str]:\n    \"\"\"Get all Git tags in a DVC repository.\n\n    Args:\n        repo (str, optional): location of the DVC repository.\n            Defaults to the current project (found by walking up from the\n            current working directory tree).\n            It can be a URL or a file system path.\n            Both HTTP and SSH protocols are supported for online Git repos\n            (e.g. [user@]server:project.git).\n    Returns:\n        List[str]: Names of the Git tags.\n    \"\"\"\n    with Repo.open(repo) as _repo:\n        return _repo.scm.list_tags()\n",
      "variables": [
        "repo",
        "_repo"
      ],
      "anonymized_code": "def all_tags(var_1: Optional[str] = None) -> list[str]:\n    \"\"\"Get all Git tags in a DVC repository.\n\n    Args:\n        var_1 (str, optional): location of the DVC repository.\n            Defaults to the current project (found by walking up from the\n            current working directory tree).\n            It can be a URL or a file system path.\n            Both HTTP and SSH protocols are supported for online Git repos\n            (e.g. [user@]server:project.git).\n    Returns:\n        List[str]: Names of the Git tags.\n    \"\"\"\n    with Repo.open(var_1) as var_2:\n        return var_2.scm.list_tags()\n",
      "lines_processed": 15,
      "total_lines": 54,
      "llm_code": "def all_tags(repo_path: Optional[str] = None) -> list[str]:\n    \"\"\"Get all Git tags in a DVC repository.\n\n    Args:\n        repo_path (str, optional): location of the DVC repository.\n            Defaults to the current project (found by walking up from the\n            current working directory tree).\n            It can be a URL or a file system path.\n            Both HTTP and SSH protocols are supported for online Git repos\n            (e.g. [user@]server:project.git).\n    Returns:\n        List[str]: Names of the Git tags.\n    \"\"\"\n    with Repo.open(repo_path) as repo:\n        return repo.scm.list_tags()\n",
      "llm_variables": [
        "repo_path",
        "repo"
      ]
    },
    {
      "file_path": "_debug.py",
      "code": "def yappi_profile(\n    path: Optional[Union[Callable[[], str], str]] = None,\n    wall_clock: Optional[bool] = True,\n    separate_threads: Optional[bool] = False,\n):\n    try:\n        import yappi\n    except ImportError:\n        print(\"Failed to run profiler, yappi is not installed\")  # noqa: T201\n        yield\n        return\n\n    yappi.set_clock_type(\"wall\" if wall_clock else \"cpu\")\n\n    yappi.start()\n    try:\n        yield\n    finally:\n        yappi.stop()\n",
      "variables": [
        "path",
        "wall_clock",
        "separate_threads"
      ],
      "anonymized_code": "def yappi_profile(\n    var_1: Optional[Union[Callable[[], str], str]] = None,\n    var_2: Optional[bool] = True,\n    var_3: Optional[bool] = False,\n):\n    try:\n        import yappi\n    except ImportError:\n        print(\"Failed to run profiler, yappi is not installed\")  # noqa: T201\n        yield\n        return\n\n    yappi.set_clock_type(\"wall\" if var_2 else \"cpu\")\n\n    yappi.start()\n    try:\n        yield\n    finally:\n        yappi.stop()\n",
      "lines_processed": 19,
      "total_lines": 288,
      "llm_code": "def yappi_profile(\n    func: Optional[Union[Callable[[], str], str]] = None,\n    use_wall_clock: Optional[bool] = True,\n    is_active: Optional[bool] = False,\n):\n    try:\n        import yappi\n    except ImportError:\n        print(\"Failed to run profiler, yappi is not installed\")  # noqa: T201\n        yield\n        return\n\n    yappi.set_clock_type(\"wall\" if use_wall_clock else \"cpu\")\n\n    yappi.start()\n    try:\n        yield\n    finally:\n        yappi.stop()\n",
      "llm_variables": [
        "func",
        "use_wall_clock",
        "is_active"
      ]
    },
    {
      "file_path": "_debug.py",
      "code": "def profile(dump_path: Optional[str] = None):\n    \"\"\"Run a cprofile\"\"\"\n    import cProfile\n\n    prof = cProfile.Profile()\n    prof.enable()\n\n    try:\n        yield\n    finally:\n        prof.disable()\n        if dump_path:\n            prof.dump_stats(dump_path)\n        else:\n            prof.print_stats(sort=\"cumtime\")\n",
      "variables": [
        "dump_path",
        "prof"
      ],
      "anonymized_code": "def profile(var_1: Optional[str] = None):\n    \"\"\"Run a cprofile\"\"\"\n    import cProfile\n\n    var_2 = cProfile.Profile()\n    var_2.enable()\n\n    try:\n        yield\n    finally:\n        var_2.disable()\n        if var_1:\n            var_2.dump_stats(var_1)\n        else:\n            var_2.print_stats(sort=\"cumtime\")\n",
      "lines_processed": 15,
      "total_lines": 288,
      "llm_code": "def profile(profile_name: Optional[str] = None):\n    \"\"\"Run a cprofile\"\"\"\n    import cProfile\n\n    profile = cProfile.Profile()\n    profile.enable()\n\n    try:\n        yield\n    finally:\n        profile.disable()\n        if profile_name:\n            profile.dump_stats(profile_name)\n        else:\n            profile.print_stats(sort=\"cumtime\")\n",
      "llm_variables": [
        "profile_name",
        "profile"
      ]
    },
    {
      "file_path": "_debug.py",
      "code": "def _sigshow(_, frame: Optional[\"FrameType\"]) -> None:\n    import sys\n    from shutil import get_terminal_size\n    from traceback import format_stack\n\n    lines = \"\\u2015\" * get_terminal_size().columns\n    stack = format_stack(frame)\n    print(lines, \"\\n\", *stack, lines, sep=\"\", file=sys.stderr)  # noqa: T201\n",
      "variables": [
        "_",
        "frame",
        "lines",
        "stack"
      ],
      "anonymized_code": "def _sigshow(var_1, var_2: Optional[\"FrameType\"]) -> None:\n    import sys\n    from shutil import get_terminal_size\n    from traceback import format_stack\n\n    var_3 = \"\\u2015\" * get_terminal_size().columns\n    var_4 = format_stack(var_2)\n    print(var_3, \"\\n\", *var_4, var_3, sep=\"\", file=sys.stderr)  # noqa: T201\n",
      "lines_processed": 8,
      "total_lines": 288,
      "llm_code": "def _sigshow(frame, frame_type: Optional[\"FrameType\"]) -> None:\n    import sys\n    from shutil import get_terminal_size\n    from traceback import format_stack\n\n    line = \"\\u2015\" * get_terminal_size().columns\n    stack = format_stack(frame_type)\n    print(line, \"\\n\", *stack, line, sep=\"\", file=sys.stderr)  # noqa: T201\n",
      "llm_variables": [
        "frame",
        "frame_type",
        "line",
        "stack"
      ]
    },
    {
      "file_path": "_debug.py",
      "code": "def viztracer_profile(\n    path: Union[Callable[[], str], str],\n    depth: int = -1,\n    log_async: bool = False,\n):\n    try:\n        import viztracer\n    except ImportError:\n        print(\"Failed to run profiler, viztracer is not installed\")  # noqa: T201\n        yield\n        return\n\n    tracer = viztracer.VizTracer(max_stack_depth=depth, log_async=log_async)\n\n    tracer.start()\n    try:\n        yield\n    finally:\n        tracer.stop()\n",
      "variables": [
        "path",
        "depth",
        "log_async",
        "tracer"
      ],
      "anonymized_code": "def viztracer_profile(\n    var_1: Union[Callable[[], str], str],\n    var_2: int = -1,\n    var_3: bool = False,\n):\n    try:\n        import viztracer\n    except ImportError:\n        print(\"Failed to run profiler, viztracer is not installed\")  # noqa: T201\n        yield\n        return\n\n    var_4 = viztracer.VizTracer(max_stack_depth=var_2, var_3=var_3)\n\n    var_4.start()\n    try:\n        yield\n    finally:\n        var_4.stop()\n",
      "lines_processed": 19,
      "total_lines": 288,
      "llm_code": "def viztracer_profile(\n    function: Union[Callable[[], str], str],\n    max_stack_depth: int = -1,\n    capture_stack: bool = False,\n):\n    try:\n        import viztracer\n    except ImportError:\n        print(\"Failed to run profiler, viztracer is not installed\")  # noqa: T201\n        yield\n        return\n\n    tracer = viztracer.VizTracer(max_stack_depth=max_stack_depth, capture_stack=capture_stack)\n\n    tracer.start()\n    try:\n        yield\n    finally:\n        tracer.stop()\n",
      "llm_variables": [
        "function",
        "max_stack_depth",
        "capture_stack",
        "tracer"
      ]
    },
    {
      "file_path": "data.py",
      "code": "def _wrap_exceptions(repo, url):\n    from dvc.config import NoRemoteError\n    from dvc.exceptions import NoOutputInExternalRepoError, NoRemoteInExternalRepoError\n\n    try:\n        yield\n    except NoRemoteError as exc:\n        raise NoRemoteInExternalRepoError(url) from exc\n    except OutputNotFoundError as exc:\n        if exc.repo is repo:\n            raise NoOutputInExternalRepoError(exc.output, repo.root_dir, url) from exc\n        raise\n    except FileMissingError as exc:\n        raise PathMissingError(exc.path, url) from exc\n",
      "variables": [
        "repo",
        "url"
      ],
      "anonymized_code": "def _wrap_exceptions(var_1, var_2):\n    from dvc.config import NoRemoteError\n    from dvc.exceptions import NoOutputInExternalRepoError, NoRemoteInExternalRepoError\n\n    try:\n        yield\n    except NoRemoteError as exc:\n        raise NoRemoteInExternalRepoError(var_2) from exc\n    except OutputNotFoundError as exc:\n        if exc.var_1 is var_1:\n            raise NoOutputInExternalRepoError(exc.output, var_1.root_dir, var_2) from exc\n        raise\n    except FileMissingError as exc:\n        raise PathMissingError(exc.path, var_2) from exc\n",
      "lines_processed": 14,
      "total_lines": 330,
      "llm_code": "def _wrap_exceptions(func, remote):\n    from dvc.config import NoRemoteError\n    from dvc.exceptions import NoOutputInExternalRepoError, NoRemoteInExternalRepoError\n\n    try:\n        yield\n    except NoRemoteError as exc:\n        raise NoRemoteInExternalRepoError(remote) from exc\n    except OutputNotFoundError as exc:\n        if exc.var_1 is func:\n            raise NoOutputInExternalRepoError(exc.output, func.root_dir, remote) from exc\n        raise\n    except FileMissingError as exc:\n        raise PathMissingError(exc.path, remote) from exc",
      "llm_variables": [
        "func",
        "remote"
      ]
    }
  ],
  "facebookresearch_detectron2": [
    {
      "file_path": "torchvision_imagenet_R_50.py",
      "code": "def build_data_loader(dataset, batch_size, num_workers, training=True):\n    return torch.utils.data.DataLoader(\n        dataset,\n        sampler=(TrainingSampler if training else InferenceSampler)(len(dataset)),\n        batch_size=batch_size,\n        num_workers=num_workers,\n        pin_memory=True,\n    )\n",
      "variables": [
        "dataset",
        "batch_size",
        "num_workers",
        "training"
      ],
      "anonymized_code": "def build_data_loader(var_1, var_2, var_3, var_4=True):\n    return torch.utils.data.DataLoader(\n        var_1,\n        sampler=(TrainingSampler if var_4 else InferenceSampler)(len(var_1)),\n        var_2=var_2,\n        var_3=var_3,\n        pin_memory=True,\n    )\n",
      "lines_processed": 8,
      "total_lines": 149,
      "llm_code": "def build_data_loader(dataset, sampler, batch_size, num_workers=True):\n    return torch.utils.data.DataLoader(\n        dataset,\n        sampler=(TrainingSampler if num_workers else InferenceSampler)(len(dataset)),\n        batch_size=batch_size,\n        num_workers=num_workers,\n        pin_memory=True,\n    )",
      "llm_variables": [
        "dataset",
        "sampler",
        "batch_size",
        "num_workers"
      ]
    }
  ],
  "celery_celery": [
    {
      "file_path": "amqp.py",
      "code": "def utf8dict(d, encoding='utf-8'):\n    return {k.decode(encoding) if isinstance(k, bytes) else k: v\n            for k, v in d.items()}\n",
      "variables": [
        "d",
        "encoding",
        "k",
        "v"
      ],
      "anonymized_code": "def utf8dict(var_1, var_2='utf-8'):\n    return {var_3.decode(var_2) if isinstance(var_3, bytes) else var_3: var_4\n            for var_3, var_4 in var_1.items()}\n",
      "lines_processed": 3,
      "total_lines": 621,
      "llm_code": "def utf8dict(data, encoding=\"utf-8\"):\n    return {item.decode(encoding) if isinstance(item, bytes) else item: value\n            for item, value in data.items()}",
      "llm_variables": [
        "data",
        "encoding",
        "item",
        "value"
      ]
    },
    {
      "file_path": "trace.py",
      "code": "def get_task_name(request, default):\n    \"\"\"Use 'shadow' in request for the task name if applicable.\"\"\"\n    # request.shadow could be None or an empty string.\n    # If so, we should use default.\n    return getattr(request, 'shadow', None) or default\n",
      "variables": [
        "request",
        "default"
      ],
      "anonymized_code": "def get_task_name(var_1, var_2):\n    \"\"\"Use 'shadow' in var_1 for the task name if applicable.\"\"\"\n    # var_1.shadow could be None or an empty string.\n    # If so, we should use var_2.\n    return getattr(var_1, 'shadow', None) or var_2\n",
      "lines_processed": 5,
      "total_lines": 739,
      "llm_code": "def get_task_name(task, default):\n    \"\"\"Use 'shadow' in task for the task name if applicable.\"\"\"\n    # task.shadow could be None or an empty string.\n    # If so, we should use default.\n    return getattr(task, 'shadow', None) or default\n",
      "llm_variables": [
        "task",
        "default"
      ]
    },
    {
      "file_path": "trace.py",
      "code": "def trace_task(task, uuid, args, kwargs, request=None, **opts):\n    \"\"\"Trace task execution.\"\"\"\n    request = {} if not request else request\n    try:\n        if task.__trace__ is None:\n            task.__trace__ = build_tracer(task.name, task, **opts)\n        return task.__trace__(uuid, args, kwargs, request)\n    except Exception as exc:\n        _signal_internal_error(task, uuid, args, kwargs, request, exc)\n        return trace_ok_t(report_internal_error(task, exc), TraceInfo(FAILURE, exc), 0.0, None)\n",
      "variables": [
        "task",
        "uuid",
        "args",
        "kwargs",
        "request",
        "opts"
      ],
      "anonymized_code": "def trace_task(var_1, var_2, var_3, var_4, var_5=None, **var_6):\n    \"\"\"Trace var_1 execution.\"\"\"\n    var_5 = {} if not var_5 else var_5\n    try:\n        if var_1.__trace__ is None:\n            var_1.__trace__ = build_tracer(var_1.name, var_1, **var_6)\n        return var_1.__trace__(var_2, var_3, var_4, var_5)\n    except Exception as exc:\n        _signal_internal_error(var_1, var_2, var_3, var_4, var_5, exc)\n        return trace_ok_t(report_internal_error(var_1, exc), TraceInfo(FAILURE, exc), 0.0, None)\n",
      "lines_processed": 10,
      "total_lines": 739,
      "llm_code": "def trace_task(task, input_data, parameters, context, trace_info=None, **kwargs):\n    \"\"\"Trace task execution.\"\"\"\n    trace_info = {} if not trace_info else trace_info\n    try:\n        if task.__trace__ is None:\n            task.__trace__ = build_tracer(task.name, task, **kwargs)\n        return task.__trace__(input_data, parameters, context, trace_info)\n    except Exception as exc:\n        _signal_internal_error(task, input_data, parameters, context, trace_info, exc)\n        return trace_ok_t(report_internal_error(task, exc), TraceInfo(FAILURE, exc), 0.0, None)\n",
      "llm_variables": [
        "task",
        "input_data",
        "parameters",
        "context",
        "trace_info",
        "kwargs"
      ]
    },
    {
      "file_path": "trace.py",
      "code": "def info(fmt, context):\n    \"\"\"Log 'fmt % context' with severity 'INFO'.\n\n    'context' is also passed in extra with key 'data' for custom handlers.\n    \"\"\"\n    logger.info(fmt, context, extra={'data': context})\n",
      "variables": [
        "fmt",
        "context"
      ],
      "anonymized_code": "def info(var_1, var_2):\n    \"\"\"Log 'var_1 % var_2' with severity 'INFO'.\n\n    'var_2' is also passed in extra with key 'data' for custom handlers.\n    \"\"\"\n    logger.info(var_1, var_2, extra={'data': var_2})\n",
      "lines_processed": 6,
      "total_lines": 739,
      "llm_code": "def info(first, second):\n    \"\"\"Log 'first % second' with severity 'INFO'.\n\n    'second' is also passed in extra with key 'data' for custom handlers.\n    \"\"\"\n    logger.info(first, second, extra={'data': second})\n",
      "llm_variables": [
        "first",
        "second"
      ]
    },
    {
      "file_path": "trace.py",
      "code": "def report_internal_error(task, exc):\n    _type, _value, _tb = sys.exc_info()\n    try:\n        _value = task.backend.prepare_exception(exc, 'pickle')\n        exc_info = ExceptionInfo((_type, _value, _tb), internal=True)\n        warn(RuntimeWarning(\n            'Exception raised outside body: {!r}:\\n{}'.format(\n                exc, exc_info.traceback)))\n        return exc_info\n    finally:\n        del _tb\n",
      "variables": [
        "task",
        "exc",
        "_type",
        "_value",
        "_tb",
        "exc_info"
      ],
      "anonymized_code": "def report_internal_error(var_1, var_2):\n    var_3, var_4, var_5 = sys.var_6()\n    try:\n        var_4 = var_1.backend.prepare_exception(var_2, 'pickle')\n        var_6 = ExceptionInfo((var_3, var_4, var_5), internal=True)\n        warn(RuntimeWarning(\n            'Exception raised outside body: {!r}:\\n{}'.format(\n                var_2, var_6.traceback)))\n        return var_6\n    finally:\n        del var_5\n",
      "lines_processed": 11,
      "total_lines": 739,
      "llm_code": "def report_internal_error(error_type, exception):\n    message, exception_info, traceback = sys.get_exception_info()\n    try:\n        exception_info = error_type.backend.prepare_exception(exception, \"pickle\")\n        error = ExceptionInfo((message, exception_info, traceback), internal=True)\n        warn(RuntimeWarning(\n            \"Exception raised outside body: {!r}:\n{}\"\n           .format(\n                exception, error.traceback)\n        ))\n        return error\n    finally:\n        del traceback",
      "llm_variables": [
        "error_type",
        "exception",
        "message",
        "exception_info",
        "traceback",
        "error"
      ]
    },
    {
      "file_path": "trace.py",
      "code": "def task_has_custom(task, attr):\n    \"\"\"Return true if the task overrides ``attr``.\"\"\"\n    return mro_lookup(task.__class__, attr, stop={BaseTask, object},\n                      monkey_patched=['celery.app.task'])\n",
      "variables": [
        "task",
        "attr"
      ],
      "anonymized_code": "def task_has_custom(var_1, var_2):\n    \"\"\"Return true if the var_1 overrides ``var_2``.\"\"\"\n    return mro_lookup(var_1.__class__, var_2, stop={BaseTask, object},\n                      monkey_patched=['celery.app.var_1'])\n",
      "lines_processed": 4,
      "total_lines": 739,
      "llm_code": "def task_has_custom(var_1, var_2):\n    \"\"\"Return true if the var_1 overrides ``var_2``.\"\"\"\n    return mro_lookup(var_1.__class__, var_2, stop={BaseTask, object},\n                      monkey_patched=[\"celery.app.var_1\"])",
      "llm_variables": [
        "var_1",
        "var_2"
      ]
    },
    {
      "file_path": "trace.py",
      "code": "def trace_task_ret(name, uuid, request, body, content_type,\n                   content_encoding, loads=loads_message, app=None,\n                   **extra_request):\n    app = app or current_app._get_current_object()\n    embed = None\n    if content_type:\n        accept = prepare_accept_content(app.conf.accept_content)\n        args, kwargs, embed = loads(\n            body, content_type, content_encoding, accept=accept,\n        )\n    else:\n        args, kwargs, embed = body\n    hostname = gethostname()\n    request.update({\n        'args': args, 'kwargs': kwargs,\n        'hostname': hostname, 'is_eager': False,\n    }, **embed or {})\n    R, I, T, Rstr = trace_task(app.tasks[name],\n                               uuid, args, kwargs, request, app=app)\n",
      "variables": [
        "name",
        "uuid",
        "request",
        "body",
        "content_type",
        "content_encoding",
        "loads",
        "app",
        "extra_request",
        "embed",
        "accept",
        "args",
        "kwargs",
        "hostname",
        "R",
        "I",
        "T",
        "Rstr"
      ],
      "anonymized_code": "def trace_task_ret(var_1, var_2, var_3, var_4, var_5,\n                   var_6, var_7=loads_message, var_8=None,\n                   **var_9):\n    var_8 = var_8 or current_app._get_current_object()\n    var_10 = None\n    if var_5:\n        var_11 = prepare_accept_content(var_8.conf.accept_content)\n        var_12, var_13, var_10 = var_7(\n            var_4, var_5, var_6, var_11=var_11,\n        )\n    else:\n        var_12, var_13, var_10 = var_4\n    var_14 = gethostname()\n    var_3.update({\n        'var_12': var_12, 'var_13': var_13,\n        'var_14': var_14, 'is_eager': False,\n    }, **var_10 or {})\n    var_15, var_16, var_17, var_18 = trace_task(var_8.tasks[var_1],\n                               var_2, var_12, var_13, var_3, var_8=var_8)\n",
      "lines_processed": 19,
      "total_lines": 739,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "registry.py",
      "code": "def _unpickle_task(name):\n    return get_current_app().tasks[name]\n",
      "variables": [
        "name"
      ],
      "anonymized_code": "def _unpickle_task(var_1):\n    return get_current_app().tasks[var_1]\n",
      "lines_processed": 2,
      "total_lines": 68,
      "llm_code": "def _unpickle_task(task_id):\n    return get_current_app().tasks[task_id]",
      "llm_variables": [
        "task_id"
      ]
    },
    {
      "file_path": "registry.py",
      "code": "def _unpickle_task_v2(name, module=None):\n    if module:\n        import_module(module)\n    return get_current_app().tasks[name]\n",
      "variables": [
        "name",
        "module"
      ],
      "anonymized_code": "def _unpickle_task_v2(var_1, var_2=None):\n    if var_2:\n        import_module(var_2)\n    return get_current_app().tasks[var_1]\n",
      "lines_processed": 4,
      "total_lines": 68,
      "llm_code": "def _unpickle_task_v2(module, task_name=None):\n    if task_name:\n        import_module(task_name)\n    return get_current_app().tasks[module]",
      "llm_variables": [
        "module",
        "task_name"
      ]
    },
    {
      "file_path": "builtins.py",
      "code": "def add_accumulate_task(app):\n    \"\"\"Task used by Task.replace when replacing task with group.\"\"\"\n    @app.task(bind=True, name='celery.accumulate', shared=False, lazy=False)\n    def accumulate(self, *args, **kwargs):\n        index = kwargs.get('index')\n        return args[index] if index is not None else args\n    return accumulate\n",
      "variables": [
        "app",
        "self",
        "args",
        "kwargs",
        "index"
      ],
      "anonymized_code": "def add_accumulate_task(var_1):\n    \"\"\"Task used by Task.replace when replacing task with group.\"\"\"\n    @var_1.task(bind=True, name='celery.accumulate', shared=False, lazy=False)\n    def accumulate(var_2, *var_3, **var_4):\n        var_5 = var_4.get('var_5')\n        return var_3[var_5] if var_5 is not None else var_3\n    return accumulate\n",
      "lines_processed": 7,
      "total_lines": 187,
      "llm_code": "def add_accumulate_task(task):\n    \"\"\"Task used by Task.replace when replacing task with group.\"\"\"\n    @task.task(bind=True, name='celery.accumulate', shared=False, lazy=False)\n    def accumulate(var_2, *var_3, **var_4):\n        var_5 = var_4.get('var_5')\n        return var_3[var_5] if var_5 is not None else var_3\n    return accumulate\n",
      "llm_variables": [
        "task",
        "var_2",
        "var_3",
        "var_4",
        "var_5"
      ]
    },
    {
      "file_path": "builtins.py",
      "code": "def add_group_task(app):\n    \"\"\"No longer used, but here for backwards compatibility.\"\"\"\n    from celery.canvas import maybe_signature\n    from celery.result import result_from_tuple\n\n    @app.task(name='celery.group', bind=True, shared=False, lazy=False)\n    def group(self, tasks, result, group_id, partial_args, add_to_parent=True):\n        app = self.app\n        result = result_from_tuple(result, app)\n        # any partial args are added to all tasks in the group\n        taskit = (maybe_signature(task, app=app).clone(partial_args)\n                  for i, task in enumerate(tasks))\n        with app.producer_or_acquire() as producer:\n            [stask.apply_async(group_id=group_id, producer=producer,\n                               add_to_parent=False) for stask in taskit]\n        parent = app.current_worker_task\n        if add_to_parent and parent:\n            parent.add_trail(result)\n        return result\n",
      "variables": [
        "app",
        "self",
        "tasks",
        "result",
        "group_id",
        "partial_args",
        "add_to_parent",
        "taskit",
        "i",
        "task",
        "producer",
        "stask",
        "parent"
      ],
      "anonymized_code": "def add_group_task(var_1):\n    \"\"\"No longer used, but here for backwards compatibility.\"\"\"\n    from celery.canvas import maybe_signature\n    from celery.var_4 import result_from_tuple\n\n    @var_1.var_10(name='celery.group', bind=True, shared=False, lazy=False)\n    def group(var_2, var_3, var_4, var_5, var_6, var_7=True):\n        var_1 = var_2.var_1\n        var_4 = result_from_tuple(var_4, var_1)\n        # any partial args are added to all var_3 in the group\n        var_8 = (maybe_signature(var_10, var_1=var_1).clone(var_6)\n                  for var_9, var_10 in enumerate(var_3))\n        with var_1.producer_or_acquire() as var_11:\n            [var_12.apply_async(var_5=var_5, var_11=var_11,\n                               var_7=False) for var_12 in var_8]\n        var_13 = var_1.current_worker_task\n        if var_7 and var_13:\n            var_13.add_trail(var_4)\n        return var_4\n",
      "lines_processed": 19,
      "total_lines": 187,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "builtins.py",
      "code": "def add_chunk_task(app):\n    from celery.canvas import chunks as _chunks\n\n    @app.task(name='celery.chunks', shared=False, lazy=False)\n    def chunks(task, it, n):\n        return _chunks.apply_chunks(task, it, n)\n    return chunks\n",
      "variables": [
        "app",
        "task",
        "it",
        "n"
      ],
      "anonymized_code": "def add_chunk_task(var_1):\n    from celery.canvas import chunks as _chunks\n\n    @var_1.var_2(name='celery.chunks', shared=False, lazy=False)\n    def chunks(var_2, var_3, var_4):\n        return _chunks.apply_chunks(var_2, var_3, var_4)\n    return chunks\n",
      "lines_processed": 7,
      "total_lines": 187,
      "llm_code": "def add_chunk_task(a):\n    from celery.canvas import chunks as _chunks\n\n    @a.var_2(name=\"celery.chunks\", shared=False, lazy=False)\n    def chunks(b, c, d):\n        return _chunks.apply_chunks(b, c, d)\n    return chunks",
      "llm_variables": [
        "a",
        "b",
        "c",
        "d"
      ]
    },
    {
      "file_path": "builtins.py",
      "code": "def add_backend_cleanup_task(app):\n    \"\"\"Task used to clean up expired results.\n\n    If the configured backend requires periodic cleanup this task is also\n    automatically configured to run every day at 4am (requires\n    :program:`celery beat` to be running).\n    \"\"\"\n    @app.task(name='celery.backend_cleanup', shared=False, lazy=False)\n    def backend_cleanup():\n        app.backend.cleanup()\n    return backend_cleanup\n",
      "variables": [
        "app"
      ],
      "anonymized_code": "def add_backend_cleanup_task(var_1):\n    \"\"\"Task used to clean up expired results.\n\n    If the configured backend requires periodic cleanup this task is also\n    automatically configured to run every day at 4am (requires\n    :program:`celery beat` to be running).\n    \"\"\"\n    @var_1.task(name='celery.backend_cleanup', shared=False, lazy=False)\n    def backend_cleanup():\n        var_1.backend.cleanup()\n    return backend_cleanup\n",
      "lines_processed": 11,
      "total_lines": 187,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "backends.py",
      "code": "def by_url(backend=None, loader=None):\n    \"\"\"Get backend class by URL.\"\"\"\n    url = None\n    if backend and '://' in backend:\n        url = backend\n        scheme, _, _ = url.partition('://')\n        if '+' in scheme:\n            backend, url = url.split('+', 1)\n        else:\n            backend = scheme\n    return by_name(backend, loader), url\n",
      "variables": [
        "backend",
        "loader",
        "url",
        "scheme",
        "_"
      ],
      "anonymized_code": "def by_url(var_1=None, var_2=None):\n    \"\"\"Get var_1 class by URL.\"\"\"\n    var_3 = None\n    if var_1 and '://' in var_1:\n        var_3 = var_1\n        var_4, var_5, var_5 = var_3.partition('://')\n        if '+' in var_4:\n            var_1, var_3 = var_3.split('+', 1)\n        else:\n            var_1 = var_4\n    return by_name(var_1, var_2), var_3\n",
      "lines_processed": 11,
      "total_lines": 69,
      "llm_code": "def by_url(url=None, name=None):\n    \"\"\"Get url class by URL.\"\"\"\n    result = None\n    if url and '://' in url:\n        result = url\n        prefix, suffix, suffix = result.partition('://')\n        if '+' in prefix:\n            url, result = result.split('+', 1)\n        else:\n            url = prefix\n    return by_name(url, name), result\n",
      "llm_variables": [
        "url",
        "name",
        "result",
        "prefix",
        "suffix"
      ]
    },
    {
      "file_path": "backends.py",
      "code": "def by_name(backend=None, loader=None,\n            extension_namespace='celery.result_backends'):\n    \"\"\"Get backend class by name/alias.\"\"\"\n    backend = backend or 'disabled'\n    loader = loader or current_app.loader\n    aliases = dict(BACKEND_ALIASES, **loader.override_backends)\n    aliases.update(load_extension_class_names(extension_namespace))\n    try:\n        cls = symbol_by_name(backend, aliases)\n    except ValueError as exc:\n        reraise(ImproperlyConfigured, ImproperlyConfigured(\n            UNKNOWN_BACKEND.strip().format(backend, exc)), sys.exc_info()[2])\n    if isinstance(cls, types.ModuleType):\n        raise ImproperlyConfigured(UNKNOWN_BACKEND.strip().format(\n            backend, 'is a Python module, not a backend class.'))\n    return cls\n",
      "variables": [
        "backend",
        "loader",
        "extension_namespace",
        "aliases",
        "cls"
      ],
      "anonymized_code": "def by_name(var_1=None, var_2=None,\n            var_3='celery.result_backends'):\n    \"\"\"Get var_1 class by name/alias.\"\"\"\n    var_1 = var_1 or 'disabled'\n    var_2 = var_2 or current_app.var_2\n    var_4 = dict(BACKEND_ALIASES, **var_2.override_backends)\n    var_4.update(load_extension_class_names(var_3))\n    try:\n        var_5 = symbol_by_name(var_1, var_4)\n    except ValueError as exc:\n        reraise(ImproperlyConfigured, ImproperlyConfigured(\n            UNKNOWN_BACKEND.strip().format(var_1, exc)), sys.exc_info()[2])\n    if isinstance(var_5, types.ModuleType):\n        raise ImproperlyConfigured(UNKNOWN_BACKEND.strip().format(\n            var_1, 'is a Python module, not a var_1 class.'))\n    return var_5\n",
      "lines_processed": 16,
      "total_lines": 69,
      "llm_code": "def by_name(name=None, alias=None,\n            result_backend='celery.result_backends'):\n    \"\"\"Get name class by name/alias.\"\"\"\n    name = name or 'disabled'\n    alias = alias or current_app.alias\n    backends = dict(BACKEND_ALIASES, **alias.override_backends)\n    backends.update(load_extension_class_names(result_backend))\n    try:\n        cls = symbol_by_name(name, backends)\n    except ValueError as exc:\n        reraise(ImproperlyConfigured, ImproperlyConfigured(\n            UNKNOWN_BACKEND.strip().format(name, exc)), sys.exc_info()[2])\n    if isinstance(cls, types.ModuleType):\n        raise ImproperlyConfigured(UNKNOWN_BACKEND.strip().format(\n            name, 'is a Python module, not a name class.'))\n    return cls\n",
      "llm_variables": [
        "name",
        "alias",
        "result_backend",
        "backends",
        "cls"
      ]
    },
    {
      "file_path": "_state.py",
      "code": "def get_current_worker_task():\n    \"\"\"Currently executing task, that was applied by the worker.\n\n    This is used to differentiate between the actual task\n    executed by the worker and any task that was called within\n    a task (using ``task.__call__`` or ``task.apply``)\n    \"\"\"\n    for task in reversed(_task_stack.stack):\n        if not task.request.called_directly:\n            return task\n",
      "variables": [
        "task"
      ],
      "anonymized_code": "def get_current_worker_task():\n    \"\"\"Currently executing var_1, that was applied by the worker.\n\n    This is used to differentiate between the actual var_1\n    executed by the worker and any var_1 that was called within\n    a var_1 (using ``var_1.__call__`` or ``var_1.apply``)\n    \"\"\"\n    for var_1 in reversed(_task_stack.stack):\n        if not var_1.request.called_directly:\n            return var_1\n",
      "lines_processed": 10,
      "total_lines": 197,
      "llm_code": "def get_current_worker_task():\n    \"\"\"Currently executing task, that was applied by the worker.\n\n    This is used to differentiate between the actual task\n    executed by the worker and any task that was called within\n    a task (using ``task.__call__`` or ``task.apply``)\n    \"\"\"\n    for task in reversed(_task_stack.stack):\n        if not task.request.called_directly:\n            return task\n",
      "llm_variables": [
        "task"
      ]
    },
    {
      "file_path": "_state.py",
      "code": "def enable_trace():\n    \"\"\"Enable tracing of app instances.\"\"\"\n    global app_or_default\n    app_or_default = _app_or_default_trace\n",
      "variables": [
        "app_or_default"
      ],
      "anonymized_code": "def enable_trace():\n    \"\"\"Enable tracing of app instances.\"\"\"\n    global var_1\n    var_1 = _app_or_default_trace\n",
      "lines_processed": 4,
      "total_lines": 197,
      "llm_code": "def enable_trace():\n    \"\"\"Enable tracing of app instances.\"\"\"\n    global trace\n    trace = _app_or_default_trace\n",
      "llm_variables": [
        "trace"
      ]
    },
    {
      "file_path": "_state.py",
      "code": "def disable_trace():\n    \"\"\"Disable tracing of app instances.\"\"\"\n    global app_or_default\n    app_or_default = _app_or_default\n",
      "variables": [
        "app_or_default"
      ],
      "anonymized_code": "def disable_trace():\n    \"\"\"Disable tracing of app instances.\"\"\"\n    global var_1\n    var_1 = _app_or_default\n",
      "lines_processed": 4,
      "total_lines": 197,
      "llm_code": "def disable_trace():\n    \"\"\"Disable tracing of app instances.\"\"\"\n    global app\n    app = _app_or_default\n",
      "llm_variables": [
        "app"
      ]
    },
    {
      "file_path": "_state.py",
      "code": "def _app_or_default(app=None):\n    if app is None:\n        return get_current_app()\n    return app\n",
      "variables": [
        "app"
      ],
      "anonymized_code": "def _app_or_default(var_1=None):\n    if var_1 is None:\n        return get_current_app()\n    return var_1\n",
      "lines_processed": 4,
      "total_lines": 197,
      "llm_code": "def _app_or_default(app=None):\n    if app is None:\n        return get_current_app()\n    return app",
      "llm_variables": [
        "app"
      ]
    },
    {
      "file_path": "_state.py",
      "code": "def set_default_app(app):\n    \"\"\"Set default app.\"\"\"\n    global default_app\n    default_app = app\n",
      "variables": [
        "app",
        "default_app"
      ],
      "anonymized_code": "def set_default_app(var_1):\n    \"\"\"Set default var_1.\"\"\"\n    global var_2\n    var_2 = var_1\n",
      "lines_processed": 4,
      "total_lines": 197,
      "llm_code": "def set_default_app(default_app):\n    \"\"\"Set default default_app.\"\"\"\n    global current_app\n    current_app = default_app\n",
      "llm_variables": [
        "default_app",
        "current_app"
      ]
    },
    {
      "file_path": "_state.py",
      "code": "def _announce_app_finalized(app):\n    callbacks = set(_on_app_finalizers)\n    for callback in callbacks:\n        callback(app)\n",
      "variables": [
        "app",
        "callbacks",
        "callback"
      ],
      "anonymized_code": "def _announce_app_finalized(var_1):\n    var_2 = set(_on_app_finalizers)\n    for var_3 in var_2:\n        var_3(var_1)\n",
      "lines_processed": 4,
      "total_lines": 197,
      "llm_code": "def _announce_app_finalized(finalized_app):\n    finalizers = set(_on_app_finalizers)\n    for finalizer in finalizers:\n        finalizer(finalized_app)",
      "llm_variables": [
        "finalized_app",
        "finalizers",
        "finalizer"
      ]
    },
    {
      "file_path": "_state.py",
      "code": "def _app_or_default_trace(app=None):  # pragma: no cover\n    from traceback import print_stack\n    try:\n        from billiard.process import current_process\n    except ImportError:\n        current_process = None\n    if app is None:\n        if getattr(_tls, 'current_app', None):\n            print('-- RETURNING TO CURRENT APP --')  # +\n            print_stack()\n            return _tls.current_app\n        if not current_process or current_process()._name == 'MainProcess':\n            raise Exception('DEFAULT APP')\n        print('-- RETURNING TO DEFAULT APP --')      # +\n        print_stack()\n        return default_app\n    return app\n",
      "variables": [
        "app",
        "current_process"
      ],
      "anonymized_code": "def _app_or_default_trace(var_1=None):  # pragma: no cover\n    from traceback import print_stack\n    try:\n        from billiard.process import var_2\n    except ImportError:\n        var_2 = None\n    if var_1 is None:\n        if getattr(_tls, 'current_app', None):\n            print('-- RETURNING TO CURRENT APP --')  # +\n            print_stack()\n            return _tls.current_app\n        if not var_2 or var_2()._name == 'MainProcess':\n            raise Exception('DEFAULT APP')\n        print('-- RETURNING TO DEFAULT APP --')      # +\n        print_stack()\n        return default_app\n    return var_1\n",
      "lines_processed": 17,
      "total_lines": 197,
      "llm_code": "def _app_or_default_trace(current_app=None):  # pragma: no cover\n    from traceback import print_stack\n    try:\n        from billiard.process import process\n    except ImportError:\n        process = None\n    if current_app is None:\n        if getattr(_tls, 'current_app', None):\n            print('-- RETURNING TO CURRENT APP --')  # +\n            print_stack()\n            return _tls.current_app\n        if not process or process()._name == 'MainProcess':\n            raise Exception('DEFAULT APP')\n        print('-- RETURNING TO DEFAULT APP --')      # +\n        print_stack()\n        return default_app\n    return current_app\n",
      "llm_variables": [
        "current_app",
        "process"
      ]
    },
    {
      "file_path": "utils.py",
      "code": "def _unpickle_app_v2(cls, kwargs):\n    \"\"\"Rebuild app for versions 3.1+.\"\"\"\n    kwargs['set_as_current'] = False\n    return cls(**kwargs)\n",
      "variables": [
        "cls",
        "kwargs"
      ],
      "anonymized_code": "def _unpickle_app_v2(var_1, var_2):\n    \"\"\"Rebuild app for versions 3.1+.\"\"\"\n    var_2['set_as_current'] = False\n    return var_1(**var_2)\n",
      "lines_processed": 4,
      "total_lines": 415,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "defaults.py",
      "code": "def find_deprecated_settings(source):  # pragma: no cover\n    from celery.utils import deprecated\n    for name, opt in flatten(NAMESPACES):\n        if (opt.deprecate_by or opt.remove_by) and getattr(source, name, None):\n            deprecated.warn(description=f'The {name!r} setting',\n                            deprecation=opt.deprecate_by,\n                            removal=opt.remove_by,\n                            alternative=f'Use the {opt.alt} instead')\n    return source\n",
      "variables": [
        "source",
        "name",
        "opt"
      ],
      "anonymized_code": "def find_deprecated_settings(var_1):  # pragma: no cover\n    from celery.utils import deprecated\n    for var_2, var_3 in flatten(NAMESPACES):\n        if (var_3.deprecate_by or var_3.remove_by) and getattr(var_1, var_2, None):\n            deprecated.warn(description=f'The {var_2!r} setting',\n                            deprecation=var_3.deprecate_by,\n                            removal=var_3.remove_by,\n                            alternative=f'Use the {var_3.alt} instead')\n    return var_1\n",
      "lines_processed": 9,
      "total_lines": 427,
      "llm_code": "def find_deprecated_settings(settings):  # pragma: no cover\n    from celery.utils import deprecated\n    for setting_name, deprecation_info in flatten(NAMESPACES):\n        if (deprecation_info.deprecate_by or deprecation_info.remove_by) and getattr(settings, setting_name, None):\n            deprecated.warn(description=f'The {setting_name!r} setting',\n                            deprecation=deprecation_info.deprecate_by,\n                            removal=deprecation_info.remove_by,\n                            alternative=f'Use the {deprecation_info.alt} instead')\n    return settings\n",
      "llm_variables": [
        "settings",
        "setting_name",
        "deprecation_info"
      ]
    },
    {
      "file_path": "defaults.py",
      "code": "def find(name, namespace='celery'):\n    \"\"\"Find setting by name.\"\"\"\n    # - Try specified name-space first.\n    namespace = namespace.lower()\n    try:\n        return searchresult(\n            namespace, name.lower(), NAMESPACES[namespace][name.lower()],\n        )\n    except KeyError:\n        # - Try all the other namespaces.\n        for ns, opts in NAMESPACES.items():\n            if ns.lower() == name.lower():\n                return searchresult(None, ns, opts)\n            elif isinstance(opts, dict):\n                try:\n                    return searchresult(ns, name.lower(), opts[name.lower()])\n                except KeyError:\n                    pass\n    # - See if name is a qualname last.\n",
      "variables": [
        "name",
        "namespace",
        "ns",
        "opts"
      ],
      "anonymized_code": "def find(var_1, var_2='celery'):\n    \"\"\"Find setting by var_1.\"\"\"\n    # - Try specified var_1-space first.\n    var_2 = var_2.lower()\n    try:\n        return searchresult(\n            var_2, var_1.lower(), NAMESPACES[var_2][var_1.lower()],\n        )\n    except KeyError:\n        # - Try all the other namespaces.\n        for var_3, var_4 in NAMESPACES.items():\n            if var_3.lower() == var_1.lower():\n                return searchresult(None, var_3, var_4)\n            elif isinstance(var_4, dict):\n                try:\n                    return searchresult(var_3, var_1.lower(), var_4[var_1.lower()])\n                except KeyError:\n                    pass\n    # - See if var_1 is a qualname last.\n",
      "lines_processed": 19,
      "total_lines": 427,
      "llm_code": "def find(name, namespace='celery'):\n    \"\"\"Find setting by name.\"\"\"\n    # - Try specified name-space first.\n    namespace = namespace.lower()\n    try:\n        return searchresult(\n            namespace, name.lower(), NAMESPACES[namespace][name.lower()],\n        )\n    except KeyError:\n        # - Try all the other namespaces.\n        for other_namespace, namespaces in NAMESPACES.items():\n            if other_namespace.lower() == name.lower():\n                return searchresult(None, other_namespace, namespaces)\n            elif isinstance(namespaces, dict):\n                try:\n                    return searchresult(other_namespace, name.lower(), namespaces[name.lower()])\n                except KeyError:\n                    pass\n    # - See if name is a qualname last.\n",
      "llm_variables": [
        "name",
        "namespace",
        "other_namespace",
        "namespaces"
      ]
    },
    {
      "file_path": "defaults.py",
      "code": "def flatten(d, root='', keyfilter=_flatten_keys):\n    \"\"\"Flatten settings.\"\"\"\n    stack = deque([(root, d)])\n    while stack:\n        ns, options = stack.popleft()\n        for key, opt in options.items():\n            if isinstance(opt, dict):\n                stack.append((ns + key + '_', opt))\n            else:\n                yield from keyfilter(ns, key, opt)\n",
      "variables": [
        "d",
        "root",
        "keyfilter",
        "stack",
        "ns",
        "options",
        "key",
        "opt"
      ],
      "anonymized_code": "def flatten(var_1, var_2='', var_3=_flatten_keys):\n    \"\"\"Flatten settings.\"\"\"\n    var_4 = deque([(var_2, var_1)])\n    while var_4:\n        var_5, var_6 = var_4.popleft()\n        for var_7, var_8 in var_6.items():\n            if isinstance(var_8, dict):\n                var_4.append((var_5 + var_7 + '_', var_8))\n            else:\n                yield from var_3(var_5, var_7, var_8)\n",
      "lines_processed": 10,
      "total_lines": 427,
      "llm_code": "def flatten(settings, prefix='', flatten_keys=_flatten_keys):\n    \"\"\"Flatten settings.\"\"\"\n    queue = deque([(prefix, settings)])\n    while queue:\n        current_prefix, current_dict = queue.popleft()\n        for key, value in current_dict.items():\n            if isinstance(value, dict):\n                queue.append((current_prefix + key + '_', value))\n            else:\n                yield from flatten_keys(current_prefix, key, value)\n",
      "llm_variables": [
        "settings",
        "prefix",
        "flatten_keys",
        "queue",
        "current_prefix",
        "current_dict",
        "key",
        "value"
      ]
    },
    {
      "file_path": "defaults.py",
      "code": "def _flatten_keys(ns, key, opt):\n    return [(ns + key, opt)]\n",
      "variables": [
        "ns",
        "key",
        "opt"
      ],
      "anonymized_code": "def _flatten_keys(var_1, var_2, var_3):\n    return [(var_1 + var_2, var_3)]\n",
      "lines_processed": 2,
      "total_lines": 427,
      "llm_code": "def _flatten_keys(keys, values, depth):\n    return [(keys + values, depth)]",
      "llm_variables": [
        "keys",
        "values",
        "depth"
      ]
    },
    {
      "file_path": "defaults.py",
      "code": "def _to_compat(ns, key, opt):\n    if opt.old:\n        return [\n            (oldkey.format(key).upper(), ns + key, opt)\n            for oldkey in opt.old\n        ]\n    return [((ns + key).upper(), ns + key, opt)]\n",
      "variables": [
        "ns",
        "key",
        "opt",
        "oldkey"
      ],
      "anonymized_code": "def _to_compat(var_1, var_2, var_3):\n    if var_3.old:\n        return [\n            (var_4.format(var_2).upper(), var_1 + var_2, var_3)\n            for var_4 in var_3.old\n        ]\n    return [((var_1 + var_2).upper(), var_1 + var_2, var_3)]\n",
      "lines_processed": 7,
      "total_lines": 427,
      "llm_code": "def _to_compat(a, b, c):\n    if c.old:\n        return [\n            (d.format(b).upper(), a + b, c)\n            for d in c.old\n        ]\n    return [((a + b).upper(), a + b, c)]",
      "llm_variables": [
        "a",
        "b",
        "c",
        "d"
      ]
    },
    {
      "file_path": "defaults.py",
      "code": "def old_ns(ns):\n    return {f'{ns}_{{0}}'}\n",
      "variables": [
        "ns"
      ],
      "anonymized_code": "def old_ns(var_1):\n    return {f'{var_1}_{{0}}'}\n",
      "lines_processed": 2,
      "total_lines": 427,
      "llm_code": "def old_ns(name):\n    return {f'{name}_{{0}}'}\n",
      "llm_variables": [
        "name"
      ]
    },
    {
      "file_path": "defaults.py",
      "code": "def Namespace(__old__=None, **options):\n    if __old__ is not None:\n        for key, opt in options.items():\n            if not opt.old:\n                opt.old = {o.format(key) for o in __old__}\n    return options\n",
      "variables": [
        "__old__",
        "options",
        "key",
        "opt",
        "o"
      ],
      "anonymized_code": "def Namespace(var_1=None, **var_2):\n    if var_1 is not None:\n        for var_3, var_4 in var_2.items():\n            if not var_4.old:\n                var_4.old = {var_5.format(var_3) for var_5 in var_1}\n    return var_2\n",
      "lines_processed": 6,
      "total_lines": 427,
      "llm_code": "def Namespace(config=None, **kwargs):\n    if config is not None:\n        for key, value in kwargs.items():\n            if not value.old:\n                value.old = {fmt.format(key) for fmt in config}\n    return kwargs",
      "llm_variables": [
        "config",
        "kwargs",
        "key",
        "value",
        "fmt"
      ]
    },
    {
      "file_path": "annotations.py",
      "code": "def resolve_all(anno, task):\n    \"\"\"Resolve all pending annotations.\"\"\"\n    return (x for x in (_first_match(anno, task), _first_match_any(anno)) if x)\n",
      "variables": [
        "anno",
        "task",
        "x"
      ],
      "anonymized_code": "def resolve_all(var_1, var_2):\n    \"\"\"Resolve all pending annotations.\"\"\"\n    return (var_3 for var_3 in (_first_match(var_1, var_2), _first_match_any(var_1)) if var_3)\n",
      "lines_processed": 3,
      "total_lines": 52,
      "llm_code": "def resolve_all(a, b):\n    \"\"\"Resolve all pending annotations.\"\"\"\n    return (var_3 for var_3 in (_first_match(a, b), _first_match_any(a)) if var_3)",
      "llm_variables": [
        "a",
        "b",
        "var_3"
      ]
    },
    {
      "file_path": "annotations.py",
      "code": "def prepare(annotations):\n    \"\"\"Expand the :setting:`task_annotations` setting.\"\"\"\n    def expand_annotation(annotation):\n        if isinstance(annotation, dict):\n            return MapAnnotation(annotation)\n        elif isinstance(annotation, str):\n            return mlazy(instantiate, annotation)\n        return annotation\n\n    if annotations is None:\n        return ()\n    elif not isinstance(annotations, (list, tuple)):\n        annotations = (annotations,)\n    return [expand_annotation(anno) for anno in annotations]\n",
      "variables": [
        "annotations",
        "annotation",
        "anno"
      ],
      "anonymized_code": "def prepare(var_1):\n    \"\"\"Expand the :setting:`task_annotations` setting.\"\"\"\n    def expand_annotation(var_2):\n        if isinstance(var_2, dict):\n            return MapAnnotation(var_2)\n        elif isinstance(var_2, str):\n            return mlazy(instantiate, var_2)\n        return var_2\n\n    if var_1 is None:\n        return ()\n    elif not isinstance(var_1, (list, tuple)):\n        var_1 = (var_1,)\n    return [expand_annotation(var_3) for var_3 in var_1]\n",
      "lines_processed": 14,
      "total_lines": 52,
      "llm_code": "def prepare(task_annotations):\n    \"\"\"Expand the :setting:`task_annotations` setting.\"\"\"\n    def expand_annotation(annotation):\n        if isinstance(annotation, dict):\n            return MapAnnotation(annotation)\n        elif isinstance(annotation, str):\n            return mlazy(instantiate, annotation)\n        return annotation\n\n    if task_annotations is None:\n        return ()\n    elif not isinstance(task_annotations, (list, tuple)):\n        task_annotations = (task_annotations,)\n    return [expand_annotation(annotation) for annotation in task_annotations]\n",
      "llm_variables": [
        "task_annotations",
        "annotation",
        "annotation"
      ]
    },
    {
      "file_path": "base.py",
      "code": "def _unpickle_appattr(reverse_name, args):\n    \"\"\"Unpickle app.\"\"\"\n    # Given an attribute name and a list of args, gets\n    # the attribute from the current app and calls it.\n    return get_current_app()._rgetattr(reverse_name)(*args)\n",
      "variables": [
        "reverse_name",
        "args"
      ],
      "anonymized_code": "def _unpickle_appattr(var_1, var_2):\n    \"\"\"Unpickle app.\"\"\"\n    # Given an attribute name and a list of var_2, gets\n    # the attribute from the current app and calls it.\n    return get_current_app()._rgetattr(var_1)(*var_2)\n",
      "lines_processed": 5,
      "total_lines": 1508,
      "llm_code": "def _unpickle_appattr(attribute_name, args):\n    \"\"\"Unpickle app.\"\"\"\n    # Given an attribute name and a list of args, gets\n    # the attribute from the current app and calls it.\n    return get_current_app()._rgetattr(attribute_name)(*args)\n",
      "llm_variables": [
        "attribute_name",
        "args"
      ]
    },
    {
      "file_path": "routes.py",
      "code": "def prepare(routes):\n    \"\"\"Expand the :setting:`task_routes` setting.\"\"\"\n\n    def expand_route(route):\n        if isinstance(route, (Mapping, list, tuple)):\n            return MapRoute(route)\n        if isinstance(route, str):\n            return mlazy(expand_router_string, route)\n        return route\n\n    if routes is None:\n        return ()\n    if not isinstance(routes, (list, tuple)):\n        routes = (routes,)\n    return [expand_route(route) for route in routes]\n",
      "variables": [
        "routes",
        "route"
      ],
      "anonymized_code": "def prepare(var_1):\n    \"\"\"Expand the :setting:`task_routes` setting.\"\"\"\n\n    def expand_route(var_2):\n        if isinstance(var_2, (Mapping, list, tuple)):\n            return MapRoute(var_2)\n        if isinstance(var_2, str):\n            return mlazy(expand_router_string, var_2)\n        return var_2\n\n    if var_1 is None:\n        return ()\n    if not isinstance(var_1, (list, tuple)):\n        var_1 = (var_1,)\n    return [expand_route(var_2) for var_2 in var_1]\n",
      "lines_processed": 15,
      "total_lines": 136,
      "llm_code": "def prepare(routes):\n    \"\"\"Expand the :setting:`task_routes` setting.\"\"\"\n\n    def expand_route(route):\n        if isinstance(route, (Mapping, list, tuple)):\n            return MapRoute(route)\n        if isinstance(route, str):\n            return mlazy(expand_router_string, route)\n        return route\n\n    if routes is None:\n        return ()\n    if not isinstance(routes, (list, tuple)):\n        routes = (routes,)\n    return [expand_route(route) for route in routes]\n",
      "llm_variables": [
        "routes",
        "route"
      ]
    },
    {
      "file_path": "routes.py",
      "code": "def expand_router_string(router):\n    router = symbol_by_name(router)\n    if hasattr(router, 'route_for_task'):\n        # need to instantiate pre 4.0 router classes\n        router = router()\n    return router\n",
      "variables": [
        "router"
      ],
      "anonymized_code": "def expand_router_string(var_1):\n    var_1 = symbol_by_name(var_1)\n    if hasattr(var_1, 'route_for_task'):\n        # need to instantiate pre 4.0 var_1 classes\n        var_1 = var_1()\n    return var_1\n",
      "lines_processed": 6,
      "total_lines": 136,
      "llm_code": "def expand_router_string(router_name):\n    router_name = symbol_by_name(router_name)\n    if hasattr(router_name, \"route_for_task\"):\n        # need to instantiate pre 4.0 router_name classes\n        router_name = router_name()\n    return router_name",
      "llm_variables": [
        "router_name"
      ]
    }
  ],
  "hpcaitech_ColossalAI": [
    {
      "file_path": "generate_release_draft.py",
      "code": "def parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--out\", type=str, help=\"output path for the release draft\", required=True)\n    parser.add_argument(\"--version\", type=str, help=\"current version to release\", required=True)\n    return parser.parse_args()\n",
      "variables": [
        "parser"
      ],
      "anonymized_code": "def parse_args():\n    var_1 = argparse.ArgumentParser()\n    var_1.add_argument(\"--out\", type=str, help=\"output path for the release draft\", required=True)\n    var_1.add_argument(\"--version\", type=str, help=\"current version to release\", required=True)\n    return var_1.parse_args()\n",
      "lines_processed": 5,
      "total_lines": 131,
      "llm_code": "def parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--out\", type=str, help=\"output path for the release draft\", required=True)\n    parser.add_argument(\"--version\", type=str, help=\"current version to release\", required=True)\n    return parser.parse_args()\n",
      "llm_variables": [
        "parser"
      ]
    },
    {
      "file_path": "generate_release_draft.py",
      "code": "def collate_release_info(commit_info_list):\n    results = dict()\n    pattern = pattern = r\"\\[.*\\]\"\n\n    for commit_info in commit_info_list:\n        author = commit_info[\"commit\"][\"author\"][\"name\"]\n\n        try:\n            author_url = commit_info[\"author\"][\"url\"]\n        except:\n            # author can be None\n            author_url = None\n        msg = commit_info[\"commit\"][\"message\"]\n        match = re.search(pattern, msg)\n\n        if match:\n            tag = match.group().lstrip(\"[\").rstrip(\"]\").capitalize()\n            if tag not in results:\n                results[tag] = []\n",
      "variables": [
        "commit_info_list",
        "results",
        "pattern",
        "commit_info",
        "author",
        "author_url",
        "msg",
        "match",
        "tag"
      ],
      "anonymized_code": "def collate_release_info(var_1):\n    var_2 = dict()\n    var_3 = var_3 = r\"\\[.*\\]\"\n\n    for var_4 in var_1:\n        var_5 = var_4[\"commit\"][\"var_5\"][\"name\"]\n\n        try:\n            var_6 = var_4[\"var_5\"][\"url\"]\n        except:\n            # var_5 can be None\n            var_6 = None\n        var_7 = var_4[\"commit\"][\"message\"]\n        var_8 = re.search(var_3, var_7)\n\n        if var_8:\n            var_9 = var_8.group().lstrip(\"[\").rstrip(\"]\").capitalize()\n            if var_9 not in var_2:\n                var_2[var_9] = []\n",
      "lines_processed": 19,
      "total_lines": 131,
      "llm_code": "def collate_release_info(release):\n    result = dict()\n    pattern = r\"\\[.*\\]\"\n\n    for item in release:\n        version = item[\"commit\"][\"version\"][\"name\"]\n\n        try:\n            url = item[\"version\"][\"url\"]\n        except:\n            # version can be None\n            url = None\n        message = item[\"commit\"][\"message\"]\n        match = re.search(pattern, message)\n\n        if match:\n            tag = match.group().lstrip(\"[\").rstrip(\")\").capitalize()\n            if tag not in result:\n                result[tag] = []",
      "llm_variables": [
        "release",
        "result",
        "pattern",
        "item",
        "version",
        "url",
        "message",
        "match",
        "tag"
      ]
    },
    {
      "file_path": "generate_release_draft.py",
      "code": "def get_commit_info(commit_hash, headers=None):\n    api = f\"{COMMIT_API}/{commit_hash}\"\n    res = requests.get(url=api, headers=headers)\n    return res.json()\n",
      "variables": [
        "commit_hash",
        "headers",
        "api",
        "res"
      ],
      "anonymized_code": "def get_commit_info(var_1, var_2=None):\n    var_3 = f\"{COMMIT_API}/{var_1}\"\n    var_4 = requests.get(url=var_3, var_2=var_2)\n    return var_4.json()\n",
      "lines_processed": 4,
      "total_lines": 131,
      "llm_code": "def get_commit_info(commit_id, branch=None):\n    url = f\"{COMMIT_API}/{commit_id}\"\n    response = requests.get(url=url, branch=branch)\n    return response.json()\n",
      "llm_variables": [
        "commit_id",
        "branch",
        "url",
        "response"
      ]
    },
    {
      "file_path": "generate_release_draft.py",
      "code": "def get_all_commit_info(since, headers=None):\n    page = 1\n    results = []\n\n    while True:\n        api = f\"{COMMIT_API}?since={since}&per_page=100&page={page}\"\n        resp = requests.get(url=api, headers=headers)\n        data = resp.json()\n\n        # exit when no more data\n        if len(data) == 0:\n            break\n\n        results.extend(data)\n        page += 1\n\n    return results\n",
      "variables": [
        "since",
        "headers",
        "page",
        "results",
        "api",
        "resp",
        "data"
      ],
      "anonymized_code": "def get_all_commit_info(var_1, var_2=None):\n    var_3 = 1\n    var_4 = []\n\n    while True:\n        var_5 = f\"{COMMIT_API}?var_1={var_1}&per_page=100&var_3={var_3}\"\n        var_6 = requests.get(url=var_5, var_2=var_2)\n        var_7 = var_6.json()\n\n        # exit when no more var_7\n        if len(var_7) == 0:\n            break\n\n        var_4.extend(var_7)\n        var_3 += 1\n\n    return var_4\n",
      "lines_processed": 17,
      "total_lines": 131,
      "llm_code": "def get_all_commit_info(page_number, per_page=None):\n    offset = 1\n    results = []\n\n    while True:\n        url = f\"{COMMIT_API}?page_number={page_number}&per_page=100&offset={offset}\"\n        response = requests.get(url=url, per_page=per_page)\n        data = response.json()\n\n        # exit when no more data\n        if len(data) == 0:\n            break\n\n        results.extend(data)\n        offset += 1\n\n    return results",
      "llm_variables": [
        "page_number",
        "per_page",
        "offset",
        "results",
        "url",
        "response",
        "data"
      ]
    },
    {
      "file_path": "generate_release_draft.py",
      "code": "def get_latest_tag_commit(headers=None):\n    res = requests.get(url=TAGS_API, headers=headers)\n    data = res.json()\n    commit_hash = data[0][\"commit\"][\"sha\"]\n    version = data[0][\"name\"]\n    return commit_hash, version\n",
      "variables": [
        "headers",
        "res",
        "data",
        "commit_hash",
        "version"
      ],
      "anonymized_code": "def get_latest_tag_commit(var_1=None):\n    var_2 = requests.get(url=TAGS_API, var_1=var_1)\n    var_3 = var_2.json()\n    var_4 = var_3[0][\"commit\"][\"sha\"]\n    var_5 = var_3[0][\"name\"]\n    return var_4, var_5\n",
      "lines_processed": 6,
      "total_lines": 131,
      "llm_code": "def get_latest_tag_commit(fetch_params=None):\n    response = requests.get(url=TAGS_API, fetch_params=fetch_params)\n    data = response.json()\n    commit_sha = data[0][\"commit\"][\"sha\"]\n    tag_name = data[0][\"name\"]\n    return commit_sha, tag_name\n",
      "llm_variables": [
        "fetch_params",
        "response",
        "data",
        "commit_sha",
        "tag_name"
      ]
    },
    {
      "file_path": "spliced_and_tokenized_dataset.py",
      "code": "def supervised_tokenize_sft(\n    data_point: Dict[str, str],\n    tokenizer: AutoTokenizer,\n    conversation_template: Conversation = default_conversation,\n    ignore_index: int = None,\n    max_length: int = 4096,\n) -> Dict[str, Union[int, str, List[int]]]:\n    \"\"\"\n    A tokenization function to tokenize an original supervised data point as following:\n        {\"messages\": [{\"from\": \"human\", \"content\": \"xxx\"}, {\"from\": \"assistant\", \"content\": \"xxx\"}]}\n    \"\"\"\n    assert tokenizer.add_bos_token is False and tokenizer.add_eos_token is False, (\n        \"Initially set `tokenizer.add_bos_token` and `tokenizer.add_eos_token` to False, \"\n        \"add <bos> and <eos> manually later\"\n    )\n\n    assert (\n        tokenizer.bos_token == conversation_template.seps[0] and tokenizer.eos_token == conversation_template.seps[1]\n    ), f\"`bos_token`{tokenizer.bos_token} and `eos_token`{tokenizer.eos_token} should be the same with `conversation_template.seps`{conversation_template.seps}.\"\n",
      "variables": [
        "data_point",
        "tokenizer",
        "conversation_template",
        "ignore_index",
        "max_length"
      ],
      "anonymized_code": "def supervised_tokenize_sft(\n    var_1: Dict[str, str],\n    var_2: AutoTokenizer,\n    var_3: Conversation = default_conversation,\n    var_4: int = None,\n    var_5: int = 4096,\n) -> Dict[str, Union[int, str, List[int]]]:\n    \"\"\"\n    A tokenization function to tokenize an original supervised data point as following:\n        {\"messages\": [{\"from\": \"human\", \"content\": \"xxx\"}, {\"from\": \"assistant\", \"content\": \"xxx\"}]}\n    \"\"\"\n    assert var_2.add_bos_token is False and var_2.add_eos_token is False, (\n        \"Initially set `var_2.add_bos_token` and `var_2.add_eos_token` to False, \"\n        \"add <bos> and <eos> manually later\"\n    )\n\n    assert (\n        var_2.bos_token == var_3.seps[0] and var_2.eos_token == var_3.seps[1]\n    ), f\"`bos_token`{var_2.bos_token} and `eos_token`{var_2.eos_token} should be the same with `var_3.seps`{var_3.seps}.\"\n",
      "lines_processed": 19,
      "total_lines": 301,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "spliced_and_tokenized_dataset.py",
      "code": "def supervised_tokenize_pretrain(\n    data_point: Dict[str, str], tokenizer: LlamaTokenizer, ignore_index: int = None, max_length: int = 4096\n) -> Dict[str, Union[int, str, List[int]]]:\n    \"\"\"\n    A tokenization function to tokenize an original pretraining data point as following:\n        {\"source\": \"\", \"target\": \"Beijing, the capital of the People's Republic of China, ...\", \"category\": \"geography\"}\n    \"\"\"\n    assert tokenizer.add_bos_token is False and tokenizer.add_eos_token is False, (\n        \"Initially set `tokenizer.add_bos_token` and `tokenizer.add_eos_token` to False, \"\n        \"add <bos> and <eos> manually later\"\n    )\n    if ignore_index is None:\n        ignore_index = IGNORE_INDEX\n\n    source_text = data_point[\"source\"]  # `str`\n    target_text = data_point[\"target\"]  # `str`\n    is_null_source = len(source_text) == 0\n\n    source_text = tokenizer.bos_token + source_text\n",
      "variables": [
        "data_point",
        "tokenizer",
        "ignore_index",
        "max_length",
        "source_text",
        "target_text",
        "is_null_source"
      ],
      "anonymized_code": "def supervised_tokenize_pretrain(\n    var_1: Dict[str, str], var_2: LlamaTokenizer, var_3: int = None, var_4: int = 4096\n) -> Dict[str, Union[int, str, List[int]]]:\n    \"\"\"\n    A tokenization function to tokenize an original pretraining data point as following:\n        {\"source\": \"\", \"target\": \"Beijing, the capital of the People's Republic of China, ...\", \"category\": \"geography\"}\n    \"\"\"\n    assert var_2.add_bos_token is False and var_2.add_eos_token is False, (\n        \"Initially set `var_2.add_bos_token` and `var_2.add_eos_token` to False, \"\n        \"add <bos> and <eos> manually later\"\n    )\n    if var_3 is None:\n        var_3 = IGNORE_INDEX\n\n    var_5 = var_1[\"source\"]  # `str`\n    var_6 = var_1[\"target\"]  # `str`\n    var_7 = len(var_5) == 0\n\n    var_5 = var_2.bos_token + var_5\n",
      "lines_processed": 19,
      "total_lines": 301,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "update_setup_for_nightly.py",
      "code": "def replace_nightly_package_info(file_lines):\n    version = datetime.today().strftime(\"%Y.%m.%d\")\n    package_name = \"colossalai-nightly\"\n\n    for idx, line in enumerate(file_lines):\n        if \"version = get_version()\" in line:\n            file_lines[idx] = f'version = \"{version}\"\\n'\n        if 'package_name = \"colossalai\"' in line:\n            file_lines[idx] = f'package_name = \"{package_name}\"\\n'\n    return file_lines\n",
      "variables": [
        "file_lines",
        "version",
        "package_name",
        "idx",
        "line"
      ],
      "anonymized_code": "def replace_nightly_package_info(var_1):\n    var_2 = datetime.today().strftime(\"%Y.%m.%d\")\n    var_3 = \"colossalai-nightly\"\n\n    for var_4, var_5 in enumerate(var_1):\n        if \"var_2 = get_version()\" in var_5:\n            var_1[var_4] = f'var_2 = \"{var_2}\"\\n'\n        if 'var_3 = \"colossalai\"' in var_5:\n            var_1[var_4] = f'var_3 = \"{var_3}\"\\n'\n    return var_1\n",
      "lines_processed": 10,
      "total_lines": 34,
      "llm_code": "def replace_nightly_package_info(package_info):\n    today_date = datetime.today().strftime(\"%Y.%m.%d\")\n    nightly_version = \"colossalai-nightly\"\n\n    for index, line in enumerate(package_info):\n        if \"today_date = get_version()\" in line:\n            package_info[index] = f'today_date = \"{today_date}\"\\n'\n        if 'nightly_version = \"colossalai\"' in line:\n            package_info[index] = f'nightly_version = \"{nightly_version}\"\\n'\n    return package_info\n",
      "llm_variables": [
        "package_info",
        "today_date",
        "nightly_version",
        "index",
        "line"
      ]
    },
    {
      "file_path": "update_setup_for_nightly.py",
      "code": "def write_setup_file(file_lines):\n    with open(\"setup.py\", \"w\") as f:\n        f.writelines(file_lines)\n",
      "variables": [
        "file_lines",
        "f"
      ],
      "anonymized_code": "def write_setup_file(var_1):\n    with open(\"setup.py\", \"w\") as var_2:\n        var_2.writelines(var_1)\n",
      "lines_processed": 3,
      "total_lines": 34,
      "llm_code": "def write_setup_file(contents):\n    with open(\"setup.py\", \"w\") as file:\n        file.writelines(contents)\n",
      "llm_variables": [
        "contents",
        "file"
      ]
    },
    {
      "file_path": "update_setup_for_nightly.py",
      "code": "def open_setup_file():\n    with open(\"setup.py\", \"r\") as f:\n        file_lines = f.readlines()\n    return file_lines\n",
      "variables": [
        "f",
        "file_lines"
      ],
      "anonymized_code": "def open_setup_file():\n    with open(\"setup.py\", \"r\") as var_1:\n        var_2 = var_1.readlines()\n    return var_2\n",
      "lines_processed": 4,
      "total_lines": 34,
      "llm_code": "def open_setup_file():\n    with open(\"setup.py\", \"r\") as file:\n        lines = file.readlines()\n    return lines\n",
      "llm_variables": [
        "file",
        "lines"
      ]
    },
    {
      "file_path": "update_setup_for_nightly.py",
      "code": "def main():\n    file_lines = open_setup_file()\n    file_lines = replace_nightly_package_info(file_lines)\n    write_setup_file(file_lines)\n",
      "variables": [
        "file_lines"
      ],
      "anonymized_code": "def main():\n    var_1 = open_setup_file()\n    var_1 = replace_nightly_package_info(var_1)\n    write_setup_file(var_1)\n",
      "lines_processed": 4,
      "total_lines": 34,
      "llm_code": "def main():\n    setup_file = open_setup_file()\n    setup_file = replace_nightly_package_info(setup_file)\n    write_setup_file(setup_file)\n",
      "llm_variables": [
        "setup_file"
      ]
    },
    {
      "file_path": "check_doc_i18n.py",
      "code": "def compare_dirs(dir1, dir2):\n    # First, we need to check if the two directories exist\n    if not os.path.exists(dir1) or not os.path.exists(dir2):\n        return False\n\n    # Now, we compare the list of items in each directory\n    items1 = os.listdir(dir1)\n    items2 = os.listdir(dir2)\n\n    # If the number of items in each directory is different, the directories are different\n    if len(items1) != len(items2):\n        return False\n\n    # For each item in the first directory, we check if there is a corresponding item in the second directory\n    for item in items1:\n        item_path1 = os.path.join(dir1, item)\n        item_path2 = os.path.join(dir2, item)\n\n        # If the corresponding item doesn't exist in the second directory, the directories are different\n",
      "variables": [
        "dir1",
        "dir2",
        "items1",
        "items2",
        "item",
        "item_path1",
        "item_path2"
      ],
      "anonymized_code": "def compare_dirs(var_1, var_2):\n    # First, we need to check if the two directories exist\n    if not os.path.exists(var_1) or not os.path.exists(var_2):\n        return False\n\n    # Now, we compare the list of items in each directory\n    var_3 = os.listdir(var_1)\n    var_4 = os.listdir(var_2)\n\n    # If the number of items in each directory is different, the directories are different\n    if len(var_3) != len(var_4):\n        return False\n\n    # For each var_5 in the first directory, we check if there is a corresponding var_5 in the second directory\n    for var_5 in var_3:\n        var_6 = os.path.join(var_1, var_5)\n        var_7 = os.path.join(var_2, var_5)\n\n        # If the corresponding var_5 doesn't exist in the second directory, the directories are different\n",
      "lines_processed": 19,
      "total_lines": 67,
      "llm_code": "def compare_dirs(dir1, dir2):\n    # First, we need to check if the two directories exist\n    if not os.path.exists(dir1) or not os.path.exists(dir2):\n        return False\n\n    # Now, we compare the list of items in each directory\n    dir1_contents = os.listdir(dir1)\n    dir2_contents = os.listdir(dir2)\n\n    # If the number of items in each directory is different, the directories are different\n    if len(dir1_contents)!= len(dir2_contents):\n        return False\n\n    # For each item in the first directory, we check if there is a corresponding item in the second directory\n    for item in dir1_contents:\n        file_path1 = os.path.join(dir1, item)\n        file_path2 = os.path.join(dir2, item)\n\n        # If the corresponding item doesn\"t exist in the second directory, the directories are different",
      "llm_variables": [
        "dir1",
        "dir2",
        "dir1_contents",
        "dir2_contents",
        "item",
        "file_path1",
        "file_path2"
      ]
    },
    {
      "file_path": "check_dispatch_inputs.py",
      "code": "def check_inputs(input_list):\n    for path in input_list:\n        real_path = os.path.join(\"examples\", path)\n        if not os.path.exists(real_path):\n            return False\n    return True\n",
      "variables": [
        "input_list",
        "path",
        "real_path"
      ],
      "anonymized_code": "def check_inputs(var_1):\n    for var_2 in var_1:\n        var_3 = os.var_2.join(\"examples\", var_2)\n        if not os.var_2.exists(var_3):\n            return False\n    return True\n",
      "lines_processed": 6,
      "total_lines": 27,
      "llm_code": "def check_inputs(paths):\n    for path in paths:\n        var_3 = os.path.join(\"examples\", path)\n        if not os.path.exists(var_3):\n            return False\n    return True\n",
      "llm_variables": [
        "paths",
        "path",
        "var_3"
      ]
    },
    {
      "file_path": "check_dispatch_inputs.py",
      "code": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-f\", \"--fileNameList\", type=str, help=\"List of file names\")\n    args = parser.parse_args()\n    name_list = args.fileNameList.split(\",\")\n    is_correct = check_inputs(name_list)\n\n    if is_correct:\n        print(\"success\")\n    else:\n        print(\"failure\")\n",
      "variables": [
        "parser",
        "args",
        "name_list",
        "is_correct"
      ],
      "anonymized_code": "def main():\n    var_1 = argparse.ArgumentParser()\n    var_1.add_argument(\"-f\", \"--fileNameList\", type=str, help=\"List of file names\")\n    var_2 = var_1.parse_args()\n    var_3 = var_2.fileNameList.split(\",\")\n    var_4 = check_inputs(var_3)\n\n    if var_4:\n        print(\"success\")\n    else:\n        print(\"failure\")\n",
      "lines_processed": 11,
      "total_lines": 27,
      "llm_code": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-f\", \"--fileNameList\", type=str, help=\"List of file names\")\n    args = parser.parse_args()\n    file_names = args.fileNameList.split(\",\")\n    input_check = check_inputs(file_names)\n\n    if input_check:\n        print(\"success\")\n    else:\n        print(\"failure\")\n",
      "llm_variables": [
        "parser",
        "args",
        "file_names",
        "input_check"
      ]
    },
    {
      "file_path": "froze.py",
      "code": "def freeze_non_embeds_parameters(model: LlamaForCausalLM) -> None:\n    \"\"\"Freeze all parameters except embeddings.\"\"\"\n    for name, params in model.named_parameters():\n        if \"embed_tokens\" not in name and \"lm_head\" not in name:\n            params.requires_grad = False\n        else:\n            params.requires_grad = True\n",
      "variables": [
        "model",
        "name",
        "params"
      ],
      "anonymized_code": "def freeze_non_embeds_parameters(var_1: LlamaForCausalLM) -> None:\n    \"\"\"Freeze all parameters except embeddings.\"\"\"\n    for var_2, var_3 in var_1.named_parameters():\n        if \"embed_tokens\" not in var_2 and \"lm_head\" not in var_2:\n            var_3.requires_grad = False\n        else:\n            var_3.requires_grad = True\n",
      "lines_processed": 7,
      "total_lines": 18,
      "llm_code": "def freeze_non_embeds_parameters(model: LlamaForCausalLM) -> None:\n    \"\"\"Freeze all parameters except embeddings.\"\"\"\n    for name, param in model.named_parameters():\n        if \"embed_tokens\" not in name and \"lm_head\" not in name:\n            param.requires_grad = False\n        else:\n            param.requires_grad = True",
      "llm_variables": [
        "model",
        "name",
        "param"
      ]
    },
    {
      "file_path": "froze.py",
      "code": "def unfreeze_parameters(model: LlamaForCausalLM) -> None:\n    for name, params in model.named_parameters():\n        params.requires_grad = False\n",
      "variables": [
        "model",
        "name",
        "params"
      ],
      "anonymized_code": "def unfreeze_parameters(var_1: LlamaForCausalLM) -> None:\n    for var_2, var_3 in var_1.named_parameters():\n        var_3.requires_grad = False\n",
      "lines_processed": 3,
      "total_lines": 18,
      "llm_code": "def unfreeze_parameters(model: LlamaForCausalLM) -> None:\n    for param_name, param in model.named_parameters():\n        param.requires_grad = False",
      "llm_variables": [
        "model",
        "param_name",
        "param"
      ]
    },
    {
      "file_path": "neftune_patch.py",
      "code": "def activate_neftune(model, neftune_noise_alpha=0.1):\n    r\"\"\"\n    Activates the neftune as presented in this code: https://github.com/neelsjain/NEFTune and paper:\n    https://arxiv.org/abs/2310.05914\n    \"\"\"\n    embeddings = unwrap(model).get_input_embeddings()\n\n    embeddings.neftune_noise_alpha = neftune_noise_alpha\n    hook_handle = embeddings.register_forward_hook(neftune_post_forward_hook)\n    neftune_hook_handle = hook_handle\n\n    return model, neftune_hook_handle\n",
      "variables": [
        "model",
        "neftune_noise_alpha",
        "embeddings",
        "hook_handle",
        "neftune_hook_handle"
      ],
      "anonymized_code": "def activate_neftune(var_1, var_2=0.1):\n    r\"\"\"\n    Activates the neftune as presented in this code: https://github.com/neelsjain/NEFTune and paper:\n    https://arxiv.org/abs/2310.05914\n    \"\"\"\n    var_3 = unwrap(var_1).get_input_embeddings()\n\n    var_3.var_2 = var_2\n    var_4 = var_3.register_forward_hook(neftune_post_forward_hook)\n    var_5 = var_4\n\n    return var_1, var_5\n",
      "lines_processed": 12,
      "total_lines": 72,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "neftune_patch.py",
      "code": "def deactivate_neftune(model, neftune_hook_handle):\n    \"\"\"\n    Deactivates the neftune method. Make sure to call `_activate_neftune` first.\n    \"\"\"\n    embeddings = unwrap(model).get_input_embeddings()\n\n    neftune_hook_handle.remove()\n    del embeddings.neftune_noise_alpha\n",
      "variables": [
        "model",
        "neftune_hook_handle",
        "embeddings"
      ],
      "anonymized_code": "def deactivate_neftune(var_1, var_2):\n    \"\"\"\n    Deactivates the neftune method. Make sure to call `_activate_neftune` first.\n    \"\"\"\n    var_3 = unwrap(var_1).get_input_embeddings()\n\n    var_2.remove()\n    del var_3.neftune_noise_alpha\n",
      "lines_processed": 8,
      "total_lines": 72,
      "llm_code": "def deactivate_neftune(input_model, embeddings):\n    \"\"\"\n    Deactivates the neftune method. Make sure to call `_activate_neftune` first.\n    \"\"\"\n    embeddings = unwrap(input_model).get_input_embeddings()\n\n    embeddings.remove()\n    del embeddings.neftune_noise_alpha",
      "llm_variables": [
        "input_model",
        "embeddings",
        "embeddings"
      ]
    },
    {
      "file_path": "neftune_patch.py",
      "code": "def neftune_post_forward_hook(module, input, output):\n    \"\"\"\n    Implements the NEFTune forward pass for the model using forward hooks. Note this works only for torch.nn.Embedding\n    layers. This method is slightly adapted from the original source code that can be found here:\n    https://github.com/neelsjain/NEFTune Simply add it to your model as follows:\n    ```python\n    model = ...\n    model.embed_tokens.neftune_noise_alpha = 0.1\n    model.embed_tokens.register_forward_hook(neftune_post_forward_hook)\n    ```\n    Args:\n        module (`torch.nn.Module`):\n            The embedding module where the hook is attached. Note that you need to set `module.neftune_noise_alpha` to\n            the desired noise alpha value.\n        input (`torch.Tensor`):\n            The input tensor to the model.\n        output (`torch.Tensor`):\n            The output tensor of the model (i.e. the embeddings).\n    \"\"\"\n",
      "variables": [
        "module",
        "input",
        "output"
      ],
      "anonymized_code": "def neftune_post_forward_hook(var_1, var_2, var_3):\n    \"\"\"\n    Implements the NEFTune forward pass for the model using forward hooks. Note this works only for torch.nn.Embedding\n    layers. This method is slightly adapted from the original source code that can be found here:\n    https://github.com/neelsjain/NEFTune Simply add it to your model as follows:\n    ```python\n    model = ...\n    model.embed_tokens.neftune_noise_alpha = 0.1\n    model.embed_tokens.register_forward_hook(neftune_post_forward_hook)\n    ```\n    Args:\n        var_1 (`torch.nn.Module`):\n            The embedding var_1 where the hook is attached. Note that you need to set `var_1.neftune_noise_alpha` to\n            the desired noise alpha value.\n        var_2 (`torch.Tensor`):\n            The var_2 tensor to the model.\n        var_3 (`torch.Tensor`):\n            The var_3 tensor of the model (i.e. the embeddings).\n    \"\"\"\n",
      "lines_processed": 19,
      "total_lines": 72,
      "llm_code": "def neftune_post_forward_hook(embedding_layer, input_tensor, embedding_tensor):\n    \"\"\"\n    Implements the NEFTune forward pass for the model using forward hooks. Note this works only for torch.nn.Embedding\n    layers. This method is slightly adapted from the original source code that can be found here:\n    https://github.com/neelsjain/NEFTune Simply add it to your model as follows:\n    ```python\n    model = ...\n    model.embed_tokens.neftune_noise_alpha = 0.1\n    model.embed_tokens.register_forward_hook(neftune_post_forward_hook)\n    ```\n    Args:\n        embedding_layer (`torch.nn.Module`):\n            The embedding embedding_layer where the hook is attached. Note that you need to set `embedding_layer.neftune_noise_alpha` to\n            the desired noise alpha value.\n        input_tensor (`torch.Tensor`):\n            The input_tensor tensor to the model.\n        embedding_tensor (`torch.Tensor`):\n            The embedding_tensor tensor of the model (i.e. the embeddings).\n    \"\"\"\n",
      "llm_variables": [
        "embedding_layer",
        "input_tensor",
        "embedding_tensor"
      ]
    },
    {
      "file_path": "neftune_patch.py",
      "code": "def unwrap(model):\n    if hasattr(model, \"module\"):\n        return model.unwrap()\n    else:\n        return model\n",
      "variables": [
        "model"
      ],
      "anonymized_code": "def unwrap(var_1):\n    if hasattr(var_1, \"module\"):\n        return var_1.unwrap()\n    else:\n        return var_1\n",
      "lines_processed": 5,
      "total_lines": 72,
      "llm_code": "def unwrap(wrapper):\n    if hasattr(wrapper, \"module\"):\n        return wrapper.unwrap()\n    else:\n        return wrapper\n",
      "llm_variables": [
        "wrapper"
      ]
    },
    {
      "file_path": "init_model.py",
      "code": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--source_model_and_tokenizer_path\",\n        type=str,\n        required=True,\n        default=None,\n        help=\"Source path of model & tokenizer\",\n    )\n    parser.add_argument(\"--target_tokenizer_path\", type=str, required=True, default=None, help=\"Target tokenizer path\")\n    parser.add_argument(\"--target_model_path\", type=str, required=True, default=None, help=\"Target model path\")\n    args = parser.parse_args()\n\n    source_tokenizer = LlamaTokenizer.from_pretrained(args.source_model_and_tokenizer_path)\n    source_tokenizer.add_bos_token = False\n    source_tokenizer.add_eos_token = False\n    if source_tokenizer.pad_token is None:\n        source_tokenizer.pad_token = source_tokenizer.unk_token\n    source_vocab = source_tokenizer.get_vocab()\n",
      "variables": [
        "parser",
        "args",
        "source_tokenizer",
        "source_vocab"
      ],
      "anonymized_code": "def main():\n    var_1 = argparse.ArgumentParser()\n    var_1.add_argument(\n        \"--source_model_and_tokenizer_path\",\n        type=str,\n        required=True,\n        default=None,\n        help=\"Source path of model & tokenizer\",\n    )\n    var_1.add_argument(\"--target_tokenizer_path\", type=str, required=True, default=None, help=\"Target tokenizer path\")\n    var_1.add_argument(\"--target_model_path\", type=str, required=True, default=None, help=\"Target model path\")\n    var_2 = var_1.parse_args()\n\n    var_3 = LlamaTokenizer.from_pretrained(var_2.source_model_and_tokenizer_path)\n    var_3.add_bos_token = False\n    var_3.add_eos_token = False\n    if var_3.pad_token is None:\n        var_3.pad_token = var_3.unk_token\n    var_4 = var_3.get_vocab()\n",
      "lines_processed": 19,
      "total_lines": 110,
      "llm_code": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--source_model_and_tokenizer_path\",\n        type=str,\n        required=True,\n        default=None,\n        help=\"Source path of model & tokenizer\",\n    )\n    parser.add_argument(\"--target_tokenizer_path\", type=str, required=True, default=None, help=\"Target tokenizer path\")\n    parser.add_argument(\"--target_model_path\", type=str, required=True, default=None, help=\"Target model path\")\n    args = parser.parse_args()\n\n    tokenizer = LlamaTokenizer.from_pretrained(args.source_model_and_tokenizer_path)\n    tokenizer.add_bos_token = False\n    tokenizer.add_eos_token = False\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.unk_token\n    vocab = tokenizer.get_vocab()\n",
      "llm_variables": [
        "parser",
        "args",
        "tokenizer",
        "vocab"
      ]
    },
    {
      "file_path": "loader.py",
      "code": "def load_tokenized_dataset(\n    dataset_paths: Union[PathType, List[PathType]], mode: str = \"train\"\n) -> Optional[DatasetType]:\n    \"\"\"\n    Load pre-tokenized dataset.\n    Each instance of dataset is a dictionary with\n    `{'input_ids': List[int], 'labels': List[int], sequence: str}` format.\n    \"\"\"\n    mode_map = {\"train\": \"train\", \"dev\": \"validation\", \"test\": \"test\"}\n    assert mode in tuple(mode_map), f\"Unsupported mode {mode}, it must be in {tuple(mode_map)}\"\n\n    if isinstance(dataset_paths, (str, os.PathLike)):\n        dataset_paths = [dataset_paths]\n\n    datasets = []  # `List[datasets.dataset_dict.Dataset]`\n    for ds_path in dataset_paths:\n        ds_path = os.path.abspath(ds_path)\n        assert os.path.exists(ds_path), f\"Not existed file path {ds_path}\"\n        ds_dict = load_from_disk(dataset_path=ds_path, keep_in_memory=False)\n",
      "variables": [
        "dataset_paths",
        "mode",
        "mode_map",
        "datasets",
        "ds_path",
        "ds_dict"
      ],
      "anonymized_code": "def load_tokenized_dataset(\n    var_1: Union[PathType, List[PathType]], var_2: str = \"train\"\n) -> Optional[DatasetType]:\n    \"\"\"\n    Load pre-tokenized dataset.\n    Each instance of dataset is a dictionary with\n    `{'input_ids': List[int], 'labels': List[int], sequence: str}` format.\n    \"\"\"\n    var_3 = {\"train\": \"train\", \"dev\": \"validation\", \"test\": \"test\"}\n    assert var_2 in tuple(var_3), f\"Unsupported var_2 {var_2}, it must be in {tuple(var_3)}\"\n\n    if isinstance(var_1, (str, os.PathLike)):\n        var_1 = [var_1]\n\n    var_4 = []  # `List[var_4.dataset_dict.Dataset]`\n    for var_5 in var_1:\n        var_5 = os.path.abspath(var_5)\n        assert os.path.exists(var_5), f\"Not existed file path {var_5}\"\n        var_6 = load_from_disk(dataset_path=var_5, keep_in_memory=False)\n",
      "lines_processed": 19,
      "total_lines": 175,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "detect_changed_example.py",
      "code": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-f\", \"--fileNameList\", type=str, help=\"The list of changed files\")\n    args = parser.parse_args()\n    name_list = args.fileNameList.split(\":\")\n    folder_need_check = set()\n    for loc in name_list:\n        # Find only the sub-sub-folder of 'example' folder\n        # the examples folder structure is like\n        # - examples\n        #   - area\n        #     - application\n        #       - file\n        if loc.split(\"/\")[0] == \"examples\" and len(loc.split(\"/\")) >= 4:\n            folder_need_check.add(\"/\".join(loc.split(\"/\")[1:3]))\n    # Output the result using print. Then the shell can get the values.\n    print(list(folder_need_check))\n",
      "variables": [
        "parser",
        "args",
        "name_list",
        "folder_need_check",
        "loc"
      ],
      "anonymized_code": "def main():\n    var_1 = argparse.ArgumentParser()\n    var_1.add_argument(\"-f\", \"--fileNameList\", type=str, help=\"The list of changed files\")\n    var_2 = var_1.parse_args()\n    var_3 = var_2.fileNameList.split(\":\")\n    var_4 = set()\n    for var_5 in var_3:\n        # Find only the sub-sub-folder of 'example' folder\n        # the examples folder structure is like\n        # - examples\n        #   - area\n        #     - application\n        #       - file\n        if var_5.split(\"/\")[0] == \"examples\" and len(var_5.split(\"/\")) >= 4:\n            var_4.add(\"/\".join(var_5.split(\"/\")[1:3]))\n    # Output the result using print. Then the shell can get the values.\n    print(list(var_4))\n",
      "lines_processed": 17,
      "total_lines": 24,
      "llm_code": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-f\", \"--fileNameList\", type=str, help=\"The list of changed files\")\n    args = parser.parse_args()\n    files = args.fileNameList.split(\":\")\n    unique_dirs = set()\n    for file in files:\n        # Find only the sub-sub-folder of 'example' folder\n        # the examples folder structure is like\n        # - examples\n        #   - area\n        #     - application\n        #       - file\n        if file.split(\"/\")[0] == \"examples\" and len(file.split(\"/\")) >= 4:\n            unique_dirs.add(\"/\".join(file.split(\"/\")[1:3]))\n    # Output the result using print. Then the shell can get the values.\n    print(list(unique_dirs))\n",
      "llm_variables": [
        "parser",
        "args",
        "files",
        "unique_dirs",
        "file"
      ]
    },
    {
      "file_path": "generate_leaderboard_and_send_to_lark.py",
      "code": "def datetime2str(dt):\n    \"\"\"\n    Convert datetime to string in the format of YYYY-MM-DDTHH:MM:SSZ\n    \"\"\"\n    return dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
      "variables": [
        "dt"
      ],
      "anonymized_code": "def datetime2str(var_1):\n    \"\"\"\n    Convert datetime to string in the format of YYYY-MM-DDTHH:MM:SSZ\n    \"\"\"\n    return var_1.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
      "lines_processed": 5,
      "total_lines": 569,
      "llm_code": "def datetime2str(dt):\n    \"\"\"Convert datetime to string in the format of YYYY-MM-DDTHH:MM:SSZ\"\"\"\n    return dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")",
      "llm_variables": [
        "dt"
      ]
    },
    {
      "file_path": "generate_leaderboard_and_send_to_lark.py",
      "code": "def get_issue_pull_request_comments(github_token: str, org_name: str, repo_name: str, since: str) -> Dict[str, int]:\n    \"\"\"\n    Retrieve the issue/PR comments made by our members in the last 7 days.\n\n    Args:\n        github_token (str): GitHub access token for API calls\n        since (str): the path parameter required by GitHub Restful APIs, in the format of YYYY-MM-DDTHH:MM:SSZ\n    \"\"\"\n    # prepare header\n    headers = {\n        \"Authorization\": f\"Bearer {github_token}\",\n        \"Accept\": \"application/vnd.github+json\",\n        \"X-GitHub-Api-Version\": \"2022-11-28\",\n    }\n\n    user_engagement_count = {}\n\n    # do pagination to the API\n    page = 1\n",
      "variables": [
        "github_token",
        "org_name",
        "repo_name",
        "since",
        "headers",
        "user_engagement_count",
        "page"
      ],
      "anonymized_code": "def get_issue_pull_request_comments(var_1: str, var_2: str, var_3: str, var_4: str) -> Dict[str, int]:\n    \"\"\"\n    Retrieve the issue/PR comments made by our members in the last 7 days.\n\n    Args:\n        var_1 (str): GitHub access token for API calls\n        var_4 (str): the path parameter required by GitHub Restful APIs, in the format of YYYY-MM-DDTHH:MM:SSZ\n    \"\"\"\n    # prepare header\n    var_5 = {\n        \"Authorization\": f\"Bearer {var_1}\",\n        \"Accept\": \"application/vnd.github+json\",\n        \"X-GitHub-Api-Version\": \"2022-11-28\",\n    }\n\n    var_6 = {}\n\n    # do pagination to the API\n    var_7 = 1\n",
      "lines_processed": 19,
      "total_lines": 569,
      "llm_code": "def get_issue_pull_request_comments(github_token: str, repo_owner: str, repo_name: str, date: str) -> Dict[str, int]:\n    \"\"\"\n    Retrieve the issue/PR comments made by our members in the last 7 days.\n\n    Args:\n        github_token (str): GitHub access token for API calls\n        date (str): the path parameter required by GitHub Restful APIs, in the format of YYYY-MM-DDTHH:MM:SSZ\n    \"\"\"\n    # prepare header\n    headers = {\n        \"Authorization\": f\"Bearer {github_token}\",\n        \"Accept\": \"application/vnd.github+json\",\n        \"X-GitHub-Api-Version\": \"2022-11-28\",\n    }\n\n    comments = {}\n\n    # do pagination to the API\n    page = 1\n",
      "llm_variables": [
        "github_token",
        "repo_owner",
        "repo_name",
        "date",
        "headers",
        "comments",
        "page"
      ]
    },
    {
      "file_path": "generate_leaderboard_and_send_to_lark.py",
      "code": "def send_image_to_lark(image_key: str, webhook_url: str) -> None:\n    \"\"\"\n    Send image to Lark.\n\n    Args:\n        image_key (str): the image key returned by Lark\n        webhook_url (str): the webhook url to send the image\n    \"\"\"\n    data = {\"msg_type\": \"image\", \"content\": {\"image_key\": image_key}}\n    requests.post(webhook_url, json=data)\n",
      "variables": [
        "image_key",
        "webhook_url",
        "data"
      ],
      "anonymized_code": "def send_image_to_lark(var_1: str, var_2: str) -> None:\n    \"\"\"\n    Send image to Lark.\n\n    Args:\n        var_1 (str): the image key returned by Lark\n        var_2 (str): the webhook url to send the image\n    \"\"\"\n    var_3 = {\"msg_type\": \"image\", \"content\": {\"var_1\": var_1}}\n    requests.post(var_2, json=var_3)\n",
      "lines_processed": 10,
      "total_lines": 569,
      "llm_code": "def send_image_to_lark(image_key: str, webhook_url: str) -> None:\n    \"\"\"\n    Send image to Lark.\n\n    Args:\n        image_key (str): the image key returned by Lark\n        webhook_url (str): the webhook url to send the image\n    \"\"\"\n    payload = {\"msg_type\": \"image\", \"content\": {image_key: image_key}}\n    requests.post(webhook_url, json=payload)",
      "llm_variables": [
        "image_key",
        "webhook_url",
        "payload"
      ]
    },
    {
      "file_path": "generate_leaderboard_and_send_to_lark.py",
      "code": "def upload_image_to_lark(lark_tenant_token: str, image_path: str) -> str:\n    \"\"\"\n    Upload image to Lark and return the image key\n\n    Args:\n        lark_tenant_token (str): Lark tenant access token\n        image_path (str): the path to the image to be uploaded\n    \"\"\"\n    url = \"https://open.feishu.cn/open-apis/im/v1/images\"\n    form = {\"image_type\": \"message\", \"image\": (open(image_path, \"rb\"))}  # \u9700\u8981\u66ff\u6362\u5177\u4f53\u7684path\n    multi_form = MultipartEncoder(form)\n    headers = {\n        \"Authorization\": f\"Bearer {lark_tenant_token}\",  ## \u83b7\u53d6tenant_access_token, \u9700\u8981\u66ff\u6362\u4e3a\u5b9e\u9645\u7684token\n    }\n    headers[\"Content-Type\"] = multi_form.content_type\n    response = requests.request(\"POST\", url, headers=headers, data=multi_form).json()\n    return response[\"data\"][\"image_key\"]\n",
      "variables": [
        "lark_tenant_token",
        "image_path",
        "url",
        "form",
        "multi_form",
        "headers",
        "response"
      ],
      "anonymized_code": "def upload_image_to_lark(var_1: str, var_2: str) -> str:\n    \"\"\"\n    Upload image to Lark and return the image key\n\n    Args:\n        var_1 (str): Lark tenant access token\n        var_2 (str): the path to the image to be uploaded\n    \"\"\"\n    var_3 = \"https://open.feishu.cn/open-apis/im/v1/images\"\n    var_4 = {\"image_type\": \"message\", \"image\": (open(var_2, \"rb\"))}  # \u9700\u8981\u66ff\u6362\u5177\u4f53\u7684path\n    var_5 = MultipartEncoder(var_4)\n    var_6 = {\n        \"Authorization\": f\"Bearer {var_1}\",  ## \u83b7\u53d6tenant_access_token, \u9700\u8981\u66ff\u6362\u4e3a\u5b9e\u9645\u7684token\n    }\n    var_6[\"Content-Type\"] = var_5.content_type\n    var_7 = requests.request(\"POST\", var_3, var_6=var_6, data=var_5).json()\n    return var_7[\"data\"][\"image_key\"]\n",
      "lines_processed": 17,
      "total_lines": 569,
      "llm_code": "def upload_image_to_lark(tenant_access_token: str, image_path: str) -> str:\n    \"\"\"\n    Upload image to Lark and return the image key\n\n    Args:\n        tenant_access_token (str): Lark tenant access token\n        image_path (str): the path to the image to be uploaded\n    \"\"\"\n    upload_url = \"https://open.feishu.cn/open-apis/im/v1/images\"\n    image_data = {\"image_type\": \"message\", \"image\": (open(image_path, \"rb\"))}  # \u9700\u8981\u66ff\u6362\u5177\u4f53\u7684path\n    encoder = MultipartEncoder(image_data)\n    headers = {\n        \"Authorization\": f\"Bearer {tenant_access_token}\",  ## \u83b7\u53d6tenant_access_token, \u9700\u8981\u66ff\u6362\u4e3a\u5b9e\u9645\u7684token\n    }\n    headers[\"Content-Type\"] = encoder.content_type\n    response = requests.request(\"POST\", upload_url, headers=headers, data=encoder).json()\n    return response[\"data\"][\"image_key\"]\n",
      "llm_variables": [
        "tenant_access_token",
        "image_path",
        "upload_url",
        "image_data",
        "encoder",
        "headers",
        "response"
      ]
    },
    {
      "file_path": "generate_leaderboard_and_send_to_lark.py",
      "code": "def generate_lark_tenant_access_token(app_id: str, app_secret: str) -> str:\n    \"\"\"\n    Generate Lark tenant access token.\n\n    Args:\n        app_id (str): Lark app id\n        app_secret (str): Lark app secret\n    \"\"\"\n    url = \"https://open.feishu.cn/open-apis/auth/v3/tenant_access_token/internal\"\n    data = {\"app_id\": app_id, \"app_secret\": app_secret}\n    response = requests.post(url, json=data).json()\n    return response[\"tenant_access_token\"]\n",
      "variables": [
        "app_id",
        "app_secret",
        "url",
        "data",
        "response"
      ],
      "anonymized_code": "def generate_lark_tenant_access_token(var_1: str, var_2: str) -> str:\n    \"\"\"\n    Generate Lark tenant access token.\n\n    Args:\n        var_1 (str): Lark app id\n        var_2 (str): Lark app secret\n    \"\"\"\n    var_3 = \"https://open.feishu.cn/open-apis/auth/v3/tenant_access_token/internal\"\n    var_4 = {\"var_1\": var_1, \"var_2\": var_2}\n    var_5 = requests.post(var_3, json=var_4).json()\n    return var_5[\"tenant_access_token\"]\n",
      "lines_processed": 12,
      "total_lines": 569,
      "llm_code": "def generate_lark_tenant_access_token(app_id: str, app_secret: str) -> str:\n    \"\"\"\n    Generate Lark tenant access token.\n\n    Args:\n        app_id (str): Lark app id\n        app_secret (str): Lark app secret\n    \"\"\"\n    url = \"https://open.feishu.cn/open-apis/auth/v3/tenant_access_token/internal\"\n    payload = {\"app_id\": app_id, \"app_secret\": app_secret}\n    response = requests.post(url, json=payload).json()\n    return response[\"tenant_access_token\"]\n",
      "llm_variables": [
        "app_id",
        "app_secret",
        "url",
        "payload",
        "response"
      ]
    },
    {
      "file_path": "generate_leaderboard_and_send_to_lark.py",
      "code": "def send_message_to_lark(message: str, webhook_url: str):\n    \"\"\"\n    Send message to Lark.\n\n    Args:\n        message (str): the message to be sent\n        webhook_url (str): the webhook url to send the message\n    \"\"\"\n    data = {\"msg_type\": \"text\", \"content\": {\"text\": message}}\n    requests.post(webhook_url, json=data)\n",
      "variables": [
        "message",
        "webhook_url",
        "data"
      ],
      "anonymized_code": "def send_message_to_lark(var_1: str, var_2: str):\n    \"\"\"\n    Send var_1 to Lark.\n\n    Args:\n        var_1 (str): the var_1 to be sent\n        var_2 (str): the webhook url to send the var_1\n    \"\"\"\n    var_3 = {\"msg_type\": \"text\", \"content\": {\"text\": var_1}}\n    requests.post(var_2, json=var_3)\n",
      "lines_processed": 10,
      "total_lines": 569,
      "llm_code": "def send_message_to_lark(message: str, webhook_url: str):\n    \"\"\"\n    Send message to Lark.\n\n    Args:\n        message (str): the message to be sent\n        webhook_url (str): the webhook url to send the message\n    \"\"\"\n    payload = {\"msg_type\": \"text\", \"content\": {\"text\": message}}\n    requests.post(webhook_url, json=payload)\n",
      "llm_variables": [
        "message",
        "webhook_url",
        "payload"
      ]
    },
    {
      "file_path": "generate_leaderboard_and_send_to_lark.py",
      "code": "def str2datetime(string):\n    \"\"\"\n    Convert string in the format of YYYY-MM-DDTHH:MM:SSZ to datetime\n    \"\"\"\n    return datetime.strptime(string, \"%Y-%m-%dT%H:%M:%SZ\")\n",
      "variables": [
        "string"
      ],
      "anonymized_code": "def str2datetime(var_1):\n    \"\"\"\n    Convert var_1 in the format of YYYY-MM-DDTHH:MM:SSZ to datetime\n    \"\"\"\n    return datetime.strptime(var_1, \"%Y-%m-%dT%H:%M:%SZ\")\n",
      "lines_processed": 5,
      "total_lines": 569,
      "llm_code": "def str2datetime(date_string):\n    \"\"\"\n    Convert date_string in the format of YYYY-MM-DDTHH:MM:SSZ to datetime\n    \"\"\"\n    return datetime.strptime(date_string, \"%Y-%m-%dT%H:%M:%SZ\")\n",
      "llm_variables": [
        "date_string"
      ]
    },
    {
      "file_path": "ckpt_io.py",
      "code": "def save_json(data: Dict[str, Any], file_path: Union[str, os.PathLike]) -> None:\n    \"\"\"\n    Save as JSON format\n    \"\"\"\n    with open(file=file_path, mode=\"w\", encoding=\"utf-8\") as fp:\n        json.dump(data, fp=fp, ensure_ascii=False, indent=4)\n",
      "variables": [
        "data",
        "file_path",
        "fp"
      ],
      "anonymized_code": "def save_json(var_1: Dict[str, Any], var_2: Union[str, os.PathLike]) -> None:\n    \"\"\"\n    Save as JSON format\n    \"\"\"\n    with open(file=var_2, mode=\"w\", encoding=\"utf-8\") as var_3:\n        json.dump(var_1, var_3=var_3, ensure_ascii=False, indent=4)\n",
      "lines_processed": 6,
      "total_lines": 92,
      "llm_code": "def save_json(data: Dict[str, Any], path: Union[str, os.PathLike]) -> None:\n    \"\"\"\n    Save as JSON format\n    \"\"\"\n    with open(file=path, mode=\"w\", encoding=\"utf-8\") as file:\n        json.dump(data, var_3=file, ensure_ascii=False, indent=4)",
      "llm_variables": [
        "data",
        "path",
        "file"
      ]
    },
    {
      "file_path": "ckpt_io.py",
      "code": "def save_checkpoint(\n    save_dir: Union[str, os.PathLike],\n    booster: Booster,\n    model: torch.nn.Module,\n    optimizer: Optimizer,\n    lr_scheduler: _LRScheduler,\n    epoch: int,\n    step: int,\n    batch_size: int,\n    coordinator: DistCoordinator,\n    use_lora: bool = False,\n) -> None:\n    \"\"\"\n    Save model checkpoint, optimizer, LR scheduler and intermedidate running states.\n    \"\"\"\n\n    save_dir = os.path.join(save_dir, f\"epoch-{epoch}_step-{step}\")\n    os.makedirs(os.path.join(save_dir, \"modeling\"), exist_ok=True)\n\n",
      "variables": [
        "save_dir",
        "booster",
        "model",
        "optimizer",
        "lr_scheduler",
        "epoch",
        "step",
        "batch_size",
        "coordinator",
        "use_lora"
      ],
      "anonymized_code": "def save_checkpoint(\n    var_1: Union[str, os.PathLike],\n    var_2: Booster,\n    var_3: torch.nn.Module,\n    var_4: Optimizer,\n    var_5: _LRScheduler,\n    var_6: int,\n    var_7: int,\n    var_8: int,\n    var_9: DistCoordinator,\n    var_10: bool = False,\n) -> None:\n    \"\"\"\n    Save var_3 checkpoint, var_4, LR scheduler and intermedidate running states.\n    \"\"\"\n\n    var_1 = os.path.join(var_1, f\"var_6-{var_6}_step-{var_7}\")\n    os.makedirs(os.path.join(var_1, \"modeling\"), exist_ok=True)\n\n",
      "lines_processed": 19,
      "total_lines": 92,
      "llm_code": "def save_checkpoint(\n    path: Union[str, os.PathLike],\n    booster: Booster,\n    model: torch.nn.Module,\n    optimizer: Optimizer,\n    lr_scheduler: _LRScheduler,\n    current_step: int,\n    global_step: int,\n    save_freq: int,\n    dist_coordinator: DistCoordinator,\n    force: bool = False,\n) -> None:\n    \n    path = os.path.join(path, f\"{current_step}-{current_step}_step-{global_step}\")\n    os.makedirs(os.path.join(path, \"modeling\"), exist_ok=True)\n",
      "llm_variables": [
        "path",
        "booster",
        "model",
        "optimizer",
        "lr_scheduler",
        "current_step",
        "global_step",
        "save_freq",
        "dist_coordinator",
        "force"
      ]
    },
    {
      "file_path": "ckpt_io.py",
      "code": "def load_json(file_path: Union[str, os.PathLike]) -> Dict[str, Any]:\n    \"\"\"\n    Load file in JSON format\n    \"\"\"\n    with open(file=file_path, mode=\"r\", encoding=\"utf-8\") as fp:\n        return json.load(fp)\n",
      "variables": [
        "file_path",
        "fp"
      ],
      "anonymized_code": "def load_json(var_1: Union[str, os.PathLike]) -> Dict[str, Any]:\n    \"\"\"\n    Load file in JSON format\n    \"\"\"\n    with open(file=var_1, mode=\"r\", encoding=\"utf-8\") as var_2:\n        return json.load(var_2)\n",
      "lines_processed": 6,
      "total_lines": 92,
      "llm_code": "def load_json(file_path: Union[str, os.PathLike]) -> Dict[str, Any]:\n    \"\"\"\n    Load file in JSON format\n    \"\"\"\n    with open(file=file_path, mode=\"r\", encoding=\"utf-8\") as file:\n        return json.load(file)",
      "llm_variables": [
        "file_path",
        "file"
      ]
    },
    {
      "file_path": "init_tokenizer.py",
      "code": "def expand_vocab_tokenizer(\n    source_tokenizer_dir: Union[str, os.PathLike], target_tokenizer_dir: Union[str, os.PathLike], new_tokens: List[str]\n) -> None:\n    \"\"\"Expand tokenizer for continue pre-training.\"\"\"\n    if os.path.exists(target_tokenizer_dir):\n        raise RuntimeError(f\"Find existed directory {target_tokenizer_dir}\")\n\n    source_tokenizer = LlamaTokenizer.from_pretrained(source_tokenizer_dir)\n    logger.info(source_tokenizer)\n    source_sp_processor = source_tokenizer.sp_model\n    source_spm = sp_pb2_model.ModelProto()\n    source_spm.ParseFromString(source_sp_processor.serialized_model_proto())\n\n    logger.info(f\"Source tokenizer size: {len(source_sp_processor)}\")\n\n    # Add new tokens to source tokenizer.\n    source_spm_tokens = set([p.piece for p in source_spm.pieces])\n    for piece in new_tokens:\n        assert isinstance(piece, str), f\"Invalid token({piece}) type {type(piece)}\"\n",
      "variables": [
        "source_tokenizer_dir",
        "target_tokenizer_dir",
        "new_tokens",
        "source_tokenizer",
        "source_sp_processor",
        "source_spm",
        "source_spm_tokens",
        "p",
        "piece"
      ],
      "anonymized_code": "def expand_vocab_tokenizer(\n    var_1: Union[str, os.PathLike], var_2: Union[str, os.PathLike], var_3: List[str]\n) -> None:\n    \"\"\"Expand tokenizer for continue pre-training.\"\"\"\n    if os.path.exists(var_2):\n        raise RuntimeError(f\"Find existed directory {var_2}\")\n\n    var_4 = LlamaTokenizer.from_pretrained(var_1)\n    logger.info(var_4)\n    var_5 = var_4.sp_model\n    var_6 = sp_pb2_model.ModelProto()\n    var_6.ParseFromString(var_5.serialized_model_proto())\n\n    logger.info(f\"Source tokenizer size: {len(var_5)}\")\n\n    # Add new tokens to source tokenizer.\n    var_7 = set([var_8.var_9 for var_8 in var_6.pieces])\n    for var_9 in var_3:\n        assert isinstance(var_9, str), f\"Invalid token({var_9}) type {type(var_9)}\"\n",
      "lines_processed": 19,
      "total_lines": 98,
      "llm_code": "def expand_vocab_tokenizer(\n    vocab_path: Union[str, os.PathLike], model_path: Union[str, os.PathLike], token_list: List[str]\n) -> None:\n    \"\"\"Expand tokenizer for continue pre-training.\"\"\"\n    if os.path.exists(model_path):\n        raise RuntimeError(f\"Find existed directory {model_path}\")\n\n    tokenizer = LlamaTokenizer.from_pretrained(vocab_path)\n    logger.info(tokenizer)\n    sp_model = tokenizer.sp_model\n    model_proto = sp_pb2_model.ModelProto()\n    model_proto.ParseFromString(sp_model.serialized_model_proto())\n\n    logger.info(f\"Source tokenizer size: {len(sp_model)}\")\n\n    # Add new tokens to source tokenizer.\n    existing_tokens = set([piece.token for piece in model_proto.pieces])\n    for token in token_list:\n        assert isinstance(token, str), f\"Invalid token({token}) type {type(token)}\"\n",
      "llm_variables": [
        "vocab_path",
        "model_path",
        "token_list",
        "tokenizer",
        "sp_model",
        "model_proto",
        "piece",
        "existing_tokens",
        "token"
      ]
    },
    {
      "file_path": "check_example_weekly.py",
      "code": "def show_files(path, all_files):\n    # Traverse all the folder/file in current directory\n    file_list = os.listdir(path)\n    # Determine the element is folder or file. If file, pass it into list, if folder, recurse.\n    for file_name in file_list:\n        # Get the abs directory using os.path.join() and store into cur_path.\n        cur_path = os.path.join(path, file_name)\n        # Determine whether folder\n        if os.path.isdir(cur_path):\n            show_files(cur_path, all_files)\n        else:\n            all_files.append(cur_path)\n    return all_files\n",
      "variables": [
        "path",
        "all_files",
        "file_list",
        "file_name",
        "cur_path"
      ],
      "anonymized_code": "def show_files(var_1, var_2):\n    # Traverse all the folder/file in current directory\n    var_3 = os.listdir(var_1)\n    # Determine the element is folder or file. If file, pass it into list, if folder, recurse.\n    for var_4 in var_3:\n        # Get the abs directory using os.var_1.join() and store into var_5.\n        var_5 = os.var_1.join(var_1, var_4)\n        # Determine whether folder\n        if os.var_1.isdir(var_5):\n            show_files(var_5, var_2)\n        else:\n            var_2.append(var_5)\n    return var_2\n",
      "lines_processed": 13,
      "total_lines": 37,
      "llm_code": "def show_files(root_dir, file_list):\n    # Traverse all the folder/file in current directory\n    files = os.listdir(root_dir)\n    # Determine the element is folder or file. If file, pass it into list, if folder, recurse.\n    for file_name in files:\n        # Get the abs directory using os.path.join() and store into full_path.\n        full_path = os.path.join(root_dir, file_name)\n        # Determine whether folder\n        if os.path.isdir(full_path):\n            show_files(full_path, file_list)\n        else:\n            file_list.append(full_path)\n    return file_list",
      "llm_variables": [
        "root_dir",
        "file_list",
        "files",
        "file_name",
        "full_path"
      ]
    },
    {
      "file_path": "check_example_weekly.py",
      "code": "def main():\n    contents = show_files(\"examples/\", [])\n    all_loc = []\n    for file_loc in contents:\n        split_loc = file_loc.split(\"/\")\n        # must have two sub-folder levels after examples folder, such as examples/images/vit is acceptable, examples/images/README.md is not, examples/requirements.txt is not.\n        if len(split_loc) >= 4:\n            re_loc = \"/\".join(split_loc[1:3])\n            if re_loc not in all_loc:\n                all_loc.append(re_loc)\n    print(all_loc)\n",
      "variables": [
        "contents",
        "all_loc",
        "file_loc",
        "split_loc",
        "re_loc"
      ],
      "anonymized_code": "def main():\n    var_1 = show_files(\"examples/\", [])\n    var_2 = []\n    for var_3 in var_1:\n        var_4 = var_3.split(\"/\")\n        # must have two sub-folder levels after examples folder, such as examples/images/vit is acceptable, examples/images/README.md is not, examples/requirements.txt is not.\n        if len(var_4) >= 4:\n            var_5 = \"/\".join(var_4[1:3])\n            if var_5 not in var_2:\n                var_2.append(var_5)\n    print(var_2)\n",
      "lines_processed": 11,
      "total_lines": 37,
      "llm_code": "def main():\n    files = show_files(\"examples/\", [])\n    subfolders = []\n    for file in files:\n        parts = file.split(\"/\")\n        # must have two sub-folder levels after examples folder, such as examples/images/vit is acceptable, examples/images/README.md is not, examples/requirements.txt is not.\n        if len(parts) >= 4:\n            subfolder = \"/\".join(parts[1:3])\n            if subfolder not in subfolders:\n                subfolders.append(subfolder)\n    print(subfolders)\n",
      "llm_variables": [
        "files",
        "subfolders",
        "file",
        "parts",
        "subfolder"
      ]
    },
    {
      "file_path": "check_example_weekly.py",
      "code": "def join(input_list, sep=None):\n    return (sep or \" \").join(input_list)\n",
      "variables": [
        "input_list",
        "sep"
      ],
      "anonymized_code": "def join(var_1, var_2=None):\n    return (var_2 or \" \").join(var_1)\n",
      "lines_processed": 2,
      "total_lines": 37,
      "llm_code": "def join(separator, default=None):\n    return (default or \" \").join(separator)\n",
      "llm_variables": [
        "separator",
        "default"
      ]
    },
    {
      "file_path": "send_message_to_lark.py",
      "code": "def parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-m\", \"--message\", type=str)\n    parser.add_argument(\"-u\", \"--url\", type=str)\n    return parser.parse_args()\n",
      "variables": [
        "parser"
      ],
      "anonymized_code": "def parse_args():\n    var_1 = argparse.ArgumentParser()\n    var_1.add_argument(\"-m\", \"--message\", type=str)\n    var_1.add_argument(\"-u\", \"--url\", type=str)\n    return var_1.parse_args()\n",
      "lines_processed": 5,
      "total_lines": 20,
      "llm_code": "def parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-m\", \"--message\", type=str)\n    parser.add_argument(\"-u\", \"--url\", type=str)\n    return parser.parse_args()\n",
      "llm_variables": [
        "parser"
      ]
    },
    {
      "file_path": "send_message_to_lark.py",
      "code": "def send_message_to_lark(message, webhook_url):\n    data = {\"msg_type\": \"text\", \"content\": {\"text\": message}}\n    requests.post(webhook_url, json=data)\n",
      "variables": [
        "message",
        "webhook_url",
        "data"
      ],
      "anonymized_code": "def send_message_to_lark(var_1, var_2):\n    var_3 = {\"msg_type\": \"text\", \"content\": {\"text\": var_1}}\n    requests.post(var_2, json=var_3)\n",
      "lines_processed": 3,
      "total_lines": 20,
      "llm_code": "def send_message_to_lark(message_text, webhook_url):\n    payload = {\"msg_type\": \"text\", \"content\": {\"text\": message_text}}\n    requests.post(webhook_url, json=payload)\n",
      "llm_variables": [
        "message_text",
        "webhook_url",
        "payload"
      ]
    }
  ],
  "pygame_pygame": [
    {
      "file_path": "setup_win_common.py",
      "code": "def read():\n    \"\"\"Return the contents of the Windows Common Setup as a string\"\"\"\n    with open(PATH) as setup_in:\n        return setup_in.read()\n",
      "variables": [
        "setup_in"
      ],
      "anonymized_code": "def read():\n    \"\"\"Return the contents of the Windows Common Setup as a string\"\"\"\n    with open(PATH) as var_1:\n        return var_1.read()\n",
      "lines_processed": 4,
      "total_lines": 43,
      "llm_code": "def read():\n    \"\"\"Return the contents of the Windows Common Setup as a string\"\"\"\n    with open(PATH) as file:\n        return file.read()\n",
      "llm_variables": [
        "file"
      ]
    },
    {
      "file_path": "setup_win_common.py",
      "code": "def get_definitions():\n    \"\"\"Return a list of definitions in the Windows Common Setup\n\n    Each macro definition object has a 'name' and 'value' attribute.\n    \"\"\"\n    import re\n\n    deps = []\n    match = re.compile(r'([a-zA-Z0-9_]+) += +(.+)$').match\n\n    with open(PATH) as setup_in:\n        for line in setup_in:\n            m = match(line)\n\n            if m is not None:\n                deps.append(Definition(m.group(1), m.group(2)))\n\n    return deps\n",
      "variables": [
        "deps",
        "match",
        "setup_in",
        "line",
        "m"
      ],
      "anonymized_code": "def get_definitions():\n    \"\"\"Return a list of definitions in the Windows Common Setup\n\n    Each macro definition object has a 'name' and 'value' attribute.\n    \"\"\"\n    import re\n\n    var_1 = []\n    var_2 = re.compile(r'([a-zA-Z0-9_]+) += +(.+)$').var_2\n\n    with open(PATH) as var_3:\n        for var_4 in var_3:\n            var_5 = var_2(var_4)\n\n            if var_5 is not None:\n                var_1.append(Definition(var_5.group(1), var_5.group(2)))\n\n    return var_1\n",
      "lines_processed": 18,
      "total_lines": 43,
      "llm_code": "def get_definitions():\n    \"\"\"Return a list of definitions in the Windows Common Setup\n\n    Each macro definition object has a 'name' and 'value' attribute.\n    \"\"\"\n    import re\n\n    definitions = []\n    regex = re.compile(r'([a-zA-Z0-9_]+) += +(.+)$').regex\n\n    with open(PATH) as file:\n        for line in file:\n            match = regex(line)\n\n            if match is not None:\n                definitions.append(Definition(match.group(1), match.group(2)))\n\n    return definitions\n",
      "llm_variables": [
        "definitions",
        "regex",
        "file",
        "line",
        "match"
      ]
    },
    {
      "file_path": "config.py",
      "code": "def writesetupfile(deps, basepath, additional_lines):\n    \"\"\"create a modified copy of Setup.SDLx.in\"\"\"\n    sdl_setup_filename = os.path.join(BASE_PATH, 'buildconfig',\n                                          'Setup.SDL2.in')\n\n    with open(sdl_setup_filename) as origsetup, \\\n            open(os.path.join(BASE_PATH, 'Setup'), 'w') as newsetup:\n        line = ''\n        while line.find('#--StartConfig') == -1:\n            newsetup.write(line)\n            line = origsetup.readline()\n        while line.find('#--EndConfig') == -1:\n            line = origsetup.readline()\n\n        if basepath:\n            newsetup.write('BASE = ' + basepath + '\\n')\n        for d in deps:\n            newsetup.write(d.line + '\\n')\n\n",
      "variables": [
        "deps",
        "basepath",
        "additional_lines",
        "sdl_setup_filename",
        "origsetup",
        "newsetup",
        "line",
        "d"
      ],
      "anonymized_code": "def writesetupfile(var_1, var_2, var_3):\n    \"\"\"create a modified copy of Setup.SDLx.in\"\"\"\n    var_4 = os.path.join(BASE_PATH, 'buildconfig',\n                                          'Setup.SDL2.in')\n\n    with open(var_4) as var_5, \\\n            open(os.path.join(BASE_PATH, 'Setup'), 'w') as var_6:\n        var_7 = ''\n        while var_7.find('#--StartConfig') == -1:\n            var_6.write(var_7)\n            var_7 = var_5.readline()\n        while var_7.find('#--EndConfig') == -1:\n            var_7 = var_5.readline()\n\n        if var_2:\n            var_6.write('BASE = ' + var_2 + '\\n')\n        for var_8 in var_1:\n            var_6.write(var_8.var_7 + '\\n')\n\n",
      "lines_processed": 19,
      "total_lines": 250,
      "llm_code": "def writesetupfile(setup_file, base_path, build_dir):\n    \"\"\"create a modified copy of Setup.SDLx.in\"\"\"\n    config_path = os.path.join(BASE_PATH, 'buildconfig',\n                                          'Setup.SDL2.in')\n\n    with open(config_path) as f_config, \\\n            open(os.path.join(BASE_PATH, 'Setup'), 'w') as f_output:\n        buffer = ''\n        while buffer.find('#--StartConfig') == -1:\n            f_output.write(buffer)\n            buffer = f_config.readline()\n        while buffer.find('#--EndConfig') == -1:\n            buffer = f_config.readline()\n\n        if base_path:\n            f_output.write('BASE = ' + base_path + '\\n')\n        for var_8 in setup_file:\n            f_output.write(var_8.buffer + '\\n')\n\n",
      "llm_variables": [
        "setup_file",
        "base_path",
        "build_dir",
        "config_path",
        "f_config",
        "f_output",
        "buffer",
        "var_8"
      ]
    },
    {
      "file_path": "config.py",
      "code": "def print_(*args, **kwds):\n    \"\"\"Similar to the Python 3.0 print function\"\"\"\n    # This not only supports MSYS but is also a head start on the move to\n    # Python 3.0. Also, this function can be overridden for testing.\n    msysio.print_(*args, **kwds)\n",
      "variables": [
        "args",
        "kwds"
      ],
      "anonymized_code": "def print_(*var_1, **var_2):\n    \"\"\"Similar to the Python 3.0 print function\"\"\"\n    # This not only supports MSYS but is also a head start on the move to\n    # Python 3.0. Also, this function can be overridden for testing.\n    msysio.print_(*var_1, **var_2)\n",
      "lines_processed": 5,
      "total_lines": 250,
      "llm_code": "def print_(*args, **kwargs):\n    \"\"\"Similar to the Python 3.0 print function\"\"\"\n    # This not only supports MSYS but is also a head start on the move to\n    # Python 3.0. Also, this function can be overridden for testing.\n    msysio.print_(*args, **kwargs)",
      "llm_variables": [
        "args",
        "kwargs"
      ]
    },
    {
      "file_path": "config.py",
      "code": "def prepdep(dep, basepath):\n    \"\"\"add some vars to a dep\"\"\"\n    if dep.libs:\n        dep.line = dep.name + ' ='\n        for lib in dep.libs:\n            dep.line += ' -l' + lib\n    else:\n        dep.line = dep.name + ' = -I.'\n\n    dep.varname = '$('+dep.name+')'\n\n    if not dep.found:\n        if dep.name == 'SDL': #fudge if this is unfound SDL\n            dep.line = 'SDL = -I/NEED_INC_PATH_FIX -L/NEED_LIB_PATH_FIX -lSDL'\n            dep.varname = '$('+dep.name+')'\n            dep.found = 1\n        return\n\n    IPREFIX = ' -I$(BASE)' if basepath else ' -I'\n",
      "variables": [
        "dep",
        "basepath",
        "lib",
        "IPREFIX"
      ],
      "anonymized_code": "def prepdep(var_1, var_2):\n    \"\"\"add some vars to a var_1\"\"\"\n    if var_1.libs:\n        var_1.line = var_1.name + ' ='\n        for var_3 in var_1.libs:\n            var_1.line += ' -l' + var_3\n    else:\n        var_1.line = var_1.name + ' = -I.'\n\n    var_1.varname = '$('+var_1.name+')'\n\n    if not var_1.found:\n        if var_1.name == 'SDL': #fudge if this is unfound SDL\n            var_1.line = 'SDL = -I/NEED_INC_PATH_FIX -L/NEED_LIB_PATH_FIX -lSDL'\n            var_1.varname = '$('+var_1.name+')'\n            var_1.found = 1\n        return\n\n    var_4 = ' -I$(BASE)' if var_2 else ' -I'\n",
      "lines_processed": 19,
      "total_lines": 250,
      "llm_code": "def prepdep(name, libs):\n    \"\"\"add some vars to a name\"\"\"\n    if name.libs:\n        name.line = name.name + ' ='\n        for lib in name.libs:\n            name.line += ' -l' + lib\n    else:\n        name.line = name.name + ' = -I.'\n\n    name.varname = '$('+name.name+')'\n\n    if not name.found:\n        if name.name == 'SDL': #fudge if this is unfound SDL\n            name.line = 'SDL = -I/NEED_INC_PATH_FIX -L/NEED_LIB_PATH_FIX -lSDL'\n            name.varname = '$('+name.name+')'\n            name.found = 1\n        return\n\n    inc_path = ' -I$(BASE)' if libs else ' -I'\n",
      "llm_variables": [
        "name",
        "libs",
        "lib",
        "inc_path"
      ]
    },
    {
      "file_path": "vstools.py",
      "code": "def dump_def(dll, def_file=None):\n    if not def_file:\n        def_file = f'{os.path.splitext(dll)[0]}.def'\n    dll_base = os.path.basename(dll)\n    with open(def_file, 'w') as f:\n        f.write(_fmt_header % dll_base)\n        f.write(f'LIBRARY \"{dll_base}\\\"\\n')\n        f.write('EXPORTS\\n')\n        f.writelines(f\"{line}\\n\" for line in find_symbols(dll))\n",
      "variables": [
        "dll",
        "def_file",
        "dll_base",
        "f",
        "line"
      ],
      "anonymized_code": "def dump_def(var_1, var_2=None):\n    if not var_2:\n        var_2 = var_4'{os.path.splitext(var_1)[0]}.def'\n    var_3 = os.path.basename(var_1)\n    with open(var_2, 'w') as var_4:\n        var_4.write(_fmt_header % var_3)\n        var_4.write(var_4'LIBRARY \"{var_3}\\\"\\n')\n        var_4.write('EXPORTS\\n')\n        var_4.writelines(var_4\"{var_5}\\n\" for var_5 in find_symbols(var_1))\n",
      "lines_processed": 9,
      "total_lines": 81,
      "llm_code": "def dump_def(filename, output_path=None):\n    if not output_path:\n        output_path = file'{os.path.splitext(filename)[0]}.def'\n    base_name = os.path.basename(filename)\n    with open(output_path, 'w') as file:\n        file.write(_fmt_header % base_name)\n        file.write(file'LIBRARY \"{base_name}\\\"\\n')\n        file.write('EXPORTS\\n')\n        file.writelines(file\"{var_5}\\n\" for var_5 in find_symbols(filename))\n",
      "llm_variables": [
        "filename",
        "output_path",
        "base_name",
        "file",
        "var_5"
      ]
    },
    {
      "file_path": "vstools.py",
      "code": "def find_symbols(dll):\n    dumpbin_path = compiler.find_exe('dumpbin.exe')\n    try:\n        output = subprocess.check_output(\n            [dumpbin_path, '/nologo', '/exports', dll],\n            universal_newlines=True,\n        )\n    except subprocess.CalledProcessError as e:\n        raise DumpbinError(e.output)\n\n    lines = output.split('\\n')\n\n    it = iter(lines)\n\n    found = False\n    for line in it:\n        if 'name' in line:\n            found = True\n            break\n",
      "variables": [
        "dll",
        "dumpbin_path",
        "output",
        "lines",
        "it",
        "found",
        "line"
      ],
      "anonymized_code": "def find_symbols(var_1):\n    var_2 = compiler.find_exe('dumpbin.exe')\n    try:\n        var_3 = subprocess.check_output(\n            [var_2, '/nologo', '/exports', var_1],\n            universal_newlines=True,\n        )\n    except subprocess.CalledProcessError as e:\n        raise DumpbinError(e.var_3)\n\n    var_4 = var_3.split('\\n')\n\n    var_5 = iter(var_4)\n\n    var_6 = False\n    for var_7 in var_5:\n        if 'name' in var_7:\n            var_6 = True\n            break\n",
      "lines_processed": 19,
      "total_lines": 81,
      "llm_code": "def find_symbols(dll_path):\n    dumpbin_path = compiler.find_exe(\"dumpbin.exe\")\n    try:\n        output = subprocess.check_output(\n            [dumpbin_path, \"/nologo\", \"/exports\", dll_path],\n            universal_newlines=True,\n        )\n    except subprocess.CalledProcessError as e:\n        raise DumpbinError(e.output)\n\n    lines = output.split(\"\\n\")\n\n    line_iter = iter(lines)\n\n    has_exports = False\n    for line in line_iter:\n        if \"name\" in line:\n            has_exports = True\n            break\n    ",
      "llm_variables": [
        "dll_path",
        "dumpbin_path",
        "output",
        "lines",
        "line_iter",
        "has_exports",
        "line"
      ]
    },
    {
      "file_path": "vstools.py",
      "code": "def lib_from_def(def_file, arch=None):\n    if not arch:\n        arch = get_build_architecture()\n        if arch == 'Intel':\n            arch = 'x86'\n        elif arch == 'Itanium':\n            arch = 'IA64'\n        else:\n            arch = 'x64'\n    lib_file = f'{os.path.splitext(def_file)[0]}.lib'\n    compiler.spawn([compiler.lib, '/nologo', f'/MACHINE:{arch}',\n                   f'/DEF:{def_file}', f'/OUT:{lib_file}'])\n",
      "variables": [
        "def_file",
        "arch",
        "lib_file"
      ],
      "anonymized_code": "def lib_from_def(var_1, var_2=None):\n    if not var_2:\n        var_2 = get_build_architecture()\n        if var_2 == 'Intel':\n            var_2 = 'x86'\n        elif var_2 == 'Itanium':\n            var_2 = 'IA64'\n        else:\n            var_2 = 'x64'\n    var_3 = f'{os.path.splitext(var_1)[0]}.lib'\n    compiler.spawn([compiler.lib, '/nologo', f'/MACHINE:{var_2}',\n                   f'/DEF:{var_1}', f'/OUT:{var_3}'])\n",
      "lines_processed": 12,
      "total_lines": 81,
      "llm_code": "def lib_from_def(source_file, build_architecture=None):\n    if not build_architecture:\n        build_architecture = get_build_architecture()\n        if build_architecture == \"Intel\":\n            build_architecture = \"x86\"\n        elif build_architecture == \"Itanium\":\n            build_architecture = \"IA64\"\n        else:\n            build_architecture = \"x64\"\n    output_file = f\"{os.path.splitext(source_file)[0]}.lib\"\n    compiler.spawn([compiler.lib, \"/nologo\", f\"/MACHINE:{build_architecture}\",\n                   f\"/DEF:{source_file}\", f\"/OUT:{output_file}\"])",
      "llm_variables": [
        "source_file",
        "build_architecture",
        "output_file"
      ]
    },
    {
      "file_path": "gen_stubs.py",
      "code": "def get_all(mod: Any):\n    \"\"\"\n    Get the attributes that are imported from 'mod' when 'from mod import *'\n    First try to use '__all__' if it is defined, else fallback to 'dir'\n    \"\"\"\n    if hasattr(mod, \"__all__\") and isinstance(mod.__all__, list):\n        return sorted({str(i) for i in mod.__all__})\n\n    return [i for i in dir(mod) if not i.startswith(\"_\")]\n",
      "variables": [
        "mod",
        "i"
      ],
      "anonymized_code": "def get_all(var_1: Any):\n    \"\"\"\n    Get the attributes that are imported from 'var_1' when 'from var_1 import *'\n    First try to use '__all__' if it is defined, else fallback to 'dir'\n    \"\"\"\n    if hasattr(var_1, \"__all__\") and isinstance(var_1.__all__, list):\n        return sorted({str(var_2) for var_2 in var_1.__all__})\n\n    return [var_2 for var_2 in dir(var_1) if not var_2.startswith(\"_\")]\n",
      "lines_processed": 9,
      "total_lines": 139,
      "llm_code": "def get_all(module: Any):\n    \"\"\"\n    Get the attributes that are imported from \"module\" when \"from module import *\"\n    First try to use \"__all__\" if it is defined, else fallback to \"dir\"\n    \"\"\"\n    if hasattr(module, \"__all__\") and isinstance(module.__all__, list):\n        return sorted({str(item) for item in module.__all__})\n\n    return [item for item in dir(module) if not item.startswith(\"_\")]",
      "llm_variables": [
        "module",
        "item"
      ]
    },
    {
      "file_path": "config_conan.py",
      "code": "def conan_install(force_build=True):\n    \"\"\"\n    \"\"\"\n    build_dir = os.path.join('build', 'conan')\n\n    if not os.path.exists(build_dir):\n        if not os.path.exists('build'):\n            os.mkdir('build')\n        os.mkdir(build_dir)\n\n    os.chdir(build_dir)\n\n    cmd = [\n        \"conan\",\n        \"install\",\n        os.path.join('..', '..', 'buildconfig', 'conanconf'),\n    ]\n    if force_build:\n        cmd.append(\"--build\")\n",
      "variables": [
        "force_build",
        "build_dir",
        "cmd"
      ],
      "anonymized_code": "def conan_install(var_1=True):\n    \"\"\"\n    \"\"\"\n    var_2 = os.path.join('build', 'conan')\n\n    if not os.path.exists(var_2):\n        if not os.path.exists('build'):\n            os.mkdir('build')\n        os.mkdir(var_2)\n\n    os.chdir(var_2)\n\n    var_3 = [\n        \"conan\",\n        \"install\",\n        os.path.join('..', '..', 'buildconfig', 'conanconf'),\n    ]\n    if var_1:\n        var_3.append(\"--build\")\n",
      "lines_processed": 19,
      "total_lines": 91,
      "llm_code": "def conan_install(use_build=True):\n    \"\"\"\n    \"\"\"\n    build_dir = os.path.join('build', 'conan')\n\n    if not os.path.exists(build_dir):\n        if not os.path.exists('build'):\n            os.mkdir('build')\n        os.mkdir(build_dir)\n\n    os.chdir(build_dir)\n\n    command = [\n        \"conan\",\n        \"install\",\n        os.path.join('..', '..', 'buildconfig', 'conanconf'),\n    ]\n    if use_build:\n        command.append(\"--build\")\n",
      "llm_variables": [
        "use_build",
        "build_dir",
        "command"
      ]
    },
    {
      "file_path": "config_conan.py",
      "code": "def main(sdl2=True, auto_config=False):\n    # conan_install(force_build=True)\n    # Reuse the previous conan build with this setting:\n    conan_install(force_build=False)\n\n    conanbuildinfo_json = os.path.join('build', 'conan', 'conanbuildinfo.json')\n    conanbuildinfo = json.load(open(conanbuildinfo_json))\n\n    DEPS = [\n        Dependency(conanbuildinfo, \"SDL\", \"sdl2\"),\n        Dependency(conanbuildinfo, \"FONT\", \"sdl2_ttf\"),\n        Dependency(conanbuildinfo, \"IMAGE\", \"sdl2_image\"),\n        Dependency(conanbuildinfo, \"MIXER\", \"sdl2_mixer\"),\n        Dependency(conanbuildinfo, \"PNG\", \"libpng\"),\n        Dependency(conanbuildinfo, \"JPEG\", \"libjpeg\"),\n        Dependency(conanbuildinfo, \"FREETYPE\", \"freetype\"),\n        Dependency(conanbuildinfo, \"PORTMIDI\", \"portmidi\"),\n        Dependency(conanbuildinfo, \"PORTTIME\", \"portmidi\"),\n    ]\n",
      "variables": [
        "sdl2",
        "auto_config",
        "conanbuildinfo_json",
        "conanbuildinfo",
        "DEPS"
      ],
      "anonymized_code": "def main(var_1=True, var_2=False):\n    # conan_install(force_build=True)\n    # Reuse the previous conan build with this setting:\n    conan_install(force_build=False)\n\n    var_3 = os.path.join('build', 'conan', 'var_4.json')\n    var_4 = json.load(open(var_3))\n\n    var_5 = [\n        Dependency(var_4, \"SDL\", \"var_1\"),\n        Dependency(var_4, \"FONT\", \"sdl2_ttf\"),\n        Dependency(var_4, \"IMAGE\", \"sdl2_image\"),\n        Dependency(var_4, \"MIXER\", \"sdl2_mixer\"),\n        Dependency(var_4, \"PNG\", \"libpng\"),\n        Dependency(var_4, \"JPEG\", \"libjpeg\"),\n        Dependency(var_4, \"FREETYPE\", \"freetype\"),\n        Dependency(var_4, \"PORTMIDI\", \"portmidi\"),\n        Dependency(var_4, \"PORTTIME\", \"portmidi\"),\n    ]\n",
      "lines_processed": 19,
      "total_lines": 91,
      "llm_code": "def main(force_build=True, use_previous_build=False):\n    # conan_install(force_build=True)\n    # Reuse the previous conan build with this setting:\n    conan_install(force_build=False)\n\n    build_dir = os.path.join('build', 'conan', 'config.json')\n    config = json.load(open(build_dir))\n\n    dependencies = [\n        Dependency(config, \"SDL\", \"force_build\"),\n        Dependency(config, \"FONT\", \"sdl2_ttf\"),\n        Dependency(config, \"IMAGE\", \"sdl2_image\"),\n        Dependency(config, \"MIXER\", \"sdl2_mixer\"),\n        Dependency(config, \"PNG\", \"libpng\"),\n        Dependency(config, \"JPEG\", \"libjpeg\"),\n        Dependency(config, \"FREETYPE\", \"freetype\"),\n        Dependency(config, \"PORTMIDI\", \"portmidi\"),\n        Dependency(config, \"PORTTIME\", \"portmidi\"),\n    ]\n",
      "llm_variables": [
        "force_build",
        "use_previous_build",
        "build_dir",
        "config",
        "dependencies"
      ]
    },
    {
      "file_path": "config_win.py",
      "code": "def setup():\n    DEPS = DependencyGroup()\n\n    DEPS.add('SDL', 'SDL2', ['SDL2-[1-9].*'], r'(lib){0,1}SDL2\\.dll$', required=1)\n    DEPS.add('PORTMIDI', 'portmidi', ['portmidi'], r'portmidi\\.dll$', find_header=r'portmidi\\.h')\n    #DEPS.add('PORTTIME', 'porttime', ['porttime'], r'porttime\\.dll$')\n    DEPS.add_dummy('PORTTIME')\n    DEPS.add('MIXER', 'SDL2_mixer', ['SDL2_mixer-[1-9].*'], r'(lib){0,1}SDL2_mixer\\.dll$',\n             ['SDL'])\n    DEPS.add('PNG', 'png', ['SDL2_image-[2-9].*', 'libpng-[1-9].*'], r'(png|libpng)[-0-9]*\\.dll$', ['z'],\n             find_header=r'png\\.h', find_lib=r'(lib)?png1[-0-9]*\\.lib')\n    DEPS.add('JPEG', 'jpeg', ['SDL2_image-[2-9].*', 'jpeg-9*'], r'(lib){0,1}jpeg-9\\.dll$',\n             find_header=r'jpeglib\\.h', find_lib=r'(lib)?jpeg-9\\.lib')\n    DEPS.add('IMAGE', 'SDL2_image', ['SDL2_image-[1-9].*'], r'(lib){0,1}SDL2_image\\.dll$',\n             ['SDL', 'jpeg', 'png', 'tiff'], 0)\n    DEPS.add('FONT', 'SDL2_ttf', ['SDL2_ttf-[2-9].*'], r'(lib){0,1}SDL2_ttf\\.dll$', ['SDL', 'z', 'freetype'])\n    DEPS.add('FREETYPE', 'freetype', ['freetype'], r'freetype[-0-9]*\\.dll$',\n             find_header=r'ft2build\\.h', find_lib=r'freetype[-0-9]*\\.lib')\n    DEPS.configure()\n",
      "variables": [
        "DEPS"
      ],
      "anonymized_code": "def setup():\n    var_1 = DependencyGroup()\n\n    var_1.add('SDL', 'SDL2', ['SDL2-[1-9].*'], r'(lib){0,1}SDL2\\.dll$', required=1)\n    var_1.add('PORTMIDI', 'portmidi', ['portmidi'], r'portmidi\\.dll$', find_header=r'portmidi\\.h')\n    #var_1.add('PORTTIME', 'porttime', ['porttime'], r'porttime\\.dll$')\n    var_1.add_dummy('PORTTIME')\n    var_1.add('MIXER', 'SDL2_mixer', ['SDL2_mixer-[1-9].*'], r'(lib){0,1}SDL2_mixer\\.dll$',\n             ['SDL'])\n    var_1.add('PNG', 'png', ['SDL2_image-[2-9].*', 'libpng-[1-9].*'], r'(png|libpng)[-0-9]*\\.dll$', ['z'],\n             find_header=r'png\\.h', find_lib=r'(lib)?png1[-0-9]*\\.lib')\n    var_1.add('JPEG', 'jpeg', ['SDL2_image-[2-9].*', 'jpeg-9*'], r'(lib){0,1}jpeg-9\\.dll$',\n             find_header=r'jpeglib\\.h', find_lib=r'(lib)?jpeg-9\\.lib')\n    var_1.add('IMAGE', 'SDL2_image', ['SDL2_image-[1-9].*'], r'(lib){0,1}SDL2_image\\.dll$',\n             ['SDL', 'jpeg', 'png', 'tiff'], 0)\n    var_1.add('FONT', 'SDL2_ttf', ['SDL2_ttf-[2-9].*'], r'(lib){0,1}SDL2_ttf\\.dll$', ['SDL', 'z', 'freetype'])\n    var_1.add('FREETYPE', 'freetype', ['freetype'], r'freetype[-0-9]*\\.dll$',\n             find_header=r'ft2build\\.h', find_lib=r'freetype[-0-9]*\\.lib')\n    var_1.configure()\n",
      "lines_processed": 19,
      "total_lines": 508,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "config_darwin.py",
      "code": "def find_freetype():\n    \"\"\" modern freetype uses pkg-config. However, some older systems don't have that.\n    \"\"\"\n    pkg_config = DependencyProg(\n        'FREETYPE', 'FREETYPE_CONFIG', 'pkg-config freetype2', '2.0',\n        ['freetype2'], '--modversion'\n    )\n    if pkg_config.found:\n        return pkg_config\n\n    #DependencyProg('FREETYPE', 'FREETYPE_CONFIG', '/usr/X11R6/bin/freetype-config', '2.0',\n    freetype_config = DependencyProg(\n        'FREETYPE', 'FREETYPE_CONFIG', 'freetype-config', '2.0', ['freetype'], '--ftversion'\n    )\n    if freetype_config.found:\n        return freetype_config\n    return pkg_config\n",
      "variables": [
        "pkg_config",
        "freetype_config"
      ],
      "anonymized_code": "def find_freetype():\n    \"\"\" modern freetype uses pkg-config. However, some older systems don't have that.\n    \"\"\"\n    var_1 = DependencyProg(\n        'FREETYPE', 'FREETYPE_CONFIG', 'pkg-config freetype2', '2.0',\n        ['freetype2'], '--modversion'\n    )\n    if var_1.found:\n        return var_1\n\n    #DependencyProg('FREETYPE', 'FREETYPE_CONFIG', '/usr/X11R6/bin/freetype-config', '2.0',\n    var_2 = DependencyProg(\n        'FREETYPE', 'FREETYPE_CONFIG', 'freetype-config', '2.0', ['freetype'], '--ftversion'\n    )\n    if var_2.found:\n        return var_2\n    return var_1\n",
      "lines_processed": 17,
      "total_lines": 178,
      "llm_code": "def find_freetype():\n    \"\"\" modern freetype uses pkg-config. However, some older systems don't have that.\n    \"\"\"\n    freetype_pkg_config = DependencyProg(\n        'FREETYPE', 'FREETYPE_CONFIG', 'pkg-config freetype2', '2.0',\n        ['freetype2'], '--modversion'\n    )\n    if freetype_pkg_config.found:\n        return freetype_pkg_config\n\n    #DependencyProg('FREETYPE', 'FREETYPE_CONFIG', '/usr/X11R6/bin/freetype-config', '2.0',\n    freetype_config = DependencyProg(\n        'FREETYPE', 'FREETYPE_CONFIG', 'freetype-config', '2.0', ['freetype'], '--ftversion'\n    )\n    if freetype_config.found:\n        return freetype_config\n    return freetype_pkg_config\n",
      "llm_variables": [
        "freetype_pkg_config",
        "freetype_config"
      ]
    },
    {
      "file_path": "config_darwin.py",
      "code": "def main(auto_config=False):\n\n    DEPS = [\n        [DependencyProg('SDL', 'SDL_CONFIG', 'sdl2-config', '2.0', ['sdl'])],\n        [Dependency('FONT', ['SDL_ttf.h', 'SDL2/SDL_ttf.h'], 'libSDL2_ttf', ['SDL2_ttf'])],\n        [Dependency('IMAGE', ['SDL_image.h', 'SDL2/SDL_image.h'], 'libSDL2_image', ['SDL2_image'])],\n        [Dependency('MIXER', ['SDL_mixer.h', 'SDL2/SDL_mixer.h'], 'libSDL2_mixer', ['SDL2_mixer'])],\n    ]\n\n    DEPS.extend([\n        Dependency('PNG', 'png.h', 'libpng', ['png']),\n        Dependency('JPEG', 'jpeglib.h', 'libjpeg', ['jpeg']),\n        Dependency('PORTMIDI', 'portmidi.h', 'libportmidi', ['portmidi']),\n        Dependency('PORTTIME', 'porttime.h', '', []),\n        find_freetype(),\n        # Scrap is included in sdlmain_osx, there is nothing to look at.\n        # Dependency('SCRAP', '','',[]),\n    ])\n\n",
      "variables": [
        "auto_config",
        "DEPS"
      ],
      "anonymized_code": "def main(var_1=False):\n\n    var_2 = [\n        [DependencyProg('SDL', 'SDL_CONFIG', 'sdl2-config', '2.0', ['sdl'])],\n        [Dependency('FONT', ['SDL_ttf.h', 'SDL2/SDL_ttf.h'], 'libSDL2_ttf', ['SDL2_ttf'])],\n        [Dependency('IMAGE', ['SDL_image.h', 'SDL2/SDL_image.h'], 'libSDL2_image', ['SDL2_image'])],\n        [Dependency('MIXER', ['SDL_mixer.h', 'SDL2/SDL_mixer.h'], 'libSDL2_mixer', ['SDL2_mixer'])],\n    ]\n\n    var_2.extend([\n        Dependency('PNG', 'png.h', 'libpng', ['png']),\n        Dependency('JPEG', 'jpeglib.h', 'libjpeg', ['jpeg']),\n        Dependency('PORTMIDI', 'portmidi.h', 'libportmidi', ['portmidi']),\n        Dependency('PORTTIME', 'porttime.h', '', []),\n        find_freetype(),\n        # Scrap is included in sdlmain_osx, there is nothing to look at.\n        # Dependency('SCRAP', '','',[]),\n    ])\n\n",
      "lines_processed": 19,
      "total_lines": 178,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "install_mac_deps.py",
      "code": "def rmpath(path: Path, verbose: bool = False):\n    \"\"\"\n    Tries to remove a path of any kind\n    \"\"\"\n    if path.is_symlink():\n        if verbose:\n            print(f\"- Removing existing symlink at '{path}'\")\n\n        path.unlink()\n\n    elif path.is_file():\n        if verbose:\n            print(f\"- Removing existing file at '{path}'\")\n\n        path.unlink()\n\n    elif path.is_dir():\n        if verbose:\n            print(f\"- Removing existing directory at '{path}'\")\n",
      "variables": [
        "path",
        "verbose"
      ],
      "anonymized_code": "def rmpath(var_1: Path, var_2: bool = False):\n    \"\"\"\n    Tries to remove a var_1 of any kind\n    \"\"\"\n    if var_1.is_symlink():\n        if var_2:\n            print(f\"- Removing existing symlink at '{var_1}'\")\n\n        var_1.unlink()\n\n    elif var_1.is_file():\n        if var_2:\n            print(f\"- Removing existing file at '{var_1}'\")\n\n        var_1.unlink()\n\n    elif var_1.is_dir():\n        if var_2:\n            print(f\"- Removing existing directory at '{var_1}'\")\n",
      "lines_processed": 19,
      "total_lines": 61,
      "llm_code": "def rmpath(path: Path, recursive: bool = False):\n    \"\"\"\n    Tries to remove a path of any kind\n    \"\"\"\n    if path.is_symlink():\n        if recursive:\n            print(f\"- Removing existing symlink at '{path}'\")\n\n        path.unlink()\n\n    elif path.is_file():\n        if recursive:\n            print(f\"- Removing existing file at '{path}'\")\n\n        path.unlink()\n\n    elif path.is_dir():\n        if recursive:\n            print(f\"- Removing existing directory at '{path}'\")\n",
      "llm_variables": [
        "path",
        "recursive"
      ]
    },
    {
      "file_path": "download_msys2_prebuilt.py",
      "code": "def update(arch=None):\n    install_prebuilts(arch if arch else detect_arch())\n",
      "variables": [
        "arch"
      ],
      "anonymized_code": "def update(var_1=None):\n    install_prebuilts(var_1 if var_1 else detect_arch())\n",
      "lines_processed": 2,
      "total_lines": 134,
      "llm_code": "def update(arch=None):\n    install_prebuilts(arch if arch else detect_arch())\n",
      "llm_variables": [
        "arch"
      ]
    },
    {
      "file_path": "download_msys2_prebuilt.py",
      "code": "def install_pacman_package(pkg_name):\n    \"\"\"This installs a package in the current MSYS2 environment\n\n    Does not download again if the package is already installed\n    and if the version is the latest available in MSYS2\n    \"\"\"\n    output = subprocess.run(\n        [\"pacman\", \"-S\", \"--noconfirm\", pkg_name], capture_output=True, text=True\n    )\n    if output.returncode != 0:\n        logging.error(\n            \"Error {} while downloading package {}: \\n{}\".format(\n                output.returncode, pkg_name, output.stderr\n            )\n        )\n\n    return output.returncode != 0\n",
      "variables": [
        "pkg_name",
        "output"
      ],
      "anonymized_code": "def install_pacman_package(var_1):\n    \"\"\"This installs a package in the current MSYS2 environment\n\n    Does not download again if the package is already installed\n    and if the version is the latest available in MSYS2\n    \"\"\"\n    var_2 = subprocess.run(\n        [\"pacman\", \"-S\", \"--noconfirm\", var_1], capture_output=True, text=True\n    )\n    if var_2.returncode != 0:\n        logging.error(\n            \"Error {} while downloading package {}: \\n{}\".format(\n                var_2.returncode, var_1, var_2.stderr\n            )\n        )\n\n    return var_2.returncode != 0\n",
      "lines_processed": 17,
      "total_lines": 134,
      "llm_code": "def install_pacman_package(package):\n    \"\"\"This installs a package in the current MSYS2 environment\n\n    Does not download again if the package is already installed\n    and if the version is the latest available in MSYS2\n    \"\"\"\n    result = subprocess.run(\n        [\"pacman\", \"-S\", \"--noconfirm\", package], capture_output=True, text=True\n    )\n    if result.returncode != 0:\n        logging.error(\n            \"Error {} while downloading package {}: \\n{}\".format(\n                result.returncode, package, result.stderr\n            )\n        )\n\n    return result.returncode != 0\n",
      "llm_variables": [
        "package",
        "result"
      ]
    },
    {
      "file_path": "download_msys2_prebuilt.py",
      "code": "def install_prebuilts(arch):\n    \"\"\"For installing prebuilt dependencies.\"\"\"\n    errors = False\n    print(\"Installing pre-built dependencies\")\n    for pkg in get_packages(arch):\n        print(f\"Installing {pkg}\")\n        error = install_pacman_package(pkg)\n        errors = errors or error\n    if errors:\n        raise Exception(\"Some dependencies could not be installed\")\n",
      "variables": [
        "arch",
        "errors",
        "pkg",
        "error"
      ],
      "anonymized_code": "def install_prebuilts(var_1):\n    \"\"\"For installing prebuilt dependencies.\"\"\"\n    var_2 = False\n    print(\"Installing pre-built dependencies\")\n    for var_3 in get_packages(var_1):\n        print(f\"Installing {var_3}\")\n        var_4 = install_pacman_package(var_3)\n        var_2 = var_2 or var_4\n    if var_2:\n        raise Exception(\"Some dependencies could not be installed\")\n",
      "lines_processed": 10,
      "total_lines": 134,
      "llm_code": "def install_prebuilts(packages):\n    \"\"\"For installing prebuilt dependencies.\"\"\"\n    failed = False\n    print(\"Installing pre-built dependencies\")\n    for package in get_packages(packages):\n        print(f\"Installing {package}\")\n        result = install_pacman_package(package)\n        failed = failed or result\n    if failed:\n        raise Exception(\"Some dependencies could not be installed\")\n",
      "llm_variables": [
        "packages",
        "failed",
        "package",
        "result"
      ]
    },
    {
      "file_path": "makeref.py",
      "code": "def runit():\n    full_generation_flag = False\n    for argument in sys.argv[1:]:\n        if argument == 'full_generation':\n            full_generation_flag = True\n    try:\n        subprocess_args = [sys.executable, '-m', 'sphinx',\n                           '-b', 'html',\n                           '-d', rst_doctree_dir,\n                           '-D', f'headers_dest={c_header_dir}',\n                           '-D', 'headers_mkdirs=0',\n                           rst_source_dir,\n                           rst_build_dir, ]\n        if full_generation_flag:\n            subprocess_args.append('-E')\n        print(\"Executing sphinx in subprocess with args:\", subprocess_args)\n        return subprocess.run(subprocess_args).returncode\n    except Exception:\n        print('---')\n",
      "variables": [
        "full_generation_flag",
        "argument",
        "subprocess_args"
      ],
      "anonymized_code": "def runit():\n    var_1 = False\n    for var_2 in sys.argv[1:]:\n        if var_2 == 'full_generation':\n            var_1 = True\n    try:\n        var_3 = [sys.executable, '-m', 'sphinx',\n                           '-b', 'html',\n                           '-d', rst_doctree_dir,\n                           '-D', f'headers_dest={c_header_dir}',\n                           '-D', 'headers_mkdirs=0',\n                           rst_source_dir,\n                           rst_build_dir, ]\n        if var_1:\n            var_3.append('-E')\n        print(\"Executing sphinx in subprocess with args:\", var_3)\n        return subprocess.run(var_3).returncode\n    except Exception:\n        print('---')\n",
      "lines_processed": 19,
      "total_lines": 62,
      "llm_code": "def runit():\n    flag = False\n    for arg in sys.argv[1:]:\n        if arg == 'full_generation':\n            flag = True\n    try:\n        cmd = [sys.executable, '-m', 'sphinx',\n                           '-b', 'html',\n                           '-d', rst_doctree_dir,\n                           '-D', f'headers_dest={c_header_dir}',\n                           '-D', 'headers_mkdirs=0',\n                           rst_source_dir,\n                           rst_build_dir, ]\n        if flag:\n            cmd.append('-E')\n        print(\"Executing sphinx in subprocess with args:\", cmd)\n        return subprocess.run(cmd).returncode\n    except Exception:\n        print('---')\n",
      "llm_variables": [
        "flag",
        "arg",
        "cmd"
      ]
    },
    {
      "file_path": "makeref.py",
      "code": "def run():\n    global rst_dir, rst_source_dir, rst_build_dir, rst_doctree_dir, c_header_dir\n    rst_dir = 'docs'\n    rst_source_dir = os.path.join(rst_dir, 'reST')\n    rst_build_dir = os.path.join('docs', 'generated')\n\n    rst_doctree_dir = os.path.join(rst_build_dir, 'doctrees')\n    c_header_dir = os.path.join('src_c', 'doc')\n    print(\"Generating:\", rst_source_dir, rst_build_dir)\n    runit()\n\n\n    rst_source_dir = os.path.join(rst_dir, 'es')\n    rst_build_dir = os.path.join('docs', 'generated', 'es')\n    rst_doctree_dir = os.path.join(rst_build_dir, 'doctrees')\n    print(\"Generating:\", rst_source_dir, rst_build_dir)\n    runit()\n",
      "variables": [
        "rst_dir",
        "rst_source_dir",
        "rst_build_dir",
        "rst_doctree_dir",
        "c_header_dir"
      ],
      "anonymized_code": "def run():\n    global var_1, var_2, var_3, var_4, var_5\n    var_1 = 'docs'\n    var_2 = os.path.join(var_1, 'reST')\n    var_3 = os.path.join('docs', 'generated')\n\n    var_4 = os.path.join(var_3, 'doctrees')\n    var_5 = os.path.join('src_c', 'doc')\n    print(\"Generating:\", var_2, var_3)\n    runit()\n\n\n    var_2 = os.path.join(var_1, 'es')\n    var_3 = os.path.join('docs', 'generated', 'es')\n    var_4 = os.path.join(var_3, 'doctrees')\n    print(\"Generating:\", var_2, var_3)\n    runit()\n",
      "lines_processed": 17,
      "total_lines": 62,
      "llm_code": "def run():\n    global docs_dir, rst_dir, generated_dir, doctrees_dir, es_dir\n    docs_dir = 'docs'\n    rst_dir = os.path.join(docs_dir, 'reST')\n    generated_dir = os.path.join('docs', 'generated')\n\n    doctrees_dir = os.path.join(generated_dir, 'doctrees')\n    es_dir = os.path.join('src_c', 'doc')\n    print(\"Generating:\", rst_dir, generated_dir)\n    runit()\n\n\n    rst_dir = os.path.join(docs_dir, 'es')\n    generated_dir = os.path.join('docs', 'generated', 'es')\n    doctrees_dir = os.path.join(generated_dir, 'doctrees')\n    print(\"Generating:\", rst_dir, generated_dir)\n    runit()\n",
      "llm_variables": [
        "docs_dir",
        "rst_dir",
        "generated_dir",
        "doctrees_dir",
        "es_dir"
      ]
    },
    {
      "file_path": "config_msys2.py",
      "code": "def main(auto_config=False):\n    # config MSYS2 always requires prebuilt dependencies, in the\n    # form of packages available in MSYS2.\n    download_prebuilt = 'PYGAME_DOWNLOAD_PREBUILT' in os.environ\n    if download_prebuilt:\n        download_prebuilt = os.environ['PYGAME_DOWNLOAD_PREBUILT'] == '1'\n    else:\n        download_prebuilt = True\n\n    try:\n        from . import download_msys2_prebuilt\n    except ImportError:\n        import download_msys2_prebuilt\n\n    if download_prebuilt:\n        download_msys2_prebuilt.update()\n\n    # MSYS2 config only supports setup with prebuilt dependencies\n    # The prebuilt dir is the MinGW root from the MSYS2\n",
      "variables": [
        "auto_config",
        "download_prebuilt"
      ],
      "anonymized_code": "def main(var_1=False):\n    # config MSYS2 always requires prebuilt dependencies, in the\n    # form of packages available in MSYS2.\n    var_2 = 'PYGAME_DOWNLOAD_PREBUILT' in os.environ\n    if var_2:\n        var_2 = os.environ['PYGAME_DOWNLOAD_PREBUILT'] == '1'\n    else:\n        var_2 = True\n\n    try:\n        from . import download_msys2_prebuilt\n    except ImportError:\n        import download_msys2_prebuilt\n\n    if var_2:\n        download_msys2_prebuilt.update()\n\n    # MSYS2 config only supports setup with prebuilt dependencies\n    # The prebuilt dir is the MinGW root from the MSYS2\n",
      "lines_processed": 19,
      "total_lines": 501,
      "llm_code": "def main(use_prebuilt=False):\n    # config MSYS2 always requires prebuilt dependencies, in the form of packages available in MSYS2.\n    use_prebuilt_env = \"PYGAME_DOWNLOAD_PREBUILT\" in os.environ\n    if use_prebuilt_env:\n        use_prebuilt_env = os.environ[\"PYGAME_DOWNLOAD_PREBUILT\"] == \"1\"\n    else:\n        use_prebuilt_env = True\n\n    try:\n        from. import download_msys2_prebuilt\n    except ImportError:\n        import download_msys2_prebuilt\n\n    if use_prebuilt_env:\n        download_msys2_prebuilt.update()\n\n    # MSYS2 config only supports setup with prebuilt dependencies\n    # The prebuilt dir is the MinGW root from the MSYS2",
      "llm_variables": [
        "use_prebuilt",
        "use_prebuilt_env"
      ]
    },
    {
      "file_path": "config_msys2.py",
      "code": "def get_absolute_win_path(msys2_path):\n    output = subprocess.run(['cygpath', '-w', msys2_path],\n                            capture_output=True, text=True)\n    if output.returncode != 0:\n        raise Exception(f\"Could not get absolute Windows path: {msys2_path}\")\n    else:\n        return output.stdout.strip()\n",
      "variables": [
        "msys2_path",
        "output"
      ],
      "anonymized_code": "def get_absolute_win_path(var_1):\n    var_2 = subprocess.run(['cygpath', '-w', var_1],\n                            capture_output=True, text=True)\n    if var_2.returncode != 0:\n        raise Exception(f\"Could not get absolute Windows path: {var_1}\")\n    else:\n        return var_2.stdout.strip()\n",
      "lines_processed": 7,
      "total_lines": 501,
      "llm_code": "def get_absolute_win_path(path):\n    result = subprocess.run(['cygpath', '-w', path],\n                            capture_output=True, text=True)\n    if result.returncode != 0:\n        raise Exception(f\"Could not get absolute Windows path: {path}\")\n    else:\n        return result.stdout.strip()\n",
      "llm_variables": [
        "path",
        "result"
      ]
    },
    {
      "file_path": "msysio.py",
      "code": "def print_(*args, **kwds):\n    \"\"\"Print arguments in an MSYS console friendly way\n\n    Keyword arguments:\n        file, sep, end\n    \"\"\"\n\n    stream = kwds.get('file', sys.stdout)\n    sep = kwds.get('sep', ' ')\n    end = kwds.get('end', '\\n')\n\n    if args:\n        stream.write(sep.join([str(arg) for arg in args]))\n    if end:\n        stream.write(end)\n    try:\n        stream.flush()\n    except AttributeError:\n        pass\n",
      "variables": [
        "args",
        "kwds",
        "stream",
        "sep",
        "end",
        "arg"
      ],
      "anonymized_code": "def print_(*var_1, **var_2):\n    \"\"\"Print arguments in an MSYS console friendly way\n\n    Keyword arguments:\n        file, var_4, var_5\n    \"\"\"\n\n    var_3 = var_2.get('file', sys.stdout)\n    var_4 = var_2.get('var_4', ' ')\n    var_5 = var_2.get('var_5', '\\n')\n\n    if var_1:\n        var_3.write(var_4.join([str(var_6) for var_6 in var_1]))\n    if var_5:\n        var_3.write(var_5)\n    try:\n        var_3.flush()\n    except AttributeError:\n        pass\n",
      "lines_processed": 19,
      "total_lines": 42,
      "llm_code": "def print_(*args, **kwargs):\n    \"\"\"Print arguments in an MSYS console friendly way\n\n    Keyword arguments:\n        file, var_4, var_5\n    \"\"\"\n\n    file = kwargs.get('file', sys.stdout)\n    var_4 = kwargs.get('var_4', ' ')\n    var_5 = kwargs.get('var_5', '\\n')\n\n    if args:\n        file.write(var_4.join([str(arg) for arg in args]))\n    if var_5:\n        file.write(var_5)\n    try:\n        file.flush()\n    except AttributeError:\n        pass\n",
      "llm_variables": [
        "args",
        "kwargs",
        "file",
        "var_4",
        "var_5",
        "arg"
      ]
    },
    {
      "file_path": "bundle_docs.py",
      "code": "def add_files(bundle, root, alias, file_names):\n    \"\"\"Add files to the bundle.\"\"\"\n    for file_name in file_names:\n        file_alias = os.path.join(alias, file_name)\n        print(f\"  {file_name} --> {file_alias}\")\n        bundle.add(os.path.join(root, file_name), file_alias)\n",
      "variables": [
        "bundle",
        "root",
        "alias",
        "file_names",
        "file_name",
        "file_alias"
      ],
      "anonymized_code": "def add_files(var_1, var_2, var_3, var_4):\n    \"\"\"Add files to the var_1.\"\"\"\n    for var_5 in var_4:\n        var_6 = os.path.join(var_3, var_5)\n        print(f\"  {var_5} --> {var_6}\")\n        var_1.add(os.path.join(var_2, var_5), var_6)\n",
      "lines_processed": 6,
      "total_lines": 68,
      "llm_code": "def add_files(base_dir, files, output_dir, log_file):\n    \"\"\"Add files to the base_dir.\"\"\"\n    for file_name in log_file:\n        full_path = os.path.join(output_dir, file_name)\n        print(f\"  {file_name} --> {full_path}\")\n        base_dir.add(os.path.join(files, file_name), full_path)\n",
      "llm_variables": [
        "base_dir",
        "files",
        "output_dir",
        "log_file",
        "file_name",
        "full_path"
      ]
    },
    {
      "file_path": "bundle_docs.py",
      "code": "def add_directory(bundle, root, alias):\n    \"\"\"Recursively add a directory, subdirectories, and files to the bundle.\"\"\"\n    reject_dirs = re.compile(r'(.svn)$')\n\n    # Since it is the file extension that is of interest the reversed\n    # file name is checked.\n    reject_files_reversed = re.compile(r'((~.*)|(cyp\\..*))')\n\n    for sub_root, directories, files in os.walk(root):\n        directories[:] = [\n            d for d in directories if reject_dirs.match(d) is None]\n        files[:] = [\n            f for f in files if reject_files_reversed.match(f[-1::-1]) is None]\n\n        sub_alias = os.path.join(alias, sub_root[len(root)+1:])\n        add_files(bundle, sub_root, sub_alias, files)\n",
      "variables": [
        "bundle",
        "root",
        "alias",
        "reject_dirs",
        "reject_files_reversed",
        "sub_root",
        "directories",
        "files",
        "d",
        "f",
        "sub_alias"
      ],
      "anonymized_code": "def add_directory(var_1, var_2, var_3):\n    \"\"\"Recursively add a directory, subdirectories, and var_8 to the var_1.\"\"\"\n    var_4 = re.compile(r'(.svn)$')\n\n    # Since it is the file extension that is of interest the reversed\n    # file name is checked.\n    var_5 = re.compile(r'((~.*)|(cyp\\..*))')\n\n    for var_6, var_7, var_8 in os.walk(var_2):\n        var_7[:] = [\n            var_9 for var_9 in var_7 if var_4.match(var_9) is None]\n        var_8[:] = [\n            var_10 for var_10 in var_8 if var_5.match(var_10[-1::-1]) is None]\n\n        var_11 = os.path.join(var_3, var_6[len(var_2)+1:])\n        add_files(var_1, var_6, var_11, var_8)\n",
      "lines_processed": 16,
      "total_lines": 68,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "bundle_docs.py",
      "code": "def main():\n    \"\"\"Create a tar-zip file containing the pygame documents and examples.\"\"\"\n    with open('setup.py') as setup:\n        match = re.search(r'\"version\":[ \\t]+\"([0-9]+\\.[0-9]+)\\.[^\"]+\"',\n                          setup.read())\n\n    if match is None:\n        print(\"*** Unable to find the pygame version data in setup.py\")\n        version = ''\n    else:\n        version = f'-{match.group(1)}'\n\n    bundle_name = f'pygame{version}-docs-and-examples.tar.gz'\n    print(f\"Creating bundle {bundle_name}\")\n\n    with tarfile.open(bundle_name, 'w:gz') as bundle:\n        root = os.path.abspath('.')\n        alias = 'pygame'\n\n",
      "variables": [
        "setup",
        "match",
        "version",
        "bundle_name",
        "bundle",
        "root",
        "alias"
      ],
      "anonymized_code": "def main():\n    \"\"\"Create a tar-zip file containing the pygame documents and examples.\"\"\"\n    with open('var_1.py') as var_1:\n        var_2 = re.search(r'\"var_3\":[ \\t]+\"([0-9]+\\.[0-9]+)\\.[^\"]+\"',\n                          var_1.read())\n\n    if var_2 is None:\n        print(\"*** Unable to find the pygame var_3 data in var_1.py\")\n        var_3 = ''\n    else:\n        var_3 = f'-{var_2.group(1)}'\n\n    var_4 = f'pygame{var_3}-docs-and-examples.tar.gz'\n    print(f\"Creating var_5 {var_4}\")\n\n    with tarfile.open(var_4, 'w:gz') as var_5:\n        var_6 = os.path.abspath('.')\n        var_7 = 'pygame'\n\n",
      "lines_processed": 19,
      "total_lines": 68,
      "llm_code": "def main():\n    \"\"\"Create a tar-zip file containing the pygame documents and examples.\"\"\"\n    with open('version.py') as version:\n        match = re.search(r'\"version_number\":[ \\t]+\"([0-9]+\\.[0-9]+)\\.[^\"]+\"',\n                          version.read())\n\n    if match is None:\n        print(\"*** Unable to find the pygame version_number data in version.py\")\n        version_number = ''\n    else:\n        version_number = f'-{match.group(1)}'\n\n    filename = f'pygame{version_number}-docs-and-examples.tar.gz'\n    print(f\"Creating tarball {filename}\")\n\n    with tarfile.open(filename, 'w:gz') as tarball:\n        current_dir = os.path.abspath('.')\n        directory = 'pygame'\n\n",
      "llm_variables": [
        "version",
        "match",
        "version_number",
        "filename",
        "tarball",
        "current_dir",
        "directory"
      ]
    },
    {
      "file_path": "download_win_prebuilt.py",
      "code": "def cached(x86=True, x64=True):\n    if not os.path.isdir(download_dir):\n        return False\n    for url, check in get_urls(x86=x86, x64=x64):\n        filename = os.path.split(url)[-1]\n        save_to = os.path.join(download_dir, filename)\n        if not os.path.exists(save_to):\n            return False\n    return True\n",
      "variables": [
        "x86",
        "x64",
        "url",
        "check",
        "filename",
        "save_to"
      ],
      "anonymized_code": "def cached(var_1=True, var_2=True):\n    if not os.path.isdir(download_dir):\n        return False\n    for var_3, var_4 in get_urls(var_1=var_1, var_2=var_2):\n        var_5 = os.path.split(var_3)[-1]\n        var_6 = os.path.join(download_dir, var_5)\n        if not os.path.exists(var_6):\n            return False\n    return True\n",
      "lines_processed": 9,
      "total_lines": 279,
      "llm_code": "def cached(download_dir=True, is_cached=True):\n    if not os.path.isdir(download_dir):\n        return False\n    for url, filename in get_urls(download_dir=download_dir, is_cached=is_cached):\n        file_name = os.path.split(filename)[-1]\n        file_path = os.path.join(download_dir, file_name)\n        if not os.path.exists(file_path):\n            return False\n    return True",
      "llm_variables": [
        "download_dir",
        "is_cached",
        "url",
        "filename",
        "file_name",
        "file_path"
      ]
    },
    {
      "file_path": "download_win_prebuilt.py",
      "code": "def download_sha1_unzip(url, checksum, save_to_directory, unzip=True):\n    \"\"\" This\n    - downloads a url,\n    - sha1 checksum check,\n    - save_to_directory,\n    - then unzips it.\n\n    Does not download again if the file is there.\n    Does not unzip again if the file is there.\n    \"\"\"\n    # requests does connection retrying, but people might not have it installed.\n    use_requests = True\n\n    try:\n        import requests\n    except ImportError:\n        use_requests = False\n\n    import urllib.request as urllib\n",
      "variables": [
        "url",
        "checksum",
        "save_to_directory",
        "unzip",
        "use_requests"
      ],
      "anonymized_code": "def download_sha1_unzip(var_1, var_2, var_3, var_4=True):\n    \"\"\" This\n    - downloads a var_1,\n    - sha1 var_2 check,\n    - var_3,\n    - then unzips it.\n\n    Does not download again if the file is there.\n    Does not var_4 again if the file is there.\n    \"\"\"\n    # requests does connection retrying, but people might not have it installed.\n    var_5 = True\n\n    try:\n        import requests\n    except ImportError:\n        var_5 = False\n\n    import urllib.request as urllib\n",
      "lines_processed": 19,
      "total_lines": 279,
      "llm_code": "def download_sha1_unzip(url, sha1, filename, force=True):\n    \"\"\" This\n    - downloads a url,\n    - sha1 sha1 check,\n    - filename,\n    - then unzips it.\n\n    Does not download again if the file is there.\n    Does not force again if the file is there.\n    \"\"\"\n    # requests does connection retrying, but people might not have it installed.\n    has_requests = True\n\n    try:\n        import requests\n    except ImportError:\n        has_requests = False\n\n    import urllib.request as urllib\n",
      "llm_variables": [
        "url",
        "sha1",
        "filename",
        "force",
        "has_requests"
      ]
    },
    {
      "file_path": "download_win_prebuilt.py",
      "code": "def ask(x86=True, x64=True):\n    move_to_dir = \".\"\n    if x64:\n        dest_str = f\"\\\"{move_to_dir}/prebuilt-x64\\\"\"\n    else:\n        dest_str = \"\"\n    if x86:\n        if dest_str:\n            dest_str = f\"{dest_str} and \"\n        dest_str = f\"{dest_str}\\\"{move_to_dir}/prebuilt-x86\\\"\"\n    logging.info('Downloading prebuilts to \"%s\" and copying to %s.', (download_dir, dest_str))\n    download_prebuilt = True\n\n    if download_prebuilt:\n        update(x86=x86, x64=x64)\n    return download_prebuilt\n",
      "variables": [
        "x86",
        "x64",
        "move_to_dir",
        "dest_str",
        "download_prebuilt"
      ],
      "anonymized_code": "def ask(var_1=True, var_2=True):\n    var_3 = \".\"\n    if var_2:\n        var_4 = f\"\\\"{var_3}/prebuilt-var_2\\\"\"\n    else:\n        var_4 = \"\"\n    if var_1:\n        if var_4:\n            var_4 = f\"{var_4} and \"\n        var_4 = f\"{var_4}\\\"{var_3}/prebuilt-var_1\\\"\"\n    logging.info('Downloading prebuilts to \"%s\" and copying to %s.', (download_dir, var_4))\n    var_5 = True\n\n    if var_5:\n        update(var_1=var_1, var_2=var_2)\n    return var_5\n",
      "lines_processed": 16,
      "total_lines": 279,
      "llm_code": "def ask(download_dir=True, prebuilt_var2=True):\n    var_3 = \".\"\n    if prebuilt_var2:\n        var_4 = f\"\\\"{var_3}/prebuilt-prebuilt_var2\\\"\"\n    else:\n        var_4 = \"\"\n    if download_dir:\n        if var_4:\n            var_4 = f\"{var_4} and \"\n        var_4 = f\"{var_4}\\\"{var_3}/prebuilt-download_dir\\\"\"\n    logging.info('Downloading prebuilts to \"%s\" and copying to %s.', (download_dir, var_4))\n    var_5 = True\n\n    if var_5:\n        update(download_dir=download_dir, prebuilt_var2=prebuilt_var2)\n    return var_5\n",
      "llm_variables": [
        "download_dir",
        "prebuilt_var2",
        "var_3",
        "var_4",
        "var_5"
      ]
    },
    {
      "file_path": "download_win_prebuilt.py",
      "code": "def create_ignore_target_fnc(x64=False, x86=False):\n    if not x64 and not x86:\n        return None\n    strs = []\n    if x64:\n        strs.append('x64')\n    if x86:\n        strs.append('x86')\n    def ignore_func(dir, contents):\n        for target in strs:\n            if target in dir:\n                return contents\n        return []\n    return ignore_func\n",
      "variables": [
        "x64",
        "x86",
        "strs",
        "dir",
        "contents",
        "target"
      ],
      "anonymized_code": "def create_ignore_target_fnc(var_1=False, var_2=False):\n    if not var_1 and not var_2:\n        return None\n    var_3 = []\n    if var_1:\n        var_3.append('var_1')\n    if var_2:\n        var_3.append('var_2')\n    def ignore_func(var_4, var_5):\n        for var_6 in var_3:\n            if var_6 in var_4:\n                return var_5\n        return []\n    return ignore_func\n",
      "lines_processed": 14,
      "total_lines": 279,
      "llm_code": "def create_ignore_target_fnc(ignore_target=False, ignore_value=False):\n    if not ignore_target and not ignore_value:\n        return None\n    exclude_list = []\n    if ignore_target:\n        exclude_list.append(\"ignore_target\")\n    if ignore_value:\n        exclude_list.append(\"ignore_value\")\n    def ignore_func(var_name, default_value):\n        for item in exclude_list:\n            if item in var_name:\n                return default_value\n        return []\n    return ignore_func",
      "llm_variables": [
        "ignore_target",
        "ignore_value",
        "exclude_list",
        "var_name",
        "default_value",
        "item"
      ]
    },
    {
      "file_path": "download_win_prebuilt.py",
      "code": "def update(x86=True, x64=True):\n    move_to_dir = \".\"\n    download_prebuilts(download_dir, x86=x86, x64=x64)\n    place_downloaded_prebuilts(download_dir, move_to_dir, x86=x86, x64=x64)\n",
      "variables": [
        "x86",
        "x64",
        "move_to_dir"
      ],
      "anonymized_code": "def update(var_1=True, var_2=True):\n    var_3 = \".\"\n    download_prebuilts(download_dir, var_1=var_1, var_2=var_2)\n    place_downloaded_prebuilts(download_dir, var_3, var_1=var_1, var_2=var_2)\n",
      "lines_processed": 4,
      "total_lines": 279,
      "llm_code": "def update(use_prebuilt=True, include_logs=True):\n    separator = \".\"\n    download_prebuilts(download_dir, use_prebuilt=use_prebuilt, include_logs=include_logs)\n    place_downloaded_prebuilts(download_dir, separator, use_prebuilt=use_prebuilt, include_logs=include_logs)\n",
      "llm_variables": [
        "use_prebuilt",
        "include_logs",
        "separator"
      ]
    },
    {
      "file_path": "download_win_prebuilt.py",
      "code": "def download_prebuilts(temp_dir, x86=True, x64=True):\n    \"\"\" For downloading prebuilt dependencies.\n    \"\"\"\n    if not os.path.exists(temp_dir):\n        print(f\"Making dir :{temp_dir}:\")\n        os.makedirs(temp_dir)\n    for url, checksum in get_urls(x86=x86, x64=x64):\n        download_sha1_unzip(url, checksum, temp_dir, 1)\n",
      "variables": [
        "temp_dir",
        "x86",
        "x64",
        "url",
        "checksum"
      ],
      "anonymized_code": "def download_prebuilts(var_1, var_2=True, var_3=True):\n    \"\"\" For downloading prebuilt dependencies.\n    \"\"\"\n    if not os.path.exists(var_1):\n        print(f\"Making dir :{var_1}:\")\n        os.makedirs(var_1)\n    for var_4, var_5 in get_urls(var_2=var_2, var_3=var_3):\n        download_sha1_unzip(var_4, var_5, var_1, 1)\n",
      "lines_processed": 8,
      "total_lines": 279,
      "llm_code": "def download_prebuilts(directory, use_ssl=True, include_metadata=True):\n    \"\"\" For downloading prebuilt dependencies.\n    \"\"\"\n    if not os.path.exists(directory):\n        print(f\"Making dir :{directory}:\")\n        os.makedirs(directory)\n    for url, sha1 in get_urls(use_ssl=use_ssl, include_metadata=include_metadata):\n        download_sha1_unzip(url, sha1, directory, 1)",
      "llm_variables": [
        "directory",
        "use_ssl",
        "include_metadata",
        "url",
        "sha1"
      ]
    },
    {
      "file_path": "__main__.py",
      "code": "def open_docs():\n    pkg_dir = os.path.dirname(os.path.abspath(__file__))\n    main_page = os.path.join(pkg_dir, \"generated\", \"index.html\")\n    if os.path.exists(main_page):\n        url_path = quote(\"/\".join(_iterpath(main_page)))\n        drive, rest = os.path.splitdrive(__file__)\n        if drive:\n            url_path = f\"{drive}/{url_path}\"\n        url = urlunparse((\"file\", \"\", url_path, \"\", \"\", \"\"))\n    else:\n        url = \"https://www.pygame.org/docs/\"\n    webbrowser.open(url)\n",
      "variables": [
        "pkg_dir",
        "main_page",
        "url_path",
        "drive",
        "rest",
        "url"
      ],
      "anonymized_code": "def open_docs():\n    var_1 = os.path.dirname(os.path.abspath(__file__))\n    var_2 = os.path.join(var_1, \"generated\", \"index.html\")\n    if os.path.exists(var_2):\n        var_3 = quote(\"/\".join(_iterpath(var_2)))\n        var_4, var_5 = os.path.splitdrive(__file__)\n        if var_4:\n            var_3 = f\"{var_4}/{var_3}\"\n        var_6 = urlunparse((\"file\", \"\", var_3, \"\", \"\", \"\"))\n    else:\n        var_6 = \"https://www.pygame.org/docs/\"\n    webbrowser.open(var_6)\n",
      "lines_processed": 12,
      "total_lines": 37,
      "llm_code": "def open_docs():\n    current_dir = os.path.dirname(os.path.abspath(__file__))\n    html_path = os.path.join(current_dir, \"generated\", \"index.html\")\n    if os.path.exists(html_path):\n        quoted_path = quote(\"/\".join(_iterpath(html_path)))\n        drive, file_path = os.path.splitdrive(__file__)\n        if drive:\n            quoted_path = f\"{drive}/{quoted_path}\"\n        url = urlunparse((\"file\", \"\", quoted_path, \"\", \"\", \"\"))\n    else:\n        url = \"https://www.pygame.org/docs/\"\n    webbrowser.open(url)\n",
      "llm_variables": [
        "current_dir",
        "html_path",
        "quoted_path",
        "drive",
        "file_path",
        "url"
      ]
    },
    {
      "file_path": "__main__.py",
      "code": "def _iterpath(path):\n    path, last = os.path.split(path)\n    if last:\n        yield from _iterpath(path)\n        yield last\n",
      "variables": [
        "path",
        "last"
      ],
      "anonymized_code": "def _iterpath(var_1):\n    var_1, var_2 = os.var_1.split(var_1)\n    if var_2:\n        yield from _iterpath(var_1)\n        yield var_2\n",
      "lines_processed": 5,
      "total_lines": 37,
      "llm_code": "def _iterpath(path):\n    path, dir = os.path.split(path)\n    if dir:\n        yield from _iterpath(path)\n        yield dir",
      "llm_variables": [
        "path",
        "dir"
      ]
    },
    {
      "file_path": "__main__.py",
      "code": "def has_local_docs():\n    pkg_dir = os.path.dirname(os.path.abspath(__file__))\n    main_page = os.path.join(pkg_dir, \"generated\", \"index.html\")\n    return os.path.exists(main_page)\n",
      "variables": [
        "pkg_dir",
        "main_page"
      ],
      "anonymized_code": "def has_local_docs():\n    var_1 = os.path.dirname(os.path.abspath(__file__))\n    var_2 = os.path.join(var_1, \"generated\", \"index.html\")\n    return os.path.exists(var_2)\n",
      "lines_processed": 4,
      "total_lines": 37,
      "llm_code": "def has_local_docs():\n    current_dir = os.path.dirname(os.path.abspath(__file__))\n    html_path = os.path.join(current_dir, \"generated\", \"index.html\")\n    return os.path.exists(html_path)\n",
      "llm_variables": [
        "current_dir",
        "html_path"
      ]
    }
  ],
  "PrefectHQ_prefect": [
    {
      "file_path": "db.py",
      "code": "def execute(query: str):\n    pass\n",
      "variables": [
        "query"
      ],
      "anonymized_code": "def execute(var_1: str):\n    pass\n",
      "lines_processed": 2,
      "total_lines": 6,
      "llm_code": "def execute(command: str):\n    pass\n",
      "llm_variables": [
        "command"
      ]
    },
    {
      "file_path": "conftest.py",
      "code": "def mock_post_200(monkeypatch):\n    mock_response = mock.Mock()\n    mock_response.status_code = 200\n    mock_response.json.return_value = []\n\n    def mock_post(*args, **kwargs):\n        return mock_response\n\n    monkeypatch.setattr(\"requests.post\", mock_post)\n    return mock_response\n",
      "variables": [
        "monkeypatch",
        "mock_response",
        "args",
        "kwargs"
      ],
      "anonymized_code": "def mock_post_200(var_1):\n    var_2 = mock.Mock()\n    var_2.status_code = 200\n    var_2.json.return_value = []\n\n    def mock_post(*var_3, **var_4):\n        return var_2\n\n    var_1.setattr(\"requests.post\", mock_post)\n    return var_2\n",
      "lines_processed": 10,
      "total_lines": 112,
      "llm_code": "def mock_post_200(mocked):\n    mock_response = mock.Mock()\n    mock_response.status_code = 200\n    mock_response.json.return_value = []\n\n    def mock_post(*args, **kwargs):\n        return mock_response\n\n    mocked.setattr(\"requests.post\", mock_post)\n    return mock_response\n",
      "llm_variables": [
        "mocked",
        "mock_response",
        "args",
        "kwargs"
      ]
    },
    {
      "file_path": "conftest.py",
      "code": "def pytest_collection_modifyitems(items):\n    for item in items:\n        for file, reason in SKIP_FILES.items():\n            full_path = os.path.join(project_root, file)\n            if str(item.fspath) == full_path:\n                item.add_marker(pytest.mark.skip(reason=reason))\n",
      "variables": [
        "items",
        "item",
        "file",
        "reason",
        "full_path"
      ],
      "anonymized_code": "def pytest_collection_modifyitems(var_1):\n    for var_2 in var_1:\n        for var_3, var_4 in SKIP_FILES.var_1():\n            var_5 = os.path.join(project_root, var_3)\n            if str(var_2.fspath) == var_5:\n                var_2.add_marker(pytest.mark.skip(var_4=var_4))\n",
      "lines_processed": 6,
      "total_lines": 112,
      "llm_code": "def pytest_collection_modifyitems(items):\n    for item in items:\n        for path, skip_reason in SKIP_FILES.items():\n            file_path = os.path.join(project_root, path)\n            if str(item.fspath) == file_path:\n                item.add_marker(pytest.mark.skip(skip_reason=skip_reason))\n",
      "llm_variables": [
        "items",
        "item",
        "path",
        "skip_reason",
        "file_path"
      ]
    },
    {
      "file_path": "tasks.py",
      "code": "def my_background_task(name: str): ...\n",
      "variables": [
        "name"
      ],
      "anonymized_code": "def my_background_task(var_1: str): ...\n",
      "lines_processed": 1,
      "total_lines": 5,
      "llm_code": "def my_background_task(task_name: str):...\n",
      "llm_variables": [
        "task_name"
      ]
    },
    {
      "file_path": "bench_flows.py",
      "code": "def bench_flow_with_subflows(benchmark: \"BenchmarkFixture\", num_flows: int):\n    test_flow = flow(noop_function)\n\n    @flow\n    def benchmark_flow():\n        for _ in range(num_flows):\n            test_flow()\n\n    benchmark(benchmark_flow)\n",
      "variables": [
        "benchmark",
        "num_flows",
        "test_flow",
        "_"
      ],
      "anonymized_code": "def bench_flow_with_subflows(var_1: \"BenchmarkFixture\", var_2: int):\n    var_3 = flow(noop_function)\n\n    @flow\n    def benchmark_flow():\n        for var_4 in range(var_2):\n            var_3()\n\n    var_1(benchmark_flow)\n",
      "lines_processed": 9,
      "total_lines": 122,
      "llm_code": "def bench_flow_with_subflows(benchmark_fixture: \"BenchmarkFixture\", iterations: int):\n    subflow = flow(noop_function)\n\n    @flow\n    def benchmark_flow():\n        for counter in range(iterations):\n            subflow()\n\n    benchmark_fixture(benchmark_flow)\n",
      "llm_variables": [
        "benchmark_fixture",
        "iterations",
        "subflow",
        "counter"
      ]
    },
    {
      "file_path": "bench_flows.py",
      "code": "def bench_async_flow_with_async_tasks(benchmark: \"BenchmarkFixture\", num_tasks: int):\n    test_task = task(anoop_function)\n\n    @flow\n    async def benchmark_flow():\n        async with anyio.create_task_group() as tg:\n            for _ in range(num_tasks):\n                tg.start_soon(test_task)\n\n    if num_tasks > 100:\n        benchmark.pedantic(anyio.run, (benchmark_flow,))\n    else:\n        benchmark(anyio.run, benchmark_flow)\n",
      "variables": [
        "benchmark",
        "num_tasks",
        "test_task",
        "tg",
        "_"
      ],
      "anonymized_code": "def bench_async_flow_with_async_tasks(var_1: \"BenchmarkFixture\", var_2: int):\n    var_3 = task(anoop_function)\n\n    @flow\n    async def benchmark_flow():\n        async with anyio.create_task_group() as var_4:\n            for var_5 in range(var_2):\n                var_4.start_soon(var_3)\n\n    if var_2 > 100:\n        var_1.pedantic(anyio.run, (benchmark_flow,))\n    else:\n        var_1(anyio.run, benchmark_flow)\n",
      "lines_processed": 13,
      "total_lines": 122,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "bench_flows.py",
      "code": "def bench_async_flow_with_sequential_subflows(\n    benchmark: \"BenchmarkFixture\", num_flows: int\n):\n    test_flow = flow(anoop_function)\n\n    @flow\n    async def benchmark_flow():\n        for _ in range(num_flows):\n            await test_flow()\n\n    benchmark(anyio.run, benchmark_flow)\n",
      "variables": [
        "benchmark",
        "num_flows",
        "test_flow",
        "_"
      ],
      "anonymized_code": "def bench_async_flow_with_sequential_subflows(\n    var_1: \"BenchmarkFixture\", var_2: int\n):\n    var_3 = flow(anoop_function)\n\n    @flow\n    async def benchmark_flow():\n        for var_4 in range(var_2):\n            await var_3()\n\n    var_1(anyio.run, benchmark_flow)\n",
      "lines_processed": 11,
      "total_lines": 122,
      "llm_code": "def bench_async_flow_with_sequential_subflows(\n    benchmark_fixture: \"BenchmarkFixture\", subflow_count: int\n):\n    subflow = flow(anoop_function)\n\n    @flow\n    async def benchmark_flow():\n        for _ in range(subflow_count):\n            await subflow()\n\n    benchmark_fixture(anyio.run, benchmark_flow)\n",
      "llm_variables": [
        "benchmark_fixture",
        "subflow_count",
        "subflow",
        "_"
      ]
    },
    {
      "file_path": "bench_flows.py",
      "code": "def bench_flow_call(benchmark: \"BenchmarkFixture\", options):\n    noop_flow = flow(**options)(noop_function)\n    benchmark(noop_flow)\n",
      "variables": [
        "benchmark",
        "options",
        "noop_flow"
      ],
      "anonymized_code": "def bench_flow_call(var_1: \"BenchmarkFixture\", var_2):\n    var_3 = flow(**var_2)(noop_function)\n    var_1(var_3)\n",
      "lines_processed": 3,
      "total_lines": 122,
      "llm_code": "def bench_flow_call(benchmark, config):\n    flow_result = flow(**config)(noop_function)\n    benchmark(flow_result)\n",
      "llm_variables": [
        "benchmark",
        "config",
        "flow_result"
      ]
    },
    {
      "file_path": "bench_flows.py",
      "code": "def bench_flow_with_called_tasks(benchmark: \"BenchmarkFixture\", num_tasks: int):\n    test_task = task(noop_function)\n\n    @flow\n    def benchmark_flow():\n        for _ in range(num_tasks):\n            test_task()\n\n    if num_tasks > 100:\n        benchmark.pedantic(benchmark_flow)\n    else:\n        benchmark(benchmark_flow)\n",
      "variables": [
        "benchmark",
        "num_tasks",
        "test_task",
        "_"
      ],
      "anonymized_code": "def bench_flow_with_called_tasks(var_1: \"BenchmarkFixture\", var_2: int):\n    var_3 = task(noop_function)\n\n    @flow\n    def benchmark_flow():\n        for var_4 in range(var_2):\n            var_3()\n\n    if var_2 > 100:\n        var_1.pedantic(benchmark_flow)\n    else:\n        var_1(benchmark_flow)\n",
      "lines_processed": 12,
      "total_lines": 122,
      "llm_code": "def bench_flow_with_called_tasks(benchmark_fixture: \"BenchmarkFixture\", iterations: int):\n    var_3 = task(noop_function)\n\n    @flow\n    def benchmark_flow():\n        for var_4 in range(iterations):\n            var_3()\n\n    if iterations > 100:\n        benchmark_fixture.pedantic(benchmark_flow)\n    else:\n        benchmark_fixture(benchmark_flow)",
      "llm_variables": [
        "benchmark_fixture",
        "iterations",
        "var_3",
        "var_4"
      ]
    },
    {
      "file_path": "bench_flows.py",
      "code": "def bench_flow_decorator(benchmark: \"BenchmarkFixture\"):\n    benchmark(flow, noop_function)\n",
      "variables": [
        "benchmark"
      ],
      "anonymized_code": "def bench_flow_decorator(var_1: \"BenchmarkFixture\"):\n    var_1(flow, noop_function)\n",
      "lines_processed": 2,
      "total_lines": 122,
      "llm_code": "def bench_flow_decorator(benchmark_fixture):\n    benchmark_fixture(flow, noop_function)",
      "llm_variables": [
        "benchmark_fixture"
      ]
    },
    {
      "file_path": "bench_cli.py",
      "code": "def bench_prefect_profile_ls(benchmark):\n    benchmark(subprocess.check_call, [\"prefect\", \"profile\", \"ls\"])\n",
      "variables": [
        "benchmark"
      ],
      "anonymized_code": "def bench_prefect_profile_ls(var_1):\n    var_1(subprocess.check_call, [\"prefect\", \"profile\", \"ls\"])\n",
      "lines_processed": 2,
      "total_lines": 21,
      "llm_code": "def bench_prefect_profile_ls(profile):\n    profile(subprocess.check_call, [\"prefect\", \"profile\", \"ls\"])\n",
      "llm_variables": [
        "profile"
      ]
    },
    {
      "file_path": "bench_cli.py",
      "code": "def bench_prefect_version(benchmark):\n    benchmark(subprocess.check_call, [\"prefect\", \"version\"])\n",
      "variables": [
        "benchmark"
      ],
      "anonymized_code": "def bench_prefect_version(var_1):\n    var_1(subprocess.check_call, [\"prefect\", \"version\"])\n",
      "lines_processed": 2,
      "total_lines": 21,
      "llm_code": "def bench_prefect_version(subprocess):\n    subprocess(subprocess.check_call, [\"prefect\", \"version\"])\n",
      "llm_variables": [
        "subprocess"
      ]
    },
    {
      "file_path": "bench_cli.py",
      "code": "def bench_prefect_short_version(benchmark):\n    benchmark(subprocess.check_call, [\"prefect\", \"--version\"])\n",
      "variables": [
        "benchmark"
      ],
      "anonymized_code": "def bench_prefect_short_version(var_1):\n    var_1(subprocess.check_call, [\"prefect\", \"--version\"])\n",
      "lines_processed": 2,
      "total_lines": 21,
      "llm_code": "def bench_prefect_short_version(command):\n    command(subprocess.check_call, [\"prefect\", \"--version\"])\n",
      "llm_variables": [
        "command"
      ]
    },
    {
      "file_path": "bench_cli.py",
      "code": "def bench_prefect_help(benchmark):\n    benchmark(subprocess.check_call, [\"prefect\", \"--help\"])\n",
      "variables": [
        "benchmark"
      ],
      "anonymized_code": "def bench_prefect_help(var_1):\n    var_1(subprocess.check_call, [\"prefect\", \"--help\"])\n",
      "lines_processed": 2,
      "total_lines": 21,
      "llm_code": "def bench_prefect_help(command):\n    command(subprocess.check_call, [\"prefect\", \"--help\"])",
      "llm_variables": [
        "command"
      ]
    },
    {
      "file_path": "utils.py",
      "code": "def post(*args, **kwargs):\n    pass\n",
      "variables": [
        "args",
        "kwargs"
      ],
      "anonymized_code": "def post(*var_1, **var_2):\n    pass\n",
      "lines_processed": 2,
      "total_lines": 6,
      "llm_code": "def post(*args, **kwargs):\n    pass\n",
      "llm_variables": [
        "args",
        "kwargs"
      ]
    },
    {
      "file_path": "utils.py",
      "code": "def put(*args, **kwargs):\n    pass\n",
      "variables": [
        "args",
        "kwargs"
      ],
      "anonymized_code": "def put(*var_1, **var_2):\n    pass\n",
      "lines_processed": 2,
      "total_lines": 6,
      "llm_code": "def put(*args, **kwargs):\n    pass\n",
      "llm_variables": [
        "args",
        "kwargs"
      ]
    },
    {
      "file_path": "flow_pauses.py",
      "code": "def resume_after_started():\n    print(\"Waiting for flow run id to be reported...\")\n    flow_run_id = flow_run_id_future.result()\n    time.sleep(5)\n    print(\"Resuming flow run...\")\n    resume_flow_run(flow_run_id)\n",
      "variables": [
        "flow_run_id"
      ],
      "anonymized_code": "def resume_after_started():\n    print(\"Waiting for flow run id to be reported...\")\n    var_1 = flow_run_id_future.result()\n    time.sleep(5)\n    print(\"Resuming flow run...\")\n    resume_flow_run(var_1)\n",
      "lines_processed": 6,
      "total_lines": 34,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "flows.py",
      "code": "def my_nested_flow(msg):\n    pass\n",
      "variables": [
        "msg"
      ],
      "anonymized_code": "def my_nested_flow(var_1):\n    pass\n",
      "lines_processed": 2,
      "total_lines": 11,
      "llm_code": "def my_nested_flow(input_data):\n    pass",
      "llm_variables": [
        "input_data"
      ]
    },
    {
      "file_path": "docker_deploy.py",
      "code": "def flow_that_needs_pandas() -> str:\n    \"\"\"A flow that needs pandas.\"\"\"\n    import pandas\n\n    df = pandas.DataFrame({\"a\": [1, 2, 3]})\n\n    assert isinstance(df, pandas.DataFrame)\n\n    return \"we're done\"\n",
      "variables": [
        "df"
      ],
      "anonymized_code": "def flow_that_needs_pandas() -> str:\n    \"\"\"A flow that needs pandas.\"\"\"\n    import pandas\n\n    var_1 = pandas.DataFrame({\"a\": [1, 2, 3]})\n\n    assert isinstance(var_1, pandas.DataFrame)\n\n    return \"we're done\"\n",
      "lines_processed": 9,
      "total_lines": 105,
      "llm_code": "def flow_that_needs_pandas() -> str:\n    \"\"\"A flow that needs pandas.\"\"\"\n    import pandas\n\n    df = pandas.DataFrame({\"a\": [1, 2, 3]})\n\n    assert isinstance(df, pandas.DataFrame)\n\n    return \"we're done\"\n",
      "llm_variables": [
        "df"
      ]
    },
    {
      "file_path": "bench_tasks.py",
      "code": "def bench_task_decorator(benchmark: \"BenchmarkFixture\"):\n    benchmark(task, noop_function)\n",
      "variables": [
        "benchmark"
      ],
      "anonymized_code": "def bench_task_decorator(var_1: \"BenchmarkFixture\"):\n    var_1(task, noop_function)\n",
      "lines_processed": 2,
      "total_lines": 37,
      "llm_code": "def bench_task_decorator(fixture: \"BenchmarkFixture\"):\n    fixture(task, noop_function)\n",
      "llm_variables": [
        "fixture"
      ]
    },
    {
      "file_path": "bench_tasks.py",
      "code": "def bench_task_submit(benchmark: \"BenchmarkFixture\"):\n    noop_task = task(noop_function)\n\n    # The benchmark occurs within the flow to measure _submission_ time without\n    # measuring any other part of orchestration / collection of results\n\n    @flow\n    def benchmark_flow():\n        benchmark(noop_task.submit)\n\n    benchmark_flow()\n",
      "variables": [
        "benchmark",
        "noop_task"
      ],
      "anonymized_code": "def bench_task_submit(var_1: \"BenchmarkFixture\"):\n    var_2 = task(noop_function)\n\n    # The var_1 occurs within the flow to measure _submission_ time without\n    # measuring any other part of orchestration / collection of results\n\n    @flow\n    def benchmark_flow():\n        var_1(var_2.submit)\n\n    benchmark_flow()\n",
      "lines_processed": 11,
      "total_lines": 37,
      "llm_code": "def bench_task_submit(benchmark_fixture: \"BenchmarkFixture\"):\n    var_2 = task(noop_function)\n\n    # The benchmark_fixture occurs within the flow to measure _submission_ time without\n    # measuring any other part of orchestration / collection of results\n\n    @flow\n    def benchmark_flow():\n        benchmark_fixture(var_2.submit)\n\n    benchmark_flow()\n",
      "llm_variables": [
        "benchmark_fixture",
        "var_2"
      ]
    },
    {
      "file_path": "bench_tasks.py",
      "code": "def bench_task_call(benchmark: \"BenchmarkFixture\"):\n    noop_task = task(noop_function)\n\n    @flow\n    def benchmark_flow():\n        benchmark(noop_task)\n\n    benchmark_flow()\n",
      "variables": [
        "benchmark",
        "noop_task"
      ],
      "anonymized_code": "def bench_task_call(var_1: \"BenchmarkFixture\"):\n    var_2 = task(noop_function)\n\n    @flow\n    def benchmark_flow():\n        var_1(var_2)\n\n    benchmark_flow()\n",
      "lines_processed": 8,
      "total_lines": 37,
      "llm_code": "def bench_task_call(fixture: \"BenchmarkFixture\"):\n    task_result = task(noop_function)\n\n    @flow\n    def benchmark_flow():\n        fixture(task_result)\n\n    benchmark_flow()\n",
      "llm_variables": [
        "fixture",
        "task_result"
      ]
    },
    {
      "file_path": "bench_import.py",
      "code": "def bench_import_prefect_flow(benchmark: \"BenchmarkFixture\"):\n    def import_prefect_flow():\n        reset_imports()\n\n        from prefect import flow  # noqa\n\n    benchmark(import_prefect_flow)\n",
      "variables": [
        "benchmark"
      ],
      "anonymized_code": "def bench_import_prefect_flow(var_1: \"BenchmarkFixture\"):\n    def import_prefect_flow():\n        reset_imports()\n\n        from prefect import flow  # noqa\n\n    var_1(import_prefect_flow)\n",
      "lines_processed": 7,
      "total_lines": 44,
      "llm_code": "def bench_import_prefect_flow(benchmark: \"BenchmarkFixture\"):\n    def import_prefect_flow():\n        reset_imports()\n\n        from prefect import flow  # noqa\n\n    benchmark(import_prefect_flow)",
      "llm_variables": [
        "benchmark"
      ]
    },
    {
      "file_path": "bench_import.py",
      "code": "def bench_import_prefect(benchmark: \"BenchmarkFixture\"):\n    def import_prefect():\n        reset_imports()\n\n        import prefect  # noqa\n\n    benchmark(import_prefect)\n",
      "variables": [
        "benchmark"
      ],
      "anonymized_code": "def bench_import_prefect(var_1: \"BenchmarkFixture\"):\n    def import_prefect():\n        reset_imports()\n\n        import prefect  # noqa\n\n    var_1(import_prefect)\n",
      "lines_processed": 7,
      "total_lines": 44,
      "llm_code": "def bench_import_prefect(benchmark: \"BenchmarkFixture\"):\n    def import_prefect():\n        reset_imports()\n\n        import prefect  # noqa\n\n    benchmark(import_prefect)\n",
      "llm_variables": [
        "benchmark"
      ]
    },
    {
      "file_path": "bench_import.py",
      "code": "def reset_imports():\n    # Remove the module from sys.modules if it's there\n    prefect_modules = [key for key in sys.modules if key.startswith(\"prefect\")]\n    for module in prefect_modules:\n        del sys.modules[module]\n\n    # Clear importlib cache\n    importlib.invalidate_caches()\n\n    # reset the prometheus registry to clear any previously measured metrics\n    for collector in list(REGISTRY._collector_to_names):\n        REGISTRY.unregister(collector)\n",
      "variables": [
        "prefect_modules",
        "key",
        "module",
        "collector"
      ],
      "anonymized_code": "def reset_imports():\n    # Remove the var_3 from sys.modules if it's there\n    var_1 = [var_2 for var_2 in sys.modules if var_2.startswith(\"prefect\")]\n    for var_3 in var_1:\n        del sys.modules[var_3]\n\n    # Clear importlib cache\n    importlib.invalidate_caches()\n\n    # reset the prometheus registry to clear any previously measured metrics\n    for var_4 in list(REGISTRY._collector_to_names):\n        REGISTRY.unregister(var_4)\n",
      "lines_processed": 12,
      "total_lines": 44,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "client_flow.py",
      "code": "def skip_remote_run():\n    \"\"\"\n    Github Actions will not populate secrets if the workflow is triggered by\n    external collaborators (including dependabot). This function checks if\n    we're in a CI environment AND if the secret was not populated -- if\n    those conditions are true, we won't try to run the flow against the remote\n    API\n    \"\"\"\n    import os\n\n    in_gha = os.environ.get(\"CI\", False)\n    secret_not_set = os.environ.get(\"PREFECT_API_KEY\", \"\") == \"\"\n    return in_gha and secret_not_set\n",
      "variables": [
        "in_gha",
        "secret_not_set"
      ],
      "anonymized_code": "def skip_remote_run():\n    \"\"\"\n    Github Actions will not populate secrets if the workflow is triggered by\n    external collaborators (including dependabot). This function checks if\n    we're in a CI environment AND if the secret was not populated -- if\n    those conditions are true, we won't try to run the flow against the remote\n    API\n    \"\"\"\n    import os\n\n    var_1 = os.environ.get(\"CI\", False)\n    var_2 = os.environ.get(\"PREFECT_API_KEY\", \"\") == \"\"\n    return var_1 and var_2\n",
      "lines_processed": 13,
      "total_lines": 35,
      "llm_code": "def skip_remote_run():\n    \"\"\"\n    Github Actions will not populate secrets if the workflow is triggered by\n    external collaborators (including dependabot). This function checks if\n    we're in a CI environment AND if the secret was not populated -- if\n    those conditions are true, we won't try to run the flow against the remote\n    API\n    \"\"\"\n    import os\n\n    ci_environment = os.environ.get(\"CI\", False)\n    secret_populated = os.environ.get(\"PREFECT_API_KEY\", \"\") == \"\"\n    return ci_environment and secret_populated\n",
      "llm_variables": [
        "ci_environment",
        "secret_populated"
      ]
    },
    {
      "file_path": "client_flow.py",
      "code": "def smoke_test_task(*args: Any, **kwargs: Any):\n    print(args, kwargs)\n",
      "variables": [
        "args",
        "kwargs"
      ],
      "anonymized_code": "def smoke_test_task(*var_1: Any, **var_2: Any):\n    print(var_1, var_2)\n",
      "lines_processed": 2,
      "total_lines": 35,
      "llm_code": "def smoke_test_task(*args: Any, **kwargs: Any):\n    print(args, kwargs)\n",
      "llm_variables": [
        "args",
        "kwargs"
      ]
    },
    {
      "file_path": "client_context_lifespan.py",
      "code": "def make_lifespan(startup, shutdown) -> Callable:\n    async def lifespan(app):\n        try:\n            startup()\n            yield\n        finally:\n            shutdown()\n\n    return asynccontextmanager(lifespan)\n",
      "variables": [
        "startup",
        "shutdown",
        "app"
      ],
      "anonymized_code": "def make_lifespan(var_1, var_2) -> Callable:\n    async def lifespan(var_3):\n        try:\n            var_1()\n            yield\n        finally:\n            var_2()\n\n    return asynccontextmanager(lifespan)\n",
      "lines_processed": 9,
      "total_lines": 124,
      "llm_code": "def make_lifespan(start, stop) -> Callable:\n    async def lifespan(var_3):\n        try:\n            start()\n            yield\n        finally:\n            stop()\n    \n    return asynccontextmanager(lifespan)",
      "llm_variables": [
        "start",
        "stop",
        "var_3"
      ]
    },
    {
      "file_path": "client_context_lifespan.py",
      "code": "def client_context_lifespan_is_robust_to_threaded_concurrency():\n    startup, shutdown = MagicMock(), MagicMock()\n    app = FastAPI(lifespan=make_lifespan(startup, shutdown))\n\n    async def enter_client(context):\n        # We must re-enter the profile context in the new thread\n        with context:\n            # Use random sleeps to interleave clients\n            await anyio.sleep(random.random())\n            async with PrefectClient(app):\n                await anyio.sleep(random.random())\n\n    threads = [\n        threading.Thread(\n            target=anyio.run,\n            args=(enter_client, prefect.context.SettingsContext.get().model_copy()),\n        )\n        for _ in range(100)\n    ]\n",
      "variables": [
        "startup",
        "shutdown",
        "app",
        "context",
        "threads",
        "_"
      ],
      "anonymized_code": "def client_context_lifespan_is_robust_to_threaded_concurrency():\n    var_1, var_2 = MagicMock(), MagicMock()\n    var_3 = FastAPI(lifespan=make_lifespan(var_1, var_2))\n\n    async def enter_client(var_4):\n        # We must re-enter the profile var_4 in the new thread\n        with var_4:\n            # Use random sleeps to interleave clients\n            await anyio.sleep(random.random())\n            async with PrefectClient(var_3):\n                await anyio.sleep(random.random())\n\n    var_5 = [\n        threading.Thread(\n            target=anyio.run,\n            args=(enter_client, prefect.var_4.SettingsContext.get().model_copy()),\n        )\n        for var_6 in range(100)\n    ]\n",
      "lines_processed": 19,
      "total_lines": 124,
      "llm_code": null,
      "llm_variables": null
    }
  ],
  "psf_requests": [
    {
      "file_path": "cookies.py",
      "code": "def cookiejar_from_dict(cookie_dict, cookiejar=None, overwrite=True):\n    \"\"\"Returns a CookieJar from a key/value dictionary.\n\n    :param cookie_dict: Dict of key/values to insert into CookieJar.\n    :param cookiejar: (optional) A cookiejar to add the cookies to.\n    :param overwrite: (optional) If False, will not replace cookies\n        already in the jar with new ones.\n    :rtype: CookieJar\n    \"\"\"\n    if cookiejar is None:\n        cookiejar = RequestsCookieJar()\n\n    if cookie_dict is not None:\n        names_from_jar = [cookie.name for cookie in cookiejar]\n        for name in cookie_dict:\n            if overwrite or (name not in names_from_jar):\n                cookiejar.set_cookie(create_cookie(name, cookie_dict[name]))\n\n    return cookiejar\n",
      "variables": [
        "cookie_dict",
        "cookiejar",
        "overwrite",
        "names_from_jar",
        "cookie",
        "name"
      ],
      "anonymized_code": "def cookiejar_from_dict(var_1, var_2=None, var_3=True):\n    \"\"\"Returns a CookieJar from a key/value dictionary.\n\n    :param var_1: Dict of key/values to insert into CookieJar.\n    :param var_2: (optional) A var_2 to add the cookies to.\n    :param var_3: (optional) If False, will not replace cookies\n        already in the jar with new ones.\n    :rtype: CookieJar\n    \"\"\"\n    if var_2 is None:\n        var_2 = RequestsCookieJar()\n\n    if var_1 is not None:\n        var_4 = [var_5.var_6 for var_5 in var_2]\n        for var_6 in var_1:\n            if var_3 or (var_6 not in var_4):\n                var_2.set_cookie(create_cookie(var_6, var_1[var_6]))\n\n    return var_2\n",
      "lines_processed": 19,
      "total_lines": 561,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "auth.py",
      "code": "def _basic_auth_str(username, password):\n    \"\"\"Returns a Basic Auth string.\"\"\"\n\n    # \"I want us to put a big-ol' comment on top of it that\n    # says that this behaviour is dumb but we need to preserve\n    # it because people are relying on it.\"\n    #    - Lukasa\n    #\n    # These are here solely to maintain backwards compatibility\n    # for things like ints. This will be removed in 3.0.0.\n    if not isinstance(username, basestring):\n        warnings.warn(\n            \"Non-string usernames will no longer be supported in Requests \"\n            \"3.0.0. Please convert the object you've passed in ({!r}) to \"\n            \"a string or bytes object in the near future to avoid \"\n            \"problems.\".format(username),\n            category=DeprecationWarning,\n        )\n        username = str(username)\n",
      "variables": [
        "username",
        "password"
      ],
      "anonymized_code": "def _basic_auth_str(var_1, var_2):\n    \"\"\"Returns a Basic Auth string.\"\"\"\n\n    # \"I want us to put a big-ol' comment on top of it that\n    # says that this behaviour is dumb but we need to preserve\n    # it because people are relying on it.\"\n    #    - Lukasa\n    #\n    # These are here solely to maintain backwards compatibility\n    # for things like ints. This will be removed in 3.0.0.\n    if not isinstance(var_1, basestring):\n        warnings.warn(\n            \"Non-string usernames will no longer be supported in Requests \"\n            \"3.0.0. Please convert the object you've passed in ({!r}) to \"\n            \"a string or bytes object in the near future to avoid \"\n            \"problems.\".format(var_1),\n            category=DeprecationWarning,\n        )\n        var_1 = str(var_1)\n",
      "lines_processed": 19,
      "total_lines": 314,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "sessions.py",
      "code": "def merge_setting(request_setting, session_setting, dict_class=OrderedDict):\n    \"\"\"Determines appropriate setting for a given request, taking into account\n    the explicit setting on that request, and the setting in the session. If a\n    setting is a dictionary, they will be merged together using `dict_class`\n    \"\"\"\n\n    if session_setting is None:\n        return request_setting\n\n    if request_setting is None:\n        return session_setting\n\n    # Bypass if not a dictionary (e.g. verify)\n    if not (\n        isinstance(session_setting, Mapping) and isinstance(request_setting, Mapping)\n    ):\n        return request_setting\n\n    merged_setting = dict_class(to_key_val_list(session_setting))\n",
      "variables": [
        "request_setting",
        "session_setting",
        "dict_class",
        "merged_setting"
      ],
      "anonymized_code": "def merge_setting(var_1, var_2, var_3=OrderedDict):\n    \"\"\"Determines appropriate setting for a given request, taking into account\n    the explicit setting on that request, and the setting in the session. If a\n    setting is a dictionary, they will be merged together using `var_3`\n    \"\"\"\n\n    if var_2 is None:\n        return var_1\n\n    if var_1 is None:\n        return var_2\n\n    # Bypass if not a dictionary (e.g. verify)\n    if not (\n        isinstance(var_2, Mapping) and isinstance(var_1, Mapping)\n    ):\n        return var_1\n\n    var_4 = var_3(to_key_val_list(var_2))\n",
      "lines_processed": 19,
      "total_lines": 831,
      "llm_code": "def merge_setting(setting, request_setting, merge_func=OrderedDict):\n    \"\"\"Determines appropriate setting for a given request, taking into account\n    the explicit setting on that request, and the setting in the session. If a\n    setting is a dictionary, they will be merged together using `merge_func`\n    \"\"\"\n\n    if request_setting is None:\n        return setting\n\n    if setting is None:\n        return request_setting\n\n    # Bypass if not a dictionary (e.g. verify)\n    if not (\n        isinstance(request_setting, Mapping) and isinstance(setting, Mapping)\n    ):\n        return setting\n\n    merged_setting = merge_func(to_key_val_list(request_setting))\n",
      "llm_variables": [
        "setting",
        "request_setting",
        "merge_func",
        "merged_setting"
      ]
    },
    {
      "file_path": "hooks.py",
      "code": "def default_hooks():\n    return {event: [] for event in HOOKS}\n",
      "variables": [
        "event"
      ],
      "anonymized_code": "def default_hooks():\n    return {var_1: [] for var_1 in HOOKS}\n",
      "lines_processed": 2,
      "total_lines": 33,
      "llm_code": "def default_hooks():\n    return {hook: [] for hook in HOOKS}\n",
      "llm_variables": [
        "hook"
      ]
    },
    {
      "file_path": "hooks.py",
      "code": "def dispatch_hook(key, hooks, hook_data, **kwargs):\n    \"\"\"Dispatches a hook dictionary on a given piece of data.\"\"\"\n    hooks = hooks or {}\n    hooks = hooks.get(key)\n    if hooks:\n        if hasattr(hooks, \"__call__\"):\n            hooks = [hooks]\n        for hook in hooks:\n            _hook_data = hook(hook_data, **kwargs)\n            if _hook_data is not None:\n                hook_data = _hook_data\n    return hook_data\n",
      "variables": [
        "key",
        "hooks",
        "hook_data",
        "kwargs",
        "hook",
        "_hook_data"
      ],
      "anonymized_code": "def dispatch_hook(var_1, var_2, var_3, **var_4):\n    \"\"\"Dispatches a var_5 dictionary on a given piece of data.\"\"\"\n    var_2 = var_2 or {}\n    var_2 = var_2.get(var_1)\n    if var_2:\n        if hasattr(var_2, \"__call__\"):\n            var_2 = [var_2]\n        for var_5 in var_2:\n            var_6 = var_5(var_3, **var_4)\n            if var_6 is not None:\n                var_3 = var_6\n    return var_3\n",
      "lines_processed": 12,
      "total_lines": 33,
      "llm_code": "def dispatch_hook(data, hook, context, **kwargs):\n    \"\"\"Dispatches a hook_item dictionary on a given piece of data.\"\"\"\n    hook = hook or {}\n    hook = hook.get(data)\n    if hook:\n        if hasattr(hook, \"__call__\"):\n            hook = [hook]\n        for hook_item in hook:\n            result = hook_item(context, **kwargs)\n            if result is not None:\n                context = result\n    return context\n",
      "llm_variables": [
        "data",
        "hook",
        "context",
        "kwargs",
        "hook_item",
        "result"
      ]
    },
    {
      "file_path": "compat.py",
      "code": "def _resolve_char_detection():\n    \"\"\"Find supported character detection libraries.\"\"\"\n    chardet = None\n    for lib in (\"chardet\", \"charset_normalizer\"):\n        if chardet is None:\n            try:\n                chardet = importlib.import_module(lib)\n            except ImportError:\n                pass\n    return chardet\n",
      "variables": [
        "chardet",
        "lib"
      ],
      "anonymized_code": "def _resolve_char_detection():\n    \"\"\"Find supported character detection libraries.\"\"\"\n    var_1 = None\n    for var_2 in (\"var_1\", \"charset_normalizer\"):\n        if var_1 is None:\n            try:\n                var_1 = importlib.import_module(var_2)\n            except ImportError:\n                pass\n    return var_1\n",
      "lines_processed": 10,
      "total_lines": 106,
      "llm_code": "def _resolve_char_detection():\n    \"\"\"Find supported character detection libraries.\"\"\"\n    library = None\n    for candidate in (\"library\", \"charset_normalizer\"):\n        if library is None:\n            try:\n                library = importlib.import_module(candidate)\n            except ImportError:\n                pass\n    return library\n",
      "llm_variables": [
        "library",
        "candidate"
      ]
    },
    {
      "file_path": "__init__.py",
      "code": "def _check_cryptography(cryptography_version):\n    # cryptography < 1.3.4\n    try:\n        cryptography_version = list(map(int, cryptography_version.split(\".\")))\n    except ValueError:\n        return\n\n    if cryptography_version < [1, 3, 4]:\n        warning = \"Old version of cryptography ({}) may cause slowdown.\".format(\n            cryptography_version\n        )\n        warnings.warn(warning, RequestsDependencyWarning)\n",
      "variables": [
        "cryptography_version",
        "warning"
      ],
      "anonymized_code": "def _check_cryptography(var_1):\n    # cryptography < 1.3.4\n    try:\n        var_1 = list(map(int, var_1.split(\".\")))\n    except ValueError:\n        return\n\n    if var_1 < [1, 3, 4]:\n        var_2 = \"Old version of cryptography ({}) may cause slowdown.\".format(\n            var_1\n        )\n        warnings.warn(var_2, RequestsDependencyWarning)\n",
      "lines_processed": 12,
      "total_lines": 184,
      "llm_code": "def _check_cryptography(version):\n    # cryptography < 1.3.4\n    try:\n        version = list(map(int, version.split(\".\")))\n    except ValueError:\n        return\n\n    if version < [1, 3, 4]:\n        message = \"Old version of cryptography ({}) may cause slowdown.\".format(\n            version\n        )\n        warnings.warn(message, RequestsDependencyWarning)\n",
      "llm_variables": [
        "version",
        "message"
      ]
    },
    {
      "file_path": "__init__.py",
      "code": "def check_compatibility(urllib3_version, chardet_version, charset_normalizer_version):\n    urllib3_version = urllib3_version.split(\".\")\n    assert urllib3_version != [\"dev\"]  # Verify urllib3 isn't installed from git.\n\n    # Sometimes, urllib3 only reports its version as 16.1.\n    if len(urllib3_version) == 2:\n        urllib3_version.append(\"0\")\n\n    # Check urllib3 for compatibility.\n    major, minor, patch = urllib3_version  # noqa: F811\n    major, minor, patch = int(major), int(minor), int(patch)\n    # urllib3 >= 1.21.1\n    assert major >= 1\n    if major == 1:\n        assert minor >= 21\n\n    # Check charset_normalizer for compatibility.\n    if chardet_version:\n        major, minor, patch = chardet_version.split(\".\")[:3]\n",
      "variables": [
        "urllib3_version",
        "chardet_version",
        "charset_normalizer_version",
        "major",
        "minor",
        "patch"
      ],
      "anonymized_code": "def check_compatibility(var_1, var_2, var_3):\n    var_1 = var_1.split(\".\")\n    assert var_1 != [\"dev\"]  # Verify urllib3 isn't installed from git.\n\n    # Sometimes, urllib3 only reports its version as 16.1.\n    if len(var_1) == 2:\n        var_1.append(\"0\")\n\n    # Check urllib3 for compatibility.\n    var_4, var_5, var_6 = var_1  # noqa: F811\n    var_4, var_5, var_6 = int(var_4), int(var_5), int(var_6)\n    # urllib3 >= 1.21.1\n    assert var_4 >= 1\n    if var_4 == 1:\n        assert var_5 >= 21\n\n    # Check charset_normalizer for compatibility.\n    if var_2:\n        var_4, var_5, var_6 = var_2.split(\".\")[:3]\n",
      "lines_processed": 19,
      "total_lines": 184,
      "llm_code": "def check_compatibility(urllib3_version, charset_normalizer_version, is_installed):\n    urllib3_version = urllib3_version.split(\".\")\n    assert urllib3_version != [\"dev\"]  # Verify urllib3 isn't installed from git.\n\n    # Sometimes, urllib3 only reports its version as 16.1.\n    if len(urllib3_version) == 2:\n        urllib3_version.append(\"0\")\n\n    # Check urllib3 for compatibility.\n    major, minor, patch = urllib3_version  # noqa: F811\n    major, minor, patch = int(major), int(minor), int(patch)\n    # urllib3 >= 1.21.1\n    assert major >= 1\n    if major == 1:\n        assert minor >= 21\n\n    # Check charset_normalizer for compatibility.\n    if charset_normalizer_version:\n        major, minor, patch = charset_normalizer_version.split(\".\")[:3]\n",
      "llm_variables": [
        "urllib3_version",
        "charset_normalizer_version",
        "is_installed",
        "major",
        "minor",
        "patch"
      ]
    },
    {
      "file_path": "status_codes.py",
      "code": "def _init():\n    for code, titles in _codes.items():\n        for title in titles:\n            setattr(codes, title, code)\n            if not title.startswith((\"\\\\\", \"/\")):\n                setattr(codes, title.upper(), code)\n\n    def doc(code):\n        names = \", \".join(f\"``{n}``\" for n in _codes[code])\n        return \"* %d: %s\" % (code, names)\n\n    global __doc__\n    __doc__ = (\n        __doc__ + \"\\n\" + \"\\n\".join(doc(code) for code in sorted(_codes))\n        if __doc__ is not None\n        else None\n    )\n",
      "variables": [
        "code",
        "titles",
        "title",
        "names",
        "n",
        "__doc__"
      ],
      "anonymized_code": "def _init():\n    for var_1, var_2 in _codes.items():\n        for var_3 in var_2:\n            setattr(codes, var_3, var_1)\n            if not var_3.startswith((\"\\\\\", \"/\")):\n                setattr(codes, var_3.upper(), var_1)\n\n    def doc(var_1):\n        var_4 = \", \".join(f\"``{var_5}``\" for var_5 in _codes[var_1])\n        return \"* %d: %s\" % (var_1, var_4)\n\n    global var_6\n    var_6 = (\n        var_6 + \"\\var_5\" + \"\\var_5\".join(doc(var_1) for var_1 in sorted(_codes))\n        if var_6 is not None\n        else None\n    )\n",
      "lines_processed": 17,
      "total_lines": 128,
      "llm_code": "def _init():\n    for name, code in _codes.items():\n        for var_3 in code:\n            setattr(codes, var_3, name)\n            if not var_3.startswith((\"\\\\\", \"/\")):\n                setattr(codes, var_3.upper(), name)\n\n    def doc(name):\n        var_4 = \", \".join(f\"``{var_5}``\" for var_5 in _codes[name])\n        return \"* %d: %s\" % (name, var_4)\n\n    global var_6\n    var_6 = (\n        var_6 + \"\\var_5\" + \"\\var_5\".join(doc(name) for name in sorted(_codes))\n        if var_6 is not None\n        else None\n    )\n",
      "llm_variables": [
        "name",
        "code",
        "var_3",
        "var_4",
        "var_5",
        "var_6"
      ]
    },
    {
      "file_path": "api.py",
      "code": "def options(url, **kwargs):\n    r\"\"\"Sends an OPTIONS request.\n\n    :param url: URL for the new :class:`Request` object.\n    :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    return request(\"options\", url, **kwargs)\n",
      "variables": [
        "url",
        "kwargs"
      ],
      "anonymized_code": "def options(var_1, **var_2):\n    r\"\"\"Sends an OPTIONS request.\n\n    :param var_1: URL for the new :class:`Request` object.\n    :param \\*\\*var_2: Optional arguments that ``request`` takes.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    return request(\"options\", var_1, **var_2)\n",
      "lines_processed": 10,
      "total_lines": 157,
      "llm_code": "def options(url, **kwargs):\n    r\"\"\"Sends an OPTIONS request.\n\n    :param url: URL for the new :class:`Request` object.\n    :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    return request(\"options\", url, **kwargs)\n",
      "llm_variables": [
        "url",
        "kwargs"
      ]
    },
    {
      "file_path": "api.py",
      "code": "def post(url, data=None, json=None, **kwargs):\n    r\"\"\"Sends a POST request.\n\n    :param url: URL for the new :class:`Request` object.\n    :param data: (optional) Dictionary, list of tuples, bytes, or file-like\n        object to send in the body of the :class:`Request`.\n    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.\n    :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    return request(\"post\", url, data=data, json=json, **kwargs)\n",
      "variables": [
        "url",
        "data",
        "json",
        "kwargs"
      ],
      "anonymized_code": "def post(var_1, var_2=None, var_3=None, **var_4):\n    r\"\"\"Sends a POST request.\n\n    :param var_1: URL for the new :class:`Request` object.\n    :param var_2: (optional) Dictionary, list of tuples, bytes, or file-like\n        object to send in the body of the :class:`Request`.\n    :param var_3: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.\n    :param \\*\\*var_4: Optional arguments that ``request`` takes.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    return request(\"post\", var_1, var_2=var_2, var_3=var_3, **var_4)\n",
      "lines_processed": 13,
      "total_lines": 157,
      "llm_code": "def post(url, data=None, json=None, **kwargs):\n    r\"\"\"Sends a POST request.\n\n    :param url: URL for the new :class:`Request` object.\n    :param data: (optional) Dictionary, list of tuples, bytes, or file-like\n        object to send in the body of the :class:`Request`.\n    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.\n    :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    return request(\"post\", url, data=data, json=json, **kwargs)\n",
      "llm_variables": [
        "url",
        "data",
        "json",
        "kwargs"
      ]
    },
    {
      "file_path": "api.py",
      "code": "def head(url, **kwargs):\n    r\"\"\"Sends a HEAD request.\n\n    :param url: URL for the new :class:`Request` object.\n    :param \\*\\*kwargs: Optional arguments that ``request`` takes. If\n        `allow_redirects` is not provided, it will be set to `False` (as\n        opposed to the default :meth:`request` behavior).\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    kwargs.setdefault(\"allow_redirects\", False)\n    return request(\"head\", url, **kwargs)\n",
      "variables": [
        "url",
        "kwargs"
      ],
      "anonymized_code": "def head(var_1, **var_2):\n    r\"\"\"Sends a HEAD request.\n\n    :param var_1: URL for the new :class:`Request` object.\n    :param \\*\\*var_2: Optional arguments that ``request`` takes. If\n        `allow_redirects` is not provided, it will be set to `False` (as\n        opposed to the default :meth:`request` behavior).\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    var_2.setdefault(\"allow_redirects\", False)\n    return request(\"head\", var_1, **var_2)\n",
      "lines_processed": 13,
      "total_lines": 157,
      "llm_code": "def head(url, **kwargs):\n    r\"\"\"Sends a HEAD request.\n\n    :param url: URL for the new :class:`Request` object.\n    :param \\*\\*kwargs: Optional arguments that ``request`` takes. If\n        `allow_redirects` is not provided, it will be set to `False` (as\n        opposed to the default :meth:`request` behavior).\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    kwargs.setdefault(\"allow_redirects\", False)\n    return request(\"head\", url, **kwargs)",
      "llm_variables": [
        "url",
        "kwargs"
      ]
    },
    {
      "file_path": "api.py",
      "code": "def get(url, params=None, **kwargs):\n    r\"\"\"Sends a GET request.\n\n    :param url: URL for the new :class:`Request` object.\n    :param params: (optional) Dictionary, list of tuples or bytes to send\n        in the query string for the :class:`Request`.\n    :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    return request(\"get\", url, params=params, **kwargs)\n",
      "variables": [
        "url",
        "params",
        "kwargs"
      ],
      "anonymized_code": "def get(var_1, var_2=None, **var_3):\n    r\"\"\"Sends a GET request.\n\n    :param var_1: URL for the new :class:`Request` object.\n    :param var_2: (optional) Dictionary, list of tuples or bytes to send\n        in the query string for the :class:`Request`.\n    :param \\*\\*var_3: Optional arguments that ``request`` takes.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    return request(\"get\", var_1, var_2=var_2, **var_3)\n",
      "lines_processed": 12,
      "total_lines": 157,
      "llm_code": "def get(url, params=None, **kwargs):\n    r\"\"\"Sends a GET request.\n\n    :param url: URL for the new :class:`Request` object.\n    :param params: (optional) Dictionary, list of tuples or bytes to send\n        in the query string for the :class:`Request`.\n    :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    return request(\"get\", url, params=params, **kwargs)\n",
      "llm_variables": [
        "url",
        "params",
        "kwargs"
      ]
    },
    {
      "file_path": "api.py",
      "code": "def put(url, data=None, **kwargs):\n    r\"\"\"Sends a PUT request.\n\n    :param url: URL for the new :class:`Request` object.\n    :param data: (optional) Dictionary, list of tuples, bytes, or file-like\n        object to send in the body of the :class:`Request`.\n    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.\n    :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    return request(\"put\", url, data=data, **kwargs)\n",
      "variables": [
        "url",
        "data",
        "kwargs"
      ],
      "anonymized_code": "def put(var_1, var_2=None, **var_3):\n    r\"\"\"Sends a PUT request.\n\n    :param var_1: URL for the new :class:`Request` object.\n    :param var_2: (optional) Dictionary, list of tuples, bytes, or file-like\n        object to send in the body of the :class:`Request`.\n    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.\n    :param \\*\\*var_3: Optional arguments that ``request`` takes.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    return request(\"put\", var_1, var_2=var_2, **var_3)\n",
      "lines_processed": 13,
      "total_lines": 157,
      "llm_code": "def put(url, data=None, **kwargs):\n    r\"\"\"Sends a PUT request.\n\n    :param url: URL for the new :class:`Request` object.\n    :param data: (optional) Dictionary, list of tuples, bytes, or file-like\n        object to send in the body of the :class:`Request`.\n    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.\n    :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    return request(\"put\", url, data=data, **kwargs)\n",
      "llm_variables": [
        "url",
        "data",
        "kwargs"
      ]
    },
    {
      "file_path": "api.py",
      "code": "def patch(url, data=None, **kwargs):\n    r\"\"\"Sends a PATCH request.\n\n    :param url: URL for the new :class:`Request` object.\n    :param data: (optional) Dictionary, list of tuples, bytes, or file-like\n        object to send in the body of the :class:`Request`.\n    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.\n    :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    return request(\"patch\", url, data=data, **kwargs)\n",
      "variables": [
        "url",
        "data",
        "kwargs"
      ],
      "anonymized_code": "def patch(var_1, var_2=None, **var_3):\n    r\"\"\"Sends a PATCH request.\n\n    :param var_1: URL for the new :class:`Request` object.\n    :param var_2: (optional) Dictionary, list of tuples, bytes, or file-like\n        object to send in the body of the :class:`Request`.\n    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.\n    :param \\*\\*var_3: Optional arguments that ``request`` takes.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    return request(\"patch\", var_1, var_2=var_2, **var_3)\n",
      "lines_processed": 13,
      "total_lines": 157,
      "llm_code": "def patch(url, data=None, **kwargs):\n    r\"\"\"Sends a PATCH request.\n\n    :param url: URL for the new :class:`Request` object.\n    :param data: (optional) Dictionary, list of tuples, bytes, or file-like\n        object to send in the body of the :class:`Request`.\n    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.\n    :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    return request(\"patch\", url, data=data, **kwargs)\n",
      "llm_variables": [
        "url",
        "data",
        "kwargs"
      ]
    },
    {
      "file_path": "api.py",
      "code": "def delete(url, **kwargs):\n    r\"\"\"Sends a DELETE request.\n\n    :param url: URL for the new :class:`Request` object.\n    :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    return request(\"delete\", url, **kwargs)\n",
      "variables": [
        "url",
        "kwargs"
      ],
      "anonymized_code": "def delete(var_1, **var_2):\n    r\"\"\"Sends a DELETE request.\n\n    :param var_1: URL for the new :class:`Request` object.\n    :param \\*\\*var_2: Optional arguments that ``request`` takes.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    return request(\"delete\", var_1, **var_2)\n",
      "lines_processed": 10,
      "total_lines": 157,
      "llm_code": "def delete(url, **kwargs):\n    r\"\"\"Sends a DELETE request.\n\n    :param url: URL for the new :class:`Request` object.\n    :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    \"\"\"\n\n    return request(\"delete\", url, **kwargs)\n",
      "llm_variables": [
        "url",
        "kwargs"
      ]
    },
    {
      "file_path": "_internal_utils.py",
      "code": "def unicode_is_ascii(u_string):\n    \"\"\"Determine if unicode string only contains ASCII characters.\n\n    :param str u_string: unicode string to check. Must be unicode\n        and not Python 2 `str`.\n    :rtype: bool\n    \"\"\"\n    assert isinstance(u_string, str)\n    try:\n        u_string.encode(\"ascii\")\n        return True\n    except UnicodeEncodeError:\n        return False\n",
      "variables": [
        "u_string"
      ],
      "anonymized_code": "def unicode_is_ascii(var_1):\n    \"\"\"Determine if unicode string only contains ASCII characters.\n\n    :param str var_1: unicode string to check. Must be unicode\n        and not Python 2 `str`.\n    :rtype: bool\n    \"\"\"\n    assert isinstance(var_1, str)\n    try:\n        var_1.encode(\"ascii\")\n        return True\n    except UnicodeEncodeError:\n        return False\n",
      "lines_processed": 13,
      "total_lines": 50,
      "llm_code": "def unicode_is_ascii(s):\n    \"\"\"Determine if unicode string only contains ASCII characters.\n\n    :param str s: unicode string to check. Must be unicode\n        and not Python 2 `str`.\n    :rtype: bool\n    \"\"\"\n    assert isinstance(s, str)\n    try:\n        s.encode(\"ascii\")\n        return True\n    except UnicodeEncodeError:\n        return False\n",
      "llm_variables": [
        "s"
      ]
    },
    {
      "file_path": "_internal_utils.py",
      "code": "def to_native_string(string, encoding=\"ascii\"):\n    \"\"\"Given a string object, regardless of type, returns a representation of\n    that string in the native string type, encoding and decoding where\n    necessary. This assumes ASCII unless told otherwise.\n    \"\"\"\n    if isinstance(string, builtin_str):\n        out = string\n    else:\n        out = string.decode(encoding)\n\n    return out\n",
      "variables": [
        "string",
        "encoding",
        "out"
      ],
      "anonymized_code": "def to_native_string(var_1, var_2=\"ascii\"):\n    \"\"\"Given a var_1 object, regardless of type, returns a representation of\n    that var_1 in the native var_1 type, var_2 and decoding where\n    necessary. This assumes ASCII unless told otherwise.\n    \"\"\"\n    if isinstance(var_1, builtin_str):\n        var_3 = var_1\n    else:\n        var_3 = var_1.decode(var_2)\n\n    return var_3\n",
      "lines_processed": 11,
      "total_lines": 50,
      "llm_code": "def to_native_string(s, encoding=\"ascii\"):\n    \"\"\"Given a s object, regardless of type, returns a representation of\n    that s in the native s type, encoding and decoding where\n    necessary. This assumes ASCII unless told otherwise.\n    \"\"\"\n    if isinstance(s, builtin_str):\n        result = s\n    else:\n        result = s.decode(encoding)\n\n    return result\n",
      "llm_variables": [
        "s",
        "encoding",
        "result"
      ]
    },
    {
      "file_path": "help.py",
      "code": "def info():\n    \"\"\"Generate information for a bug report.\"\"\"\n    try:\n        platform_info = {\n            \"system\": platform.system(),\n            \"release\": platform.release(),\n        }\n    except OSError:\n        platform_info = {\n            \"system\": \"Unknown\",\n            \"release\": \"Unknown\",\n        }\n\n    implementation_info = _implementation()\n    urllib3_info = {\"version\": urllib3.__version__}\n    charset_normalizer_info = {\"version\": None}\n    chardet_info = {\"version\": None}\n    if charset_normalizer:\n        charset_normalizer_info = {\"version\": charset_normalizer.__version__}\n",
      "variables": [
        "platform_info",
        "implementation_info",
        "urllib3_info",
        "charset_normalizer_info",
        "chardet_info"
      ],
      "anonymized_code": "def info():\n    \"\"\"Generate information for a bug report.\"\"\"\n    try:\n        var_1 = {\n            \"system\": platform.system(),\n            \"release\": platform.release(),\n        }\n    except OSError:\n        var_1 = {\n            \"system\": \"Unknown\",\n            \"release\": \"Unknown\",\n        }\n\n    var_2 = _implementation()\n    var_3 = {\"version\": urllib3.__version__}\n    var_4 = {\"version\": None}\n    var_5 = {\"version\": None}\n    if charset_normalizer:\n        var_4 = {\"version\": charset_normalizer.__version__}\n",
      "lines_processed": 19,
      "total_lines": 134,
      "llm_code": "def info():\n    \"\"\"Generate information for a bug report.\"\"\"\n    try:\n        system_info = {\n            \"system\": platform.system(),\n            \"release\": platform.release(),\n        }\n    except OSError:\n        system_info = {\n            \"system\": \"Unknown\",\n            \"release\": \"Unknown\",\n        }\n\n    implementation_info = _implementation()\n    version_info = {\"version\": urllib3.__version__}\n    default_version = {\"version\": None}\n    another_version = {\"version\": None}\n    if charset_normalizer:\n        default_version = {\"version\": charset_normalizer.__version__}\n",
      "llm_variables": [
        "system_info",
        "implementation_info",
        "version_info",
        "default_version",
        "another_version"
      ]
    }
  ],
  "pallets_jinja": [
    {
      "file_path": "generate_identifier_pattern.py",
      "code": "def build_pattern(ranges):\n    \"\"\"Output the regex pattern for ranges of characters.\n\n    One and two character ranges output the individual characters.\n    \"\"\"\n    out = []\n\n    for a, b in ranges:\n        if a == b:  # single char\n            out.append(a)\n        elif ord(b) - ord(a) == 1:  # two chars, range is redundant\n            out.append(a)\n            out.append(b)\n        else:\n            out.append(f\"{a}-{b}\")\n\n    return \"\".join(out)\n",
      "variables": [
        "ranges",
        "out",
        "a",
        "b"
      ],
      "anonymized_code": "def build_pattern(var_1):\n    \"\"\"Output the regex pattern for var_1 of characters.\n\n    One and two character var_1 output the individual characters.\n    \"\"\"\n    var_2 = []\n\n    for var_3, var_4 in var_1:\n        if var_3 == var_4:  # single char\n            var_2.append(var_3)\n        elif ord(var_4) - ord(var_3) == 1:  # two chars, range is redundant\n            var_2.append(var_3)\n            var_2.append(var_4)\n        else:\n            var_2.append(f\"{var_3}-{var_4}\")\n\n    return \"\".join(var_2)\n",
      "lines_processed": 17,
      "total_lines": 73,
      "llm_code": "def build_pattern(characters):\n    \"\"\"Output the regex pattern for characters of characters.\n\n    One and two character characters output the individual characters.\n    \"\"\"\n    result = []\n\n    for index, char in characters:\n        if index == char:  # single char\n            result.append(index)\n        elif ord(char) - ord(index) == 1:  # two chars, range is redundant\n            result.append(index)\n            result.append(char)\n        else:\n            result.append(f\"{index}-{char}\")\n\n    return \"\".join(result)\n",
      "llm_variables": [
        "characters",
        "result",
        "index",
        "char"
      ]
    },
    {
      "file_path": "generate_identifier_pattern.py",
      "code": "def get_characters():\n    \"\"\"Find every Unicode character that is valid in a Python `identifier`_ but\n    is not matched by the regex ``\\\\w`` group.\n\n    ``\\\\w`` matches some characters that aren't valid in identifiers, but\n    :meth:`str.isidentifier` will catch that later in lexing.\n\n    All start characters are valid continue characters, so we only test for\n    continue characters.\n\n    _identifier: https://docs.python.org/3/reference/lexical_analysis.html#identifiers\n    \"\"\"\n    for cp in range(sys.maxunicode + 1):\n        s = chr(cp)\n\n        if (\"a\" + s).isidentifier() and not re.match(r\"\\w\", s):\n            yield s\n",
      "variables": [
        "cp",
        "s"
      ],
      "anonymized_code": "def get_characters():\n    \"\"\"Find every Unicode character that is valid in a Python `identifier`_ but\n    is not matched by the regex ``\\\\w`` group.\n\n    ``\\\\w`` matches some characters that aren't valid in identifiers, but\n    :meth:`str.isidentifier` will catch that later in lexing.\n\n    All start characters are valid continue characters, so we only test for\n    continue characters.\n\n    _identifier: https://docs.python.org/3/reference/lexical_analysis.html#identifiers\n    \"\"\"\n    for var_1 in range(sys.maxunicode + 1):\n        var_2 = chr(var_1)\n\n        if (\"a\" + var_2).isidentifier() and not re.match(r\"\\w\", var_2):\n            yield var_2\n",
      "lines_processed": 17,
      "total_lines": 73,
      "llm_code": "def get_characters():\n    \"\"\"Find every Unicode character that is valid in a Python `identifier`_ but\n    is not matched by the regex ``\\\\w`` group.\n\n    ``\\\\w`` matches some characters that aren't valid in identifiers, but\n    :meth:`str.isidentifier` will catch that later in lexing.\n\n    All start characters are valid continue characters, so we only test for\n    continue characters.\n\n    _identifier: https://docs.python.org/3/reference/lexical_analysis.html#identifiers\n    \"\"\"\n    for code_point in range(sys.maxunicode + 1):\n        character = chr(code_point)\n\n        if (\"a\" + character).isidentifier() and not re.match(r\"\\w\", character):\n            yield character\n",
      "llm_variables": [
        "code_point",
        "character"
      ]
    },
    {
      "file_path": "generate_identifier_pattern.py",
      "code": "def main():\n    \"\"\"Build the regex pattern and write it to ``jinja2/_identifier.py``.\"\"\"\n    pattern = build_pattern(collapse_ranges(get_characters()))\n    filename = os.path.abspath(\n        os.path.join(os.path.dirname(__file__), \"..\", \"src\", \"jinja2\", \"_identifier.py\")\n    )\n\n    with open(filename, \"w\", encoding=\"utf8\") as f:\n        f.write(\"# generated by scripts/generate_identifier_pattern.py\")\n        f.write(f\"# Python {sys.version_info[0]}.{sys.version_info[1]}\\n\")\n        f.write(\"import re\\n\\n\")\n        f.write(\"pattern = re.compile(\\n\")\n        f.write(f'    r\"[\\\\w{pattern}]+\"  # noqa: B950\\n')\n        f.write(\")\\n\")\n",
      "variables": [
        "pattern",
        "filename",
        "f"
      ],
      "anonymized_code": "def main():\n    \"\"\"Build the regex var_1 and write it to ``jinja2/_identifier.py``.\"\"\"\n    var_1 = build_pattern(collapse_ranges(get_characters()))\n    var_2 = os.path.abspath(\n        os.path.join(os.path.dirname(__file__), \"..\", \"src\", \"jinja2\", \"_identifier.py\")\n    )\n\n    with open(var_2, \"w\", encoding=\"utf8\") as var_3:\n        var_3.write(\"# generated by scripts/generate_identifier_pattern.py\")\n        var_3.write(var_3\"# Python {sys.version_info[0]}.{sys.version_info[1]}\\n\")\n        var_3.write(\"import re\\n\\n\")\n        var_3.write(\"var_1 = re.compile(\\n\")\n        var_3.write(var_3'    r\"[\\\\w{var_1}]+\"  # noqa: B950\\n')\n        var_3.write(\")\\n\")\n",
      "lines_processed": 14,
      "total_lines": 73,
      "llm_code": "def main():\n    \"\"\"Build the regex pattern and write it to ``jinja2/_identifier.py``.\"\"\"\n    pattern = build_pattern(collapse_ranges(get_characters()))\n    file_path = os.path.abspath(\n        os.path.join(os.path.dirname(__file__), \"..\", \"src\", \"jinja2\", \"_identifier.py\")\n    )\n\n    with open(file_path, \"w\", encoding=\"utf8\") as file:\n        file.write(\"# generated by scripts/generate_identifier_pattern.py\")\n        file.write(file\"# Python {sys.version_info[0]}.{sys.version_info[1]}\\n\")\n        file.write(\"import re\\n\\n\")\n        file.write(\"pattern = re.compile(\\n\")\n        file.write(file'    r\"[\\\\w{pattern}]+\"  # noqa: B950\\n')\n        file.write(\")\\n\")\n",
      "llm_variables": [
        "pattern",
        "file_path",
        "file"
      ]
    },
    {
      "file_path": "generate_identifier_pattern.py",
      "code": "def collapse_ranges(data):\n    \"\"\"Given a sorted list of unique characters, generate ranges representing\n    sequential code points.\n\n    Source: https://stackoverflow.com/a/4629241/400617\n    \"\"\"\n    for _, g in itertools.groupby(enumerate(data), lambda x: ord(x[1]) - x[0]):\n        lb = list(g)\n        yield lb[0][1], lb[-1][1]\n",
      "variables": [
        "data",
        "_",
        "g",
        "x",
        "lb"
      ],
      "anonymized_code": "def collapse_ranges(var_1):\n    \"\"\"Given a sorted list of unique characters, generate ranges representing\n    sequential code points.\n\n    Source: https://stackoverflow.com/a/4629241/400617\n    \"\"\"\n    for var_2, var_3 in itertools.groupby(enumerate(var_1), lambda var_4: ord(var_4[1]) - var_4[0]):\n        var_5 = list(var_3)\n        yield var_5[0][1], var_5[-1][1]\n",
      "lines_processed": 9,
      "total_lines": 73,
      "llm_code": "def collapse_ranges(chars):\n    \"\"\"Given a sorted list of unique characters, generate ranges representing\n    sequential code points.\n\n    Source: https://stackoverflow.com/a/4629241/400617\n    \"\"\"\n    for group_key, group in itertools.groupby(enumerate(chars), lambda x: ord(x[1]) - x[0]):\n        group_list = list(group)\n        yield group_list[0][1], group_list[-1][1]\n",
      "llm_variables": [
        "chars",
        "group_key",
        "group",
        "x",
        "group_list"
      ]
    },
    {
      "file_path": "async_utils.py",
      "code": "def auto_aiter(\n    iterable: \"t.AsyncIterable[V] | t.Iterable[V]\",\n) -> \"t.AsyncIterator[V]\":\n    if hasattr(iterable, \"__aiter__\"):\n        return iterable.__aiter__()\n    else:\n        return _IteratorToAsyncIterator(iter(iterable))\n",
      "variables": [
        "iterable"
      ],
      "anonymized_code": "def auto_aiter(\n    var_1: \"t.AsyncIterable[V] | t.Iterable[V]\",\n) -> \"t.AsyncIterator[V]\":\n    if hasattr(var_1, \"__aiter__\"):\n        return var_1.__aiter__()\n    else:\n        return _IteratorToAsyncIterator(iter(var_1))\n",
      "lines_processed": 7,
      "total_lines": 99,
      "llm_code": "def auto_aiter(\n    iterable: \"t.AsyncIterable[V] | t.Iterable[V]\",\n) -> \"t.AsyncIterator[V]\":\n    if hasattr(iterable, \"__aiter__\"):\n        return iterable.__aiter__()\n    else:\n        return _IteratorToAsyncIterator(iter(iterable))\n",
      "llm_variables": [
        "iterable"
      ]
    },
    {
      "file_path": "async_utils.py",
      "code": "def async_variant(normal_func):  # type: ignore\n    def decorator(async_func):  # type: ignore\n        pass_arg = _PassArg.from_obj(normal_func)\n        need_eval_context = pass_arg is None\n\n        if pass_arg is _PassArg.environment:\n\n            def is_async(args: t.Any) -> bool:\n                return t.cast(bool, args[0].is_async)\n\n        else:\n\n            def is_async(args: t.Any) -> bool:\n                return t.cast(bool, args[0].environment.is_async)\n\n        # Take the doc and annotations from the sync function, but the\n        # name from the async function. Pallets-Sphinx-Themes\n        # build_function_directive expects __wrapped__ to point to the\n        # sync function.\n",
      "variables": [
        "normal_func",
        "async_func",
        "pass_arg",
        "need_eval_context",
        "args"
      ],
      "anonymized_code": "def async_variant(var_1):  # type: ignore\n    def decorator(var_2):  # type: ignore\n        var_3 = _PassArg.from_obj(var_1)\n        var_4 = var_3 is None\n\n        if var_3 is _PassArg.environment:\n\n            def is_async(var_5: t.Any) -> bool:\n                return t.cast(bool, var_5[0].is_async)\n\n        else:\n\n            def is_async(var_5: t.Any) -> bool:\n                return t.cast(bool, var_5[0].environment.is_async)\n\n        # Take the doc and annotations from the sync function, but the\n        # name from the async function. Pallets-Sphinx-Themes\n        # build_function_directive expects __wrapped__ to point to the\n        # sync function.\n",
      "lines_processed": 19,
      "total_lines": 99,
      "llm_code": "def async_variant(func):  # type: ignore\n    def decorator(decorator_func):  # type: ignore\n        pass_arg = _PassArg.from_obj(func)\n        is_pass_arg_none = pass_arg is None\n\n        if pass_arg is _PassArg.environment:\n\n            def is_async(arg: t.Any) -> bool:\n                return t.cast(bool, arg[0].is_async)\n\n        else:\n\n            def is_async(arg: t.Any) -> bool:\n                return t.cast(bool, arg[0].environment.is_async)\n\n        # Take the doc and annotations from the sync function, but the\n        # name from the async function. Pallets-Sphinx-Themes\n        # build_function_directive expects __wrapped__ to point to the\n        # sync function.",
      "llm_variables": [
        "func",
        "decorator_func",
        "pass_arg",
        "is_pass_arg_none",
        "arg"
      ]
    },
    {
      "file_path": "__init__.py",
      "code": "def __getattr__(name: str) -> t.Any:\n    if name == \"__version__\":\n        import importlib.metadata\n        import warnings\n\n        warnings.warn(\n            \"The `__version__` attribute is deprecated and will be removed in\"\n            \" Jinja 3.3. Use feature detection or\"\n            ' `importlib.metadata.version(\"jinja2\")` instead.',\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return importlib.metadata.version(\"jinja2\")\n\n    raise AttributeError(name)\n",
      "variables": [
        "name"
      ],
      "anonymized_code": "def __getattr__(var_1: str) -> t.Any:\n    if var_1 == \"__version__\":\n        import importlib.metadata\n        import warnings\n\n        warnings.warn(\n            \"The `__version__` attribute is deprecated and will be removed in\"\n            \" Jinja 3.3. Use feature detection or\"\n            ' `importlib.metadata.version(\"jinja2\")` instead.',\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return importlib.metadata.version(\"jinja2\")\n\n    raise AttributeError(var_1)\n",
      "lines_processed": 15,
      "total_lines": 57,
      "llm_code": "def __getattr__(name: str) -> t.Any:\n    if name == \"__version__\":\n        import importlib.metadata\n        import warnings\n\n        warnings.warn(\n            \"The `__version__` attribute is deprecated and will be removed in\"\n            \" Jinja 3.3. Use feature detection or\"\n            ' `importlib.metadata.version(\"jinja2\")` instead.',\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return importlib.metadata.version(\"jinja2\")\n\n    raise AttributeError(name)\n",
      "llm_variables": [
        "name"
      ]
    },
    {
      "file_path": "debug.py",
      "code": "def rewrite_traceback_stack(source: str | None = None) -> BaseException:\n    \"\"\"Rewrite the current exception to replace any tracebacks from\n    within compiled template code with tracebacks that look like they\n    came from the template source.\n\n    This must be called within an ``except`` block.\n\n    :param source: For ``TemplateSyntaxError``, the original source if\n        known.\n    :return: The original exception with the rewritten traceback.\n    \"\"\"\n    _, exc_value, tb = sys.exc_info()\n    exc_value = t.cast(BaseException, exc_value)\n    tb = t.cast(TracebackType, tb)\n\n    if isinstance(exc_value, TemplateSyntaxError) and not exc_value.translated:\n        exc_value.translated = True\n        exc_value.source = source\n        # Remove the old traceback, otherwise the frames from the\n",
      "variables": [
        "source",
        "_",
        "exc_value",
        "tb"
      ],
      "anonymized_code": "def rewrite_traceback_stack(var_1: str | None = None) -> BaseException:\n    \"\"\"Rewrite the current exception to replace any tracebacks from\n    within compiled template code with tracebacks that look like they\n    came from the template var_1.\n\n    This must be called within an ``except`` block.\n\n    :param var_1: For ``TemplateSyntaxError``, the original var_1 if\n        known.\n    :return: The original exception with the rewritten traceback.\n    \"\"\"\n    var_2, var_3, var_4 = sys.exc_info()\n    var_3 = t.cast(BaseException, var_3)\n    var_4 = t.cast(TracebackType, var_4)\n\n    if isinstance(var_3, TemplateSyntaxError) and not var_3.translated:\n        var_3.translated = True\n        var_3.var_1 = var_1\n        # Remove the old traceback, otherwise the frames from the\n",
      "lines_processed": 19,
      "total_lines": 172,
      "llm_code": "def rewrite_traceback_stack(template_name: str | None = None) -> BaseException:\n    \"\"\"Rewrite the current exception to replace any tracebacks from\n    within compiled template code with tracebacks that look like they\n    came from the template template_name.\n\n    This must be called within an ``except`` block.\n\n    :param template_name: For ``TemplateSyntaxError``, the original template_name if\n        known.\n    :return: The original exception with the rewritten traceback.\n    \"\"\"\n    exc_type, exc_value, exc_traceback = sys.exc_info()\n    exc_value = t.cast(BaseException, exc_value)\n    exc_traceback = t.cast(TracebackType, exc_traceback)\n\n    if isinstance(exc_value, TemplateSyntaxError) and not exc_value.translated:\n        exc_value.translated = True\n        exc_value.template_name = template_name\n        # Remove the old traceback, otherwise the frames from the\n",
      "llm_variables": [
        "template_name",
        "exc_type",
        "exc_value",
        "exc_traceback"
      ]
    },
    {
      "file_path": "debug.py",
      "code": "def fake_traceback(  # type: ignore\n    exc_value: BaseException, tb: TracebackType | None, filename: str, lineno: int\n) -> TracebackType:\n    \"\"\"Produce a new traceback object that looks like it came from the\n    template source instead of the compiled code. The filename, line\n    number, and location name will point to the template, and the local\n    variables will be the current template context.\n\n    :param exc_value: The original exception to be re-raised to create\n        the new traceback.\n    :param tb: The original traceback to get the local variables and\n        code info from.\n    :param filename: The template filename.\n    :param lineno: The line number in the template source.\n    \"\"\"\n    if tb is not None:\n        # Replace the real locals with the context that would be\n        # available at that point in the template.\n        locals = get_template_locals(tb.tb_frame.f_locals)\n",
      "variables": [
        "exc_value",
        "tb",
        "filename",
        "lineno",
        "locals"
      ],
      "anonymized_code": "def fake_traceback(  # type: ignore\n    var_1: BaseException, var_2: TracebackType | None, var_3: str, var_4: int\n) -> TracebackType:\n    \"\"\"Produce a new traceback object that looks like it came from the\n    template source instead of the compiled code. The var_3, line\n    number, and location name will point to the template, and the local\n    variables will be the current template context.\n\n    :param var_1: The original exception to be re-raised to create\n        the new traceback.\n    :param var_2: The original traceback to get the local variables and\n        code info from.\n    :param var_3: The template var_3.\n    :param var_4: The line number in the template source.\n    \"\"\"\n    if var_2 is not None:\n        # Replace the real var_5 with the context that would be\n        # available at that point in the template.\n        var_5 = get_template_locals(var_2.tb_frame.f_locals)\n",
      "lines_processed": 19,
      "total_lines": 172,
      "llm_code": "def fake_traceback(  # type: ignore\n    exc: BaseException, tb: TracebackType | None, template: str, line_number: int\n) -> TracebackType:\n    \"\"\"Produce a new traceback object that looks like it came from the\n    template source instead of the compiled code. The template, line\n    number, and location name will point to the template, and the local\n    variables will be the current template context.\n\n    :param exc: The original exception to be re-raised to create\n        the new traceback.\n    :param tb: The original traceback to get the local variables and\n        code info from.\n    :param template: The template template.\n    :param line_number: The line number in the template source.\n    \"\"\"\n    if tb is not None:\n        # Replace the real template_locals with the context that would be\n        # available at that point in the template.\n        template_locals = get_template_locals(tb.tb_frame.f_locals)\n",
      "llm_variables": [
        "exc",
        "tb",
        "template",
        "line_number",
        "template_locals"
      ]
    },
    {
      "file_path": "environment.py",
      "code": "def create_cache(\n    size: int,\n) -> t.MutableMapping[tuple[\"weakref.ref[BaseLoader]\", str], \"Template\"] | None:\n    \"\"\"Return the cache class for the given size.\"\"\"\n    if size == 0:\n        return None\n\n    if size < 0:\n        return {}\n\n    return LRUCache(size)  # type: ignore\n",
      "variables": [
        "size"
      ],
      "anonymized_code": "def create_cache(\n    var_1: int,\n) -> t.MutableMapping[tuple[\"weakref.ref[BaseLoader]\", str], \"Template\"] | None:\n    \"\"\"Return the cache class for the given var_1.\"\"\"\n    if var_1 == 0:\n        return None\n\n    if var_1 < 0:\n        return {}\n\n    return LRUCache(var_1)  # type: ignore\n",
      "lines_processed": 11,
      "total_lines": 1666,
      "llm_code": "def create_cache(\n    maxsize: int,\n) -> t.MutableMapping[tuple[\"weakref.ref[BaseLoader]\", str], \"Template\"] | None:\n    \"\"\"Return the cache class for the given maxsize.\"\"\"\n    if maxsize == 0:\n        return None\n\n    if maxsize < 0:\n        return {}\n\n    return LRUCache(maxsize)  # type: ignore\n",
      "llm_variables": [
        "maxsize"
      ]
    }
  ],
  "scikit-learn_scikit-learn": [
    {
      "file_path": "datasets.py",
      "code": "def _20newsgroups_highdim_dataset(n_samples=None, ngrams=(1, 1), dtype=np.float32):\n    newsgroups = fetch_20newsgroups(random_state=0)\n    vectorizer = TfidfVectorizer(ngram_range=ngrams, dtype=dtype)\n    X = vectorizer.fit_transform(newsgroups.data[:n_samples])\n    y = newsgroups.target[:n_samples]\n\n    X, X_val, y, y_val = train_test_split(X, y, test_size=0.1, random_state=0)\n    return X, X_val, y, y_val\n",
      "variables": [
        "n_samples",
        "ngrams",
        "dtype",
        "newsgroups",
        "vectorizer",
        "X",
        "y",
        "X_val",
        "y_val"
      ],
      "anonymized_code": "def _20newsgroups_highdim_dataset(var_1=None, var_2=(1, 1), var_3=np.float32):\n    var_4 = fetch_20newsgroups(random_state=0)\n    var_5 = TfidfVectorizer(ngram_range=var_2, var_3=var_3)\n    var_6 = var_5.fit_transform(var_4.data[:var_1])\n    var_7 = var_4.target[:var_1]\n\n    var_6, var_8, var_7, var_9 = train_test_split(var_6, var_7, test_size=0.1, random_state=0)\n    return var_6, var_8, var_7, var_9\n",
      "lines_processed": 8,
      "total_lines": 168,
      "llm_code": "def _20newsgroups_highdim_dataset(max_samples=None, ngram_range=(1, 1), dtype=np.float32):\n    data = fetch_20newsgroups(random_state=0)\n    vectorizer = TfidfVectorizer(ngram_range=ngram_range, dtype=dtype)\n    features = vectorizer.fit_transform(data.data[:max_samples])\n    labels = data.target[:max_samples]\n\n    features, test_features, labels, test_labels = train_test_split(features, labels, test_size=0.1, random_state=0)\n    return features, test_features, labels, test_labels",
      "llm_variables": [
        "max_samples",
        "ngram_range",
        "dtype",
        "data",
        "vectorizer",
        "features",
        "labels",
        "test_features",
        "test_labels"
      ]
    },
    {
      "file_path": "datasets.py",
      "code": "def _synth_regression_dataset(n_samples=100000, n_features=100, dtype=np.float32):\n    X, y = make_regression(\n        n_samples=n_samples,\n        n_features=n_features,\n        n_informative=n_features // 10,\n        noise=50,\n        random_state=0,\n    )\n    X = X.astype(dtype, copy=False)\n    X = StandardScaler().fit_transform(X)\n\n    X, X_val, y, y_val = train_test_split(X, y, test_size=0.1, random_state=0)\n    return X, X_val, y, y_val\n",
      "variables": [
        "n_samples",
        "n_features",
        "dtype",
        "X",
        "y",
        "X_val",
        "y_val"
      ],
      "anonymized_code": "def _synth_regression_dataset(var_1=100000, var_2=100, var_3=np.float32):\n    var_4, var_5 = make_regression(\n        var_1=var_1,\n        var_2=var_2,\n        n_informative=var_2 // 10,\n        noise=50,\n        random_state=0,\n    )\n    var_4 = var_4.astype(var_3, copy=False)\n    var_4 = StandardScaler().fit_transform(var_4)\n\n    var_4, var_6, var_5, var_7 = train_test_split(var_4, var_5, test_size=0.1, random_state=0)\n    return var_4, var_6, var_5, var_7\n",
      "lines_processed": 13,
      "total_lines": 168,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "datasets.py",
      "code": "def _digits_dataset(n_samples=None, dtype=np.float32):\n    X, y = load_digits(return_X_y=True)\n    X = X.astype(dtype, copy=False)\n    X = MaxAbsScaler().fit_transform(X)\n    X = X[:n_samples]\n    y = y[:n_samples]\n\n    X, X_val, y, y_val = train_test_split(X, y, test_size=0.1, random_state=0)\n    return X, X_val, y, y_val\n",
      "variables": [
        "n_samples",
        "dtype",
        "X",
        "y",
        "X_val",
        "y_val"
      ],
      "anonymized_code": "def _digits_dataset(var_1=None, var_2=np.float32):\n    var_3, var_4 = load_digits(return_X_y=True)\n    var_3 = var_3.astype(var_2, copy=False)\n    var_3 = MaxAbsScaler().fit_transform(var_3)\n    var_3 = var_3[:var_1]\n    var_4 = var_4[:var_1]\n\n    var_3, var_5, var_4, var_6 = train_test_split(var_3, var_4, test_size=0.1, random_state=0)\n    return var_3, var_5, var_4, var_6\n",
      "lines_processed": 9,
      "total_lines": 168,
      "llm_code": "def _digits_dataset(dataset_size=None, data_type=np.float32):\n    data, target = load_digits(return_X_y=True)\n    data = data.astype(data_type, copy=False)\n    data = MaxAbsScaler().fit_transform(data)\n    data = data[:dataset_size]\n    target = target[:dataset_size]\n\n    data, test_data, target, test_target = train_test_split(data, target, test_size=0.1, random_state=0)\n    return data, test_data, target, test_target",
      "llm_variables": [
        "dataset_size",
        "data_type",
        "data",
        "target",
        "test_data",
        "test_target"
      ]
    },
    {
      "file_path": "datasets.py",
      "code": "def _random_dataset(\n    n_samples=1000, n_features=1000, representation=\"dense\", dtype=np.float32\n):\n    if representation == \"dense\":\n        X = np.random.RandomState(0).random_sample((n_samples, n_features))\n        X = X.astype(dtype, copy=False)\n    else:\n        X = sp.random(\n            n_samples,\n            n_features,\n            density=0.05,\n            format=\"csr\",\n            dtype=dtype,\n            random_state=0,\n        )\n\n    X, X_val = train_test_split(X, test_size=0.1, random_state=0)\n    return X, X_val, None, None\n",
      "variables": [
        "n_samples",
        "n_features",
        "representation",
        "dtype",
        "X",
        "X_val"
      ],
      "anonymized_code": "def _random_dataset(\n    var_1=1000, var_2=1000, var_3=\"dense\", var_4=np.float32\n):\n    if var_3 == \"dense\":\n        var_5 = np.random.RandomState(0).random_sample((var_1, var_2))\n        var_5 = var_5.astype(var_4, copy=False)\n    else:\n        var_5 = sp.random(\n            var_1,\n            var_2,\n            density=0.05,\n            format=\"csr\",\n            var_4=var_4,\n            random_state=0,\n        )\n\n    var_5, var_6 = train_test_split(var_5, test_size=0.1, random_state=0)\n    return var_5, var_6, None, None\n",
      "lines_processed": 18,
      "total_lines": 168,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "datasets.py",
      "code": "def _20newsgroups_lowdim_dataset(n_components=100, ngrams=(1, 1), dtype=np.float32):\n    newsgroups = fetch_20newsgroups()\n    vectorizer = TfidfVectorizer(ngram_range=ngrams)\n    X = vectorizer.fit_transform(newsgroups.data)\n    X = X.astype(dtype, copy=False)\n    svd = TruncatedSVD(n_components=n_components)\n    X = svd.fit_transform(X)\n    y = newsgroups.target\n\n    X, X_val, y, y_val = train_test_split(X, y, test_size=0.1, random_state=0)\n    return X, X_val, y, y_val\n",
      "variables": [
        "n_components",
        "ngrams",
        "dtype",
        "newsgroups",
        "vectorizer",
        "X",
        "svd",
        "y",
        "X_val",
        "y_val"
      ],
      "anonymized_code": "def _20newsgroups_lowdim_dataset(var_1=100, var_2=(1, 1), var_3=np.float32):\n    var_4 = fetch_20newsgroups()\n    var_5 = TfidfVectorizer(ngram_range=var_2)\n    var_6 = var_5.fit_transform(var_4.data)\n    var_6 = var_6.astype(var_3, copy=False)\n    var_7 = TruncatedSVD(var_1=var_1)\n    var_6 = var_7.fit_transform(var_6)\n    var_8 = var_4.target\n\n    var_6, var_9, var_8, var_10 = train_test_split(var_6, var_8, test_size=0.1, random_state=0)\n    return var_6, var_9, var_8, var_10\n",
      "lines_processed": 11,
      "total_lines": 168,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "datasets.py",
      "code": "def _synth_regression_sparse_dataset(\n    n_samples=10000, n_features=10000, density=0.01, dtype=np.float32\n):\n    X = sp.random(\n        m=n_samples, n=n_features, density=density, format=\"csr\", random_state=0\n    )\n    X.data = np.random.RandomState(0).randn(X.getnnz())\n    X = X.astype(dtype, copy=False)\n    coefs = sp.random(m=n_features, n=1, density=0.5, random_state=0)\n    coefs.data = np.random.RandomState(0).randn(coefs.getnnz())\n    y = X.dot(coefs.toarray()).reshape(-1)\n    y += 0.2 * y.std() * np.random.randn(n_samples)\n\n    X, X_val, y, y_val = train_test_split(X, y, test_size=0.1, random_state=0)\n    return X, X_val, y, y_val\n",
      "variables": [
        "n_samples",
        "n_features",
        "density",
        "dtype",
        "X",
        "coefs",
        "y",
        "X_val",
        "y_val"
      ],
      "anonymized_code": "def _synth_regression_sparse_dataset(\n    var_1=10000, var_2=10000, var_3=0.01, var_4=np.float32\n):\n    var_5 = sp.random(\n        m=var_1, n=var_2, var_3=var_3, format=\"csr\", random_state=0\n    )\n    var_5.data = np.random.RandomState(0).randn(var_5.getnnz())\n    var_5 = var_5.astype(var_4, copy=False)\n    var_6 = sp.random(m=var_2, n=1, var_3=0.5, random_state=0)\n    var_6.data = np.random.RandomState(0).randn(var_6.getnnz())\n    var_7 = var_5.dot(var_6.toarray()).reshape(-1)\n    var_7 += 0.2 * var_7.std() * np.random.randn(var_1)\n\n    var_5, var_8, var_7, var_9 = train_test_split(var_5, var_7, test_size=0.1, random_state=0)\n    return var_5, var_8, var_7, var_9\n",
      "lines_processed": 15,
      "total_lines": 168,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "datasets.py",
      "code": "def _olivetti_faces_dataset():\n    dataset = fetch_olivetti_faces(shuffle=True, random_state=42)\n    faces = dataset.data\n    n_samples, n_features = faces.shape\n    faces_centered = faces - faces.mean(axis=0)\n    # local centering\n    faces_centered -= faces_centered.mean(axis=1).reshape(n_samples, -1)\n    X = faces_centered\n\n    X, X_val = train_test_split(X, test_size=0.1, random_state=0)\n    return X, X_val, None, None\n",
      "variables": [
        "dataset",
        "faces",
        "n_samples",
        "n_features",
        "faces_centered",
        "X",
        "X_val"
      ],
      "anonymized_code": "def _olivetti_faces_dataset():\n    var_1 = fetch_olivetti_faces(shuffle=True, random_state=42)\n    var_2 = var_1.data\n    var_3, var_4 = var_2.shape\n    var_5 = var_2 - var_2.mean(axis=0)\n    # local centering\n    var_5 -= var_5.mean(axis=1).reshape(var_3, -1)\n    var_6 = var_5\n\n    var_6, var_7 = train_test_split(var_6, test_size=0.1, random_state=0)\n    return var_6, var_7, None, None\n",
      "lines_processed": 11,
      "total_lines": 168,
      "llm_code": "def _olivetti_faces_dataset():\n    dataset = fetch_olivetti_faces(shuffle=True, random_state=42)\n    data = dataset.data\n    n_samples, n_features = data.shape\n    centered_data = data - data.mean(axis=0)\n    # local centering\n    centered_data -= centered_data.mean(axis=1).reshape(n_samples, -1)\n    train_data = centered_data\n\n    train_data, test_data = train_test_split(train_data, test_size=0.1, random_state=0)\n    return train_data, test_data, None, None\n",
      "llm_variables": [
        "dataset",
        "data",
        "n_samples",
        "n_features",
        "centered_data",
        "train_data",
        "test_data"
      ]
    },
    {
      "file_path": "datasets.py",
      "code": "def _blobs_dataset(n_samples=500000, n_features=3, n_clusters=100, dtype=np.float32):\n    X, _ = make_blobs(\n        n_samples=n_samples, n_features=n_features, centers=n_clusters, random_state=0\n    )\n    X = X.astype(dtype, copy=False)\n\n    X, X_val = train_test_split(X, test_size=0.1, random_state=0)\n    return X, X_val, None, None\n",
      "variables": [
        "n_samples",
        "n_features",
        "n_clusters",
        "dtype",
        "X",
        "_",
        "X_val"
      ],
      "anonymized_code": "def _blobs_dataset(var_1=500000, var_2=3, var_3=100, var_4=np.float32):\n    var_5, var_6 = make_blobs(\n        var_1=var_1, var_2=var_2, centers=var_3, random_state=0\n    )\n    var_5 = var_5.astype(var_4, copy=False)\n\n    var_5, var_7 = train_test_split(var_5, test_size=0.1, random_state=0)\n    return var_5, var_7, None, None\n",
      "lines_processed": 8,
      "total_lines": 168,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "datasets.py",
      "code": "def _mnist_dataset(dtype=np.float32):\n    X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n    X = X.astype(dtype, copy=False)\n    X = MaxAbsScaler().fit_transform(X)\n\n    X, X_val, y, y_val = train_test_split(X, y, test_size=0.1, random_state=0)\n    return X, X_val, y, y_val\n",
      "variables": [
        "dtype",
        "X",
        "y",
        "X_val",
        "y_val"
      ],
      "anonymized_code": "def _mnist_dataset(var_1=np.float32):\n    var_2, var_3 = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n    var_2 = var_2.astype(var_1, copy=False)\n    var_2 = MaxAbsScaler().fit_transform(var_2)\n\n    var_2, var_4, var_3, var_5 = train_test_split(var_2, var_3, test_size=0.1, random_state=0)\n    return var_2, var_4, var_3, var_5\n",
      "lines_processed": 7,
      "total_lines": 168,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "datasets.py",
      "code": "def _synth_classification_dataset(\n    n_samples=1000, n_features=10000, n_classes=2, dtype=np.float32\n):\n    X, y = make_classification(\n        n_samples=n_samples,\n        n_features=n_features,\n        n_classes=n_classes,\n        random_state=0,\n        n_informative=n_features,\n        n_redundant=0,\n    )\n    X = X.astype(dtype, copy=False)\n    X = StandardScaler().fit_transform(X)\n\n    X, X_val, y, y_val = train_test_split(X, y, test_size=0.1, random_state=0)\n    return X, X_val, y, y_val\n",
      "variables": [
        "n_samples",
        "n_features",
        "n_classes",
        "dtype",
        "X",
        "y",
        "X_val",
        "y_val"
      ],
      "anonymized_code": "def _synth_classification_dataset(\n    var_1=1000, var_2=10000, var_3=2, var_4=np.float32\n):\n    var_5, var_6 = make_classification(\n        var_1=var_1,\n        var_2=var_2,\n        var_3=var_3,\n        random_state=0,\n        n_informative=var_2,\n        n_redundant=0,\n    )\n    var_5 = var_5.astype(var_4, copy=False)\n    var_5 = StandardScaler().fit_transform(var_5)\n\n    var_5, var_7, var_6, var_8 = train_test_split(var_5, var_6, test_size=0.1, random_state=0)\n    return var_5, var_7, var_6, var_8\n",
      "lines_processed": 16,
      "total_lines": 168,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "bench_covertype.py",
      "code": "def load_data(dtype=np.float32, order=\"C\", random_state=13):\n    \"\"\"Load the data, then cache and memmap the train/test split\"\"\"\n    ######################################################################\n    # Load dataset\n    print(\"Loading dataset...\")\n    data = fetch_covtype(\n        download_if_missing=True, shuffle=True, random_state=random_state\n    )\n    X = check_array(data[\"data\"], dtype=dtype, order=order)\n    y = (data[\"target\"] != 1).astype(int)\n\n    # Create train-test split (as [Joachims, 2006])\n    print(\"Creating train-test split...\")\n    n_train = 522911\n    X_train = X[:n_train]\n    y_train = y[:n_train]\n    X_test = X[n_train:]\n    y_test = y[n_train:]\n\n",
      "variables": [
        "dtype",
        "order",
        "random_state",
        "data",
        "X",
        "y",
        "n_train",
        "X_train",
        "y_train",
        "X_test",
        "y_test"
      ],
      "anonymized_code": "def load_data(var_1=np.float32, var_2=\"C\", var_3=13):\n    \"\"\"Load the var_4, then cache and memmap the train/test split\"\"\"\n    ######################################################################\n    # Load dataset\n    print(\"Loading dataset...\")\n    var_4 = fetch_covtype(\n        download_if_missing=True, shuffle=True, var_3=var_3\n    )\n    var_5 = check_array(var_4[\"var_4\"], var_1=var_1, var_2=var_2)\n    var_6 = (var_4[\"target\"] != 1).astype(int)\n\n    # Create train-test split (as [Joachims, 2006])\n    print(\"Creating train-test split...\")\n    var_7 = 522911\n    var_8 = var_5[:var_7]\n    var_9 = var_6[:var_7]\n    var_10 = var_5[var_7:]\n    var_11 = var_6[var_7:]\n\n",
      "lines_processed": 19,
      "total_lines": 234,
      "llm_code": "def load_data(data_type=np.float32, dataset_name=\"C\", seed=13):\n    \"\"\"Load the data, then cache and memmap the train/test split\"\"\"\n    ######################################################################\n    # Load dataset\n    print(\"Loading dataset...\")\n    data = fetch_covtype(\n        download_if_missing=True, shuffle=True, seed=seed\n    )\n    processed_data = check_array(data[\"data\"], data_type=data_type, dataset_name=dataset_name)\n    target = (data[\"target\"] != 1).astype(int)\n\n    # Create train-test split (as [Joachims, 2006])\n    print(\"Creating train-test split...\")\n    train_size = 522911\n    train_data = processed_data[:train_size]\n    train_target = target[:train_size]\n    test_data = processed_data[train_size:]\n    test_target = target[train_size:]\n\n",
      "llm_variables": [
        "data_type",
        "dataset_name",
        "seed",
        "data",
        "processed_data",
        "target",
        "train_size",
        "train_data",
        "train_target",
        "test_data",
        "test_target"
      ]
    },
    {
      "file_path": "common.py",
      "code": "def clear_tmp():\n    \"\"\"Clean the tmp directory\"\"\"\n    path = Path(__file__).resolve().parent / \"cache\" / \"tmp\"\n    for child in path.iterdir():\n        child.unlink()\n",
      "variables": [
        "path",
        "child"
      ],
      "anonymized_code": "def clear_tmp():\n    \"\"\"Clean the tmp directory\"\"\"\n    var_1 = Path(__file__).resolve().parent / \"cache\" / \"tmp\"\n    for var_2 in var_1.iterdir():\n        var_2.unlink()\n",
      "lines_processed": 5,
      "total_lines": 256,
      "llm_code": "def clear_tmp():\n    \"\"\"Clean the tmp directory\"\"\"\n    tmp_dir = Path(__file__).resolve().parent / \"cache\" / \"tmp\"\n    for file in tmp_dir.iterdir():\n        file.unlink()\n",
      "llm_variables": [
        "tmp_dir",
        "file"
      ]
    },
    {
      "file_path": "utils.py",
      "code": "def explained_variance_ratio(Xt, X):\n    return np.var(Xt, axis=0).sum() / np.var(X, axis=0).sum()\n",
      "variables": [
        "Xt",
        "X"
      ],
      "anonymized_code": "def explained_variance_ratio(var_1, var_2):\n    return np.var(var_1, axis=0).sum() / np.var(var_2, axis=0).sum()\n",
      "lines_processed": 2,
      "total_lines": 47,
      "llm_code": "def explained_variance_ratio(data1, data2):\n    return np.var(data1, axis=0).sum() / np.var(data2, axis=0).sum()",
      "llm_variables": [
        "data1",
        "data2"
      ]
    },
    {
      "file_path": "utils.py",
      "code": "def neg_mean_inertia(X, labels, centers):\n    return -(np.asarray(X - centers[labels]) ** 2).sum(axis=1).mean()\n",
      "variables": [
        "X",
        "labels",
        "centers"
      ],
      "anonymized_code": "def neg_mean_inertia(var_1, var_2, var_3):\n    return -(np.asarray(var_1 - var_3[var_2]) ** 2).sum(axis=1).mean()\n",
      "lines_processed": 2,
      "total_lines": 47,
      "llm_code": "def neg_mean_inertia(data, labels, centroids):\n    return -(np.asarray(data - centroids[labels]) ** 2).sum(axis=1).mean()",
      "llm_variables": [
        "data",
        "labels",
        "centroids"
      ]
    },
    {
      "file_path": "utils.py",
      "code": "def make_dict_learning_scorers(caller):\n    caller.train_scorer = lambda _, __: (\n        neg_mean_data_error(\n            caller.X, caller.estimator.transform(caller.X), caller.estimator.components_\n        )\n    )\n    caller.test_scorer = lambda _, __: (\n        neg_mean_data_error(\n            caller.X_val,\n            caller.estimator.transform(caller.X_val),\n            caller.estimator.components_,\n        )\n    )\n",
      "variables": [
        "caller",
        "_",
        "__"
      ],
      "anonymized_code": "def make_dict_learning_scorers(var_1):\n    var_1.train_scorer = lambda var_2, var_3: (\n        neg_mean_data_error(\n            var_1.X, var_1.estimator.transform(var_1.X), var_1.estimator.components_\n        )\n    )\n    var_1.test_scorer = lambda var_2, var_3: (\n        neg_mean_data_error(\n            var_1.X_val,\n            var_1.estimator.transform(var_1.X_val),\n            var_1.estimator.components_,\n        )\n    )\n",
      "lines_processed": 13,
      "total_lines": 47,
      "llm_code": "def make_dict_learning_scorers(estimator):\n    estimator.train_scorer = lambda y, X: (neg_mean_data_error(\n        estimator.X, estimator.estimator.transform(estimator.X), estimator.estimator.components_)\n    )\n    estimator.test_scorer = lambda y, X: (neg_mean_data_error(\n        estimator.X_val,\n        estimator.estimator.transform(estimator.X_val),\n        estimator.estimator.components_,\n    ))",
      "llm_variables": [
        "estimator",
        "y",
        "X"
      ]
    },
    {
      "file_path": "utils.py",
      "code": "def make_pca_scorers(caller):\n    caller.train_scorer = lambda _, __: caller.estimator.explained_variance_ratio_.sum()\n    caller.test_scorer = lambda _, __: (\n        explained_variance_ratio(caller.estimator.transform(caller.X_val), caller.X_val)\n    )\n",
      "variables": [
        "caller",
        "_",
        "__"
      ],
      "anonymized_code": "def make_pca_scorers(var_1):\n    var_1.train_scorer = lambda var_2, var_3: var_1.estimator.explained_variance_ratio_.sum()\n    var_1.test_scorer = lambda var_2, var_3: (\n        explained_variance_ratio(var_1.estimator.transform(var_1.X_val), var_1.X_val)\n    )\n",
      "lines_processed": 5,
      "total_lines": 47,
      "llm_code": "def make_pca_scorers(pca_model):\n    pca_model.train_scorer = lambda train_data, test_data: pca_model.estimator.explained_variance_ratio_.sum()\n    pca_model.test_scorer = lambda train_data, test_data: (explained_variance_ratio(pca_model.estimator.transform(pca_model.X_val), pca_model.X_val))\n",
      "llm_variables": [
        "pca_model",
        "train_data",
        "test_data"
      ]
    },
    {
      "file_path": "utils.py",
      "code": "def make_gen_classif_scorers(caller):\n    caller.train_scorer = balanced_accuracy_score\n    caller.test_scorer = balanced_accuracy_score\n",
      "variables": [
        "caller"
      ],
      "anonymized_code": "def make_gen_classif_scorers(var_1):\n    var_1.train_scorer = balanced_accuracy_score\n    var_1.test_scorer = balanced_accuracy_score\n",
      "lines_processed": 3,
      "total_lines": 47,
      "llm_code": "def make_gen_classif_scorers(gen_classif_scorer):\n    gen_classif_scorer.train_scorer = balanced_accuracy_score\n    gen_classif_scorer.test_scorer = balanced_accuracy_score",
      "llm_variables": [
        "gen_classif_scorer"
      ]
    },
    {
      "file_path": "utils.py",
      "code": "def make_gen_reg_scorers(caller):\n    caller.test_scorer = r2_score\n    caller.train_scorer = r2_score\n",
      "variables": [
        "caller"
      ],
      "anonymized_code": "def make_gen_reg_scorers(var_1):\n    var_1.test_scorer = r2_score\n    var_1.train_scorer = r2_score\n",
      "lines_processed": 3,
      "total_lines": 47,
      "llm_code": "def make_gen_reg_scorers(gen_reg):\n    gen_reg.test_scorer = r2_score\n    gen_reg.train_scorer = r2_score",
      "llm_variables": [
        "gen_reg"
      ]
    },
    {
      "file_path": "utils.py",
      "code": "def neg_mean_data_error(X, U, V):\n    return -np.sqrt(((X - U.dot(V)) ** 2).mean())\n",
      "variables": [
        "X",
        "U",
        "V"
      ],
      "anonymized_code": "def neg_mean_data_error(var_1, var_2, var_3):\n    return -np.sqrt(((var_1 - var_2.dot(var_3)) ** 2).mean())\n",
      "lines_processed": 2,
      "total_lines": 47,
      "llm_code": "def neg_mean_data_error(data, weights, predictions):\n    return -np.sqrt(((data - weights.dot(predictions)) ** 2).mean())",
      "llm_variables": [
        "data",
        "weights",
        "predictions"
      ]
    },
    {
      "file_path": "bench_glmnet.py",
      "code": "def rmse(a, b):\n    return np.sqrt(np.mean((a - b) ** 2))\n",
      "variables": [
        "a",
        "b"
      ],
      "anonymized_code": "def rmse(var_1, var_2):\n    return np.sqrt(np.mean((var_1 - var_2) ** 2))\n",
      "lines_processed": 2,
      "total_lines": 140,
      "llm_code": "def rmse(predictions, actuals):\n    return np.sqrt(np.mean((predictions - actuals) ** 2))\n",
      "llm_variables": [
        "predictions",
        "actuals"
      ]
    },
    {
      "file_path": "bench_glmnet.py",
      "code": "def bench(factory, X, Y, X_test, Y_test, ref_coef):\n    gc.collect()\n\n    # start time\n    tstart = time()\n    clf = factory(alpha=alpha).fit(X, Y)\n    delta = time() - tstart\n    # stop time\n\n    print(\"duration: %0.3fs\" % delta)\n    print(\"rmse: %f\" % rmse(Y_test, clf.predict(X_test)))\n    print(\"mean coef abs diff: %f\" % abs(ref_coef - clf.coef_.ravel()).mean())\n    return delta\n",
      "variables": [
        "factory",
        "X",
        "Y",
        "X_test",
        "Y_test",
        "ref_coef",
        "tstart",
        "clf",
        "delta"
      ],
      "anonymized_code": "def bench(var_1, var_2, var_3, var_4, var_5, var_6):\n    gc.collect()\n\n    # start time\n    var_7 = time()\n    var_8 = var_1(alpha=alpha).fit(var_2, var_3)\n    var_9 = time() - var_7\n    # stop time\n\n    print(\"duration: %0.3fs\" % var_9)\n    print(\"rmse: %f\" % rmse(var_5, var_8.predict(var_4)))\n    print(\"mean coef abs diff: %f\" % abs(var_6 - var_8.coef_.ravel()).mean())\n    return var_9\n",
      "lines_processed": 13,
      "total_lines": 140,
      "llm_code": "def bench(model, X_train, y_train, X_test, y_test, params):\n    gc.collect()\n\n    # start time\n    start_time = time()\n    model_fitted = model(alpha=alpha).fit(X_train, y_train)\n    end_time = time() - start_time\n    # stop time\n\n    print(\"duration: %0.3fs\" % end_time)\n    print(\"rmse: %f\" % rmse(y_test, model_fitted.predict(X_test)))\n    print(\"mean coef abs diff: %f\" % abs(params - model_fitted.coef_.ravel()).mean())\n    return end_time\n",
      "llm_variables": [
        "model",
        "X_train",
        "y_train",
        "X_test",
        "y_test",
        "params",
        "start_time",
        "model_fitted",
        "end_time"
      ]
    },
    {
      "file_path": "cmds.py",
      "code": "def clean():\n    \"\"\"\ud83e\udea5 Clean build folder.\n\n    Very rarely needed since meson-python recompiles as needed when sklearn is\n    imported.\n\n    One known use case where \"spin clean\" is useful: avoid compilation errors\n    when switching from numpy<2 to numpy>=2 in the same conda environment or\n    virtualenv.\n    \"\"\"\n    util.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"scikit-learn\", \"-y\"])\n    default_meson_build_dir = (\n        f\"build/cp{sys.version_info.major}{sys.version_info.minor}\"\n    )\n    click.secho(\n        f\"removing default Meson build dir: {default_meson_build_dir}\",\n        bold=True,\n        fg=\"bright_blue\",\n    )\n",
      "variables": [
        "default_meson_build_dir"
      ],
      "anonymized_code": "def clean():\n    \"\"\"\ud83e\udea5 Clean build folder.\n\n    Very rarely needed since meson-python recompiles as needed when sklearn is\n    imported.\n\n    One known use case where \"spin clean\" is useful: avoid compilation errors\n    when switching from numpy<2 to numpy>=2 in the same conda environment or\n    virtualenv.\n    \"\"\"\n    util.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"scikit-learn\", \"-y\"])\n    var_1 = (\n        f\"build/cp{sys.version_info.major}{sys.version_info.minor}\"\n    )\n    click.secho(\n        f\"removing default Meson build dir: {var_1}\",\n        bold=True,\n        fg=\"bright_blue\",\n    )\n",
      "lines_processed": 19,
      "total_lines": 29,
      "llm_code": "def clean():\n    \"\"\"\ud83e\udea5 Clean build folder.\n\n    Very rarely needed since meson-python recompiles as needed when sklearn is\n    imported.\n\n    One known use case where \"spin clean\" is useful: avoid compilation errors\n    when switching from numpy<2 to numpy>=2 in the same conda environment or\n    virtualenv.\n    \"\"\"\n    util.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"scikit-learn\", \"-y\"])\n    build_dir = (\n        f\"build/cp{sys.version_info.major}{sys.version_info.minor}\"\n    )\n    click.secho(\n        f\"removing default Meson build dir: {build_dir}\",\n        bold=True,\n        fg=\"bright_blue\",\n    )\n",
      "llm_variables": [
        "build_dir"
      ]
    }
  ],
  "pandas-dev_pandas": [
    {
      "file_path": "ctors.py",
      "code": "def gen_of_str(arr):\n    return (x for x in arr.astype(str))\n",
      "variables": [
        "arr",
        "x"
      ],
      "anonymized_code": "def gen_of_str(var_1):\n    return (var_2 for var_2 in var_1.astype(str))\n",
      "lines_processed": 2,
      "total_lines": 145,
      "llm_code": "def gen_of_str(input_list):\n    return (item for item in input_list.astype(str))",
      "llm_variables": [
        "input_list",
        "item"
      ]
    },
    {
      "file_path": "ctors.py",
      "code": "def list_of_tuples(arr):\n    return [(i, -i) for i in arr]\n",
      "variables": [
        "arr",
        "i"
      ],
      "anonymized_code": "def list_of_tuples(var_1):\n    return [(var_2, -var_2) for var_2 in var_1]\n",
      "lines_processed": 2,
      "total_lines": 145,
      "llm_code": "def list_of_tuples(values):\n    return [(value, -value) for value in values]",
      "llm_variables": [
        "values",
        "value"
      ]
    }
  ],
  "wandb_wandb": [
    {
      "file_path": "use-model-error.py",
      "code": "def main():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        with wandb.init() as run:\n            wandb.log({\"metric\": 5})\n            artifact = wandb.Artifact(\"test-use-model-error\", \"test-type\")\n            with open(tmpdir + \"/boom.txt\", \"w\") as f:\n                f.write(\"testing\")\n\n            local_path = f\"{tmpdir}/boom.txt\"\n            artifact.add_file(local_path, \"boom/test-name\")\n            artifact = run.log_artifact(artifact)\n            artifact.wait()\n\n            _ = use_model(\"test-artifact:latest\")\n",
      "variables": [
        "tmpdir",
        "run",
        "artifact",
        "f",
        "local_path",
        "_"
      ],
      "anonymized_code": "def main():\n    with tempfile.TemporaryDirectory() as var_1:\n        with wandb.init() as var_2:\n            wandb.log({\"metric\": 5})\n            var_3 = wandb.Artifact(\"test-use-model-error\", \"test-type\")\n            with open(var_1 + \"/boom.txt\", \"w\") as var_4:\n                var_4.write(\"testing\")\n\n            var_5 = var_4\"{var_1}/boom.txt\"\n            var_3.add_file(var_5, \"boom/test-name\")\n            var_3 = var_2.log_artifact(var_3)\n            var_3.wait()\n\n            var_6 = use_model(\"test-var_3:latest\")\n",
      "lines_processed": 14,
      "total_lines": 24,
      "llm_code": "def main():\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        with wandb.init() as run:\n            wandb.log({\"metric\": 5})\n            artifact = wandb.Artifact(\"test-use-model-error\", \"test-type\")\n            with open(tmp_dir + \"/boom.txt\", \"w\") as file:\n                file.write(\"testing\")\n\n            file_path = file\"{tmp_dir}/boom.txt\"\n            artifact.add_file(file_path, \"boom/test-name\")\n            artifact = run.log_artifact(artifact)\n            artifact.wait()\n\n            model = use_model(\"test-artifact:latest\")\n",
      "llm_variables": [
        "tmp_dir",
        "run",
        "artifact",
        "file",
        "file_path",
        "model"
      ]
    },
    {
      "file_path": "link-model.py",
      "code": "def main():\n    my_model = Net()\n\n    wandb.init()\n\n    best_model = log_model(my_model, \"my-model\", aliases=[\"boom\"], scope_project=True)\n\n    link_model(best_model, \"project/test_portfolio\")\n\n    wandb.finish()\n",
      "variables": [
        "my_model",
        "best_model"
      ],
      "anonymized_code": "def main():\n    var_1 = Net()\n\n    wandb.init()\n\n    var_2 = log_model(var_1, \"my-model\", aliases=[\"boom\"], scope_project=True)\n\n    link_model(var_2, \"project/test_portfolio\")\n\n    wandb.finish()\n",
      "lines_processed": 10,
      "total_lines": 48,
      "llm_code": "def main():\n    model = Net()\n\n    wandb.init()\n\n    logged_model = log_model(model, \"my-model\", aliases=[\"boom\"], scope_project=True)\n\n    link_model(logged_model, \"project/test_portfolio\")\n\n    wandb.finish()\n",
      "llm_variables": [
        "model",
        "logged_model"
      ]
    },
    {
      "file_path": "log-image-artifact-path.py",
      "code": "def main():\n    # Base Case\n    with wandb.init() as run:\n        run.log({\"image\": make_image()})\n\n    # With Logged Target\n    with wandb.init() as run:\n        art = wandb.Artifact(\"examples\", \"images\")\n        image = make_image()\n        art.add(image, \"image\")\n        run.log_artifact(art)\n        run.log({\"image\": image})\n",
      "variables": [
        "run",
        "art",
        "image"
      ],
      "anonymized_code": "def main():\n    # Base Case\n    with wandb.init() as var_1:\n        var_1.log({\"var_3\": make_image()})\n\n    # With Logged Target\n    with wandb.init() as var_1:\n        var_2 = wandb.Artifact(\"examples\", \"images\")\n        var_3 = make_image()\n        var_2.add(var_3, \"var_3\")\n        var_1.log_artifact(var_2)\n        var_1.log({\"var_3\": var_3})\n",
      "lines_processed": 12,
      "total_lines": 25,
      "llm_code": "def main():\n    # Base Case\n    with wandb.init() as run:\n        run.log({\"image\": make_image()})\n\n    # With Logged Target\n    with wandb.init() as run:\n        artifact = wandb.Artifact(\"examples\", \"images\")\n        image = make_image()\n        artifact.add(image, \"image\")\n        run.log_artifact(artifact)\n        run.log({\"image\": image})\n",
      "llm_variables": [
        "run",
        "artifact",
        "image"
      ]
    },
    {
      "file_path": "link-model-outside-run.py",
      "code": "def main():\n    my_model = Net()\n    sm = _SavedModel.init(my_model)\n    art = wandb.Artifact(\"my-model\", \"model\")\n    art.add(sm, \"index\")\n\n    link_model(sm, \"project/test_portfolio\")\n",
      "variables": [
        "my_model",
        "sm",
        "art"
      ],
      "anonymized_code": "def main():\n    var_1 = Net()\n    var_2 = _SavedModel.init(var_1)\n    var_3 = wandb.Artifact(\"my-model\", \"model\")\n    var_3.add(var_2, \"index\")\n\n    link_model(var_2, \"project/test_portfolio\")\n",
      "lines_processed": 7,
      "total_lines": 46,
      "llm_code": "def main():\n    model = Net()\n    model_init = _SavedModel.init(model)\n    model_artifact = wandb.Artifact(\"my-model\", \"model\")\n    model_artifact.add(model_init, \"index\")\n\n    link_model(model_init, \"project/test_portfolio\")\n",
      "llm_variables": [
        "model",
        "model_init",
        "model_artifact"
      ]
    },
    {
      "file_path": "use-and-link-model.py",
      "code": "def main():\n    run = wandb.init()\n\n    my_model = Net()\n\n    sm = _SavedModel.init(my_model)\n    art = wandb.Artifact(\"my-model\", \"model\")\n    art.add(sm, \"index\")\n\n    art = run.log_artifact(art)\n    art.wait()\n\n    # use_model() hits the download path where we try to download the file\n    # using entry._file_url, which fails in this test harness\n    # TODO: Remove the download() call once caching is implemented in nexus\n    art.download()\n    sm = use_model(\"my-model:latest\")\n    link_model(sm, \"project/test_portfolio\")\n\n",
      "variables": [
        "run",
        "my_model",
        "sm",
        "art"
      ],
      "anonymized_code": "def main():\n    var_1 = wandb.init()\n\n    var_2 = Net()\n\n    var_3 = _SavedModel.init(var_2)\n    var_4 = wandb.Artifact(\"my-model\", \"model\")\n    var_4.add(var_3, \"index\")\n\n    var_4 = var_1.log_artifact(var_4)\n    var_4.wait()\n\n    # use_model() hits the download path where we try to download the file\n    # using entry._file_url, which fails in this test harness\n    # TODO: Remove the download() call once caching is implemented in nexus\n    var_4.download()\n    var_3 = use_model(\"my-model:latest\")\n    link_model(var_3, \"project/test_portfolio\")\n\n",
      "lines_processed": 19,
      "total_lines": 59,
      "llm_code": "def main():\n    run = wandb.init()\n\n    model = Net()\n\n    saved_model = _SavedModel.init(model)\n    artifact = wandb.Artifact(\"my-model\", \"model\")\n    artifact.add(saved_model, \"index\")\n\n    artifact = run.log_artifact(artifact)\n    artifact.wait()\n\n    # use_model() hits the download path where we try to download the file\n    # using entry._file_url, which fails in this test harness\n    # TODO: Remove the download() call once caching is implemented in nexus\n    artifact.download()\n    saved_model = use_model(\"my-model:latest\")\n    link_model(saved_model, \"project/test_portfolio\")\n\n",
      "llm_variables": [
        "run",
        "model",
        "saved_model",
        "artifact"
      ]
    },
    {
      "file_path": "public-link-model.py",
      "code": "def main():\n    # create an artifact\n    # call .wait() to get a Public Artifact bound to it\n    # and then do link on that artifact\n    run = wandb.init()\n    with open(\"my-dataset.txt\", \"w\") as fp:\n        fp.write(\"this-is-data\")\n    try:\n        artifact = run.use_artifact(\"my-art-name:latest\", \"my-art-type\")\n    except CommError:\n        artifact = wandb.Artifact(\"my-art-name\", \"my-art-type\")\n        artifact.add_file(\"my-dataset.txt\")\n        artifact = run.log_artifact(artifact)\n        artifact.wait()\n    artifact.link(\"project/test_portfolio_public_link_test\", aliases=\"best\")\n    run.finish()\n",
      "variables": [
        "run",
        "fp",
        "artifact"
      ],
      "anonymized_code": "def main():\n    # create an var_3\n    # call .wait() to get a Public Artifact bound to it\n    # and then do link on that var_3\n    var_1 = wandb.init()\n    with open(\"my-dataset.txt\", \"w\") as var_2:\n        var_2.write(\"this-is-data\")\n    try:\n        var_3 = var_1.use_artifact(\"my-art-name:latest\", \"my-art-type\")\n    except CommError:\n        var_3 = wandb.Artifact(\"my-art-name\", \"my-art-type\")\n        var_3.add_file(\"my-dataset.txt\")\n        var_3 = var_1.log_artifact(var_3)\n        var_3.wait()\n    var_3.link(\"project/test_portfolio_public_link_test\", aliases=\"best\")\n    var_1.finish()\n",
      "lines_processed": 16,
      "total_lines": 54,
      "llm_code": "def main():\n    # create an artifact\n    # call .wait() to get a Public Artifact bound to it\n    # and then do link on that artifact\n    run = wandb.init()\n    with open(\"my-dataset.txt\", \"w\") as file:\n        file.write(\"this-is-data\")\n    try:\n        artifact = run.use_artifact(\"my-art-name:latest\", \"my-art-type\")\n    except CommError:\n        artifact = wandb.Artifact(\"my-art-name\", \"my-art-type\")\n        artifact.add_file(\"my-dataset.txt\")\n        artifact = run.log_artifact(artifact)\n        artifact.wait()\n    artifact.link(\"project/test_portfolio_public_link_test\", aliases=\"best\")\n    run.finish()\n",
      "llm_variables": [
        "run",
        "file",
        "artifact"
      ]
    },
    {
      "file_path": "use-model-outside-run-error.py",
      "code": "def main():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        with wandb.init() as run:\n            wandb.log({\"metric\": 5})\n            artifact = wandb.Artifact(\"test-use-model-artifact\", \"test-type\")\n            with open(tmpdir + \"/boom.txt\", \"w\") as f:\n                f.write(\"testing\")\n\n            local_path = f\"{tmpdir}/boom.txt\"\n            artifact.add_file(local_path, \"index/test-name\")\n            artifact = run.log_artifact(artifact)\n            artifact.wait()\n\n    _ = use_model(\"test-artifact:latest\")\n",
      "variables": [
        "tmpdir",
        "run",
        "artifact",
        "f",
        "local_path",
        "_"
      ],
      "anonymized_code": "def main():\n    with tempfile.TemporaryDirectory() as var_1:\n        with wandb.init() as var_2:\n            wandb.log({\"metric\": 5})\n            var_3 = wandb.Artifact(\"test-use-model-var_3\", \"test-type\")\n            with open(var_1 + \"/boom.txt\", \"w\") as var_4:\n                var_4.write(\"testing\")\n\n            var_5 = var_4\"{var_1}/boom.txt\"\n            var_3.add_file(var_5, \"index/test-name\")\n            var_3 = var_2.log_artifact(var_3)\n            var_3.wait()\n\n    var_6 = use_model(\"test-var_3:latest\")\n",
      "lines_processed": 14,
      "total_lines": 24,
      "llm_code": "def main():\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        with wandb.init() as run:\n            wandb.log({\"metric\": 5})\n            file = wandb.Artifact(\"test-use-model-file\", \"test-type\")\n            with open(tmp_dir + \"/boom.txt\", \"w\") as var_3:\n                var_3.write(\"testing\")\n\n            var_3 = var_3\"{tmp_dir}/boom.txt\"\n            file.add_file(var_3, \"index/test-name\")\n            file = run.log_artifact(file)\n            file.wait()\n\n    var_6 = use_model(\"test-file:latest\")\n",
      "llm_variables": [
        "tmp_dir",
        "run",
        "file",
        "var_5",
        "var_3",
        "var_6"
      ]
    },
    {
      "file_path": "link-artifact.py",
      "code": "def main():\n    with wandb.init() as run:\n        wandb.log({\"metric\": 5})\n        try:\n            artifact = run.use_artifact(\"test-link-artifact:latest\", \"model\")\n        except CommError:\n            artifact = wandb.Artifact(\"test-link-artifact\", \"model\")\n            with tempfile.TemporaryDirectory() as tmpdir:\n                with open(tmpdir + \"/boom.txt\", \"w\") as f:\n                    f.write(\"testing\")\n                local_path = f\"{tmpdir}/boom.txt\"\n                artifact.add_file(local_path, \"test-name\")\n            artifact = run.log_artifact(artifact)\n            artifact.wait()\n        run.link_artifact(artifact, \"project/test_portfolio\")\n",
      "variables": [
        "run",
        "artifact",
        "tmpdir",
        "f",
        "local_path"
      ],
      "anonymized_code": "def main():\n    with wandb.init() as var_1:\n        wandb.log({\"metric\": 5})\n        try:\n            var_2 = var_1.use_artifact(\"test-link-var_2:latest\", \"model\")\n        except CommError:\n            var_2 = wandb.Artifact(\"test-link-var_2\", \"model\")\n            with tempfile.TemporaryDirectory() as var_3:\n                with open(var_3 + \"/boom.txt\", \"w\") as var_4:\n                    var_4.write(\"testing\")\n                var_5 = var_4\"{var_3}/boom.txt\"\n                var_2.add_file(var_5, \"test-name\")\n            var_2 = var_1.log_artifact(var_2)\n            var_2.wait()\n        var_1.link_artifact(var_2, \"project/test_portfolio\")\n",
      "lines_processed": 15,
      "total_lines": 30,
      "llm_code": "def main():\n    with wandb.init() as run:\n        wandb.log({\"metric\": 5})\n        try:\n            artifact = run.use_artifact(\"test-link-artifact:latest\", \"model\")\n        except CommError:\n            artifact = wandb.Artifact(\"test-link-artifact\", \"model\")\n            with tempfile.TemporaryDirectory() as temp_dir:\n                with open(temp_dir + \"/boom.txt\", \"w\") as file:\n                    file.write(\"testing\")\n                file_path = file\"{temp_dir}/boom.txt\"\n                artifact.add_file(file_path, \"test-name\")\n            artifact = run.log_artifact(artifact)\n            artifact.wait()\n        run.link_artifact(artifact, \"project/test_portfolio\")\n",
      "llm_variables": [
        "run",
        "artifact",
        "temp_dir",
        "file",
        "file_path"
      ]
    },
    {
      "file_path": "public_collections.py",
      "code": "def main():\n    run = wandb.init(entity=\"mock_server_entity\", project=\"test\")\n    art = wandb.Artifact(\"test_artifact\", type=\"model\")\n    art.add_file(\"public_collection.py\")\n    run.link_artifact(art, \"mock_server_entity/test/test_port\")\n    run.finish()\n\n    collections = wandb.Api().artifact_type(\"model\", \"test\").collections()\n    assert len(collections) == 2\n",
      "variables": [
        "run",
        "art",
        "collections"
      ],
      "anonymized_code": "def main():\n    var_1 = wandb.init(entity=\"mock_server_entity\", project=\"test\")\n    var_2 = wandb.Artifact(\"test_artifact\", type=\"model\")\n    var_2.add_file(\"public_collection.py\")\n    var_1.link_artifact(var_2, \"mock_server_entity/test/test_port\")\n    var_1.finish()\n\n    var_3 = wandb.Api().artifact_type(\"model\", \"test\").var_3()\n    assert len(var_3) == 2\n",
      "lines_processed": 9,
      "total_lines": 16,
      "llm_code": "def main():\n    run = wandb.init(entity=\"mock_server_entity\", project=\"test\")\n    artifact = wandb.Artifact(\"test_artifact\", type=\"model\")\n    artifact.add_file(\"public_collection.py\")\n    run.link_artifact(artifact, \"mock_server_entity/test/test_port\")\n    run.finish()\n\n    artifact_type = wandb.Api().artifact_type(\"model\", \"test\").artifact_type()\n    assert len(artifact_type) == 2\n",
      "llm_variables": [
        "run",
        "artifact",
        "artifact_type"
      ]
    },
    {
      "file_path": "hatch_build.py",
      "code": "def _get_env_bool(name: str, default: bool) -> bool:\n    \"\"\"Returns the value of a boolean environment variable.\"\"\"\n    value = os.getenv(name)\n\n    if value is None:\n        return default\n    elif value.lower() in (\"1\", \"true\"):\n        return True\n    elif value.lower() in (\"0\", \"false\"):\n        return False\n    else:\n        raise ValueError(\n            f\"Environment variable '{name}' has invalid value '{value}'\"\n            \" expected one of {1,true,0,false}.\"\n        )\n",
      "variables": [
        "name",
        "default",
        "value"
      ],
      "anonymized_code": "def _get_env_bool(var_1: str, var_2: bool) -> bool:\n    \"\"\"Returns the var_3 of a boolean environment variable.\"\"\"\n    var_3 = os.getenv(var_1)\n\n    if var_3 is None:\n        return var_2\n    elif var_3.lower() in (\"1\", \"true\"):\n        return True\n    elif var_3.lower() in (\"0\", \"false\"):\n        return False\n    else:\n        raise ValueError(\n            f\"Environment variable '{var_1}' has invalid var_3 '{var_3}'\"\n            \" expected one of {1,true,0,false}.\"\n        )\n",
      "lines_processed": 15,
      "total_lines": 245,
      "llm_code": "def _get_env_bool(env_var: str, default_value: bool) -> bool:\n    \"\"\"Returns the env_var_value of a boolean environment variable.\"\"\"\n    env_var_value = os.getenv(env_var)\n\n    if env_var_value is None:\n        return default_value\n    elif env_var_value.lower() in (\"1\", \"true\"):\n        return True\n    elif env_var_value.lower() in (\"0\", \"false\"):\n        return False\n    else:\n        raise ValueError(\n            f\"Environment variable '{env_var}' has invalid env_var_value '{env_var_value}'\"\n            \" expected one of {1,true,0,false}.\"\n        )\n",
      "llm_variables": [
        "env_var",
        "default_value",
        "env_var_value"
      ]
    },
    {
      "file_path": "hatch_build.py",
      "code": "def _to_goarch(arch: str) -> str:\n    \"\"\"Returns a valid GOARCH value or the empty string.\"\"\"\n    return {\n        # amd64 synonyms\n        \"amd64\": \"amd64\",\n        \"x86_64\": \"amd64\",\n        # arm64 synonyms\n        \"arm64\": \"arm64\",\n        \"aarch64\": \"arm64\",\n    }.get(arch, \"\")\n",
      "variables": [
        "arch"
      ],
      "anonymized_code": "def _to_goarch(var_1: str) -> str:\n    \"\"\"Returns a valid GOARCH value or the empty string.\"\"\"\n    return {\n        # amd64 synonyms\n        \"amd64\": \"amd64\",\n        \"x86_64\": \"amd64\",\n        # arm64 synonyms\n        \"arm64\": \"arm64\",\n        \"aarch64\": \"arm64\",\n    }.get(var_1, \"\")\n",
      "lines_processed": 10,
      "total_lines": 245,
      "llm_code": "def _to_goarch(goarch: str) -> str:\n    \"\"\"Returns a valid GOARCH value or the empty string.\"\"\"\n    return {\n        # amd64 synonyms\n        \"amd64\": \"amd64\",\n        \"x86_64\": \"amd64\",\n        # arm64 synonyms\n        \"arm64\": \"arm64\",\n        \"aarch64\": \"arm64\",\n    }.get(goarch, \"\")",
      "llm_variables": [
        "goarch"
      ]
    },
    {
      "file_path": "log-artifact.py",
      "code": "def main():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        with wandb.init() as run:\n            wandb.log({\"metric\": 5})\n            artifact = wandb.Artifact(\"test-log-artifact\", \"test-type\")\n            with open(tmpdir + \"/boom.txt\", \"w\") as f:\n                f.write(\"testing\")\n\n            local_path = f\"{tmpdir}/boom.txt\"\n            artifact.add_file(local_path, \"test-name\")\n            run.log_artifact(artifact)\n",
      "variables": [
        "tmpdir",
        "run",
        "artifact",
        "f",
        "local_path"
      ],
      "anonymized_code": "def main():\n    with tempfile.TemporaryDirectory() as var_1:\n        with wandb.init() as var_2:\n            wandb.log({\"metric\": 5})\n            var_3 = wandb.Artifact(\"test-log-var_3\", \"test-type\")\n            with open(var_1 + \"/boom.txt\", \"w\") as var_4:\n                var_4.write(\"testing\")\n\n            var_5 = var_4\"{var_1}/boom.txt\"\n            var_3.add_file(var_5, \"test-name\")\n            var_2.log_artifact(var_3)\n",
      "lines_processed": 11,
      "total_lines": 20,
      "llm_code": "def main():\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        with wandb.init() as run:\n            wandb.log({\"metric\": 5})\n            file = wandb.Artifact(\"test-log-file\", \"test-type\")\n            with open(tmp_dir + \"/boom.txt\", \"w\") as var_3:\n                var_3.write(\"testing\")\n\n            var_5 = var_3\"{tmp_dir}/boom.txt\"\n            file.add_file(var_5, \"test-name\")\n            run.log_artifact(file)\n",
      "llm_variables": [
        "tmp_dir",
        "run",
        "file",
        "var_3",
        "var_5"
      ]
    },
    {
      "file_path": "use-model.py",
      "code": "def main():\n    run = wandb.init()\n\n    my_model = Net()\n\n    sm = _SavedModel.init(my_model)\n    art = wandb.Artifact(\"my-model\", \"model\")\n    art.add(sm, \"index\")\n\n    art = run.log_artifact(art)\n    art.wait()\n\n    # use_model() hits the download path where we try to download the file\n    # using entry._file_url, which fails in this test harness\n    # TODO: Remove the download() call once caching is implemented in nexus\n    art.download()\n    _ = use_model(\"my-model:latest\")\n\n    run.finish()\n",
      "variables": [
        "run",
        "my_model",
        "sm",
        "art",
        "_"
      ],
      "anonymized_code": "def main():\n    var_1 = wandb.init()\n\n    var_2 = Net()\n\n    var_3 = _SavedModel.init(var_2)\n    var_4 = wandb.Artifact(\"my-model\", \"model\")\n    var_4.add(var_3, \"index\")\n\n    var_4 = var_1.log_artifact(var_4)\n    var_4.wait()\n\n    # use_model() hits the download path where we try to download the file\n    # using entry._file_url, which fails in this test harness\n    # TODO: Remove the download() call once caching is implemented in nexus\n    var_4.download()\n    var_5 = use_model(\"my-model:latest\")\n\n    var_1.finish()\n",
      "lines_processed": 19,
      "total_lines": 58,
      "llm_code": "def main():\n    run = wandb.init()\n\n    model = Net()\n\n    saved_model = _SavedModel.init(model)\n    artifact = wandb.Artifact(\"my-model\", \"model\")\n    artifact.add(saved_model, \"index\")\n\n    artifact = run.log_artifact(artifact)\n    artifact.wait()\n\n    # use_model() hits the download path where we try to download the file\n    # using entry._file_url, which fails in this test harness\n    # TODO: Remove the download() call once caching is implemented in nexus\n    artifact.download()\n    model = use_model(\"my-model:latest\")\n\n    run.finish()\n",
      "llm_variables": [
        "run",
        "model",
        "saved_model",
        "artifact",
        "model"
      ]
    },
    {
      "file_path": "log-model.py",
      "code": "def main():\n    my_model = Net()\n\n    _ = log_model(my_model, \"my-model\", aliases=[\"boom\"], scope_project=True)\n\n    wandb.finish()\n",
      "variables": [
        "my_model",
        "_"
      ],
      "anonymized_code": "def main():\n    var_1 = Net()\n\n    var_2 = log_model(var_1, \"my-model\", aliases=[\"boom\"], scope_project=True)\n\n    wandb.finish()\n",
      "lines_processed": 6,
      "total_lines": 44,
      "llm_code": "def main():\n    model = Net()\n\n    log_model_result = log_model(model, \"my-model\", aliases=[\"boom\"], scope_project=True)\n\n    wandb.finish()\n",
      "llm_variables": [
        "model",
        "log_model_result"
      ]
    },
    {
      "file_path": "log-unicode-artifact.py",
      "code": "def main():\n    with wandb.init() as run:\n        artifact = wandb.Artifact(\"my_artifact\", type=\"unicode_artifact\")\n        with artifact.new_file(\"euler.txt\", mode=\"w\", encoding=\"utf-8\") as f:\n            f.write(\"e^(i\u03c0)+1=0\")\n\n        run.log_artifact(artifact)\n",
      "variables": [
        "run",
        "artifact",
        "f"
      ],
      "anonymized_code": "def main():\n    with wandb.init() as var_1:\n        var_2 = wandb.Artifact(\"my_artifact\", type=\"unicode_artifact\")\n        with var_2.new_file(\"euler.txt\", mode=\"w\", encoding=\"utf-8\") as var_3:\n            var_3.write(\"e^(i\u03c0)+1=0\")\n\n        var_1.log_artifact(var_2)\n",
      "lines_processed": 7,
      "total_lines": 14,
      "llm_code": "def main():\n    with wandb.init() as run:\n        artifact = wandb.Artifact(\"my_artifact\", type=\"unicode_artifact\")\n        with artifact.new_file(\"euler.txt\", mode=\"w\", encoding=\"utf-8\") as file:\n            file.write(\"e^(i\u03c0)+1=0\")\n\n        run.log_artifact(artifact)\n",
      "llm_variables": [
        "run",
        "artifact",
        "file"
      ]
    }
  ],
  "matplotlib_matplotlib": [
    {
      "file_path": "missing_references.py",
      "code": "def _write_missing_references_json(records, json_path):\n    \"\"\"\n    Convert ignored references to a format which we can write as JSON\n\n    Convert from ``{(domain_type, target): locations}`` to\n    ``{domain_type: {target: locations}}`` since JSON can't serialize tuples.\n    \"\"\"\n    # Sorting records and keys avoids needlessly big diffs when\n    # missing_references.json is regenerated.\n    transformed_records = defaultdict(dict)\n    for (domain_type, target), paths in records.items():\n        transformed_records[domain_type][target] = sorted(paths)\n    with json_path.open(\"w\") as stream:\n        json.dump(transformed_records, stream, sort_keys=True, indent=2)\n        stream.write(\"\\n\")  # Silence pre-commit no-newline-at-end-of-file warning.\n",
      "variables": [
        "records",
        "json_path",
        "transformed_records",
        "domain_type",
        "target",
        "paths",
        "stream"
      ],
      "anonymized_code": "def _write_missing_references_json(var_1, var_2):\n    \"\"\"\n    Convert ignored references to a format which we can write as JSON\n\n    Convert from ``{(var_4, var_5): locations}`` to\n    ``{var_4: {var_5: locations}}`` since JSON can't serialize tuples.\n    \"\"\"\n    # Sorting var_1 and keys avoids needlessly big diffs when\n    # missing_references.json is regenerated.\n    var_3 = defaultdict(dict)\n    for (var_4, var_5), var_6 in var_1.items():\n        var_3[var_4][var_5] = sorted(var_6)\n    with var_2.open(\"w\") as var_7:\n        json.dump(var_3, var_7, sort_keys=True, indent=2)\n        var_7.write(\"\\n\")  # Silence pre-commit no-newline-at-end-of-file warning.\n",
      "lines_processed": 15,
      "total_lines": 232,
      "llm_code": "def _write_missing_references_json(missing_references, output_file):\n    \"\"\"\n    Convert ignored references to a format which we can write as JSON\n\n    Convert from ``{(key1, key2): locations}`` to\n    ``{key1: {key2: locations}}`` since JSON can't serialize tuples.\n    \"\"\"\n    # Sorting missing_references and keys avoids needlessly big diffs when\n    # missing_references.json is regenerated.\n    result = defaultdict(dict)\n    for (key1, key2), locations in missing_references.items():\n        result[key1][key2] = sorted(locations)\n    with output_file.open(\"w\") as file:\n        json.dump(result, file, sort_keys=True, indent=2)\n        file.write(\"\\n\")  # Silence pre-commit no-newline-at-end-of-file warning.\n",
      "llm_variables": [
        "missing_references",
        "output_file",
        "result",
        "key1",
        "key2",
        "locations",
        "file"
      ]
    },
    {
      "file_path": "missing_references.py",
      "code": "def _read_missing_references_json(json_path):\n    \"\"\"\n    Convert from the JSON file to the form used internally by this\n    extension.\n\n    The JSON file is stored as ``{domain_type: {target: [locations,]}}``\n    since JSON can't store dictionary keys which are tuples. We convert\n    this back to ``{(domain_type, target):[locations]}`` for internal use.\n\n    \"\"\"\n    with json_path.open(\"r\") as stream:\n        data = json.load(stream)\n\n    ignored_references = {}\n    for domain_type, targets in data.items():\n        for target, locations in targets.items():\n            ignored_references[(domain_type, target)] = locations\n    return ignored_references\n",
      "variables": [
        "json_path",
        "stream",
        "data",
        "ignored_references",
        "domain_type",
        "targets",
        "target",
        "locations"
      ],
      "anonymized_code": "def _read_missing_references_json(var_1):\n    \"\"\"\n    Convert from the JSON file to the form used internally by this\n    extension.\n\n    The JSON file is stored as ``{var_5: {var_7: [var_8,]}}``\n    since JSON can't store dictionary keys which are tuples. We convert\n    this back to ``{(var_5, var_7):[var_8]}`` for internal use.\n\n    \"\"\"\n    with var_1.open(\"r\") as var_2:\n        var_3 = json.load(var_2)\n\n    var_4 = {}\n    for var_5, var_6 in var_3.items():\n        for var_7, var_8 in var_6.items():\n            var_4[(var_5, var_7)] = var_8\n    return var_4\n",
      "lines_processed": 18,
      "total_lines": 232,
      "llm_code": "def _read_missing_references_json(json_file):\n    \"\"\"\n    Convert from the JSON file to the form used internally by this\n    extension.\n\n    The JSON file is stored as ``{key1: {key2: [value,]}}``\n    since JSON can't store dictionary keys which are tuples. We convert\n    this back to ``{(key1, key2):[value]}`` for internal use.\n\n    \"\"\"\n    with json_file.open(\"r\") as file:\n        data = json.load(file)\n\n    result = {}\n    for key1, key2 in data.items():\n        for key2, value in key2.items():\n            result[(key1, key2)] = value\n    return result\n",
      "llm_variables": [
        "json_file",
        "file",
        "data",
        "result",
        "key1",
        "key2",
        "key2",
        "value"
      ]
    },
    {
      "file_path": "missing_references.py",
      "code": "def handle_missing_reference(app, domain, node):\n    \"\"\"\n    Handle the warn-missing-reference Sphinx event.\n\n    This function will:\n\n    #. record missing references for saving/comparing with ignored list.\n    #. prevent Sphinx from raising a warning on ignored references.\n    \"\"\"\n    refdomain = node[\"refdomain\"]\n    reftype = node[\"reftype\"]\n    target = node[\"reftarget\"]\n    location = get_location(node, app)\n    domain_type = f\"{refdomain}:{reftype}\"\n\n    app.env.missing_references_events[(domain_type, target)].add(location)\n\n    # If we're ignoring this event, return True so that Sphinx thinks we handled it,\n    # even though we didn't print or warn. If we aren't ignoring it, Sphinx will print a\n",
      "variables": [
        "app",
        "domain",
        "node",
        "refdomain",
        "reftype",
        "target",
        "location",
        "domain_type"
      ],
      "anonymized_code": "def handle_missing_reference(var_1, var_2, var_3):\n    \"\"\"\n    Handle the warn-missing-reference Sphinx event.\n\n    This function will:\n\n    #. record missing references for saving/comparing with ignored list.\n    #. prevent Sphinx from raising a warning on ignored references.\n    \"\"\"\n    var_4 = var_3[\"var_4\"]\n    var_5 = var_3[\"var_5\"]\n    var_6 = var_3[\"reftarget\"]\n    var_7 = get_location(var_3, var_1)\n    var_8 = f\"{var_4}:{var_5}\"\n\n    var_1.env.missing_references_events[(var_8, var_6)].add(var_7)\n\n    # If we're ignoring this event, return True so that Sphinx thinks we handled it,\n    # even though we didn't print or warn. If we aren't ignoring it, Sphinx will print a\n",
      "lines_processed": 19,
      "total_lines": 232,
      "llm_code": "def handle_missing_reference(missing_ref, ignored_list, event):\n    \"\"\"\n    Handle the warn-missing-reference Sphinx event.\n\n    This function will:\n\n    #. record missing references for saving/comparing with ignored list.\n    #. prevent Sphinx from raising a warning on ignored references.\n    \"\"\"\n    ref_type = event[\"ref_type\"]\n    ref_target = event[\"ref_target\"]\n    reftarget = event[\"reftarget\"]\n    location = get_location(event, missing_ref)\n    reference_key = f\"{ref_type}:{ref_target}\"\n\n    missing_ref.env.missing_references_events[(reference_key, reftarget)].add(location)\n\n    # If we're ignoring this event, return True so that Sphinx thinks we handled it,\n    # even though we didn't print or warn. If we aren't ignoring it, Sphinx will print a\n",
      "llm_variables": [
        "missing_ref",
        "ignored_list",
        "event",
        "ref_type",
        "ref_target",
        "reftarget",
        "location",
        "reference_key"
      ]
    },
    {
      "file_path": "missing_references.py",
      "code": "def save_missing_references(app, exc):\n    \"\"\"\n    Write a new JSON file containing missing references.\n    \"\"\"\n    json_path = Path(app.confdir) / app.config.missing_references_filename\n    references_warnings = app.env.missing_references_events\n    _write_missing_references_json(references_warnings, json_path)\n",
      "variables": [
        "app",
        "exc",
        "json_path",
        "references_warnings"
      ],
      "anonymized_code": "def save_missing_references(var_1, var_2):\n    \"\"\"\n    Write a new JSON file containing missing references.\n    \"\"\"\n    var_3 = Path(var_1.confdir) / var_1.config.missing_references_filename\n    var_4 = var_1.env.missing_references_events\n    _write_missing_references_json(var_4, var_3)\n",
      "lines_processed": 7,
      "total_lines": 232,
      "llm_code": "def save_missing_references(config, env):\n    \n    var_3 = Path(config.confdir) / config.config.missing_references_filename\n    var_4 = env.missing_references_events\n    _write_missing_references_json(var_4, var_3)\n    \n    ",
      "llm_variables": [
        "config",
        "env",
        "var_3",
        "var_4"
      ]
    },
    {
      "file_path": "missing_references.py",
      "code": "def prepare_missing_references_setup(app):\n    \"\"\"\n    Initialize this extension once the configuration is ready.\n    \"\"\"\n    if not app.config.missing_references_enabled:\n        # no-op when we are disabled.\n        return\n\n    app.connect(\"warn-missing-reference\", handle_missing_reference)\n    if app.config.missing_references_warn_unused_ignores:\n        app.connect(\"build-finished\", warn_unused_missing_references)\n    if app.config.missing_references_write_json:\n        app.connect(\"build-finished\", save_missing_references)\n\n    json_path = Path(app.confdir) / app.config.missing_references_filename\n    app.env.missing_references_ignored_references = (\n        _read_missing_references_json(json_path) if json_path.exists() else {}\n    )\n    app.env.missing_references_events = defaultdict(set)\n",
      "variables": [
        "app",
        "json_path"
      ],
      "anonymized_code": "def prepare_missing_references_setup(var_1):\n    \"\"\"\n    Initialize this extension once the configuration is ready.\n    \"\"\"\n    if not var_1.config.missing_references_enabled:\n        # no-op when we are disabled.\n        return\n\n    var_1.connect(\"warn-missing-reference\", handle_missing_reference)\n    if var_1.config.missing_references_warn_unused_ignores:\n        var_1.connect(\"build-finished\", warn_unused_missing_references)\n    if var_1.config.missing_references_write_json:\n        var_1.connect(\"build-finished\", save_missing_references)\n\n    var_2 = Path(var_1.confdir) / var_1.config.missing_references_filename\n    var_1.env.missing_references_ignored_references = (\n        _read_missing_references_json(var_2) if var_2.exists() else {}\n    )\n    var_1.env.missing_references_events = defaultdict(set)\n",
      "lines_processed": 19,
      "total_lines": 232,
      "llm_code": "def prepare_missing_references_setup(config):\n    \"\"\"\n    Initialize this extension once the configuration is ready.\n    \"\"\"\n    if not config.config.missing_references_enabled:\n        # no-op when we are disabled.\n        return\n\n    config.connect(\"warn-missing-reference\", handle_missing_reference)\n    if config.config.missing_references_warn_unused_ignores:\n        config.connect(\"build-finished\", warn_unused_missing_references)\n    if config.config.missing_references_write_json:\n        config.connect(\"build-finished\", save_missing_references)\n\n    var_2 = Path(config.confdir) / config.config.missing_references_filename\n    config.env.missing_references_ignored_references = (\n        _read_missing_references_json(var_2) if var_2.exists() else {}\n    )\n    config.env.missing_references_events = defaultdict(set)\n",
      "llm_variables": [
        "config",
        "var_2"
      ]
    },
    {
      "file_path": "missing_references.py",
      "code": "def _truncate_location(location):\n    \"\"\"\n    Cuts off anything after the first colon in location strings.\n\n    This allows for easy comparison even when line numbers change\n    (as they do regularly).\n    \"\"\"\n    return location.split(\":\", 1)[0]\n",
      "variables": [
        "location"
      ],
      "anonymized_code": "def _truncate_location(var_1):\n    \"\"\"\n    Cuts off anything after the first colon in var_1 strings.\n\n    This allows for easy comparison even when line numbers change\n    (as they do regularly).\n    \"\"\"\n    return var_1.split(\":\", 1)[0]\n",
      "lines_processed": 8,
      "total_lines": 232,
      "llm_code": "def _truncate_location(path):\n    \"\"\"\n    Cuts off anything after the first colon in path strings.\n\n    This allows for easy comparison even when line numbers change\n    (as they do regularly).\n    \"\"\"\n    return path.split(\":\", 1)[0]\n",
      "llm_variables": [
        "path"
      ]
    },
    {
      "file_path": "missing_references.py",
      "code": "def get_location(node, app):\n    \"\"\"\n    Given a docutils node and a sphinx application, return a string\n    representation of the source location of this node.\n\n    Usually, this will be of the form \"path/to/file:linenumber\". Two\n    special values can be emitted, \"<external>\" for paths which are\n    not contained in this source tree (e.g. docstrings included from\n    other modules) or \"<unknown>\", indicating that the sphinx application\n    cannot locate the original source file (usually because an extension\n    has injected text into the sphinx parsing engine).\n    \"\"\"\n    source, line = get_source_line(node)\n\n    if source:\n        # 'source' can have the form '/some/path:docstring of some.api' but the\n        # colons are forbidden on windows, but on posix just passes through.\n        if ':docstring of' in source:\n            path, *post = source.rpartition(':docstring of')\n",
      "variables": [
        "node",
        "app",
        "source",
        "line",
        "path",
        "post"
      ],
      "anonymized_code": "def get_location(var_1, var_2):\n    \"\"\"\n    Given a docutils var_1 and a sphinx application, return a string\n    representation of the var_3 location of this var_1.\n\n    Usually, this will be of the form \"var_5/to/file:linenumber\". Two\n    special values can be emitted, \"<external>\" for paths which are\n    not contained in this var_3 tree (e.g. docstrings included from\n    other modules) or \"<unknown>\", indicating that the sphinx application\n    cannot locate the original var_3 file (usually because an extension\n    has injected text into the sphinx parsing engine).\n    \"\"\"\n    var_3, var_4 = get_source_line(var_1)\n\n    if var_3:\n        # 'var_3' can have the form '/some/var_5:docstring of some.api' but the\n        # colons are forbidden on windows, but on posix just passes through.\n        if ':docstring of' in var_3:\n            var_5, *var_6 = var_3.rpartition(':docstring of')\n",
      "lines_processed": 19,
      "total_lines": 232,
      "llm_code": "def get_location(docutils, sphinx):\n    \"\"\"\n    Given a docutils docutils and a sphinx application, return a string\n    representation of the source location of this docutils.\n\n    Usually, this will be of the form \"path/to/file:linenumber\". Two\n    special values can be emitted, \"<external>\" for paths which are\n    not contained in this source tree (e.g. docstrings included from\n    other modules) or \"<unknown>\", indicating that the sphinx application\n    cannot locate the original source file (usually because an extension\n    has injected text into the sphinx parsing engine).\n    \"\"\"\n    source, line = get_source_line(docutils)\n\n    if source:\n        # 'source' can have the form '/some/path:docstring of some.api' but the\n        # colons are forbidden on windows, but on posix just passes through.\n        if ':docstring of' in source:\n            path, *rest = source.rpartition(':docstring of')\n",
      "llm_variables": [
        "docutils",
        "sphinx",
        "source",
        "line",
        "path",
        "rest"
      ]
    },
    {
      "file_path": "missing_references.py",
      "code": "def setup(app):\n    app.add_config_value(\"missing_references_enabled\", True, \"env\")\n    app.add_config_value(\"missing_references_write_json\", False, \"env\")\n    app.add_config_value(\"missing_references_warn_unused_ignores\", True, \"env\")\n    app.add_config_value(\"missing_references_filename\",\n                         \"missing-references.json\", \"env\")\n\n    app.connect(\"builder-inited\", prepare_missing_references_setup)\n\n    return {'parallel_read_safe': True}\n",
      "variables": [
        "app"
      ],
      "anonymized_code": "def setup(var_1):\n    var_1.add_config_value(\"missing_references_enabled\", True, \"env\")\n    var_1.add_config_value(\"missing_references_write_json\", False, \"env\")\n    var_1.add_config_value(\"missing_references_warn_unused_ignores\", True, \"env\")\n    var_1.add_config_value(\"missing_references_filename\",\n                         \"missing-references.json\", \"env\")\n\n    var_1.connect(\"builder-inited\", prepare_missing_references_setup)\n\n    return {'parallel_read_safe': True}\n",
      "lines_processed": 10,
      "total_lines": 232,
      "llm_code": "def setup(config):\n    config.add_config_value(\"missing_references_enabled\", True, \"env\")\n    config.add_config_value(\"missing_references_write_json\", False, \"env\")\n    config.add_config_value(\"missing_references_warn_unused_ignores\", True, \"env\")\n    config.add_config_value(\"missing_references_filename\",\n                         \"missing-references.json\", \"env\")\n\n    config.connect(\"builder-inited\", prepare_missing_references_setup)\n\n    return {'parallel_read_safe': True}\n",
      "llm_variables": [
        "config"
      ]
    },
    {
      "file_path": "axes_margins.py",
      "code": "def arrow(p1, p2, **props):\n    ax.annotate(\"\", p1, p2,\n                arrowprops=dict(arrowstyle=\"<->\", shrinkA=0, shrinkB=0, **props))\n",
      "variables": [
        "p1",
        "p2",
        "props"
      ],
      "anonymized_code": "def arrow(var_1, var_2, **var_3):\n    ax.annotate(\"\", var_1, var_2,\n                arrowprops=dict(arrowstyle=\"<->\", shrinkA=0, shrinkB=0, **var_3))\n",
      "lines_processed": 3,
      "total_lines": 42,
      "llm_code": "def arrow(start, end, **kwargs):\n    ax.annotate(\"\", start, end,\n                arrowprops=dict(arrowstyle=\"<->\", shrinkA=0, shrinkB=0, **kwargs))\n",
      "llm_variables": [
        "start",
        "end",
        "kwargs"
      ]
    },
    {
      "file_path": "util.py",
      "code": "def matplotlib_reduced_latex_scraper(block, block_vars, gallery_conf,\n                                     **kwargs):\n    \"\"\"\n    Reduce srcset when creating a PDF.\n\n    Because sphinx-gallery runs *very* early, we cannot modify this even in the\n    earliest builder-inited signal. Thus we do it at scraping time.\n    \"\"\"\n    from sphinx_gallery.scrapers import matplotlib_scraper\n\n    if gallery_conf['builder_name'] == 'latex':\n        gallery_conf['image_srcset'] = []\n    return matplotlib_scraper(block, block_vars, gallery_conf, **kwargs)\n",
      "variables": [
        "block",
        "block_vars",
        "gallery_conf",
        "kwargs"
      ],
      "anonymized_code": "def matplotlib_reduced_latex_scraper(var_1, var_2, var_3,\n                                     **var_4):\n    \"\"\"\n    Reduce srcset when creating a PDF.\n\n    Because sphinx-gallery runs *very* early, we cannot modify this even in the\n    earliest builder-inited signal. Thus we do it at scraping time.\n    \"\"\"\n    from sphinx_gallery.scrapers import matplotlib_scraper\n\n    if var_3['builder_name'] == 'latex':\n        var_3['image_srcset'] = []\n    return matplotlib_scraper(var_1, var_2, var_3, **var_4)\n",
      "lines_processed": 13,
      "total_lines": 21,
      "llm_code": "def matplotlib_reduced_latex_scraper(srcset, image_info, builder_options, **kwargs):\n    \"\"\"\n    Reduce srcset when creating a PDF.\n\n    Because sphinx-gallery runs *very* very early, we cannot modify this even in the\n    earliest builder-inited signal. Thus we do it at scraping time.\n    \"\"\"\n    from sphinx_gallery.scrapers import matplotlib_scraper\n\n    if builder_options[\"builder_name\"] == \"latex\":\n        builder_options[\"image_srcset\"] = []\n    return matplotlib_scraper(srcset, image_info, builder_options, **kwargs)",
      "llm_variables": [
        "srcset",
        "image_info",
        "builder_options",
        "kwargs"
      ]
    },
    {
      "file_path": "util.py",
      "code": "def clear_basic_units(gallery_conf, fname):\n    return sys.modules.pop('basic_units', None)\n",
      "variables": [
        "gallery_conf",
        "fname"
      ],
      "anonymized_code": "def clear_basic_units(var_1, var_2):\n    return sys.modules.pop('basic_units', None)\n",
      "lines_processed": 2,
      "total_lines": 21,
      "llm_code": "def clear_basic_units(module, package):\n    return sys.modules.pop(package, None)\n",
      "llm_variables": [
        "module",
        "package"
      ]
    },
    {
      "file_path": "vendor_schemas.py",
      "code": "def print_progress(block_count, block_size, total_size):\n    size = block_count * block_size\n    if total_size != -1:\n        size = min(size, total_size)\n        width = 50\n        percent = size / total_size * 100\n        filled = int(percent // (100 // width))\n        percent_str = '\\N{Full Block}' * filled + '\\N{Light Shade}' * (width - filled)\n    print(f'{percent_str} {size:6d} / {total_size:6d}', end='\\r')\n",
      "variables": [
        "block_count",
        "block_size",
        "total_size",
        "size",
        "width",
        "percent",
        "filled",
        "percent_str"
      ],
      "anonymized_code": "def print_progress(var_1, var_2, var_3):\n    var_4 = var_1 * var_2\n    if var_3 != -1:\n        var_4 = min(var_4, var_3)\n        var_5 = 50\n        var_6 = var_4 / var_3 * 100\n        var_7 = int(var_6 // (100 // var_5))\n        var_8 = '\\N{Full Block}' * var_7 + '\\N{Light Shade}' * (var_5 - var_7)\n    print(f'{var_8} {var_4:6d} / {var_3:6d}', end='\\r')\n",
      "lines_processed": 9,
      "total_lines": 50,
      "llm_code": "def print_progress(iteration, total, max_value):\n    current = iteration * total\n    if max_value != -1:\n        current = min(current, max_value)\n        bar_length = 50\n        percentage = current / max_value * 100\n        filled_length = int(percentage // (100 // bar_length))\n        bar = '\\N{Full Block}' * filled_length + '\\N{Light Shade}' * (bar_length - filled_length)\n    print(f'{bar} {current:6d} / {max_value:6d}', end='\\r')\n",
      "llm_variables": [
        "iteration",
        "total",
        "max_value",
        "current",
        "bar_length",
        "percentage",
        "filled_length",
        "bar"
      ]
    },
    {
      "file_path": "math_symbol_table.py",
      "code": "def setup(app):\n    app.add_directive(\"math_symbol_table\", MathSymbolTableDirective)\n\n    metadata = {'parallel_read_safe': True, 'parallel_write_safe': True}\n    return metadata\n",
      "variables": [
        "app",
        "metadata"
      ],
      "anonymized_code": "def setup(var_1):\n    var_1.add_directive(\"math_symbol_table\", MathSymbolTableDirective)\n\n    var_2 = {'parallel_read_safe': True, 'parallel_write_safe': True}\n    return var_2\n",
      "lines_processed": 5,
      "total_lines": 152,
      "llm_code": "def setup(config):\n    config.add_directive(\"math_symbol_table\", MathSymbolTableDirective)\n\n    var_2 = {'parallel_read_safe': True, 'parallel_write_safe': True}\n    return var_2\n",
      "llm_variables": [
        "config",
        "var_2"
      ]
    },
    {
      "file_path": "conf.py",
      "code": "def tutorials_download_error(record):\n    if re.match(\"download file not readable: .*tutorials_(python|jupyter).zip\",\n                record.msg):\n        return False\n",
      "variables": [
        "record"
      ],
      "anonymized_code": "def tutorials_download_error(var_1):\n    if re.match(\"download file not readable: .*tutorials_(python|jupyter).zip\",\n                var_1.msg):\n        return False\n",
      "lines_processed": 4,
      "total_lines": 928,
      "llm_code": "def tutorials_download_error(error):\n    if re.match(\"download file not readable:.*tutorials_(python|jupyter).zip\",\n                error.msg):\n        return False",
      "llm_variables": [
        "error"
      ]
    },
    {
      "file_path": "conf.py",
      "code": "def _check_dependencies():\n    names = {\n        **{ext: ext.split(\".\")[0] for ext in extensions},\n        # Explicitly list deps that are not extensions, or whose PyPI package\n        # name does not match the (toplevel) module name.\n        \"colorspacious\": 'colorspacious',\n        \"mpl_sphinx_theme\": 'mpl_sphinx_theme',\n        \"sphinxcontrib.inkscapeconverter\": 'sphinxcontrib-svg2pdfconverter',\n    }\n    missing = []\n    for name in names:\n        try:\n            __import__(name)\n        except ImportError:\n            missing.append(names[name])\n    if missing:\n        raise ImportError(\n            \"The following dependencies are missing to build the \"\n            f\"documentation: {', '.join(missing)}\")\n",
      "variables": [
        "names",
        "ext",
        "missing",
        "name"
      ],
      "anonymized_code": "def _check_dependencies():\n    var_1 = {\n        **{var_2: var_2.split(\".\")[0] for var_2 in extensions},\n        # Explicitly list deps that are not extensions, or whose PyPI package\n        # var_4 does not match the (toplevel) module var_4.\n        \"colorspacious\": 'colorspacious',\n        \"mpl_sphinx_theme\": 'mpl_sphinx_theme',\n        \"sphinxcontrib.inkscapeconverter\": 'sphinxcontrib-svg2pdfconverter',\n    }\n    var_3 = []\n    for var_4 in var_1:\n        try:\n            __import__(var_4)\n        except ImportError:\n            var_3.append(var_1[var_4])\n    if var_3:\n        raise ImportError(\n            \"The following dependencies are var_3 to build the \"\n            f\"documentation: {', '.join(var_3)}\")\n",
      "lines_processed": 19,
      "total_lines": 928,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "conf.py",
      "code": "def setup(app):\n    if any(st in version for st in ('post', 'dev', 'alpha', 'beta')):\n        bld_type = 'dev'\n    else:\n        bld_type = 'rel'\n    app.add_config_value('skip_sub_dirs', 0, '')\n    app.add_config_value('releaselevel', bld_type, 'env')\n    app.connect('autodoc-process-bases', autodoc_process_bases)\n    if sphinx.version_info[:2] < (7, 1):\n        app.connect('html-page-context', add_html_cache_busting, priority=1000)\n    generate_ScalarMappable_docs()\n",
      "variables": [
        "app",
        "st",
        "bld_type"
      ],
      "anonymized_code": "def setup(var_1):\n    if any(var_2 in version for var_2 in ('post', 'dev', 'alpha', 'beta')):\n        var_3 = 'dev'\n    else:\n        var_3 = 'rel'\n    var_1.add_config_value('skip_sub_dirs', 0, '')\n    var_1.add_config_value('releaselevel', var_3, 'env')\n    var_1.connect('autodoc-process-bases', autodoc_process_bases)\n    if sphinx.version_info[:2] < (7, 1):\n        var_1.connect('html-page-context', add_html_cache_busting, priority=1000)\n    generate_ScalarMappable_docs()\n",
      "lines_processed": 11,
      "total_lines": 928,
      "llm_code": "def setup(config):\n    if any(level in version for level in (\"post\", \"dev\", \"alpha\", \"beta\")):\n        releaselevel = \"dev\"\n    else:\n        releaselevel = \"rel\"\n    config.add_config_value(\"skip_sub_dirs\", 0, \"\")\n    config.add_config_value(\"releaselevel\", releaselevel, \"env\")\n    config.connect(\"autodoc-process-bases\", autodoc_process_bases)\n    if sphinx.version_info[:2] < (7, 1):\n        config.connect(\"html-page-context\", add_html_cache_busting, priority=1000)\n    generate_ScalarMappable_docs()",
      "llm_variables": [
        "config",
        "level",
        "releaselevel"
      ]
    },
    {
      "file_path": "conf.py",
      "code": "def add_html_cache_busting(app, pagename, templatename, context, doctree):\n    \"\"\"\n    Add cache busting query on CSS and JavaScript assets.\n\n    This adds the Matplotlib version as a query to the link reference in the\n    HTML, if the path is not absolute (i.e., it comes from the `_static`\n    directory) and doesn't already have a query.\n\n    .. note:: Sphinx 7.1 provides asset checksums; so this hook only runs on\n              Sphinx 7.0 and earlier.\n    \"\"\"\n    from sphinx.builders.html import Stylesheet, JavaScript\n\n    css_tag = context['css_tag']\n    js_tag = context['js_tag']\n\n    def css_tag_with_cache_busting(css):\n        if isinstance(css, Stylesheet) and css.filename is not None:\n            url = urlsplit(css.filename)\n",
      "variables": [
        "app",
        "pagename",
        "templatename",
        "context",
        "doctree",
        "css_tag",
        "js_tag",
        "css",
        "url"
      ],
      "anonymized_code": "def add_html_cache_busting(var_1, var_2, var_3, var_4, var_5):\n    \"\"\"\n    Add cache busting query on CSS and JavaScript assets.\n\n    This adds the Matplotlib version as a query to the link reference in the\n    HTML, if the path is not absolute (i.e., it comes from the `_static`\n    directory) and doesn't already have a query.\n\n    .. note:: Sphinx 7.1 provides asset checksums; so this hook only runs on\n              Sphinx 7.0 and earlier.\n    \"\"\"\n    from sphinx.builders.html import Stylesheet, JavaScript\n\n    var_6 = var_4['var_6']\n    var_7 = var_4['var_7']\n\n    def css_tag_with_cache_busting(var_8):\n        if isinstance(var_8, Stylesheet) and var_8.filename is not None:\n            var_9 = urlsplit(var_8.filename)\n",
      "lines_processed": 19,
      "total_lines": 928,
      "llm_code": "def add_html_cache_busting(html, config, assets, urls, version):\n    \"\"\"\n    Add cache busting query on CSS and JavaScript assets.\n\n    This adds the Matplotlib version as a query to the link reference in the\n    HTML, if the path is not absolute (i.e., it comes from the `_static`\n    directory) and doesn\"t already have a query.\n\n   .. note:: Sphinx 7.1 provides asset checksums; so this hook only runs on\n              Sphinx 7.0 and earlier.\n    \"\"\"\n    from sphinx.builders.html import Stylesheet, JavaScript\n\n    cache_busting = urls[\"cache_busting\"]\n    asset_path = urls[\"asset_path\"]\n\n    def css_tag_with_cache_busting(tag):\n        if isinstance(tag, Stylesheet) and tag.filename is not None:\n            split_url = urlsplit(tag.filename)",
      "llm_variables": [
        "html",
        "config",
        "assets",
        "urls",
        "version",
        "cache_busting",
        "asset_path",
        "tag",
        "split_url"
      ]
    },
    {
      "file_path": "conf.py",
      "code": "def autodoc_process_bases(app, name, obj, options, bases):\n    \"\"\"\n    Hide pybind11 base object from inheritance tree.\n\n    Note, *bases* must be modified in place.\n    \"\"\"\n    for cls in bases[:]:\n        if not isinstance(cls, type):\n            continue\n        if cls.__module__ == 'pybind11_builtins' and cls.__name__ == 'pybind11_object':\n            bases.remove(cls)\n",
      "variables": [
        "app",
        "name",
        "obj",
        "options",
        "bases",
        "cls"
      ],
      "anonymized_code": "def autodoc_process_bases(var_1, var_2, var_3, var_4, var_5):\n    \"\"\"\n    Hide pybind11 base object from inheritance tree.\n\n    Note, *var_5* must be modified in place.\n    \"\"\"\n    for var_6 in var_5[:]:\n        if not isinstance(var_6, type):\n            continue\n        if var_6.__module__ == 'pybind11_builtins' and var_6.__name__ == 'pybind11_object':\n            var_5.remove(var_6)\n",
      "lines_processed": 11,
      "total_lines": 928,
      "llm_code": "def autodoc_process_bases(parent, child, bases, base_objects, base_classes):\n    \"\"\"\n    Hide pybind11 base object from inheritance tree.\n\n    Note, *base_classes* must be modified in place.\n    \"\"\"\n    for base_class in base_classes[:]:\n        if not isinstance(base_class, type):\n            continue\n        if base_class.__module__ == \"pybind11_builtins\" and base_class.__name__ == \"pybind11_object\":\n            base_classes.remove(base_class)",
      "llm_variables": [
        "parent",
        "child",
        "bases",
        "base_objects",
        "base_classes",
        "base_class"
      ]
    },
    {
      "file_path": "generate_credits.py",
      "code": "def generate_credits():\n    text = subprocess.check_output(['git', 'shortlog', '--summary'])\n    lines = text.decode('utf8').split('\\n')\n    contributors = [line.split('\\t', 1)[1].strip() for line in lines if line]\n    contributors.sort(key=locale.strxfrm)\n    with open('credits.rst', 'w') as f:\n        f.write(TEMPLATE.format(contributors=',\\n'.join(contributors)))\n",
      "variables": [
        "text",
        "lines",
        "contributors",
        "line",
        "f"
      ],
      "anonymized_code": "def generate_credits():\n    var_1 = subprocess.check_output(['git', 'shortlog', '--summary'])\n    var_2 = var_1.decode('utf8').split('\\n')\n    var_3 = [var_4.split('\\t', 1)[1].strip() for var_4 in var_2 if var_4]\n    var_3.sort(key=locale.strxfrm)\n    with open('credits.rst', 'w') as var_5:\n        var_5.write(TEMPLATE.format(var_3=',\\n'.join(var_3)))\n",
      "lines_processed": 7,
      "total_lines": 89,
      "llm_code": "def generate_credits():\n    git_log = subprocess.check_output(['git', 'shortlog', '--summary'])\n    git_output = git_log.decode('utf8').split('\\n')\n    contributors = [line.split('\\t', 1)[1].strip() for line in git_output if line]\n    contributors.sort(key=locale.strxfrm)\n    with open('credits.rst', 'w') as file:\n        file.write(TEMPLATE.format(contributors=',\\n'.join(contributors)))\n",
      "llm_variables": [
        "git_log",
        "git_output",
        "contributors",
        "line",
        "file"
      ]
    },
    {
      "file_path": "generate_credits.py",
      "code": "def check_duplicates():\n    text = subprocess.check_output(['git', 'shortlog', '--summary', '--email'])\n    lines = text.decode('utf8').split('\\n')\n    contributors = [line.split('\\t', 1)[1].strip() for line in lines if line]\n    emails = [re.match('.*<(.*)>', line).group(1) for line in contributors]\n    email_counter = Counter(emails)\n\n    if email_counter.most_common(1)[0][1] > 1:\n        print('DUPLICATE CHECK: The following email addresses are used with '\n              'more than one name.\\nConsider adding them to .mailmap.\\n')\n        for email, count in email_counter.items():\n            if count > 1:\n                print('{}\\n{}'.format(\n                    email, '\\n'.join(l for l in lines if email in l)))\n",
      "variables": [
        "text",
        "lines",
        "contributors",
        "line",
        "emails",
        "email_counter",
        "email",
        "count",
        "l"
      ],
      "anonymized_code": "def check_duplicates():\n    var_1 = subprocess.check_output(['git', 'shortlog', '--summary', '--var_7'])\n    var_2 = var_1.decode('utf8').split('\\n')\n    var_3 = [var_4.split('\\t', 1)[1].strip() for var_4 in var_2 if var_4]\n    var_5 = [re.match('.*<(.*)>', var_4).group(1) for var_4 in var_3]\n    var_6 = Counter(var_5)\n\n    if var_6.most_common(1)[0][1] > 1:\n        print('DUPLICATE CHECK: The following var_7 addresses are used with '\n              'more than one name.\\nConsider adding them to .mailmap.\\n')\n        for var_7, var_8 in var_6.items():\n            if var_8 > 1:\n                print('{}\\n{}'.format(\n                    var_7, '\\n'.join(var_9 for var_9 in var_2 if var_7 in var_9)))\n",
      "lines_processed": 14,
      "total_lines": 89,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "mock_gui_toolkits.py",
      "code": "def setup(app):\n    sys.modules.update(\n        cairocffi=MyCairoCffi(),\n    )\n    return {'parallel_read_safe': True, 'parallel_write_safe': True}\n",
      "variables": [
        "app"
      ],
      "anonymized_code": "def setup(var_1):\n    sys.modules.update(\n        cairocffi=MyCairoCffi(),\n    )\n    return {'parallel_read_safe': True, 'parallel_write_safe': True}\n",
      "lines_processed": 5,
      "total_lines": 13,
      "llm_code": "def setup(caps):\n    sys.modules.update(\n        cairocffi=MyCairoCffi(),\n    )\n    return {\"parallel_read_safe\": True, \"parallel_write_safe\": True}",
      "llm_variables": [
        "caps"
      ]
    },
    {
      "file_path": "redirect_from.py",
      "code": "def _generate_redirects(app, exception):\n    builder = app.builder\n    if builder.name != \"html\" or exception:\n        return\n    for k, v in app.env.get_domain('redirect_from').redirects.items():\n        p = Path(app.outdir, k + builder.out_suffix)\n        html = HTML_TEMPLATE.format(v=builder.get_relative_uri(k, v))\n        if p.is_file():\n            if p.read_text() != html:\n                logger.warning('A redirect-from directive is trying to '\n                               'create %s, but that file already exists '\n                               '(perhaps you need to run \"make clean\")', p)\n        else:\n            logger.info('making refresh html file: %s redirect to %s', k, v)\n            p.parent.mkdir(parents=True, exist_ok=True)\n            p.write_text(html, encoding='utf-8')\n",
      "variables": [
        "app",
        "exception",
        "builder",
        "k",
        "v",
        "p",
        "html"
      ],
      "anonymized_code": "def _generate_redirects(var_1, var_2):\n    var_3 = var_1.var_3\n    if var_3.name != \"var_7\" or var_2:\n        return\n    for var_4, var_5 in var_1.env.get_domain('redirect_from').redirects.items():\n        var_6 = Path(var_1.outdir, var_4 + var_3.out_suffix)\n        var_7 = HTML_TEMPLATE.format(var_5=var_3.get_relative_uri(var_4, var_5))\n        if var_6.is_file():\n            if var_6.read_text() != var_7:\n                logger.warning('A redirect-from directive is trying to '\n                               'create %s, but that file already exists '\n                               '(perhaps you need to run \"make clean\")', var_6)\n        else:\n            logger.info('making refresh var_7 file: %s redirect to %s', var_4, var_5)\n            var_6.parent.mkdir(parents=True, exist_ok=True)\n            var_6.write_text(var_7, encoding='utf-8')\n",
      "lines_processed": 16,
      "total_lines": 127,
      "llm_code": "def _generate_redirects(page, config):\n    redirect_from = page.redirect_from\n    if redirect_from.name != \"redirect_template\" or config:\n        return\n    for path, redirect_to in page.env.get_domain('redirect_from').redirects.items():\n        output_path = Path(page.outdir, path + redirect_from.out_suffix)\n        redirect_template = HTML_TEMPLATE.format(redirect_to=redirect_from.get_relative_uri(path, redirect_to))\n        if output_path.is_file():\n            if output_path.read_text() != redirect_template:\n                logger.warning('A redirect-from directive is trying to '\n                               'create %s, but that file already exists '\n                               '(perhaps you need to run \"make clean\")', output_path)\n        else:\n            logger.info('making refresh redirect_template file: %s redirect to %s', path, redirect_to)\n            output_path.parent.mkdir(parents=True, exist_ok=True)\n            output_path.write_text(redirect_template, encoding='utf-8')\n",
      "llm_variables": [
        "page",
        "config",
        "redirect_from",
        "path",
        "redirect_to",
        "output_path",
        "redirect_template"
      ]
    },
    {
      "file_path": "redirect_from.py",
      "code": "def setup(app):\n    app.add_directive(\"redirect-from\", RedirectFrom)\n    app.add_domain(RedirectFromDomain)\n    app.connect(\"builder-inited\", _clear_redirects)\n    app.connect(\"build-finished\", _generate_redirects)\n\n    metadata = {'parallel_read_safe': True}\n    return metadata\n",
      "variables": [
        "app",
        "metadata"
      ],
      "anonymized_code": "def setup(var_1):\n    var_1.add_directive(\"redirect-from\", RedirectFrom)\n    var_1.add_domain(RedirectFromDomain)\n    var_1.connect(\"builder-inited\", _clear_redirects)\n    var_1.connect(\"build-finished\", _generate_redirects)\n\n    var_2 = {'parallel_read_safe': True}\n    return var_2\n",
      "lines_processed": 8,
      "total_lines": 127,
      "llm_code": "def setup(builder):\n    builder.add_directive(\"redirect-from\", RedirectFrom)\n    builder.add_domain(RedirectFromDomain)\n    builder.connect(\"builder-inited\", _clear_redirects)\n    builder.connect(\"build-finished\", _generate_redirects)\n\n    config = {'parallel_read_safe': True}\n    return config\n",
      "llm_variables": [
        "builder",
        "config"
      ]
    },
    {
      "file_path": "redirect_from.py",
      "code": "def _clear_redirects(app):\n    domain = app.env.get_domain('redirect_from')\n    if domain.redirects:\n        logger.info('clearing cached redirects')\n        domain.redirects.clear()\n",
      "variables": [
        "app",
        "domain"
      ],
      "anonymized_code": "def _clear_redirects(var_1):\n    var_2 = var_1.env.get_domain('redirect_from')\n    if var_2.redirects:\n        logger.info('clearing cached redirects')\n        var_2.redirects.clear()\n",
      "lines_processed": 5,
      "total_lines": 127,
      "llm_code": "def _clear_redirects(env):\n    domain = env.env.get_domain('redirect_from')\n    if domain.redirects:\n        logger.info('clearing cached redirects')\n        domain.redirects.clear()\n",
      "llm_variables": [
        "env",
        "domain"
      ]
    },
    {
      "file_path": "skip_deprecated.py",
      "code": "def setup(app):\n    app.connect('autodoc-skip-member', skip_deprecated)\n\n    metadata = {'parallel_read_safe': True, 'parallel_write_safe': True}\n    return metadata\n",
      "variables": [
        "app",
        "metadata"
      ],
      "anonymized_code": "def setup(var_1):\n    var_1.connect('autodoc-skip-member', skip_deprecated)\n\n    var_2 = {'parallel_read_safe': True, 'parallel_write_safe': True}\n    return var_2\n",
      "lines_processed": 5,
      "total_lines": 17,
      "llm_code": "def setup(conn):\n    conn.connect('autodoc-skip-member', skip_deprecated)\n\n    config = {'parallel_read_safe': True, 'parallel_write_safe': True}\n    return config\n",
      "llm_variables": [
        "conn",
        "config"
      ]
    },
    {
      "file_path": "skip_deprecated.py",
      "code": "def skip_deprecated(app, what, name, obj, skip, options):\n    if skip:\n        return skip\n    skipped = {\"matplotlib.colors\": [\"ColorConverter\", \"hex2color\", \"rgb2hex\"]}\n    skip_list = skipped.get(getattr(obj, \"__module__\", None))\n    if skip_list is not None:\n        return getattr(obj, \"__name__\", None) in skip_list\n",
      "variables": [
        "app",
        "what",
        "name",
        "obj",
        "skip",
        "options",
        "skipped",
        "skip_list"
      ],
      "anonymized_code": "def skip_deprecated(var_1, var_2, var_3, var_4, var_5, var_6):\n    if var_5:\n        return var_5\n    var_7 = {\"matplotlib.colors\": [\"ColorConverter\", \"hex2color\", \"rgb2hex\"]}\n    var_8 = var_7.get(getattr(var_4, \"__module__\", None))\n    if var_8 is not None:\n        return getattr(var_4, \"__name__\", None) in var_8\n",
      "lines_processed": 7,
      "total_lines": 17,
      "llm_code": "def skip_deprecated(imports, deprecated, modules, module, is_deprecated, has_deprecated):\n    if is_deprecated:\n        return is_deprecated\n    var_7 = {\"matplotlib.colors\": [\"ColorConverter\", \"hex2color\", \"rgb2hex\"]}\n    var_8 = var_7.get(getattr(module, \"__module__\", None))\n    if var_8 is not None:\n        return getattr(module, \"__name__\", None) in var_8\n",
      "llm_variables": [
        "imports",
        "deprecated",
        "modules",
        "module",
        "is_deprecated",
        "has_deprecated",
        "var_7",
        "var_8"
      ]
    },
    {
      "file_path": "github.py",
      "code": "def setup(app):\n    \"\"\"\n    Install the plugin.\n\n    :param app: Sphinx application context.\n    \"\"\"\n    app.add_role('ghissue', ghissue_role)\n    app.add_role('ghpull', ghissue_role)\n    app.add_role('ghuser', ghuser_role)\n    app.add_role('ghcommit', ghcommit_role)\n    app.add_config_value('github_project_url', None, 'env')\n\n    metadata = {'parallel_read_safe': True, 'parallel_write_safe': True}\n    return metadata\n",
      "variables": [
        "app",
        "metadata"
      ],
      "anonymized_code": "def setup(var_1):\n    \"\"\"\n    Install the plugin.\n\n    :param var_1: Sphinx application context.\n    \"\"\"\n    var_1.add_role('ghissue', ghissue_role)\n    var_1.add_role('ghpull', ghissue_role)\n    var_1.add_role('ghuser', ghuser_role)\n    var_1.add_role('ghcommit', ghcommit_role)\n    var_1.add_config_value('github_project_url', None, 'env')\n\n    var_2 = {'parallel_read_safe': True, 'parallel_write_safe': True}\n    return var_2\n",
      "lines_processed": 14,
      "total_lines": 167,
      "llm_code": "def setup(app):\n    \"\"\"\n    Install the plugin.\n\n    :param app: Sphinx application context.\n    \"\"\"\n    app.add_role('ghissue', ghissue_role)\n    app.add_role('ghpull', ghissue_role)\n    app.add_role('ghuser', ghuser_role)\n    app.add_role('ghcommit', ghcommit_role)\n    app.add_config_value('github_project_url', None, 'env')\n\n    config = {'parallel_read_safe': True, 'parallel_write_safe': True}\n    return config\n",
      "llm_variables": [
        "app",
        "config"
      ]
    },
    {
      "file_path": "github.py",
      "code": "def ghuser_role(name, rawtext, text, lineno, inliner, options={}, content=[]):\n    \"\"\"\n    Link to a GitHub user.\n\n    Returns 2 part tuple containing list of nodes to insert into the\n    document and a list of system messages.  Both are allowed to be\n    empty.\n\n    :param name: The role name used in the document.\n    :param rawtext: The entire markup snippet, with role.\n    :param text: The text marked with the role.\n    :param lineno: The line number where rawtext appears in the input.\n    :param inliner: The inliner instance that called us.\n    :param options: Directive options for customization.\n    :param content: The directive content for customization.\n    \"\"\"\n    ref = 'https://www.github.com/' + text\n    node = nodes.reference(rawtext, text, refuri=ref, **options)\n    return [node], []\n",
      "variables": [
        "name",
        "rawtext",
        "text",
        "lineno",
        "inliner",
        "options",
        "content",
        "ref",
        "node"
      ],
      "anonymized_code": "def ghuser_role(var_1, var_2, var_3, var_4, var_5, var_6={}, var_7=[]):\n    \"\"\"\n    Link to a GitHub user.\n\n    Returns 2 part tuple containing list of nodes to insert into the\n    document and a list of system messages.  Both are allowed to be\n    empty.\n\n    :param var_1: The role var_1 used in the document.\n    :param var_2: The entire markup snippet, with role.\n    :param var_3: The var_3 marked with the role.\n    :param var_4: The line number where var_2 appears in the input.\n    :param var_5: The var_5 instance that called us.\n    :param var_6: Directive var_6 for customization.\n    :param var_7: The directive var_7 for customization.\n    \"\"\"\n    var_8 = 'https://www.github.com/' + var_3\n    var_9 = nodes.reference(var_2, var_3, refuri=var_8, **var_6)\n    return [var_9], []\n",
      "lines_processed": 19,
      "total_lines": 167,
      "llm_code": null,
      "llm_variables": null
    }
  ],
  "python-poetry_poetry": [
    {
      "file_path": "config.py",
      "code": "def boolean_normalizer(val: str) -> bool:\n    return val.lower() in [\"true\", \"1\"]\n",
      "variables": [
        "val"
      ],
      "anonymized_code": "def boolean_normalizer(var_1: str) -> bool:\n    return var_1.lower() in [\"true\", \"1\"]\n",
      "lines_processed": 2,
      "total_lines": 432,
      "llm_code": "def boolean_normalizer(string: str) -> bool:\n    return string.lower() in [\"true\", \"1\"]\n",
      "llm_variables": [
        "string"
      ]
    },
    {
      "file_path": "config_source.py",
      "code": "def drop_empty_config_category(\n    keys: list[str], config: dict[Any, Any]\n) -> dict[Any, Any]:\n    config_ = {}\n\n    for key, value in config.items():\n        if not keys or key != keys[0]:\n            config_[key] = value\n            continue\n        if keys and key == keys[0]:\n            if isinstance(value, dict):\n                value = drop_empty_config_category(keys[1:], value)\n\n            if value != {}:\n                config_[key] = value\n\n    return config_\n",
      "variables": [
        "keys",
        "config",
        "config_",
        "key",
        "value"
      ],
      "anonymized_code": "def drop_empty_config_category(\n    var_1: list[str], var_2: dict[Any, Any]\n) -> dict[Any, Any]:\n    var_3 = {}\n\n    for var_4, var_5 in var_2.items():\n        if not var_1 or var_4 != var_1[0]:\n            var_3[var_4] = var_5\n            continue\n        if var_1 and var_4 == var_1[0]:\n            if isinstance(var_5, dict):\n                var_5 = drop_empty_config_category(var_1[1:], var_5)\n\n            if var_5 != {}:\n                var_3[var_4] = var_5\n\n    return var_3\n",
      "lines_processed": 17,
      "total_lines": 99,
      "llm_code": "def drop_empty_config_category(\n    categories: list[str], config: dict[Any, Any]\n) -> dict[Any, Any]:\n    result = {}\n\n    for category, sub_config in config.items():\n        if not categories or category!= categories[0]:\n            result[category] = sub_config\n            continue\n        if categories and category == categories[0]:\n            if isinstance(sub_config, dict):\n                sub_config = drop_empty_config_category(categories[1:], sub_config)\n\n            if sub_config!= {}:\n                result[category] = sub_config\n\n    return result",
      "llm_variables": [
        "categories",
        "config",
        "result",
        "category",
        "sub_config"
      ]
    }
  ],
  "PaddlePaddle_PaddleOCR": [
    {
      "file_path": "make_shrink_map.py",
      "code": "def shrink_polygon_pyclipper(polygon, shrink_ratio):\n    from shapely.geometry import Polygon\n    import pyclipper\n\n    polygon_shape = Polygon(polygon)\n    distance = (\n        polygon_shape.area * (1 - np.power(shrink_ratio, 2)) / polygon_shape.length\n    )\n    subject = [tuple(l) for l in polygon]\n    padding = pyclipper.PyclipperOffset()\n    padding.AddPath(subject, pyclipper.JT_ROUND, pyclipper.ET_CLOSEDPOLYGON)\n    shrunk = padding.Execute(-distance)\n    if shrunk == []:\n        shrunk = np.array(shrunk)\n    else:\n        shrunk = np.array(shrunk[0]).reshape(-1, 2)\n    return shrunk\n",
      "variables": [
        "polygon",
        "shrink_ratio",
        "polygon_shape",
        "distance",
        "subject",
        "l",
        "padding",
        "shrunk"
      ],
      "anonymized_code": "def shrink_polygon_pyclipper(var_1, var_2):\n    from shapely.geometry import Polygon\n    import pyclipper\n\n    var_3 = Polygon(var_1)\n    var_4 = (\n        var_3.area * (1 - np.power(var_2, 2)) / var_3.length\n    )\n    var_5 = [tuple(var_6) for var_6 in var_1]\n    var_7 = pyclipper.PyclipperOffset()\n    var_7.AddPath(var_5, pyclipper.JT_ROUND, pyclipper.ET_CLOSEDPOLYGON)\n    var_8 = var_7.Execute(-var_4)\n    if var_8 == []:\n        var_8 = np.array(var_8)\n    else:\n        var_8 = np.array(var_8[0]).reshape(-1, 2)\n    return var_8\n",
      "lines_processed": 17,
      "total_lines": 129,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "make_shrink_map.py",
      "code": "def shrink_polygon_py(polygon, shrink_ratio):\n    \"\"\"\n    \u5bf9\u6846\u8fdb\u884c\u7f29\u653e\uff0c\u8fd4\u56de\u53bb\u7684\u6bd4\u4f8b\u4e3a1/shrink_ratio \u5373\u53ef\n    \"\"\"\n    cx = polygon[:, 0].mean()\n    cy = polygon[:, 1].mean()\n    polygon[:, 0] = cx + (polygon[:, 0] - cx) * shrink_ratio\n    polygon[:, 1] = cy + (polygon[:, 1] - cy) * shrink_ratio\n    return polygon\n",
      "variables": [
        "polygon",
        "shrink_ratio",
        "cx",
        "cy"
      ],
      "anonymized_code": "def shrink_polygon_py(var_1, var_2):\n    \"\"\"\n    \u5bf9\u6846\u8fdb\u884c\u7f29\u653e\uff0c\u8fd4\u56de\u53bb\u7684\u6bd4\u4f8b\u4e3a1/var_2 \u5373\u53ef\n    \"\"\"\n    var_3 = var_1[:, 0].mean()\n    var_4 = var_1[:, 1].mean()\n    var_1[:, 0] = var_3 + (var_1[:, 0] - var_3) * var_2\n    var_1[:, 1] = var_4 + (var_1[:, 1] - var_4) * var_2\n    return var_1\n",
      "lines_processed": 9,
      "total_lines": 129,
      "llm_code": "def shrink_polygon_py(polygon, scale):\n    \"\"\"\n    \u5bf9\u6846\u8fdb\u884c\u7f29\u653e\uff0c\u8fd4\u56de\u53bb\u7684\u6bd4\u4f8b\u4e3a1/scale \u5373\u53ef\n    \"\"\"\n    x_mean = polygon[:, 0].mean()\n    y_mean = polygon[:, 1].mean()\n    polygon[:, 0] = x_mean + (polygon[:, 0] - x_mean) * scale\n    polygon[:, 1] = y_mean + (polygon[:, 1] - y_mean) * scale\n    return polygon\n",
      "llm_variables": [
        "polygon",
        "scale",
        "x_mean",
        "y_mean"
      ]
    },
    {
      "file_path": "__init__.py",
      "code": "def build_loss(config):\n    copy_config = copy.deepcopy(config)\n    loss_type = copy_config.pop(\"type\")\n    assert loss_type in support_loss, f\"all support loss is {support_loss}\"\n    criterion = eval(loss_type)(**copy_config)\n    return criterion\n",
      "variables": [
        "config",
        "copy_config",
        "loss_type",
        "criterion"
      ],
      "anonymized_code": "def build_loss(var_1):\n    var_2 = copy.deepcopy(var_1)\n    var_3 = var_2.pop(\"type\")\n    assert var_3 in support_loss, f\"all support loss is {support_loss}\"\n    var_4 = eval(var_3)(**var_2)\n    return var_4\n",
      "lines_processed": 6,
      "total_lines": 16,
      "llm_code": "def build_loss(loss_config):\n    config = copy.deepcopy(loss_config)\n    loss_type = config.pop(\"type\")\n    assert loss_type in support_loss, f\"all support loss is {support_loss}\"\n    loss_instance = eval(loss_type)(**config)\n    return loss_instance\n",
      "llm_variables": [
        "loss_config",
        "config",
        "loss_type",
        "loss_instance"
      ]
    },
    {
      "file_path": "augment.py",
      "code": "def resize_image(img, short_size):\n    height, width, _ = img.shape\n    if height < width:\n        new_height = short_size\n        new_width = new_height / height * width\n    else:\n        new_width = short_size\n        new_height = new_width / width * height\n    new_height = int(round(new_height / 32) * 32)\n    new_width = int(round(new_width / 32) * 32)\n    resized_img = cv2.resize(img, (new_width, new_height))\n    return resized_img, (new_width / width, new_height / height)\n",
      "variables": [
        "img",
        "short_size",
        "height",
        "width",
        "_",
        "new_height",
        "new_width",
        "resized_img"
      ],
      "anonymized_code": "def resize_image(var_1, var_2):\n    var_3, var_4, var_5 = var_1.shape\n    if var_3 < var_4:\n        var_6 = var_2\n        var_7 = var_6 / var_3 * var_4\n    else:\n        var_7 = var_2\n        var_6 = var_7 / var_4 * var_3\n    var_6 = int(round(var_6 / 32) * 32)\n    var_7 = int(round(var_7 / 32) * 32)\n    var_8 = cv2.resize(var_1, (var_7, var_6))\n    return var_8, (var_7 / var_4, var_6 / var_3)\n",
      "lines_processed": 12,
      "total_lines": 308,
      "llm_code": "def resize_image(width, height):\n    original_height, original_width, channels = width.shape\n    if original_height < original_width:\n        new_height = height\n        new_width = new_height / original_height * original_width\n    else:\n        new_width = height\n        new_height = new_width / original_width * original_width\n    new_width = int(round(new_width / 32) * 32)\n    new_height = int(round(new_height / 32) * 32)\n    resized_image = cv2.resize(width, (new_height, new_width))\n    return resized_image, (new_height / original_width, new_width / original_height)",
      "llm_variables": [
        "width",
        "height",
        "original_height",
        "original_width",
        "channels",
        "new_height",
        "new_width",
        "resized_image"
      ]
    },
    {
      "file_path": "resnet.py",
      "code": "def load_models(model, model_name):\n    import torch.utils.model_zoo as model_zoo\n\n    torch_patams = model_zoo.load_url(model_urls[model_name])\n    load_torch_params(model, torch_patams)\n",
      "variables": [
        "model",
        "model_name",
        "torch_patams"
      ],
      "anonymized_code": "def load_models(var_1, var_2):\n    import torch.utils.model_zoo as model_zoo\n\n    var_3 = model_zoo.load_url(model_urls[var_2])\n    load_torch_params(var_1, var_3)\n",
      "lines_processed": 5,
      "total_lines": 366,
      "llm_code": "def load_models(model1, model2):\n    import torch.utils.model_zoo as model_zoo\n\n    model3 = model_zoo.load_url(model_urls[model2])\n    load_torch_params(model1, model3)",
      "llm_variables": [
        "model1",
        "model2",
        "model3"
      ]
    },
    {
      "file_path": "resnet.py",
      "code": "def resnet152(pretrained=True, **kwargs):\n    \"\"\"Constructs a ResNet-152 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        assert (\n            kwargs.get(\"in_channels\", 3) == 3\n        ), \"in_channels must be 3 when pretrained is True\"\n        model.load_state_dict(model_zoo.load_url(model_urls[\"resnet152\"]), strict=False)\n    return model\n",
      "variables": [
        "pretrained",
        "kwargs",
        "model"
      ],
      "anonymized_code": "def resnet152(var_1=True, **var_2):\n    \"\"\"Constructs a ResNet-152 var_3.\n    Args:\n        var_1 (bool): If True, returns a var_3 pre-trained on ImageNet\n    \"\"\"\n    var_3 = ResNet(Bottleneck, [3, 8, 36, 3], **var_2)\n    if var_1:\n        assert (\n            var_2.get(\"in_channels\", 3) == 3\n        ), \"in_channels must be 3 when var_1 is True\"\n        var_3.load_state_dict(model_zoo.load_url(model_urls[\"resnet152\"]), strict=False)\n    return var_3\n",
      "lines_processed": 12,
      "total_lines": 366,
      "llm_code": "def resnet152(pretrained=True, **kwargs):\n    \"\"\"Constructs a ResNet-152 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        assert (\n            kwargs.get(\"in_channels\", 3) == 3\n        ), \"in_channels must be 3 when pretrained is True\"\n        model.load_state_dict(model_zoo.load_url(model_urls[\"resnet152\"]), strict=False)\n    return model\n",
      "llm_variables": [
        "pretrained",
        "kwargs",
        "model"
      ]
    },
    {
      "file_path": "resnet.py",
      "code": "def resnet101(pretrained=True, **kwargs):\n    \"\"\"Constructs a ResNet-101 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        assert (\n            kwargs.get(\"in_channels\", 3) == 3\n        ), \"in_channels must be 3 when pretrained is True\"\n        model.load_state_dict(model_zoo.load_url(model_urls[\"resnet101\"]), strict=False)\n    return model\n",
      "variables": [
        "pretrained",
        "kwargs",
        "model"
      ],
      "anonymized_code": "def resnet101(var_1=True, **var_2):\n    \"\"\"Constructs a ResNet-101 var_3.\n    Args:\n        var_1 (bool): If True, returns a var_3 pre-trained on ImageNet\n    \"\"\"\n    var_3 = ResNet(Bottleneck, [3, 4, 23, 3], **var_2)\n    if var_1:\n        assert (\n            var_2.get(\"in_channels\", 3) == 3\n        ), \"in_channels must be 3 when var_1 is True\"\n        var_3.load_state_dict(model_zoo.load_url(model_urls[\"resnet101\"]), strict=False)\n    return var_3\n",
      "lines_processed": 12,
      "total_lines": 366,
      "llm_code": "def resnet101(pretrained=True, **kwargs):\n    \"\"\"Constructs a ResNet-101 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        assert (\n            kwargs.get(\"in_channels\", 3) == 3\n        ), \"in_channels must be 3 when pretrained is True\"\n        model.load_state_dict(model_zoo.load_url(model_urls[\"resnet101\"]), strict=False)\n    return model\n",
      "llm_variables": [
        "pretrained",
        "kwargs",
        "model"
      ]
    }
  ],
  "Stability-AI_stablediffusion": [
    {
      "file_path": "sampling_util.py",
      "code": "def norm_thresholding(x0, value):\n    s = append_dims(x0.pow(2).flatten(1).mean(1).sqrt().clamp(min=value), x0.ndim)\n    return x0 * (value / s)\n",
      "variables": [
        "x0",
        "value",
        "s"
      ],
      "anonymized_code": "def norm_thresholding(var_1, var_2):\n    var_3 = append_dims(var_1.pow(2).flatten(1).mean(1).sqrt().clamp(min=var_2), var_1.ndim)\n    return var_1 * (var_2 / var_3)\n",
      "lines_processed": 3,
      "total_lines": 22,
      "llm_code": "def norm_thresholding(threshold, input_tensor):\n    normalized = append_dims(input_tensor.pow(2).flatten(1).mean(1).sqrt().clamp(min=threshold), input_tensor.ndim)\n    return input_tensor * (threshold / normalized)",
      "llm_variables": [
        "threshold",
        "input_tensor",
        "normalized"
      ]
    },
    {
      "file_path": "sampling_util.py",
      "code": "def spatial_norm_thresholding(x0, value):\n    # b c h w\n    s = x0.pow(2).mean(1, keepdim=True).sqrt().clamp(min=value)\n    return x0 * (value / s)",
      "variables": [
        "x0",
        "value",
        "s"
      ],
      "anonymized_code": "def spatial_norm_thresholding(var_1, var_2):\n    # b c h w\n    var_3 = var_1.pow(2).mean(1, keepdim=True).sqrt().clamp(min=var_2)\n    return var_1 * (var_2 / var_3)",
      "lines_processed": 4,
      "total_lines": 22,
      "llm_code": "def spatial_norm_thresholding(x, eps):\n    # b c h w\n    std = x.pow(2).mean(1, keepdim=True).sqrt().clamp(min=eps)\n    return x * (eps / std)",
      "llm_variables": [
        "x",
        "eps",
        "std"
      ]
    },
    {
      "file_path": "sampling_util.py",
      "code": "def append_dims(x, target_dims):\n    \"\"\"Appends dimensions to the end of a tensor until it has target_dims dimensions.\n    From https://github.com/crowsonkb/k-diffusion/blob/master/k_diffusion/utils.py\"\"\"\n    dims_to_append = target_dims - x.ndim\n    if dims_to_append < 0:\n        raise ValueError(f'input has {x.ndim} dims but target_dims is {target_dims}, which is less')\n    return x[(...,) + (None,) * dims_to_append]\n",
      "variables": [
        "x",
        "target_dims",
        "dims_to_append"
      ],
      "anonymized_code": "def append_dims(var_1, var_2):\n    \"\"\"Appends dimensions to the end of a tensor until it has var_2 dimensions.\n    From https://github.com/crowsonkb/k-diffusion/blob/master/k_diffusion/utils.py\"\"\"\n    var_3 = var_2 - var_1.ndim\n    if var_3 < 0:\n        raise ValueError(f'input has {var_1.ndim} dims but var_2 is {var_2}, which is less')\n    return var_1[(...,) + (None,) * var_3]\n",
      "lines_processed": 7,
      "total_lines": 22,
      "llm_code": "def append_dims(a, b):\n    \"\"\"Appends dimensions to the end of a tensor until it has b dimensions.\n    From https://github.com/crowsonkb/k-diffusion/blob/master/k_diffusion/utils.py\"\"\"\n    result = b - a.ndim\n    if result < 0:\n        raise ValueError(f'input has {a.ndim} dims but b is {b}, which is less')\n    return a[(...,) + (None,) * result]\n",
      "llm_variables": [
        "a",
        "b",
        "result"
      ]
    },
    {
      "file_path": "attention.py",
      "code": "def exists(val):\n    return val is not None\n",
      "variables": [
        "val"
      ],
      "anonymized_code": "def exists(var_1):\n    return var_1 is not None\n",
      "lines_processed": 2,
      "total_lines": 341,
      "llm_code": "def exists(value):\n    return value is not None",
      "llm_variables": [
        "value"
      ]
    },
    {
      "file_path": "attention.py",
      "code": "def zero_module(module):\n    \"\"\"\n    Zero out the parameters of a module and return it.\n    \"\"\"\n    for p in module.parameters():\n        p.detach().zero_()\n    return module\n",
      "variables": [
        "module",
        "p"
      ],
      "anonymized_code": "def zero_module(var_1):\n    \"\"\"\n    Zero out the parameters of a var_1 and return it.\n    \"\"\"\n    for var_2 in var_1.parameters():\n        var_2.detach().zero_()\n    return var_1\n",
      "lines_processed": 7,
      "total_lines": 341,
      "llm_code": "def zero_module(model):\n    \"\"\"\n    Zero out the parameters of a model and return it.\n    \"\"\"\n    for param in model.parameters():\n        param.detach().zero_()\n    return model",
      "llm_variables": [
        "model",
        "param"
      ]
    },
    {
      "file_path": "util.py",
      "code": "def noise_like(shape, device, repeat=False):\n    repeat_noise = lambda: torch.randn((1, *shape[1:]), device=device).repeat(shape[0], *((1,) * (len(shape) - 1)))\n    noise = lambda: torch.randn(shape, device=device)\n    return repeat_noise() if repeat else noise()\n",
      "variables": [
        "shape",
        "device",
        "repeat",
        "repeat_noise",
        "noise"
      ],
      "anonymized_code": "def noise_like(var_1, var_2, var_3=False):\n    var_4 = lambda: torch.randn((1, *var_1[1:]), var_2=var_2).var_3(var_1[0], *((1,) * (len(var_1) - 1)))\n    var_5 = lambda: torch.randn(var_1, var_2=var_2)\n    return var_4() if var_3 else var_5()\n",
      "lines_processed": 4,
      "total_lines": 278,
      "llm_code": "def noise_like(input_shape, num_channels, use_var_3=False):\n    var_4 = lambda: torch.randn((1, *input_shape[1:]), var_2=num_channels).var_3(input_shape[0], *((1,) * (len(input_shape) - 1)))\n    var_5 = lambda: torch.randn(input_shape, var_2=num_channels)\n    return var_4() if use_var_3 else var_5()",
      "llm_variables": [
        "input_shape",
        "num_channels",
        "use_var_3",
        "var_4",
        "var_5"
      ]
    },
    {
      "file_path": "util.py",
      "code": "def checkpoint(func, inputs, params, flag):\n    \"\"\"\n    Evaluate a function without caching intermediate activations, allowing for\n    reduced memory at the expense of extra compute in the backward pass.\n    :param func: the function to evaluate.\n    :param inputs: the argument sequence to pass to `func`.\n    :param params: a sequence of parameters `func` depends on but does not\n                   explicitly take as arguments.\n    :param flag: if False, disable gradient checkpointing.\n    \"\"\"\n    if flag:\n        args = tuple(inputs) + tuple(params)\n        return CheckpointFunction.apply(func, len(inputs), *args)\n    else:\n        return func(*inputs)\n",
      "variables": [
        "func",
        "inputs",
        "params",
        "flag",
        "args"
      ],
      "anonymized_code": "def checkpoint(var_1, var_2, var_3, var_4):\n    \"\"\"\n    Evaluate a function without caching intermediate activations, allowing for\n    reduced memory at the expense of extra compute in the backward pass.\n    :param var_1: the function to evaluate.\n    :param var_2: the argument sequence to pass to `var_1`.\n    :param var_3: a sequence of parameters `var_1` depends on but does not\n                   explicitly take as arguments.\n    :param var_4: if False, disable gradient checkpointing.\n    \"\"\"\n    if var_4:\n        var_5 = tuple(var_2) + tuple(var_3)\n        return CheckpointFunction.apply(var_1, len(var_2), *var_5)\n    else:\n        return var_1(*var_2)\n",
      "lines_processed": 15,
      "total_lines": 278,
      "llm_code": "def checkpoint(func, args, params, use_cache):\n    \"\"\"\n    Evaluate a function without caching intermediate activations, allowing for\n    reduced memory at the expense of extra compute in the backward pass.\n    :param func: the function to evaluate.\n    :param args: the argument sequence to pass to `func`.\n    :param params: a sequence of parameters `func` depends on but does not\n                   explicitly take as arguments.\n    :param use_cache: if False, disable gradient checkpointing.\n    \"\"\"\n    if use_cache:\n        cached_args = tuple(args) + tuple(params)\n        return CheckpointFunction.apply(func, len(args), *cached_args)\n    else:\n        return func(*args)\n",
      "llm_variables": [
        "func",
        "args",
        "params",
        "use_cache",
        "cached_args"
      ]
    },
    {
      "file_path": "util.py",
      "code": "def zero_module(module):\n    \"\"\"\n    Zero out the parameters of a module and return it.\n    \"\"\"\n    for p in module.parameters():\n        p.detach().zero_()\n    return module\n",
      "variables": [
        "module",
        "p"
      ],
      "anonymized_code": "def zero_module(var_1):\n    \"\"\"\n    Zero out the parameters of a var_1 and return it.\n    \"\"\"\n    for var_2 in var_1.parameters():\n        var_2.detach().zero_()\n    return var_1\n",
      "lines_processed": 7,
      "total_lines": 278,
      "llm_code": "def zero_module(module):\n    \"\"\"\n    Zero out the parameters of a module and return it.\n    \"\"\"\n    for param in module.parameters():\n        param.detach().zero_()\n    return module",
      "llm_variables": [
        "module",
        "param"
      ]
    },
    {
      "file_path": "util.py",
      "code": "def normalization(channels):\n    \"\"\"\n    Make a standard normalization layer.\n    :param channels: number of input channels.\n    :return: an nn.Module for normalization.\n    \"\"\"\n    return GroupNorm32(32, channels)\n",
      "variables": [
        "channels"
      ],
      "anonymized_code": "def normalization(var_1):\n    \"\"\"\n    Make a standard normalization layer.\n    :param var_1: number of input var_1.\n    :return: an nn.Module for normalization.\n    \"\"\"\n    return GroupNorm32(32, var_1)\n",
      "lines_processed": 7,
      "total_lines": 278,
      "llm_code": "def normalization(num_features):\n    \"\"\"\n    Make a standard normalization layer.\n    :param num_features: number of input num_features.\n    :return: an nn.Module for normalization.\n    \"\"\"\n    return GroupNorm32(32, num_features)\n",
      "llm_variables": [
        "num_features"
      ]
    },
    {
      "file_path": "util.py",
      "code": "def avg_pool_nd(dims, *args, **kwargs):\n    \"\"\"\n    Create a 1D, 2D, or 3D average pooling module.\n    \"\"\"\n    if dims == 1:\n        return nn.AvgPool1d(*args, **kwargs)\n    elif dims == 2:\n        return nn.AvgPool2d(*args, **kwargs)\n    elif dims == 3:\n        return nn.AvgPool3d(*args, **kwargs)\n    raise ValueError(f\"unsupported dimensions: {dims}\")\n",
      "variables": [
        "dims",
        "args",
        "kwargs"
      ],
      "anonymized_code": "def avg_pool_nd(var_1, *var_2, **var_3):\n    \"\"\"\n    Create a 1D, 2D, or 3D average pooling module.\n    \"\"\"\n    if var_1 == 1:\n        return nn.AvgPool1d(*var_2, **var_3)\n    elif var_1 == 2:\n        return nn.AvgPool2d(*var_2, **var_3)\n    elif var_1 == 3:\n        return nn.AvgPool3d(*var_2, **var_3)\n    raise ValueError(f\"unsupported dimensions: {var_1}\")\n",
      "lines_processed": 11,
      "total_lines": 278,
      "llm_code": "def avg_pool_nd(dim, *kernel_size, **kwargs):\n    \"\"\"\n    Create a 1D, 2D, or 3D average pooling module.\n    \"\"\"\n    if dim == 1:\n        return nn.AvgPool1d(*kernel_size, **kwargs)\n    elif dim == 2:\n        return nn.AvgPool2d(*kernel_size, **kwargs)\n    elif dim == 3:\n        return nn.AvgPool3d(*kernel_size, **kwargs)\n    raise ValueError(f\"unsupported dimensions: {dim}\")",
      "llm_variables": [
        "dim",
        "kernel_size",
        "kwargs"
      ]
    },
    {
      "file_path": "model.py",
      "code": "def get_timestep_embedding(timesteps, embedding_dim):\n    \"\"\"\n    This matches the implementation in Denoising Diffusion Probabilistic Models:\n    From Fairseq.\n    Build sinusoidal embeddings.\n    This matches the implementation in tensor2tensor, but differs slightly\n    from the description in Section 3.5 of \"Attention Is All You Need\".\n    \"\"\"\n    assert len(timesteps.shape) == 1\n\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = emb.to(device=timesteps.device)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:  # zero pad\n        emb = torch.nn.functional.pad(emb, (0,1,0,0))\n    return emb\n",
      "variables": [
        "timesteps",
        "embedding_dim",
        "half_dim",
        "emb"
      ],
      "anonymized_code": "def get_timestep_embedding(var_1, var_2):\n    \"\"\"\n    This matches the implementation in Denoising Diffusion Probabilistic Models:\n    From Fairseq.\n    Build sinusoidal embeddings.\n    This matches the implementation in tensor2tensor, but differs slightly\n    from the description in Section 3.5 of \"Attention Is All You Need\".\n    \"\"\"\n    assert len(var_1.shape) == 1\n\n    var_3 = var_2 // 2\n    var_4 = math.log(10000) / (var_3 - 1)\n    var_4 = torch.exp(torch.arange(var_3, dtype=torch.float32) * -var_4)\n    var_4 = var_4.to(device=var_1.device)\n    var_4 = var_1.float()[:, None] * var_4[None, :]\n    var_4 = torch.cat([torch.sin(var_4), torch.cos(var_4)], dim=1)\n    if var_2 % 2 == 1:  # zero pad\n        var_4 = torch.nn.functional.pad(var_4, (0,1,0,0))\n    return var_4\n",
      "lines_processed": 19,
      "total_lines": 852,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "openaimodel.py",
      "code": "def convert_module_to_f32(x):\n    pass\n",
      "variables": [
        "x"
      ],
      "anonymized_code": "def convert_module_to_f32(var_1):\n    pass\n",
      "lines_processed": 2,
      "total_lines": 807,
      "llm_code": "def convert_module_to_f32(module):\n    pass\n",
      "llm_variables": [
        "module"
      ]
    }
  ],
  "langflow-ai_langflow": [
    {
      "file_path": "1b8b740a6fa3_remove_fk_constraint_in_message_.py",
      "code": "def downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    conn = op.get_bind()\n\n    # Recreate tables with constraints\n    # 1. Handle vertex_build table\n    if migration.table_exists(\"vertex_build\", conn):\n        # Create a temporary table with the constraint\n        temp_table_name = \"temp_vertex_build\"\n        pk_name = \"pk_vertex_build\"\n        fk_name = \"fk_vertex_build_flow_id_flow\"\n\n        # Check if constraints already exist\n        if constraint_exists(pk_name, conn):\n            pk_name = f\"pk_temp_vertex_build\"\n\n        if constraint_exists(fk_name, conn):\n            fk_name = f\"fk_vertex_build_flow_id_flow_{revision[:8]}\"\n\n",
      "variables": [
        "conn",
        "temp_table_name",
        "pk_name",
        "fk_name"
      ],
      "anonymized_code": "def downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    var_1 = op.get_bind()\n\n    # Recreate tables with constraints\n    # 1. Handle vertex_build table\n    if migration.table_exists(\"vertex_build\", var_1):\n        # Create a temporary table with the constraint\n        var_2 = \"temp_vertex_build\"\n        var_3 = \"pk_vertex_build\"\n        var_4 = \"fk_vertex_build_flow_id_flow\"\n\n        # Check if constraints already exist\n        if constraint_exists(var_3, var_1):\n            var_3 = f\"pk_temp_vertex_build\"\n\n        if constraint_exists(var_4, var_1):\n            var_4 = f\"fk_vertex_build_flow_id_flow_{revision[:8]}\"\n\n",
      "lines_processed": 19,
      "total_lines": 337,
      "llm_code": "def downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    db = op.get_bind()\n\n    # Recreate tables with constraints\n    # 1. Handle vertex_build table\n    if migration.table_exists(\"vertex_build\", db):\n        # Create a temporary table with the constraint\n        temp_table_name = \"temp_vertex_build\"\n        primary_key_constraint = \"pk_vertex_build\"\n        foreign_key_constraint = \"fk_vertex_build_flow_id_flow\"\n\n        # Check if constraints already exist\n        if constraint_exists(primary_key_constraint, db):\n            primary_key_constraint = f\"pk_temp_vertex_build\"\n\n        if constraint_exists(foreign_key_constraint, db):\n            foreign_key_constraint = f\"fk_vertex_build_flow_id_flow_{revision[:8]}\"\n\n",
      "llm_variables": [
        "db",
        "temp_table_name",
        "primary_key_constraint",
        "foreign_key_constraint"
      ]
    },
    {
      "file_path": "update_pyproject_name.py",
      "code": "def main() -> None:\n    if len(sys.argv) != ARGUMENT_NUMBER:\n        msg = \"Must specify project name and build type, e.g. langflow-nightly base\"\n        raise ValueError(msg)\n    new_project_name = sys.argv[1]\n    build_type = sys.argv[2]\n\n    if build_type == \"base\":\n        update_pyproject_name(\"src/backend/base/pyproject.toml\", new_project_name)\n        update_uv_dep(\"pyproject.toml\", new_project_name)\n    elif build_type == \"main\":\n        update_pyproject_name(\"pyproject.toml\", new_project_name)\n        update_uv_dep(\"pyproject.toml\", new_project_name)\n    else:\n        msg = f\"Invalid build type: {build_type}\"\n        raise ValueError(msg)\n",
      "variables": [
        "msg",
        "new_project_name",
        "build_type"
      ],
      "anonymized_code": "def main() -> None:\n    if len(sys.argv) != ARGUMENT_NUMBER:\n        var_1 = \"Must specify project name and build type, e.g. langflow-nightly base\"\n        raise ValueError(var_1)\n    var_2 = sys.argv[1]\n    var_3 = sys.argv[2]\n\n    if var_3 == \"base\":\n        update_pyproject_name(\"src/backend/base/pyproject.toml\", var_2)\n        update_uv_dep(\"pyproject.toml\", var_2)\n    elif var_3 == \"main\":\n        update_pyproject_name(\"pyproject.toml\", var_2)\n        update_uv_dep(\"pyproject.toml\", var_2)\n    else:\n        var_1 = f\"Invalid build type: {var_3}\"\n        raise ValueError(var_1)\n",
      "lines_processed": 16,
      "total_lines": 69,
      "llm_code": "def main() -> None:\n    if len(sys.argv) != ARGUMENT_NUMBER:\n        message = \"Must specify project name and build type, e.g. langflow-nightly base\"\n        raise ValueError(message)\n    project_name = sys.argv[1]\n    build_type = sys.argv[2]\n\n    if build_type == \"base\":\n        update_pyproject_name(\"src/backend/base/pyproject.toml\", project_name)\n        update_uv_dep(\"pyproject.toml\", project_name)\n    elif build_type == \"main\":\n        update_pyproject_name(\"pyproject.toml\", project_name)\n        update_uv_dep(\"pyproject.toml\", project_name)\n    else:\n        message = f\"Invalid build type: {build_type}\"\n        raise ValueError(message)\n",
      "llm_variables": [
        "message",
        "project_name",
        "build_type"
      ]
    },
    {
      "file_path": "update_pyproject_name.py",
      "code": "def update_pyproject_name(pyproject_path: str, new_project_name: str) -> None:\n    \"\"\"Update the project name in pyproject.toml.\"\"\"\n    filepath = BASE_DIR / pyproject_path\n    content = filepath.read_text(encoding=\"utf-8\")\n\n    # Regex to match the version line under [tool.poetry]\n    pattern = re.compile(r'(?<=^name = \")[^\"]+(?=\")', re.MULTILINE)\n\n    if not pattern.search(content):\n        msg = f'Project name not found in \"{filepath}\"'\n        raise ValueError(msg)\n    content = pattern.sub(new_project_name, content)\n\n    filepath.write_text(content, encoding=\"utf-8\")\n",
      "variables": [
        "pyproject_path",
        "new_project_name",
        "filepath",
        "content",
        "pattern",
        "msg"
      ],
      "anonymized_code": "def update_pyproject_name(var_1: str, var_2: str) -> None:\n    \"\"\"Update the project name in pyproject.toml.\"\"\"\n    var_3 = BASE_DIR / var_1\n    var_4 = var_3.read_text(encoding=\"utf-8\")\n\n    # Regex to match the version line under [tool.poetry]\n    var_5 = re.compile(r'(?<=^name = \")[^\"]+(?=\")', re.MULTILINE)\n\n    if not var_5.search(var_4):\n        var_6 = f'Project name not found in \"{var_3}\"'\n        raise ValueError(var_6)\n    var_4 = var_5.sub(var_2, var_4)\n\n    var_3.write_text(var_4, encoding=\"utf-8\")\n",
      "lines_processed": 14,
      "total_lines": 69,
      "llm_code": "def update_pyproject_name(project_name: str, config_file: str) -> None:\n    \"\"\"Update the project name in pyproject.toml.\"\"\"\n    project_path = BASE_DIR / project_name\n    project_content = project_path.read_text(encoding=\"utf-8\")\n\n    # Regex to match the version line under [tool.poetry]\n    name_pattern = re.compile(r'(?<=^name = \")[^\"]+(?=\")', re.MULTILINE)\n\n    if not name_pattern.search(project_content):\n        error_message = f'Project name not found in \"{project_path}\"'\n        raise ValueError(error_message)\n    project_content = name_pattern.sub(config_file, project_content)\n\n    project_path.write_text(project_content, encoding=\"utf-8\")\n",
      "llm_variables": [
        "project_name",
        "config_file",
        "project_path",
        "project_content",
        "name_pattern",
        "error_message"
      ]
    },
    {
      "file_path": "update_pyproject_name.py",
      "code": "def update_uv_dep(pyproject_path: str, new_project_name: str) -> None:\n    \"\"\"Update the langflow-base dependency in pyproject.toml.\"\"\"\n    filepath = BASE_DIR / pyproject_path\n    content = filepath.read_text(encoding=\"utf-8\")\n\n    if new_project_name == \"langflow-nightly\":\n        pattern = re.compile(r\"langflow = \\{ workspace = true \\}\")\n        replacement = \"langflow-nightly = { workspace = true }\"\n    elif new_project_name == \"langflow-base-nightly\":\n        pattern = re.compile(r\"langflow-base = \\{ workspace = true \\}\")\n        replacement = \"langflow-base-nightly = { workspace = true }\"\n    else:\n        msg = f\"Invalid project name: {new_project_name}\"\n        raise ValueError(msg)\n\n    # Updates the dependency name for uv\n    if not pattern.search(content):\n        msg = f\"{replacement} uv dependency not found in {filepath}\"\n        raise ValueError(msg)\n",
      "variables": [
        "pyproject_path",
        "new_project_name",
        "filepath",
        "content",
        "pattern",
        "replacement",
        "msg"
      ],
      "anonymized_code": "def update_uv_dep(var_1: str, var_2: str) -> None:\n    \"\"\"Update the langflow-base dependency in pyproject.toml.\"\"\"\n    var_3 = BASE_DIR / var_1\n    var_4 = var_3.read_text(encoding=\"utf-8\")\n\n    if var_2 == \"langflow-nightly\":\n        var_5 = re.compile(r\"langflow = \\{ workspace = true \\}\")\n        var_6 = \"langflow-nightly = { workspace = true }\"\n    elif var_2 == \"langflow-base-nightly\":\n        var_5 = re.compile(r\"langflow-base = \\{ workspace = true \\}\")\n        var_6 = \"langflow-base-nightly = { workspace = true }\"\n    else:\n        var_7 = f\"Invalid project name: {var_2}\"\n        raise ValueError(var_7)\n\n    # Updates the dependency name for uv\n    if not var_5.search(var_4):\n        var_7 = f\"{var_6} uv dependency not found in {var_3}\"\n        raise ValueError(var_7)\n",
      "lines_processed": 19,
      "total_lines": 69,
      "llm_code": "def update_uv_dep(package: str, target: str) -> None:\n    \"\"\"Update the langflow-base dependency in pyproject.toml.\"\"\"\n    path = BASE_DIR / package\n    content = path.read_text(encoding=\"utf-8\")\n\n    if target == \"langflow-nightly\":\n        pattern = re.compile(r\"langflow = \\{ workspace = true \\}\")\n        replacement = \"langflow-nightly = { workspace = true }\"\n    elif target == \"langflow-base-nightly\":\n        pattern = re.compile(r\"langflow-base = \\{ workspace = true \\}\")\n        replacement = \"langflow-base-nightly = { workspace = true }\"\n    else:\n        error_msg = f\"Invalid project name: {target}\"\n        raise ValueError(error_msg)\n\n    # Updates the dependency name for uv\n    if not pattern.search(content):\n        error_msg = f\"{replacement} uv dependency not found in {path}\"\n        raise ValueError(error_msg)\n\n",
      "llm_variables": [
        "package",
        "target",
        "path",
        "content",
        "pattern",
        "replacement",
        "error_msg"
      ]
    },
    {
      "file_path": "env.py",
      "code": "def _sqlite_do_connect(\n    dbapi_connection,\n    connection_record,  # noqa: ARG001\n):\n    # disable pysqlite's emitting of the BEGIN statement entirely.\n    # also stops it from emitting COMMIT before any DDL.\n    dbapi_connection.isolation_level = None\n",
      "variables": [
        "dbapi_connection",
        "connection_record"
      ],
      "anonymized_code": "def _sqlite_do_connect(\n    var_1,\n    var_2,  # noqa: ARG001\n):\n    # disable pysqlite's emitting of the BEGIN statement entirely.\n    # also stops it from emitting COMMIT before any DDL.\n    var_1.isolation_level = None\n",
      "lines_processed": 7,
      "total_lines": 124,
      "llm_code": "def _sqlite_do_connect(\n    connection,\n    unused,  # noqa: ARG001\n):\n    # disable pysqlite's emitting of the BEGIN statement entirely.\n    # also stops it from emitting COMMIT before any DDL.\n    connection.isolation_level = None\n",
      "llm_variables": [
        "connection",
        "unused"
      ]
    },
    {
      "file_path": "env.py",
      "code": "def _do_run_migrations(connection):\n    context.configure(\n        connection=connection, target_metadata=target_metadata, render_as_batch=True, prepare_threshold=None\n    )\n\n    with context.begin_transaction():\n        if connection.dialect.name == \"postgresql\":\n            connection.execute(text(\"SET LOCAL lock_timeout = '60s';\"))\n            connection.execute(text(\"SELECT pg_advisory_xact_lock(112233);\"))\n        context.run_migrations()\n",
      "variables": [
        "connection"
      ],
      "anonymized_code": "def _do_run_migrations(var_1):\n    context.configure(\n        var_1=var_1, target_metadata=target_metadata, render_as_batch=True, prepare_threshold=None\n    )\n\n    with context.begin_transaction():\n        if var_1.dialect.name == \"postgresql\":\n            var_1.execute(text(\"SET LOCAL lock_timeout = '60s';\"))\n            var_1.execute(text(\"SELECT pg_advisory_xact_lock(112233);\"))\n        context.run_migrations()\n",
      "lines_processed": 10,
      "total_lines": 124,
      "llm_code": "def _do_run_migrations(connection):\n    context.configure(\n        connection=connection, target_metadata=target_metadata, render_as_batch=True, prepare_threshold=None\n    )\n\n    with context.begin_transaction():\n        if connection.dialect.name == \"postgresql\":\n            connection.execute(text(\"SET LOCAL lock_timeout = '60s';\"))\n            connection.execute(text(\"SELECT pg_advisory_xact_lock(112233);\"))\n        context.run_migrations()\n",
      "llm_variables": [
        "connection"
      ]
    },
    {
      "file_path": "env.py",
      "code": "def _sqlite_do_begin(conn):\n    # emit our own BEGIN\n    conn.exec_driver_sql(\"PRAGMA busy_timeout = 60000\")\n    conn.exec_driver_sql(\"BEGIN EXCLUSIVE\")\n",
      "variables": [
        "conn"
      ],
      "anonymized_code": "def _sqlite_do_begin(var_1):\n    # emit our own BEGIN\n    var_1.exec_driver_sql(\"PRAGMA busy_timeout = 60000\")\n    var_1.exec_driver_sql(\"BEGIN EXCLUSIVE\")\n",
      "lines_processed": 4,
      "total_lines": 124,
      "llm_code": "def _sqlite_do_begin(conn):\n    # emit our own BEGIN\n    conn.exec_driver_sql(\"PRAGMA busy_timeout = 60000\")\n    conn.exec_driver_sql(\"BEGIN EXCLUSIVE\")",
      "llm_variables": [
        "conn"
      ]
    },
    {
      "file_path": "006b3990db50_add_unique_constraints.py",
      "code": "def upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    conn = op.get_bind()\n    inspector = sa.inspect(conn)  # type: ignore\n    api_key_constraints = inspector.get_unique_constraints(\"apikey\")\n    flow_constraints = inspector.get_unique_constraints(\"flow\")\n    user_constraints = inspector.get_unique_constraints(\"user\")\n    try:\n        if not any(constraint[\"column_names\"] == [\"id\"] for constraint in api_key_constraints):\n            with op.batch_alter_table(\"apikey\", schema=None) as batch_op:\n                batch_op.create_unique_constraint(\"uq_apikey_id\", [\"id\"])\n        if not any(constraint[\"column_names\"] == [\"id\"] for constraint in flow_constraints):\n            with op.batch_alter_table(\"flow\", schema=None) as batch_op:\n                batch_op.create_unique_constraint(\"uq_flow_id\", [\"id\"])\n        if not any(constraint[\"column_names\"] == [\"id\"] for constraint in user_constraints):\n            with op.batch_alter_table(\"user\", schema=None) as batch_op:\n                batch_op.create_unique_constraint(\"uq_user_id\", [\"id\"])\n    except Exception as e:\n        print(e)\n",
      "variables": [
        "conn",
        "inspector",
        "api_key_constraints",
        "flow_constraints",
        "user_constraints",
        "constraint",
        "batch_op"
      ],
      "anonymized_code": "def upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    var_1 = op.get_bind()\n    var_2 = sa.inspect(var_1)  # type: ignore\n    var_3 = var_2.get_unique_constraints(\"apikey\")\n    var_4 = var_2.get_unique_constraints(\"flow\")\n    var_5 = var_2.get_unique_constraints(\"user\")\n    try:\n        if not any(var_6[\"column_names\"] == [\"id\"] for var_6 in var_3):\n            with op.batch_alter_table(\"apikey\", schema=None) as var_7:\n                var_7.create_unique_constraint(\"uq_apikey_id\", [\"id\"])\n        if not any(var_6[\"column_names\"] == [\"id\"] for var_6 in var_4):\n            with op.batch_alter_table(\"flow\", schema=None) as var_7:\n                var_7.create_unique_constraint(\"uq_flow_id\", [\"id\"])\n        if not any(var_6[\"column_names\"] == [\"id\"] for var_6 in var_5):\n            with op.batch_alter_table(\"user\", schema=None) as var_7:\n                var_7.create_unique_constraint(\"uq_user_id\", [\"id\"])\n    except Exception as e:\n        print(e)\n",
      "lines_processed": 19,
      "total_lines": 66,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "006b3990db50_add_unique_constraints.py",
      "code": "def downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    conn = op.get_bind()\n    inspector = sa.inspect(conn)  # type: ignore\n    api_key_constraints = inspector.get_unique_constraints(\"apikey\")\n    flow_constraints = inspector.get_unique_constraints(\"flow\")\n    user_constraints = inspector.get_unique_constraints(\"user\")\n    try:\n        if any(constraint[\"name\"] == \"uq_apikey_id\" for constraint in api_key_constraints):\n            with op.batch_alter_table(\"user\", schema=None) as batch_op:\n                batch_op.drop_constraint(\"uq_user_id\", type_=\"unique\")\n        if any(constraint[\"name\"] == \"uq_flow_id\" for constraint in flow_constraints):\n            with op.batch_alter_table(\"flow\", schema=None) as batch_op:\n                batch_op.drop_constraint(\"uq_flow_id\", type_=\"unique\")\n        if any(constraint[\"name\"] == \"uq_user_id\" for constraint in user_constraints):\n            with op.batch_alter_table(\"apikey\", schema=None) as batch_op:\n                batch_op.drop_constraint(\"uq_apikey_id\", type_=\"unique\")\n    except Exception as e:\n        print(e)\n",
      "variables": [
        "conn",
        "inspector",
        "api_key_constraints",
        "flow_constraints",
        "user_constraints",
        "constraint",
        "batch_op"
      ],
      "anonymized_code": "def downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    var_1 = op.get_bind()\n    var_2 = sa.inspect(var_1)  # type: ignore\n    var_3 = var_2.get_unique_constraints(\"apikey\")\n    var_4 = var_2.get_unique_constraints(\"flow\")\n    var_5 = var_2.get_unique_constraints(\"user\")\n    try:\n        if any(var_6[\"name\"] == \"uq_apikey_id\" for var_6 in var_3):\n            with op.batch_alter_table(\"user\", schema=None) as var_7:\n                var_7.drop_constraint(\"uq_user_id\", type_=\"unique\")\n        if any(var_6[\"name\"] == \"uq_flow_id\" for var_6 in var_4):\n            with op.batch_alter_table(\"flow\", schema=None) as var_7:\n                var_7.drop_constraint(\"uq_flow_id\", type_=\"unique\")\n        if any(var_6[\"name\"] == \"uq_user_id\" for var_6 in var_5):\n            with op.batch_alter_table(\"apikey\", schema=None) as var_7:\n                var_7.drop_constraint(\"uq_apikey_id\", type_=\"unique\")\n    except Exception as e:\n        print(e)\n",
      "lines_processed": 19,
      "total_lines": 66,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "update_pyproject_combined.py",
      "code": "def main():\n    \"\"\"Universal update script that handles both base and main updates in a single run.\n\n    Usage:\n    update_pyproject_combined.py main <main_tag> <base_tag>\n    \"\"\"\n    arg_count = 4\n    if len(sys.argv) != arg_count:\n        print(\"Usage:\")\n        print(\"  update_pyproject_combined.py main <main_tag> <base_tag>\")\n        sys.exit(1)\n\n    mode = sys.argv[1]\n    if mode != \"main\":\n        print(\"Only 'main' mode is supported\")\n        print(\"Usage: update_pyproject_combined.py main <main_tag> <base_tag>\")\n        sys.exit(1)\n\n    main_tag = sys.argv[2]\n",
      "variables": [
        "arg_count",
        "mode",
        "main_tag"
      ],
      "anonymized_code": "def main():\n    \"\"\"Universal update script that handles both base and main updates in a single run.\n\n    Usage:\n    update_pyproject_combined.py main <var_3> <base_tag>\n    \"\"\"\n    var_1 = 4\n    if len(sys.argv) != var_1:\n        print(\"Usage:\")\n        print(\"  update_pyproject_combined.py main <var_3> <base_tag>\")\n        sys.exit(1)\n\n    var_2 = sys.argv[1]\n    if var_2 != \"main\":\n        print(\"Only 'main' var_2 is supported\")\n        print(\"Usage: update_pyproject_combined.py main <var_3> <base_tag>\")\n        sys.exit(1)\n\n    var_3 = sys.argv[2]\n",
      "lines_processed": 19,
      "total_lines": 52,
      "llm_code": "def main():\n    \"\"\"Universal update script that handles both base and main updates in a single run.\n\n    Usage:\n    update_pyproject_combined.py main <var_3> <base_tag>\n    \"\"\"\n    arg_count = 4\n    if len(sys.argv) != arg_count:\n        print(\"Usage:\")\n        print(\"  update_pyproject_combined.py main <var_3> <base_tag>\")\n        sys.exit(1)\n\n    var_2 = sys.argv[1]\n    if var_2 != \"main\":\n        print(\"Only 'main' var_2 is supported\")\n        print(\"Usage: update_pyproject_combined.py main <var_3> <base_tag>\")\n        sys.exit(1)\n\n    var_3 = sys.argv[2]\n",
      "llm_variables": [
        "arg_count",
        "var_2",
        "var_3"
      ]
    },
    {
      "file_path": "update_uv_dependency.py",
      "code": "def main() -> None:\n    if len(sys.argv) != ARGUMENT_NUMBER:\n        msg = \"specify base version\"\n        raise ValueError(msg)\n    base_version = sys.argv[1]\n    base_version = base_version.lstrip(\"v\")\n    update_uv_dep(base_version)\n",
      "variables": [
        "msg",
        "base_version"
      ],
      "anonymized_code": "def main() -> None:\n    if len(sys.argv) != ARGUMENT_NUMBER:\n        var_1 = \"specify base version\"\n        raise ValueError(var_1)\n    var_2 = sys.argv[1]\n    var_2 = var_2.lstrip(\"v\")\n    update_uv_dep(var_2)\n",
      "lines_processed": 7,
      "total_lines": 44,
      "llm_code": "def main() -> None:\n    if len(sys.argv) != ARGUMENT_NUMBER:\n        var_1 = \"specify base version\"\n        raise ValueError(var_1)\n    var_2 = sys.argv[1]\n    var_2 = var_2.lstrip(\"v\")\n    update_uv_dep(var_2)\n",
      "llm_variables": [
        "var_1",
        "var_2"
      ]
    },
    {
      "file_path": "update_uv_dependency.py",
      "code": "def update_uv_dep(base_version: str) -> None:\n    \"\"\"Update the langflow-base dependency in pyproject.toml.\"\"\"\n    pyproject_path = BASE_DIR / \"pyproject.toml\"\n\n    # Read the pyproject.toml file content\n    content = pyproject_path.read_text(encoding=\"utf-8\")\n\n    # For the main project, update the langflow-base dependency in the UV section\n    pattern = re.compile(r'(dependencies\\s*=\\s*\\[\\s*\\n\\s*)(\"langflow-base==[\\d.]+\")')\n    replacement = rf'\\1\"langflow-base-nightly=={base_version}\"'\n\n    # Check if the pattern is found\n    if not pattern.search(content):\n        msg = f\"{pattern} UV dependency not found in {pyproject_path}\"\n        raise ValueError(msg)\n\n    # Replace the matched pattern with the new one\n    content = pattern.sub(replacement, content)\n\n",
      "variables": [
        "base_version",
        "pyproject_path",
        "content",
        "pattern",
        "replacement",
        "msg"
      ],
      "anonymized_code": "def update_uv_dep(var_1: str) -> None:\n    \"\"\"Update the langflow-base dependency in pyproject.toml.\"\"\"\n    var_2 = BASE_DIR / \"pyproject.toml\"\n\n    # Read the pyproject.toml file var_3\n    var_3 = var_2.read_text(encoding=\"utf-8\")\n\n    # For the main project, update the langflow-base dependency in the UV section\n    var_4 = re.compile(r'(dependencies\\s*=\\s*\\[\\s*\\n\\s*)(\"langflow-base==[\\d.]+\")')\n    var_5 = rf'\\1\"langflow-base-nightly=={var_1}\"'\n\n    # Check if the var_4 is found\n    if not var_4.search(var_3):\n        var_6 = f\"{var_4} UV dependency not found in {var_2}\"\n        raise ValueError(var_6)\n\n    # Replace the matched var_4 with the new one\n    var_3 = var_4.sub(var_5, var_3)\n\n",
      "lines_processed": 19,
      "total_lines": 44,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "update_lf_base_dependency.py",
      "code": "def verify_pep440(version):\n    \"\"\"Verify if version is PEP440 compliant.\n\n    https://github.com/pypa/packaging/blob/16.7/packaging/version.py#L191\n    \"\"\"\n    return packaging.version.Version(version)\n",
      "variables": [
        "version"
      ],
      "anonymized_code": "def verify_pep440(var_1):\n    \"\"\"Verify if var_1 is PEP440 compliant.\n\n    https://github.com/pypa/packaging/blob/16.7/packaging/var_1.py#L191\n    \"\"\"\n    return packaging.var_1.Version(var_1)\n",
      "lines_processed": 6,
      "total_lines": 51,
      "llm_code": "def verify_pep440(version):\n    \"\"\"Verify if version is PEP440 compliant.\n\n    https://github.com/pypa/packaging/blob/16.7/packaging/version.py#L191\n    \"\"\"\n    return packaging.version.Version(version)\n",
      "llm_variables": [
        "version"
      ]
    },
    {
      "file_path": "update_lf_base_dependency.py",
      "code": "def update_base_dep(pyproject_path: str, new_version: str) -> None:\n    \"\"\"Update the langflow-base dependency in pyproject.toml.\"\"\"\n    filepath = BASE_DIR / pyproject_path\n    content = filepath.read_text(encoding=\"utf-8\")\n\n    replacement = f'langflow-base-nightly = \"{new_version}\"'\n\n    # Updates the pattern for poetry\n    pattern = re.compile(r'langflow-base = \\{ path = \"\\./src/backend/base\", develop = true \\}')\n    if not pattern.search(content):\n        msg = f'langflow-base poetry dependency not found in \"{filepath}\"'\n        raise ValueError(msg)\n    content = pattern.sub(replacement, content)\n    filepath.write_text(content, encoding=\"utf-8\")\n",
      "variables": [
        "pyproject_path",
        "new_version",
        "filepath",
        "content",
        "replacement",
        "pattern",
        "msg"
      ],
      "anonymized_code": "def update_base_dep(var_1: str, var_2: str) -> None:\n    \"\"\"Update the langflow-base dependency in pyproject.toml.\"\"\"\n    var_3 = BASE_DIR / var_1\n    var_4 = var_3.read_text(encoding=\"utf-8\")\n\n    var_5 = f'langflow-base-nightly = \"{var_2}\"'\n\n    # Updates the var_6 for poetry\n    var_6 = re.compile(r'langflow-base = \\{ path = \"\\./src/backend/base\", develop = true \\}')\n    if not var_6.search(var_4):\n        var_7 = f'langflow-base poetry dependency not found in \"{var_3}\"'\n        raise ValueError(var_7)\n    var_4 = var_6.sub(var_5, var_4)\n    var_3.write_text(var_4, encoding=\"utf-8\")\n",
      "lines_processed": 14,
      "total_lines": 51,
      "llm_code": "def update_base_dep(package_name: str, new_version: str) -> None:\n    \"\"\"Update the langflow-base dependency in pyproject.toml.\"\"\"\n    config_path = BASE_DIR / package_name\n    config_content = config_path.read_text(encoding=\"utf-8\")\n\n    new_dependency_line = f'langflow-base-nightly = \"{new_version}\"'\n\n    # Updates the dependency_regex for poetry\n    dependency_regex = re.compile(r'langflow-base = \\{ path = \"\\./src/backend/base\", develop = true \\}')\n    if not dependency_regex.search(config_content):\n        missing_dependency_msg = f'langflow-base poetry dependency not found in \"{config_path}\"'\n        raise ValueError(missing_dependency_msg)\n    config_content = dependency_regex.sub(new_dependency_line, config_content)\n    config_path.write_text(config_content, encoding=\"utf-8\")\n",
      "llm_variables": [
        "package_name",
        "new_version",
        "config_path",
        "config_content",
        "new_dependency_line",
        "dependency_regex",
        "missing_dependency_msg"
      ]
    },
    {
      "file_path": "update_lf_base_dependency.py",
      "code": "def main() -> None:\n    if len(sys.argv) != ARGUMENT_NUMBER:\n        msg = \"New version not specified\"\n        raise ValueError(msg)\n    base_version = sys.argv[1]\n\n    # Strip \"v\" prefix from version if present\n    base_version = base_version.removeprefix(\"v\")\n\n    verify_pep440(base_version)\n    update_base_dep(\"pyproject.toml\", base_version)\n",
      "variables": [
        "msg",
        "base_version"
      ],
      "anonymized_code": "def main() -> None:\n    if len(sys.argv) != ARGUMENT_NUMBER:\n        var_1 = \"New version not specified\"\n        raise ValueError(var_1)\n    var_2 = sys.argv[1]\n\n    # Strip \"v\" prefix from version if present\n    var_2 = var_2.removeprefix(\"v\")\n\n    verify_pep440(var_2)\n    update_base_dep(\"pyproject.toml\", var_2)\n",
      "lines_processed": 11,
      "total_lines": 51,
      "llm_code": "def main() -> None:\n    if len(sys.argv) != ARGUMENT_NUMBER:\n        message = \"New version not specified\"\n        raise ValueError(message)\n    argument = sys.argv[1]\n\n    # Strip \"v\" prefix from version if present\n    argument = argument.removeprefix(\"v\")\n\n    verify_pep440(argument)\n    update_base_dep(\"pyproject.toml\", argument)\n",
      "llm_variables": [
        "message",
        "argument"
      ]
    },
    {
      "file_path": "0d60fcbd4e8e_create_vertex_builds_table.py",
      "code": "def downgrade() -> None:\n    conn = op.get_bind()\n    if migration.table_exists(\"vertex_build\", conn):\n        op.drop_table(\"vertex_build\")\n    pass\n",
      "variables": [
        "conn"
      ],
      "anonymized_code": "def downgrade() -> None:\n    var_1 = op.get_bind()\n    if migration.table_exists(\"vertex_build\", var_1):\n        op.drop_table(\"vertex_build\")\n    pass\n",
      "lines_processed": 5,
      "total_lines": 51,
      "llm_code": "def downgrade() -> None:\n    connection = op.get_bind()\n    if migration.table_exists(\"vertex_build\", connection):\n        op.drop_table(\"vertex_build\")\n    pass\n",
      "llm_variables": [
        "connection"
      ]
    },
    {
      "file_path": "update_pyproject_version.py",
      "code": "def update_pyproject_version(pyproject_path: str, new_version: str) -> None:\n    \"\"\"Update the version in pyproject.toml.\"\"\"\n    filepath = BASE_DIR / pyproject_path\n    content = filepath.read_text(encoding=\"utf-8\")\n\n    # Regex to match the version line under [tool.poetry]\n    pattern = re.compile(r'(?<=^version = \")[^\"]+(?=\")', re.MULTILINE)\n\n    if not pattern.search(content):\n        msg = f'Project version not found in \"{filepath}\"'\n        raise ValueError(msg)\n\n    content = pattern.sub(new_version, content)\n\n    filepath.write_text(content, encoding=\"utf-8\")\n",
      "variables": [
        "pyproject_path",
        "new_version",
        "filepath",
        "content",
        "pattern",
        "msg"
      ],
      "anonymized_code": "def update_pyproject_version(var_1: str, var_2: str) -> None:\n    \"\"\"Update the version in pyproject.toml.\"\"\"\n    var_3 = BASE_DIR / var_1\n    var_4 = var_3.read_text(encoding=\"utf-8\")\n\n    # Regex to match the version line under [tool.poetry]\n    var_5 = re.compile(r'(?<=^version = \")[^\"]+(?=\")', re.MULTILINE)\n\n    if not var_5.search(var_4):\n        var_6 = f'Project version not found in \"{var_3}\"'\n        raise ValueError(var_6)\n\n    var_4 = var_5.sub(var_2, var_4)\n\n    var_3.write_text(var_4, encoding=\"utf-8\")\n",
      "lines_processed": 15,
      "total_lines": 61,
      "llm_code": "def update_pyproject_version(version_file: str, new_version: str) -> None:\n    \"\"\"Update the version in pyproject.toml.\"\"\"\n    project_path = BASE_DIR / version_file\n    file_content = project_path.read_text(encoding=\"utf-8\")\n\n    # Regex to match the version line under [tool.poetry]\n    version_pattern = re.compile(r'(?<=^version = \")[^\"]+(?=\")', re.MULTILINE)\n\n    if not version_pattern.search(file_content):\n        error_message = f'Project version not found in \"{project_path}\"'\n        raise ValueError(error_message)\n\n    file_content = version_pattern.sub(new_version, file_content)\n\n    project_path.write_text(file_content, encoding=\"utf-8\")\n",
      "llm_variables": [
        "version_file",
        "new_version",
        "project_path",
        "file_content",
        "version_pattern",
        "error_message"
      ]
    },
    {
      "file_path": "update_pyproject_version.py",
      "code": "def verify_pep440(version):\n    \"\"\"Verify if version is PEP440 compliant.\n\n    https://github.com/pypa/packaging/blob/16.7/packaging/version.py#L191\n    \"\"\"\n    return packaging.version.Version(version)\n",
      "variables": [
        "version"
      ],
      "anonymized_code": "def verify_pep440(var_1):\n    \"\"\"Verify if var_1 is PEP440 compliant.\n\n    https://github.com/pypa/packaging/blob/16.7/packaging/var_1.py#L191\n    \"\"\"\n    return packaging.var_1.Version(var_1)\n",
      "lines_processed": 6,
      "total_lines": 61,
      "llm_code": "def verify_pep440(version):\n    \"\"\"Verify if version is PEP440 compliant.\n\n    https://github.com/pypa/packaging/blob/16.7/packaging/version.py#L191\n    \"\"\"\n    return packaging.version.Version(version)\n",
      "llm_variables": [
        "version"
      ]
    },
    {
      "file_path": "update_pyproject_version.py",
      "code": "def main() -> None:\n    if len(sys.argv) != ARGUMENT_NUMBER:\n        msg = \"New version not specified\"\n        raise ValueError(msg)\n    new_version = sys.argv[1]\n\n    # Strip \"v\" prefix from version if present\n    new_version = new_version.removeprefix(\"v\")\n\n    build_type = sys.argv[2]\n\n    verify_pep440(new_version)\n\n    if build_type == \"base\":\n        update_pyproject_version(\"src/backend/base/pyproject.toml\", new_version)\n    elif build_type == \"main\":\n        update_pyproject_version(\"pyproject.toml\", new_version)\n    else:\n        msg = f\"Invalid build type: {build_type}\"\n",
      "variables": [
        "msg",
        "new_version",
        "build_type"
      ],
      "anonymized_code": "def main() -> None:\n    if len(sys.argv) != ARGUMENT_NUMBER:\n        var_1 = \"New version not specified\"\n        raise ValueError(var_1)\n    var_2 = sys.argv[1]\n\n    # Strip \"v\" prefix from version if present\n    var_2 = var_2.removeprefix(\"v\")\n\n    var_3 = sys.argv[2]\n\n    verify_pep440(var_2)\n\n    if var_3 == \"base\":\n        update_pyproject_version(\"src/backend/base/pyproject.toml\", var_2)\n    elif var_3 == \"main\":\n        update_pyproject_version(\"pyproject.toml\", var_2)\n    else:\n        var_1 = f\"Invalid build type: {var_3}\"\n",
      "lines_processed": 19,
      "total_lines": 61,
      "llm_code": "def main() -> None:\n    if len(sys.argv) != ARGUMENT_NUMBER:\n        new_version_not_specified = \"New version not specified\"\n        raise ValueError(new_version_not_specified)\n    version = sys.argv[1]\n\n    # Strip \"v\" prefix from version if present\n    version = version.removeprefix(\"v\")\n\n    build_type = sys.argv[2]\n\n    verify_pep440(version)\n\n    if build_type == \"base\":\n        update_pyproject_version(\"src/backend/base/pyproject.toml\", version)\n    elif build_type == \"main\":\n        update_pyproject_version(\"pyproject.toml\", version)\n    else:\n        new_version_not_specified = f\"Invalid build type: {build_type}\"\n",
      "llm_variables": [
        "new_version_not_specified",
        "version",
        "build_type"
      ]
    },
    {
      "file_path": "pypi_nightly_tag.py",
      "code": "def get_latest_published_version(build_type: str, *, is_nightly: bool) -> Version:\n    import requests\n\n    url = \"\"\n    if build_type == \"base\":\n        url = PYPI_LANGFLOW_BASE_NIGHTLY_URL if is_nightly else PYPI_LANGFLOW_BASE_URL\n    elif build_type == \"main\":\n        url = PYPI_LANGFLOW_NIGHTLY_URL if is_nightly else PYPI_LANGFLOW_URL\n    else:\n        msg = f\"Invalid build type: {build_type}\"\n        raise ValueError(msg)\n\n    res = requests.get(url, timeout=10)\n    try:\n        version_str = res.json()[\"info\"][\"version\"]\n    except Exception as e:\n        msg = \"Got unexpected response from PyPI\"\n        raise RuntimeError(msg) from e\n    return Version(version_str)\n",
      "variables": [
        "build_type",
        "is_nightly",
        "url",
        "msg",
        "res",
        "version_str"
      ],
      "anonymized_code": "def get_latest_published_version(var_1: str, *, var_2: bool) -> Version:\n    import requests\n\n    var_3 = \"\"\n    if var_1 == \"base\":\n        var_3 = PYPI_LANGFLOW_BASE_NIGHTLY_URL if var_2 else PYPI_LANGFLOW_BASE_URL\n    elif var_1 == \"main\":\n        var_3 = PYPI_LANGFLOW_NIGHTLY_URL if var_2 else PYPI_LANGFLOW_URL\n    else:\n        var_4 = f\"Invalid build type: {var_1}\"\n        raise ValueError(var_4)\n\n    var_5 = requests.get(var_3, timeout=10)\n    try:\n        var_6 = var_5.json()[\"info\"][\"version\"]\n    except Exception as e:\n        var_4 = \"Got unexpected response from PyPI\"\n        raise RuntimeError(var_4) from e\n    return Version(var_6)\n",
      "lines_processed": 19,
      "total_lines": 85,
      "llm_code": "def get_latest_published_version(build_type: str, *, is_nightly: bool) -> Version:\n    import requests\n\n    base_url = \"\"\n    if build_type == \"base\":\n        base_url = PYPI_LANGFLOW_BASE_NIGHTLY_URL if is_nightly else PYPI_LANGFLOW_BASE_URL\n    elif build_type == \"main\":\n        base_url = PYPI_LANGFLOW_NIGHTLY_URL if is_nightly else PYPI_LANGFLOW_URL\n    else:\n        error_message = f\"Invalid build type: {build_type}\"\n        raise ValueError(error_message)\n\n    response = requests.get(base_url, timeout=10)\n    try:\n        version = response.json()[\"info\"][\"version\"]\n    except Exception as e:\n        error_message = \"Got unexpected response from PyPI\"\n        raise RuntimeError(error_message) from e\n    return Version(version)\n",
      "llm_variables": [
        "build_type",
        "is_nightly",
        "base_url",
        "error_message",
        "response",
        "version"
      ]
    },
    {
      "file_path": "pypi_nightly_tag.py",
      "code": "def create_tag(build_type: str):\n    current_version = get_latest_published_version(build_type, is_nightly=False)\n    current_nightly_version = get_latest_published_version(build_type, is_nightly=True)\n\n    build_number = \"0\"\n    latest_base_version = current_version.base_version\n    nightly_base_version = current_nightly_version.base_version\n\n    if latest_base_version == nightly_base_version:\n        # If the latest version is the same as the nightly version, increment the build number\n        build_number = str(current_nightly_version.dev + 1)\n\n    new_nightly_version = latest_base_version + \".dev\" + build_number\n\n    # Prepend \"v\" to the version, if DNE.\n    # This is an update to the nightly version format.\n    if not new_nightly_version.startswith(\"v\"):\n        new_nightly_version = \"v\" + new_nightly_version\n\n",
      "variables": [
        "build_type",
        "current_version",
        "current_nightly_version",
        "build_number",
        "latest_base_version",
        "nightly_base_version",
        "new_nightly_version"
      ],
      "anonymized_code": "def create_tag(var_1: str):\n    var_2 = get_latest_published_version(var_1, is_nightly=False)\n    var_3 = get_latest_published_version(var_1, is_nightly=True)\n\n    var_4 = \"0\"\n    var_5 = var_2.base_version\n    var_6 = var_3.base_version\n\n    if var_5 == var_6:\n        # If the latest version is the same as the nightly version, increment the build number\n        var_4 = str(var_3.dev + 1)\n\n    var_7 = var_5 + \".dev\" + var_4\n\n    # Prepend \"v\" to the version, if DNE.\n    # This is an update to the nightly version format.\n    if not var_7.startswith(\"v\"):\n        var_7 = \"v\" + var_7\n\n",
      "lines_processed": 19,
      "total_lines": 85,
      "llm_code": "def create_tag(version: str):\n    latest_version = get_latest_published_version(version, is_nightly=False)\n    nightly_version = get_latest_published_version(version, is_nightly=True)\n\n    build_number = \"0\"\n    base_version = latest_version.base_version\n    nightly_base_version = nightly_version.base_version\n\n    if base_version == nightly_base_version:\n        # If the latest version is the same as the nightly version, increment the build number\n        build_number = str(nightly_version.dev + 1)\n\n    version_tag = base_version + \".dev\" + build_number\n\n    # Prepend \"v\" to the version, if DNE.\n    # This is an update to the nightly version format.\n    if not version_tag.startswith(\"v\"):\n        version_tag = \"v\" + version_tag\n\n",
      "llm_variables": [
        "version",
        "latest_version",
        "nightly_version",
        "build_number",
        "base_version",
        "nightly_base_version",
        "version_tag"
      ]
    },
    {
      "file_path": "__main__.py",
      "code": "def get_number_of_workers(workers=None):\n    if workers == -1 or workers is None:\n        workers = (cpu_count() * 2) + 1\n    logger.debug(f\"Number of workers: {workers}\")\n    return workers\n",
      "variables": [
        "workers"
      ],
      "anonymized_code": "def get_number_of_workers(var_1=None):\n    if var_1 == -1 or var_1 is None:\n        var_1 = (cpu_count() * 2) + 1\n    logger.debug(f\"Number of var_1: {var_1}\")\n    return var_1\n",
      "lines_processed": 5,
      "total_lines": 672,
      "llm_code": "def get_number_of_workers(number_of_workers=None):\n    if number_of_workers == -1 or number_of_workers is None:\n        number_of_workers = (cpu_count() * 2) + 1\n    logger.debug(f\"Number of number_of_workers: {number_of_workers}\")\n    return number_of_workers\n",
      "llm_variables": [
        "number_of_workers"
      ]
    },
    {
      "file_path": "__main__.py",
      "code": "def migration(\n    test: bool = typer.Option(default=True, help=\"Run migrations in test mode.\"),  # noqa: FBT001\n    fix: bool = typer.Option(  # noqa: FBT001\n        default=False,\n        help=\"Fix migrations. This is a destructive operation, and should only be used if you know what you are doing.\",\n    ),\n) -> None:\n    \"\"\"Run or test migrations.\"\"\"\n    if fix and not typer.confirm(\n        \"This will delete all data necessary to fix migrations. Are you sure you want to continue?\"\n    ):\n        raise typer.Abort\n\n    asyncio.run(_migration(test=test, fix=fix))\n",
      "variables": [
        "test",
        "fix"
      ],
      "anonymized_code": "def migration(\n    var_1: bool = typer.Option(default=True, help=\"Run migrations in var_1 mode.\"),  # noqa: FBT001\n    var_2: bool = typer.Option(  # noqa: FBT001\n        default=False,\n        help=\"Fix migrations. This is a destructive operation, and should only be used if you know what you are doing.\",\n    ),\n) -> None:\n    \"\"\"Run or var_1 migrations.\"\"\"\n    if var_2 and not typer.confirm(\n        \"This will delete all data necessary to var_2 migrations. Are you sure you want to continue?\"\n    ):\n        raise typer.Abort\n\n    asyncio.run(_migration(var_1=var_1, var_2=var_2))\n",
      "lines_processed": 14,
      "total_lines": 672,
      "llm_code": "def migration(\n    run_migrations: bool = typer.Option(default=True, help=\"Run migrations in run_migrations mode.\"),  # noqa: FBT001\n    fix_migrations: bool = typer.Option(  # noqa: FBT001\n        default=False,\n        help=\"Fix migrations. This is a destructive operation, and should only be used if you know what you are doing.\",\n    ),\n) -> None:\n    \"\"\"Run or run_migrations migrations.\"\"\"\n    if fix_migrations and not typer.confirm(\n        \"This will delete all data necessary to fix_migrations migrations. Are you sure you want to continue?\"\n    ):\n        raise typer.Abort\n\n    asyncio.run(_migration(run_migrations=run_migrations, fix_migrations=fix_migrations))\n",
      "llm_variables": [
        "run_migrations",
        "fix_migrations"
      ]
    },
    {
      "file_path": "__main__.py",
      "code": "def run_langflow(host, port, log_level, options, app) -> None:\n    \"\"\"Run Langflow server on localhost.\"\"\"\n    if platform.system() == \"Windows\":\n        import uvicorn\n\n        uvicorn.run(\n            app,\n            host=host,\n            port=port,\n            log_level=log_level.lower(),\n            loop=\"asyncio\",\n            ssl_keyfile=options[\"keyfile\"],\n            ssl_certfile=options[\"certfile\"],\n        )\n    else:\n        from langflow.server import LangflowApplication\n\n        server = LangflowApplication(app, options)\n\n",
      "variables": [
        "host",
        "port",
        "log_level",
        "options",
        "app",
        "server"
      ],
      "anonymized_code": "def run_langflow(var_1, var_2, var_3, var_4, var_5) -> None:\n    \"\"\"Run Langflow var_6 on localhost.\"\"\"\n    if platform.system() == \"Windows\":\n        import uvicorn\n\n        uvicorn.run(\n            var_5,\n            var_1=var_1,\n            var_2=var_2,\n            var_3=var_3.lower(),\n            loop=\"asyncio\",\n            ssl_keyfile=var_4[\"keyfile\"],\n            ssl_certfile=var_4[\"certfile\"],\n        )\n    else:\n        from langflow.var_6 import LangflowApplication\n\n        var_6 = LangflowApplication(var_5, var_4)\n\n",
      "lines_processed": 19,
      "total_lines": 672,
      "llm_code": "def run_langflow(app, config, host, port, debug) -> None:\n    \"\"\"Run Langflow server on localhost.\"\"\"\n    if platform.system() == \"Windows\":\n        import uvicorn\n\n        uvicorn.run(\n            debug,\n            app=app,\n            config=config,\n            host=host.lower(),\n            loop=\"asyncio\",\n            ssl_keyfile=port[\"keyfile\"],\n            ssl_certfile=port[\"certfile\"],\n        )\n    else:\n        from langflow.server import LangflowApplication\n\n        server = LangflowApplication(debug, port)\n\n",
      "llm_variables": [
        "app",
        "config",
        "host",
        "port",
        "debug",
        "server"
      ]
    },
    {
      "file_path": "__main__.py",
      "code": "def wait_for_server_ready(host, port, protocol) -> None:\n    \"\"\"Wait for the server to become ready by polling the health endpoint.\"\"\"\n    status_code = 0\n    while status_code != httpx.codes.OK:\n        try:\n            status_code = httpx.get(\n                f\"{protocol}://{host}:{port}/health\", verify=host not in (\"127.0.0.1\", \"localhost\")\n            ).status_code\n        except HTTPError:\n            time.sleep(1)\n        except Exception:  # noqa: BLE001\n            logger.opt(exception=True).debug(\"Error while waiting for the server to become ready.\")\n            time.sleep(1)\n",
      "variables": [
        "host",
        "port",
        "protocol",
        "status_code"
      ],
      "anonymized_code": "def wait_for_server_ready(var_1, var_2, var_3) -> None:\n    \"\"\"Wait for the server to become ready by polling the health endpoint.\"\"\"\n    var_4 = 0\n    while var_4 != httpx.codes.OK:\n        try:\n            var_4 = httpx.get(\n                f\"{var_3}://{var_1}:{var_2}/health\", verify=var_1 not in (\"127.0.0.1\", \"localhost\")\n            ).var_4\n        except HTTPError:\n            time.sleep(1)\n        except Exception:  # noqa: BLE001\n            logger.opt(exception=True).debug(\"Error while waiting for the server to become ready.\")\n            time.sleep(1)\n",
      "lines_processed": 13,
      "total_lines": 672,
      "llm_code": "def wait_for_server_ready(host, port, base_url) -> None:\n    \"\"\"Wait for the server to become ready by polling the health endpoint.\"\"\"\n    retries = 0\n    while retries!= httpx.codes.OK:\n        try:\n            retries = httpx.get(\n                f\"{base_url}://{host}:{port}/health\", verify=host not in (\"127.0.0.1\", \"localhost\")\n            ).status_code\n        except HTTPError:\n            time.sleep(1)\n        except Exception:  # noqa: BLE001\n            logger.opt(exception=True).debug(\"Error while waiting for the server to become ready.\")\n            time.sleep(1)",
      "llm_variables": [
        "host",
        "port",
        "base_url",
        "retries"
      ]
    },
    {
      "file_path": "__main__.py",
      "code": "def print_banner(host: str, port: int, protocol: str) -> None:\n    notices = []\n    package_names = []  # Track package names for pip install instructions\n    is_pre_release = False  # Track if any package is a pre-release\n    package_name = \"\"\n\n    # Use langflow.utils.version to get the version info\n    version_info = get_version_info()\n    langflow_version = version_info[\"version\"]\n    package_name = version_info[\"package\"]\n    is_pre_release |= langflow_is_pre_release(langflow_version)  # Update pre-release status\n\n    notice = build_version_notice(langflow_version, package_name)\n\n    notice = stylize_text(notice, package_name, is_prerelease=is_pre_release)\n    if notice:\n        notices.append(notice)\n    package_names.append(package_name)\n\n",
      "variables": [
        "host",
        "port",
        "protocol",
        "notices",
        "package_names",
        "is_pre_release",
        "package_name",
        "version_info",
        "langflow_version",
        "notice"
      ],
      "anonymized_code": "def print_banner(var_1: str, var_2: int, var_3: str) -> None:\n    var_4 = []\n    var_5 = []  # Track package names for pip install instructions\n    var_6 = False  # Track if any package is a pre-release\n    var_7 = \"\"\n\n    # Use langflow.utils.version to get the version info\n    var_8 = get_version_info()\n    var_9 = var_8[\"version\"]\n    var_7 = var_8[\"package\"]\n    var_6 |= langflow_is_pre_release(var_9)  # Update pre-release status\n\n    var_10 = build_version_notice(var_9, var_7)\n\n    var_10 = stylize_text(var_10, var_7, is_prerelease=var_6)\n    if var_10:\n        var_4.append(var_10)\n    var_5.append(var_7)\n\n",
      "lines_processed": 19,
      "total_lines": 672,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "__main__.py",
      "code": "def show_version(*, value: bool):\n    if value:\n        default = \"DEV\"\n        raw_info = get_version_info()\n        version = raw_info.get(\"version\", default) if raw_info else default\n        typer.echo(f\"langflow {version}\")\n        raise typer.Exit\n",
      "variables": [
        "value",
        "default",
        "raw_info",
        "version"
      ],
      "anonymized_code": "def show_version(*, var_1: bool):\n    if var_1:\n        var_2 = \"DEV\"\n        var_3 = get_version_info()\n        var_4 = var_3.get(\"var_4\", var_2) if var_3 else var_2\n        typer.echo(f\"langflow {var_4}\")\n        raise typer.Exit\n",
      "lines_processed": 7,
      "total_lines": 672,
      "llm_code": "def show_version(*, is_dev: bool):\n    if is_dev:\n        dev_version = \"DEV\"\n        version_info = get_version_info()\n        final_version = version_info.get(\"final_version\", dev_version) if version_info else dev_version\n        typer.echo(f\"langflow {final_version}\")\n        raise typer.Exit\n",
      "llm_variables": [
        "is_dev",
        "dev_version",
        "version_info",
        "final_version"
      ]
    }
  ],
  "pallets_flask": [
    {
      "file_path": "auth.py",
      "code": "def login():\n    \"\"\"Log in a registered user by adding the user id to the session.\"\"\"\n    if request.method == \"POST\":\n        username = request.form[\"username\"]\n        password = request.form[\"password\"]\n        db = get_db()\n        error = None\n        user = db.execute(\n            \"SELECT * FROM user WHERE username = ?\", (username,)\n        ).fetchone()\n\n        if user is None:\n            error = \"Incorrect username.\"\n        elif not check_password_hash(user[\"password\"], password):\n            error = \"Incorrect password.\"\n\n        if error is None:\n            # store the user id in a new session and return to the index\n            session.clear()\n",
      "variables": [
        "username",
        "password",
        "db",
        "error",
        "user"
      ],
      "anonymized_code": "def login():\n    \"\"\"Log in a registered var_5 by adding the var_5 id to the session.\"\"\"\n    if request.method == \"POST\":\n        var_1 = request.form[\"var_1\"]\n        var_2 = request.form[\"var_2\"]\n        var_3 = get_db()\n        var_4 = None\n        var_5 = var_3.execute(\n            \"SELECT * FROM var_5 WHERE var_1 = ?\", (var_1,)\n        ).fetchone()\n\n        if var_5 is None:\n            var_4 = \"Incorrect var_1.\"\n        elif not check_password_hash(var_5[\"var_2\"], var_2):\n            var_4 = \"Incorrect var_2.\"\n\n        if var_4 is None:\n            # store the var_5 id in a new session and return to the index\n            session.clear()\n",
      "lines_processed": 19,
      "total_lines": 116,
      "llm_code": "def login():\n    \"\"\"Log in a registered user by adding the user id to the session.\"\"\"\n    if request.method == \"POST\":\n        username = request.form[\"username\"]\n        password = request.form[\"password\"]\n        db = get_db()\n        error = None\n        user = db.execute(\n            \"SELECT * FROM user WHERE username = ?\", (username,)\n        ).fetchone()\n\n        if user is None:\n            error = \"Incorrect username.\"\n        elif not check_password_hash(user[\"password\"], password):\n            error = \"Incorrect password.\"\n\n        if error is None:\n            # store the user id in a new session and return to the index\n            session.clear()\n",
      "llm_variables": [
        "username",
        "password",
        "db",
        "error",
        "user"
      ]
    },
    {
      "file_path": "auth.py",
      "code": "def load_logged_in_user():\n    \"\"\"If a user id is stored in the session, load the user object from\n    the database into ``g.user``.\"\"\"\n    user_id = session.get(\"user_id\")\n\n    if user_id is None:\n        g.user = None\n    else:\n        g.user = (\n            get_db().execute(\"SELECT * FROM user WHERE id = ?\", (user_id,)).fetchone()\n        )\n",
      "variables": [
        "user_id"
      ],
      "anonymized_code": "def load_logged_in_user():\n    \"\"\"If a user id is stored in the session, load the user object from\n    the database into ``g.user``.\"\"\"\n    var_1 = session.get(\"var_1\")\n\n    if var_1 is None:\n        g.user = None\n    else:\n        g.user = (\n            get_db().execute(\"SELECT * FROM user WHERE id = ?\", (var_1,)).fetchone()\n        )\n",
      "lines_processed": 11,
      "total_lines": 116,
      "llm_code": "def load_logged_in_user():\n    \"\"\"If a user id is stored in the session, load the user object from\n    the database into ``g.user``.\"\"\"\n    user_id = session.get(\"user_id\")\n\n    if user_id is None:\n        g.user = None\n    else:\n        g.user = (\n            get_db().execute(\"SELECT * FROM user WHERE id = ?\", (user_id,)).fetchone()\n        )\n",
      "llm_variables": [
        "user_id"
      ]
    },
    {
      "file_path": "auth.py",
      "code": "def login_required(view):\n    \"\"\"View decorator that redirects anonymous users to the login page.\"\"\"\n\n    @functools.wraps(view)\n    def wrapped_view(**kwargs):\n        if g.user is None:\n            return redirect(url_for(\"auth.login\"))\n\n        return view(**kwargs)\n\n    return wrapped_view\n",
      "variables": [
        "view",
        "kwargs"
      ],
      "anonymized_code": "def login_required(var_1):\n    \"\"\"View decorator that redirects anonymous users to the login page.\"\"\"\n\n    @functools.wraps(var_1)\n    def wrapped_view(**var_2):\n        if g.user is None:\n            return redirect(url_for(\"auth.login\"))\n\n        return var_1(**var_2)\n\n    return wrapped_view\n",
      "lines_processed": 11,
      "total_lines": 116,
      "llm_code": "def login_required(view):\n    \"\"\"View decorator that redirects anonymous users to the login page.\"\"\"\n\n    @functools.wraps(view)\n    def wrapped_view(**kwargs):\n        if g.user is None:\n            return redirect(url_for(\"auth.login\"))\n\n        return view(**kwargs)\n\n    return wrapped_view\n",
      "llm_variables": [
        "view",
        "kwargs"
      ]
    },
    {
      "file_path": "db.py",
      "code": "def close_db(e=None):\n    \"\"\"If this request connected to the database, close the\n    connection.\n    \"\"\"\n    db = g.pop(\"db\", None)\n\n    if db is not None:\n        db.close()\n",
      "variables": [
        "e",
        "db"
      ],
      "anonymized_code": "def close_db(var_1=None):\n    \"\"\"If this request connected to the database, close the\n    connection.\n    \"\"\"\n    var_2 = g.pop(\"var_2\", None)\n\n    if var_2 is not None:\n        var_2.close()\n",
      "lines_processed": 8,
      "total_lines": 56,
      "llm_code": "def close_db(connection=None):\n    \"\"\"If this request connected to the database, close the\n    connection.\n    \"\"\"\n    db_connection = g.pop(\"db_connection\", None)\n\n    if db_connection is not None:\n        db_connection.close()\n",
      "llm_variables": [
        "connection",
        "db_connection"
      ]
    },
    {
      "file_path": "db.py",
      "code": "def init_db():\n    \"\"\"Clear existing data and create new tables.\"\"\"\n    db = get_db()\n\n    with current_app.open_resource(\"schema.sql\") as f:\n        db.executescript(f.read().decode(\"utf8\"))\n",
      "variables": [
        "db",
        "f"
      ],
      "anonymized_code": "def init_db():\n    \"\"\"Clear existing data and create new tables.\"\"\"\n    var_1 = get_db()\n\n    with current_app.open_resource(\"schema.sql\") as var_2:\n        var_1.executescript(var_2.read().decode(\"utf8\"))\n",
      "lines_processed": 6,
      "total_lines": 56,
      "llm_code": "def init_db():\n    \"\"\"Clear existing data and create new tables.\"\"\"\n    db = get_db()\n\n    with current_app.open_resource(\"schema.sql\") as schema_file:\n        db.executescript(schema_file.read().decode(\"utf8\"))\n",
      "llm_variables": [
        "db",
        "schema_file"
      ]
    },
    {
      "file_path": "db.py",
      "code": "def init_app(app):\n    \"\"\"Register database functions with the Flask app. This is called by\n    the application factory.\n    \"\"\"\n    app.teardown_appcontext(close_db)\n    app.cli.add_command(init_db_command)\n",
      "variables": [
        "app"
      ],
      "anonymized_code": "def init_app(var_1):\n    \"\"\"Register database functions with the Flask var_1. This is called by\n    the application factory.\n    \"\"\"\n    var_1.teardown_appcontext(close_db)\n    var_1.cli.add_command(init_db_command)\n",
      "lines_processed": 6,
      "total_lines": 56,
      "llm_code": "def init_app(app):\n    \"\"\"Register database functions with the Flask app. This is called by\n    the application factory.\n    \"\"\"\n    app.teardown_appcontext(close_db)\n    app.cli.add_command(init_db_command)\n",
      "llm_variables": [
        "app"
      ]
    },
    {
      "file_path": "test_js_example.py",
      "code": "def test_add(client, a, b, result):\n    response = client.post(\"/add\", data={\"a\": a, \"b\": b})\n    assert response.get_json()[\"result\"] == result\n",
      "variables": [
        "client",
        "a",
        "b",
        "result",
        "response"
      ],
      "anonymized_code": "def test_add(var_1, var_2, var_3, var_4):\n    var_5 = var_1.post(\"/add\", data={\"var_2\": var_2, \"var_3\": var_3})\n    assert var_5.get_json()[\"var_4\"] == var_4\n",
      "lines_processed": 3,
      "total_lines": 27,
      "llm_code": "def test_add(a, b, c, d):\n    var_5 = a.post(\"/add\", data={\"b\": b, \"c\": c})\n    assert var_5.get_json()[\"d\"] == d\n",
      "llm_variables": [
        "a",
        "b",
        "c",
        "d",
        "var_5"
      ]
    },
    {
      "file_path": "test_js_example.py",
      "code": "def test_index(app, client, path, template_name):\n    def check(sender, template, context):\n        assert template.name == template_name\n\n    with template_rendered.connected_to(check, app):\n        client.get(path)\n",
      "variables": [
        "app",
        "client",
        "path",
        "template_name",
        "sender",
        "template",
        "context"
      ],
      "anonymized_code": "def test_index(var_1, var_2, var_3, var_4):\n    def check(var_5, var_6, var_7):\n        assert var_6.name == var_4\n\n    with template_rendered.connected_to(check, var_1):\n        var_2.get(var_3)\n",
      "lines_processed": 6,
      "total_lines": 27,
      "llm_code": "def test_index(a, b, c, d):\n    def check(e, f, g):\n        assert f.name == d\n    \n    with template_rendered.connected_to(check, a):\n        b.get(c)\n",
      "llm_variables": [
        "a",
        "b",
        "c",
        "d",
        "e",
        "f",
        "g"
      ]
    },
    {
      "file_path": "conftest.py",
      "code": "def auth(client):\n    return AuthActions(client)\n",
      "variables": [
        "client"
      ],
      "anonymized_code": "def auth(var_1):\n    return AuthActions(var_1)\n",
      "lines_processed": 2,
      "total_lines": 62,
      "llm_code": "def auth(user):\n    return AuthActions(user)\n",
      "llm_variables": [
        "user"
      ]
    },
    {
      "file_path": "conftest.py",
      "code": "def app():\n    \"\"\"Create and configure a new app instance for each test.\"\"\"\n    # create a temporary file to isolate the database for each test\n    db_fd, db_path = tempfile.mkstemp()\n    # create the app with common test config\n    app = create_app({\"TESTING\": True, \"DATABASE\": db_path})\n\n    # create the database and load test data\n    with app.app_context():\n        init_db()\n        get_db().executescript(_data_sql)\n\n    yield app\n\n    # close and remove the temporary database\n    os.close(db_fd)\n    os.unlink(db_path)\n",
      "variables": [
        "db_fd",
        "db_path",
        "app"
      ],
      "anonymized_code": "def var_3():\n    \"\"\"Create and configure a new var_3 instance for each test.\"\"\"\n    # create a temporary file to isolate the database for each test\n    var_1, var_2 = tempfile.mkstemp()\n    # create the var_3 with common test config\n    var_3 = create_app({\"TESTING\": True, \"DATABASE\": var_2})\n\n    # create the database and load test data\n    with var_3.app_context():\n        init_db()\n        get_db().executescript(_data_sql)\n\n    yield var_3\n\n    # close and remove the temporary database\n    os.close(var_1)\n    os.unlink(var_2)\n",
      "lines_processed": 17,
      "total_lines": 62,
      "llm_code": "def app():\n    \"\"\"Create and configure a new app instance for each test.\"\"\"\n    # create a temporary file to isolate the database for each test\n    fd, temp_db_path = tempfile.mkstemp()\n    # create the app with common test config\n    app = create_app({\"TESTING\": True, \"DATABASE\": temp_db_path})\n\n    # create the database and load test data\n    with app.app_context():\n        init_db()\n        get_db().executescript(_data_sql)\n\n    yield app\n\n    # close and remove the temporary database\n    os.close(fd)\n    os.unlink(temp_db_path)\n",
      "llm_variables": [
        "fd",
        "temp_db_path",
        "app"
      ]
    },
    {
      "file_path": "conftest.py",
      "code": "def client(app):\n    \"\"\"A test client for the app.\"\"\"\n    return app.test_client()\n",
      "variables": [
        "app"
      ],
      "anonymized_code": "def client(var_1):\n    \"\"\"A test client for the var_1.\"\"\"\n    return var_1.test_client()\n",
      "lines_processed": 3,
      "total_lines": 62,
      "llm_code": "def client(client):\n    \"\"\"A test client for the client.\"\"\"\n    return client.test_client()\n",
      "llm_variables": [
        "client"
      ]
    },
    {
      "file_path": "conftest.py",
      "code": "def runner(app):\n    \"\"\"A test runner for the app's Click commands.\"\"\"\n    return app.test_cli_runner()\n",
      "variables": [
        "app"
      ],
      "anonymized_code": "def runner(var_1):\n    \"\"\"A test runner for the var_1's Click commands.\"\"\"\n    return var_1.test_cli_runner()\n",
      "lines_processed": 3,
      "total_lines": 62,
      "llm_code": "def runner(cli):\n    \"\"\"A test runner for the cli's Click commands.\"\"\"\n    return cli.test_cli_runner()\n",
      "llm_variables": [
        "cli"
      ]
    },
    {
      "file_path": "tasks.py",
      "code": "def process(self: Task, total: int) -> object:\n    for i in range(total):\n        self.update_state(state=\"PROGRESS\", meta={\"current\": i + 1, \"total\": total})\n        time.sleep(1)\n\n    return {\"current\": total, \"total\": total}\n",
      "variables": [
        "self",
        "total",
        "i"
      ],
      "anonymized_code": "def process(var_1: Task, var_2: int) -> object:\n    for var_3 in range(var_2):\n        var_1.update_state(state=\"PROGRESS\", meta={\"current\": var_3 + 1, \"var_2\": var_2})\n        time.sleep(1)\n\n    return {\"current\": var_2, \"var_2\": var_2}\n",
      "lines_processed": 6,
      "total_lines": 23,
      "llm_code": "def process(task: Task, iterations: int) -> object:\n    for step in range(iterations):\n        task.update_state(state=\"PROGRESS\", meta={\"current\": step + 1, \"iterations\": iterations})\n        time.sleep(1)\n\n    return {\"current\": iterations, \"iterations\": iterations}\n",
      "llm_variables": [
        "task",
        "iterations",
        "step"
      ]
    },
    {
      "file_path": "tasks.py",
      "code": "def add(a: int, b: int) -> int:\n    return a + b\n",
      "variables": [
        "a",
        "b"
      ],
      "anonymized_code": "def add(var_1: int, var_2: int) -> int:\n    return var_1 + var_2\n",
      "lines_processed": 2,
      "total_lines": 23,
      "llm_code": "def add(a: int, b: int) -> int:\n    return a + b",
      "llm_variables": [
        "a",
        "b"
      ]
    },
    {
      "file_path": "test_auth.py",
      "code": "def test_login_validate_input(auth, username, password, message):\n    response = auth.login(username, password)\n    assert message in response.data\n",
      "variables": [
        "auth",
        "username",
        "password",
        "message",
        "response"
      ],
      "anonymized_code": "def test_login_validate_input(var_1, var_2, var_3, var_4):\n    var_5 = var_1.login(var_2, var_3)\n    assert var_4 in var_5.data\n",
      "lines_processed": 3,
      "total_lines": 69,
      "llm_code": "def test_login_validate_input(username, password, expected_data, input_data):\n    response = input_data.login(username, password)\n    assert expected_data in response.data",
      "llm_variables": [
        "username",
        "password",
        "expected_data",
        "input_data",
        "response"
      ]
    },
    {
      "file_path": "test_auth.py",
      "code": "def test_register(client, app):\n    # test that viewing the page renders without template errors\n    assert client.get(\"/auth/register\").status_code == 200\n\n    # test that successful registration redirects to the login page\n    response = client.post(\"/auth/register\", data={\"username\": \"a\", \"password\": \"a\"})\n    assert response.headers[\"Location\"] == \"/auth/login\"\n\n    # test that the user was inserted into the database\n    with app.app_context():\n        assert (\n            get_db().execute(\"SELECT * FROM user WHERE username = 'a'\").fetchone()\n            is not None\n        )\n",
      "variables": [
        "client",
        "app",
        "response"
      ],
      "anonymized_code": "def test_register(var_1, var_2):\n    # test that viewing the page renders without template errors\n    assert var_1.get(\"/auth/register\").status_code == 200\n\n    # test that successful registration redirects to the login page\n    var_3 = var_1.post(\"/auth/register\", data={\"username\": \"a\", \"password\": \"a\"})\n    assert var_3.headers[\"Location\"] == \"/auth/login\"\n\n    # test that the user was inserted into the database\n    with var_2.app_context():\n        assert (\n            get_db().execute(\"SELECT * FROM user WHERE username = 'a'\").fetchone()\n            is not None\n        )\n",
      "lines_processed": 14,
      "total_lines": 69,
      "llm_code": "def test_register(client, db):\n    # test that viewing the page renders without template errors\n    assert client.get(\"/auth/register\").status_code == 200\n\n    # test that successful registration redirects to the login page\n    response = client.post(\"/auth/register\", data={\"username\": \"a\", \"password\": \"a\"})\n    assert response.headers[\"Location\"] == \"/auth/login\"\n\n    # test that the user was inserted into the database\n    with db.app_context():\n        assert (\n            get_db().execute(\"SELECT * FROM user WHERE username = 'a'\").fetchone()\n            is not None\n        )\n",
      "llm_variables": [
        "client",
        "db",
        "response"
      ]
    },
    {
      "file_path": "test_auth.py",
      "code": "def test_register_validate_input(client, username, password, message):\n    response = client.post(\n        \"/auth/register\", data={\"username\": username, \"password\": password}\n    )\n    assert message in response.data\n",
      "variables": [
        "client",
        "username",
        "password",
        "message",
        "response"
      ],
      "anonymized_code": "def test_register_validate_input(var_1, var_2, var_3, var_4):\n    var_5 = var_1.post(\n        \"/auth/register\", data={\"var_2\": var_2, \"var_3\": var_3}\n    )\n    assert var_4 in var_5.data\n",
      "lines_processed": 5,
      "total_lines": 69,
      "llm_code": "def test_register_validate_input(first_name, last_name, email, password):\n    response = first_name.post(\n        \"/auth/register\", data={\"last_name\": last_name, \"email\": email}\n    )\n    assert password in response.data\n",
      "llm_variables": [
        "first_name",
        "last_name",
        "email",
        "password",
        "response"
      ]
    },
    {
      "file_path": "test_auth.py",
      "code": "def test_login(client, auth):\n    # test that viewing the page renders without template errors\n    assert client.get(\"/auth/login\").status_code == 200\n\n    # test that successful login redirects to the index page\n    response = auth.login()\n    assert response.headers[\"Location\"] == \"/\"\n\n    # login request set the user_id in the session\n    # check that the user is loaded from the session\n    with client:\n        client.get(\"/\")\n        assert session[\"user_id\"] == 1\n        assert g.user[\"username\"] == \"test\"\n",
      "variables": [
        "client",
        "auth",
        "response"
      ],
      "anonymized_code": "def test_login(var_1, var_2):\n    # test that viewing the page renders without template errors\n    assert var_1.get(\"/var_2/login\").status_code == 200\n\n    # test that successful login redirects to the index page\n    var_3 = var_2.login()\n    assert var_3.headers[\"Location\"] == \"/\"\n\n    # login request set the user_id in the session\n    # check that the user is loaded from the session\n    with var_1:\n        var_1.get(\"/\")\n        assert session[\"user_id\"] == 1\n        assert g.user[\"username\"] == \"test\"\n",
      "lines_processed": 14,
      "total_lines": 69,
      "llm_code": "def test_login(client, app):\n    # test that viewing the page renders without template errors\n    assert client.get(\"/app/login\").status_code == 200\n\n    # test that successful login redirects to the index page\n    response = app.login()\n    assert response.headers[\"Location\"] == \"/\"\n\n    # login request set the user_id in the session\n    # check that the user is loaded from the session\n    with client:\n        client.get(\"/\")\n        assert session[\"user_id\"] == 1\n        assert g.user[\"username\"] == \"test\"\n",
      "llm_variables": [
        "client",
        "app",
        "response"
      ]
    },
    {
      "file_path": "test_auth.py",
      "code": "def test_logout(client, auth):\n    auth.login()\n\n    with client:\n        auth.logout()\n        assert \"user_id\" not in session\n",
      "variables": [
        "client",
        "auth"
      ],
      "anonymized_code": "def test_logout(var_1, var_2):\n    var_2.login()\n\n    with var_1:\n        var_2.logout()\n        assert \"user_id\" not in session\n",
      "lines_processed": 6,
      "total_lines": 69,
      "llm_code": "def test_logout(user, session):\n    session.login()\n\n    with user:\n        session.logout()\n        assert \"user_id\" not in session\n",
      "llm_variables": [
        "user",
        "session"
      ]
    },
    {
      "file_path": "test_db.py",
      "code": "def test_init_db_command(runner, monkeypatch):\n    class Recorder:\n        called = False\n\n    def fake_init_db():\n        Recorder.called = True\n\n    monkeypatch.setattr(\"flaskr.db.init_db\", fake_init_db)\n    result = runner.invoke(args=[\"init-db\"])\n    assert \"Initialized\" in result.output\n    assert Recorder.called\n",
      "variables": [
        "runner",
        "monkeypatch",
        "called",
        "result"
      ],
      "anonymized_code": "def test_init_db_command(var_1, var_2):\n    class Recorder:\n        var_3 = False\n\n    def fake_init_db():\n        Recorder.var_3 = True\n\n    var_2.setattr(\"flaskr.db.init_db\", fake_init_db)\n    var_4 = var_1.invoke(args=[\"init-db\"])\n    assert \"Initialized\" in var_4.output\n    assert Recorder.var_3\n",
      "lines_processed": 11,
      "total_lines": 29,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "test_db.py",
      "code": "def test_get_close_db(app):\n    with app.app_context():\n        db = get_db()\n        assert db is get_db()\n\n    with pytest.raises(sqlite3.ProgrammingError) as e:\n        db.execute(\"SELECT 1\")\n\n    assert \"closed\" in str(e.value)\n",
      "variables": [
        "app",
        "db",
        "e"
      ],
      "anonymized_code": "def test_get_close_db(var_1):\n    with var_1.app_context():\n        var_2 = get_db()\n        assert var_2 is get_db()\n\n    with pytest.raises(sqlite3.ProgrammingError) as var_3:\n        var_2.execute(\"SELECT 1\")\n\n    assert \"closed\" in str(var_3.value)\n",
      "lines_processed": 9,
      "total_lines": 29,
      "llm_code": "def test_get_close_db(conn):\n    with conn.app_context():\n        db = get_db()\n        assert db is get_db()\n\n    with pytest.raises(sqlite3.ProgrammingError) as error:\n        db.execute(\"SELECT 1\")\n\n    assert \"closed\" in str(error.value)\n",
      "llm_variables": [
        "conn",
        "db",
        "error"
      ]
    },
    {
      "file_path": "conf.py",
      "code": "def setup(app):\n    app.add_role(\"gh\", github_link)\n",
      "variables": [
        "app"
      ],
      "anonymized_code": "def setup(var_1):\n    var_1.add_role(\"gh\", github_link)\n",
      "lines_processed": 2,
      "total_lines": 101,
      "llm_code": "def setup(account):\n    account.add_role(\"gh\", github_link)\n",
      "llm_variables": [
        "account"
      ]
    },
    {
      "file_path": "conf.py",
      "code": "def github_link(name, rawtext, text, lineno, inliner, options=None, content=None):\n    app = inliner.document.settings.env.app\n    release = app.config.release\n    base_url = \"https://github.com/pallets/flask/tree/\"\n\n    if text.endswith(\">\"):\n        words, text = text[:-1].rsplit(\"<\", 1)\n        words = words.strip()\n    else:\n        words = None\n\n    if packaging.version.parse(release).is_devrelease:\n        url = f\"{base_url}main/{text}\"\n    else:\n        url = f\"{base_url}{release}/{text}\"\n\n    if words is None:\n        words = url\n\n",
      "variables": [
        "name",
        "rawtext",
        "text",
        "lineno",
        "inliner",
        "options",
        "content",
        "app",
        "release",
        "base_url",
        "words",
        "url"
      ],
      "anonymized_code": "def github_link(var_1, var_2, var_3, var_4, var_5, var_6=None, var_7=None):\n    var_8 = var_5.document.settings.env.var_8\n    var_9 = var_8.config.var_9\n    var_10 = \"https://github.com/pallets/flask/tree/\"\n\n    if var_3.endswith(\">\"):\n        var_11, var_3 = var_3[:-1].rsplit(\"<\", 1)\n        var_11 = var_11.strip()\n    else:\n        var_11 = None\n\n    if packaging.version.parse(var_9).is_devrelease:\n        var_12 = f\"{var_10}main/{var_3}\"\n    else:\n        var_12 = f\"{var_10}{var_9}/{var_3}\"\n\n    if var_11 is None:\n        var_11 = var_12\n\n",
      "lines_processed": 19,
      "total_lines": 101,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "test_factory.py",
      "code": "def test_hello(client):\n    response = client.get(\"/hello\")\n    assert response.data == b\"Hello, World!\"\n",
      "variables": [
        "client",
        "response"
      ],
      "anonymized_code": "def test_hello(var_1):\n    var_2 = var_1.get(\"/hello\")\n    assert var_2.data == b\"Hello, World!\"\n",
      "lines_processed": 3,
      "total_lines": 12,
      "llm_code": "def test_hello(name):\n    greeting = name.get(\"/hello\")\n    assert greeting.data == b\"Hello, World!\"\n",
      "llm_variables": [
        "name",
        "greeting"
      ]
    },
    {
      "file_path": "blog.py",
      "code": "def delete(id):\n    \"\"\"Delete a post.\n\n    Ensures that the post exists and that the logged in user is the\n    author of the post.\n    \"\"\"\n    get_post(id)\n    db = get_db()\n    db.execute(\"DELETE FROM post WHERE id = ?\", (id,))\n    db.commit()\n    return redirect(url_for(\"blog.index\"))\n",
      "variables": [
        "id",
        "db"
      ],
      "anonymized_code": "def delete(var_1):\n    \"\"\"Delete a post.\n\n    Ensures that the post exists and that the logged in user is the\n    author of the post.\n    \"\"\"\n    get_post(var_1)\n    var_2 = get_db()\n    var_2.execute(\"DELETE FROM post WHERE var_1 = ?\", (var_1,))\n    var_2.commit()\n    return redirect(url_for(\"blog.index\"))\n",
      "lines_processed": 11,
      "total_lines": 125,
      "llm_code": "def delete(post_id):\n    \"\"\"Delete a post.\n\n    Ensures that the post exists and that the logged in user is the\n    author of the post.\n    \"\"\"\n    get_post(post_id)\n    db = get_db()\n    db.execute(\"DELETE FROM post WHERE post_id = ?\", (post_id,))\n    db.commit()\n    return redirect(url_for(\"blog.index\"))\n",
      "llm_variables": [
        "post_id",
        "db"
      ]
    },
    {
      "file_path": "blog.py",
      "code": "def update(id):\n    \"\"\"Update a post if the current user is the author.\"\"\"\n    post = get_post(id)\n\n    if request.method == \"POST\":\n        title = request.form[\"title\"]\n        body = request.form[\"body\"]\n        error = None\n\n        if not title:\n            error = \"Title is required.\"\n\n        if error is not None:\n            flash(error)\n        else:\n            db = get_db()\n            db.execute(\n                \"UPDATE post SET title = ?, body = ? WHERE id = ?\", (title, body, id)\n            )\n",
      "variables": [
        "id",
        "post",
        "title",
        "body",
        "error",
        "db"
      ],
      "anonymized_code": "def update(var_1):\n    \"\"\"Update a var_2 if the current user is the author.\"\"\"\n    var_2 = get_post(var_1)\n\n    if request.method == \"POST\":\n        var_3 = request.form[\"var_3\"]\n        var_4 = request.form[\"var_4\"]\n        var_5 = None\n\n        if not var_3:\n            var_5 = \"Title is required.\"\n\n        if var_5 is not None:\n            flash(var_5)\n        else:\n            var_6 = get_db()\n            var_6.execute(\n                \"UPDATE var_2 SET var_3 = ?, var_4 = ? WHERE var_1 = ?\", (var_3, var_4, var_1)\n            )\n",
      "lines_processed": 19,
      "total_lines": 125,
      "llm_code": "def update(post_id):\n    \"\"\"Update a post if the current user is the author.\"\"\"\n    post = get_post(post_id)\n\n    if request.method == \"POST\":\n        title = request.form[\"title\"]\n        content = request.form[\"content\"]\n        error = None\n\n        if not title:\n            error = \"Title is required.\"\n\n        if error is not None:\n            flash(error)\n        else:\n            db = get_db()\n            db.execute(\n                \"UPDATE post SET title =?, content =? WHERE id =?\",\n                (title, content, post_id)\n            )\n",
      "llm_variables": [
        "post_id",
        "post",
        "title",
        "content",
        "error",
        "db"
      ]
    },
    {
      "file_path": "blog.py",
      "code": "def index():\n    \"\"\"Show all the posts, most recent first.\"\"\"\n    db = get_db()\n    posts = db.execute(\n        \"SELECT p.id, title, body, created, author_id, username\"\n        \" FROM post p JOIN user u ON p.author_id = u.id\"\n        \" ORDER BY created DESC\"\n    ).fetchall()\n    return render_template(\"blog/index.html\", posts=posts)\n",
      "variables": [
        "db",
        "posts"
      ],
      "anonymized_code": "def index():\n    \"\"\"Show all the var_2, most recent first.\"\"\"\n    var_1 = get_db()\n    var_2 = var_1.execute(\n        \"SELECT p.id, title, body, created, author_id, username\"\n        \" FROM post p JOIN user u ON p.author_id = u.id\"\n        \" ORDER BY created DESC\"\n    ).fetchall()\n    return render_template(\"blog/index.html\", var_2=var_2)\n",
      "lines_processed": 9,
      "total_lines": 125,
      "llm_code": "def index():\n    \"\"\"Show all the posts, most recent first.\"\"\"\n    db = get_db()\n    posts = db.execute(\n        \"SELECT p.id, title, body, created, author_id, username\"\n        \" FROM post p JOIN user u ON p.author_id = u.id\"\n        \" ORDER BY created DESC\"\n    ).fetchall()\n    return render_template(\"blog/index.html\", posts=posts)\n",
      "llm_variables": [
        "db",
        "posts"
      ]
    },
    {
      "file_path": "blog.py",
      "code": "def create():\n    \"\"\"Create a new post for the current user.\"\"\"\n    if request.method == \"POST\":\n        title = request.form[\"title\"]\n        body = request.form[\"body\"]\n        error = None\n\n        if not title:\n            error = \"Title is required.\"\n\n        if error is not None:\n            flash(error)\n        else:\n            db = get_db()\n            db.execute(\n                \"INSERT INTO post (title, body, author_id) VALUES (?, ?, ?)\",\n                (title, body, g.user[\"id\"]),\n            )\n            db.commit()\n",
      "variables": [
        "title",
        "body",
        "error",
        "db"
      ],
      "anonymized_code": "def create():\n    \"\"\"Create a new post for the current user.\"\"\"\n    if request.method == \"POST\":\n        var_1 = request.form[\"var_1\"]\n        var_2 = request.form[\"var_2\"]\n        var_3 = None\n\n        if not var_1:\n            var_3 = \"Title is required.\"\n\n        if var_3 is not None:\n            flash(var_3)\n        else:\n            var_4 = get_db()\n            var_4.execute(\n                \"INSERT INTO post (var_1, var_2, author_id) VALUES (?, ?, ?)\",\n                (var_1, var_2, g.user[\"id\"]),\n            )\n            var_4.commit()\n",
      "lines_processed": 19,
      "total_lines": 125,
      "llm_code": "def create():\n    \"\"\"Create a new post for the current user.\"\"\"\n    if request.method == \"POST\":\n        title = request.form[\"title\"]\n        content = request.form[\"content\"]\n        error = None\n\n        if not title:\n            error = \"Title is required.\"\n\n        if error is not None:\n            flash(error)\n        else:\n            db = get_db()\n            db.execute(\n                \"INSERT INTO post (title, content, author_id) VALUES (?, ?, ?)\",\n                (title, content, g.user[\"id\"]),\n            )\n            db.commit()\n",
      "llm_variables": [
        "title",
        "content",
        "error",
        "db"
      ]
    },
    {
      "file_path": "views.py",
      "code": "def index(js):\n    return render_template(f\"{js}.html\", js=js)\n",
      "variables": [
        "js"
      ],
      "anonymized_code": "def index(var_1):\n    return render_template(f\"{var_1}.html\", var_1=var_1)\n",
      "lines_processed": 2,
      "total_lines": 18,
      "llm_code": "def index(name):\n    return render_template(f\"{name}.html\", name=name)",
      "llm_variables": [
        "name"
      ]
    },
    {
      "file_path": "views.py",
      "code": "def add():\n    a = request.form.get(\"a\", 0, type=float)\n    b = request.form.get(\"b\", 0, type=float)\n    return jsonify(result=a + b)\n",
      "variables": [
        "a",
        "b"
      ],
      "anonymized_code": "def add():\n    var_1 = request.form.get(\"var_1\", 0, type=float)\n    var_2 = request.form.get(\"var_2\", 0, type=float)\n    return jsonify(result=var_1 + var_2)\n",
      "lines_processed": 4,
      "total_lines": 18,
      "llm_code": "def add():\n    var_1 = request.form.get(\"var_1\", 0, type=float)\n    var_2 = request.form.get(\"var_2\", 0, type=float)\n    return jsonify(result=var_1 + var_2)\n",
      "llm_variables": [
        "var_1",
        "var_2"
      ]
    },
    {
      "file_path": "test_blog.py",
      "code": "def test_delete(client, auth, app):\n    auth.login()\n    response = client.post(\"/1/delete\")\n    assert response.headers[\"Location\"] == \"/\"\n\n    with app.app_context():\n        db = get_db()\n        post = db.execute(\"SELECT * FROM post WHERE id = 1\").fetchone()\n        assert post is None\n",
      "variables": [
        "client",
        "auth",
        "app",
        "response",
        "db",
        "post"
      ],
      "anonymized_code": "def test_delete(var_1, var_2, var_3):\n    var_2.login()\n    var_4 = var_1.var_6(\"/1/delete\")\n    assert var_4.headers[\"Location\"] == \"/\"\n\n    with var_3.app_context():\n        var_5 = get_db()\n        var_6 = var_5.execute(\"SELECT * FROM var_6 WHERE id = 1\").fetchone()\n        assert var_6 is None\n",
      "lines_processed": 9,
      "total_lines": 83,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "test_blog.py",
      "code": "def test_create_update_validate(client, auth, path):\n    auth.login()\n    response = client.post(path, data={\"title\": \"\", \"body\": \"\"})\n    assert b\"Title is required.\" in response.data\n",
      "variables": [
        "client",
        "auth",
        "path",
        "response"
      ],
      "anonymized_code": "def test_create_update_validate(var_1, var_2, var_3):\n    var_2.login()\n    var_4 = var_1.post(var_3, data={\"title\": \"\", \"body\": \"\"})\n    assert b\"Title is required.\" in var_4.data\n",
      "lines_processed": 4,
      "total_lines": 83,
      "llm_code": "def test_create_update_validate(user, client, db):\n    client.login()\n    response = user.post(db, data={\"title\": \"\", \"body\": \"\"})\n    assert b\"Title is required.\" in response.data\n",
      "llm_variables": [
        "user",
        "client",
        "db",
        "response"
      ]
    },
    {
      "file_path": "test_blog.py",
      "code": "def test_author_required(app, client, auth):\n    # change the post author to another user\n    with app.app_context():\n        db = get_db()\n        db.execute(\"UPDATE post SET author_id = 2 WHERE id = 1\")\n        db.commit()\n\n    auth.login()\n    # current user can't modify other user's post\n    assert client.post(\"/1/update\").status_code == 403\n    assert client.post(\"/1/delete\").status_code == 403\n    # current user doesn't see edit link\n    assert b'href=\"/1/update\"' not in client.get(\"/\").data\n",
      "variables": [
        "app",
        "client",
        "auth",
        "db"
      ],
      "anonymized_code": "def test_author_required(var_1, var_2, var_3):\n    # change the post author to another user\n    with var_1.app_context():\n        var_4 = get_db()\n        var_4.execute(\"UPDATE post SET author_id = 2 WHERE id = 1\")\n        var_4.commit()\n\n    var_3.login()\n    # current user can't modify other user's post\n    assert var_2.post(\"/1/update\").status_code == 403\n    assert var_2.post(\"/1/delete\").status_code == 403\n    # current user doesn't see edit link\n    assert b'href=\"/1/update\"' not in var_2.get(\"/\").data\n",
      "lines_processed": 13,
      "total_lines": 83,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "test_blog.py",
      "code": "def test_update(client, auth, app):\n    auth.login()\n    assert client.get(\"/1/update\").status_code == 200\n    client.post(\"/1/update\", data={\"title\": \"updated\", \"body\": \"\"})\n\n    with app.app_context():\n        db = get_db()\n        post = db.execute(\"SELECT * FROM post WHERE id = 1\").fetchone()\n        assert post[\"title\"] == \"updated\"\n",
      "variables": [
        "client",
        "auth",
        "app",
        "db",
        "post"
      ],
      "anonymized_code": "def test_update(var_1, var_2, var_3):\n    var_2.login()\n    assert var_1.get(\"/1/update\").status_code == 200\n    var_1.var_5(\"/1/update\", data={\"title\": \"updated\", \"body\": \"\"})\n\n    with var_3.app_context():\n        var_4 = get_db()\n        var_5 = var_4.execute(\"SELECT * FROM var_5 WHERE id = 1\").fetchone()\n        assert var_5[\"title\"] == \"updated\"\n",
      "lines_processed": 9,
      "total_lines": 83,
      "llm_code": "def test_update(user, db, app):\n    db.login()\n    assert user.get(\"/1/update\").status_code == 200\n    user.db_result(\"/1/update\", data={\"title\": \"updated\", \"body\": \"\"})\n\n    with app.app_context():\n        db = get_db()\n        db_result = db.execute(\"SELECT * FROM db_result WHERE id = 1\").fetchone()\n        assert db_result[\"title\"] == \"updated\"\n",
      "llm_variables": [
        "user",
        "db",
        "app",
        "db",
        "db_result"
      ]
    },
    {
      "file_path": "test_blog.py",
      "code": "def test_exists_required(client, auth, path):\n    auth.login()\n    assert client.post(path).status_code == 404\n",
      "variables": [
        "client",
        "auth",
        "path"
      ],
      "anonymized_code": "def test_exists_required(var_1, var_2, var_3):\n    var_2.login()\n    assert var_1.post(var_3).status_code == 404\n",
      "lines_processed": 3,
      "total_lines": 83,
      "llm_code": "def test_exists_required(user, login, post):\n    login.login()\n    assert user.post(post).status_code == 404",
      "llm_variables": [
        "user",
        "login",
        "post"
      ]
    },
    {
      "file_path": "test_blog.py",
      "code": "def test_index(client, auth):\n    response = client.get(\"/\")\n    assert b\"Log In\" in response.data\n    assert b\"Register\" in response.data\n\n    auth.login()\n    response = client.get(\"/\")\n    assert b\"test title\" in response.data\n    assert b\"by test on 2018-01-01\" in response.data\n    assert b\"test\\nbody\" in response.data\n    assert b'href=\"/1/update\"' in response.data\n",
      "variables": [
        "client",
        "auth",
        "response"
      ],
      "anonymized_code": "def test_index(var_1, var_2):\n    var_3 = var_1.get(\"/\")\n    assert b\"Log In\" in var_3.data\n    assert b\"Register\" in var_3.data\n\n    var_2.login()\n    var_3 = var_1.get(\"/\")\n    assert b\"test title\" in var_3.data\n    assert b\"by test on 2018-01-01\" in var_3.data\n    assert b\"test\\nbody\" in var_3.data\n    assert b'href=\"/1/update\"' in var_3.data\n",
      "lines_processed": 11,
      "total_lines": 83,
      "llm_code": "def test_index(client, user):\n    response = client.get(\"/\")\n    assert b\"Log In\" in response.data\n    assert b\"Register\" in response.data\n\n    user.login()\n    response = client.get(\"/\")\n    assert b\"test title\" in response.data\n    assert b\"by test on 2018-01-01\" in response.data\n    assert b\"test\\nbody\" in response.data\n    assert b'href=\"/1/update\"' in response.data\n",
      "llm_variables": [
        "client",
        "user",
        "response"
      ]
    },
    {
      "file_path": "test_blog.py",
      "code": "def test_login_required(client, path):\n    response = client.post(path)\n    assert response.headers[\"Location\"] == \"/auth/login\"\n",
      "variables": [
        "client",
        "path",
        "response"
      ],
      "anonymized_code": "def test_login_required(var_1, var_2):\n    var_3 = var_1.post(var_2)\n    assert var_3.headers[\"Location\"] == \"/auth/login\"\n",
      "lines_processed": 3,
      "total_lines": 83,
      "llm_code": "def test_login_required(client, url):\n    response = client.post(url)\n    assert response.headers[\"Location\"] == \"/auth/login\"\n",
      "llm_variables": [
        "client",
        "url",
        "response"
      ]
    },
    {
      "file_path": "test_blog.py",
      "code": "def test_create(client, auth, app):\n    auth.login()\n    assert client.get(\"/create\").status_code == 200\n    client.post(\"/create\", data={\"title\": \"created\", \"body\": \"\"})\n\n    with app.app_context():\n        db = get_db()\n        count = db.execute(\"SELECT COUNT(id) FROM post\").fetchone()[0]\n        assert count == 2\n",
      "variables": [
        "client",
        "auth",
        "app",
        "db",
        "count"
      ],
      "anonymized_code": "def test_create(var_1, var_2, var_3):\n    var_2.login()\n    assert var_1.get(\"/create\").status_code == 200\n    var_1.post(\"/create\", data={\"title\": \"created\", \"body\": \"\"})\n\n    with var_3.app_context():\n        var_4 = get_db()\n        var_5 = var_4.execute(\"SELECT COUNT(id) FROM post\").fetchone()[0]\n        assert var_5 == 2\n",
      "lines_processed": 9,
      "total_lines": 83,
      "llm_code": "def test_create(user, client, db):\n    client.login()\n    assert user.get(\"/create\").status_code == 200\n    user.post(\"/create\", data={\"title\": \"created\", \"body\": \"\"})\n\n    with db.app_context():\n        db_ = get_db()\n        count = db_.execute(\"SELECT COUNT(id) FROM post\").fetchone()[0]\n        assert count == 2\n",
      "llm_variables": [
        "user",
        "client",
        "db",
        "db_",
        "count"
      ]
    }
  ],
  "huggingface_diffusers": [
    {
      "file_path": "train_dreambooth_lora_sd15_advanced.py",
      "code": "def save_model_card(\n    repo_id: str,\n    use_dora: bool,\n    images: list = None,\n    base_model: str = None,\n    train_text_encoder=False,\n    train_text_encoder_ti=False,\n    token_abstraction_dict=None,\n    instance_prompt=None,\n    validation_prompt=None,\n    repo_folder=None,\n    vae_path=None,\n):\n    lora = \"lora\" if not use_dora else \"dora\"\n\n    widget_dict = []\n    if images is not None:\n        for i, image in enumerate(images):\n            image.save(os.path.join(repo_folder, f\"image_{i}.png\"))\n",
      "variables": [
        "repo_id",
        "use_dora",
        "images",
        "base_model",
        "train_text_encoder",
        "train_text_encoder_ti",
        "token_abstraction_dict",
        "instance_prompt",
        "validation_prompt",
        "repo_folder",
        "vae_path",
        "lora",
        "widget_dict",
        "i",
        "image"
      ],
      "anonymized_code": "def save_model_card(\n    var_1: str,\n    var_2: bool,\n    var_3: list = None,\n    var_4: str = None,\n    var_5=False,\n    var_6=False,\n    var_7=None,\n    var_8=None,\n    var_9=None,\n    var_10=None,\n    var_11=None,\n):\n    var_12 = \"var_12\" if not var_2 else \"dora\"\n\n    var_13 = []\n    if var_3 is not None:\n        for var_14, var_15 in enumerate(var_3):\n            var_15.save(os.path.join(var_10, f\"image_{var_14}.png\"))\n",
      "lines_processed": 19,
      "total_lines": 2066,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "train_dreambooth_lora_sd15_advanced.py",
      "code": "def tokenize_prompt(tokenizer, prompt, add_special_tokens=False):\n    text_inputs = tokenizer(\n        prompt,\n        padding=\"max_length\",\n        max_length=tokenizer.model_max_length,\n        truncation=True,\n        add_special_tokens=add_special_tokens,\n        return_tensors=\"pt\",\n    )\n    text_input_ids = text_inputs.input_ids\n    return text_input_ids\n",
      "variables": [
        "tokenizer",
        "prompt",
        "add_special_tokens",
        "text_inputs",
        "text_input_ids"
      ],
      "anonymized_code": "def tokenize_prompt(var_1, var_2, var_3=False):\n    var_4 = var_1(\n        var_2,\n        padding=\"max_length\",\n        max_length=var_1.model_max_length,\n        truncation=True,\n        var_3=var_3,\n        return_tensors=\"pt\",\n    )\n    var_5 = var_4.input_ids\n    return var_5\n",
      "lines_processed": 11,
      "total_lines": 2066,
      "llm_code": "def tokenize_prompt(prompt, tokenizer, model_max_length=False):\n    tokenized = prompt(\n        tokenizer,\n        padding=\"max_length\",\n        max_length=prompt.model_max_length,\n        truncation=True,\n        model_max_length=model_max_length,\n        return_tensors=\"pt\",\n    )\n    input_ids = tokenized.input_ids\n    return input_ids\n",
      "llm_variables": [
        "prompt",
        "tokenizer",
        "model_max_length",
        "tokenized",
        "input_ids"
      ]
    },
    {
      "file_path": "train_dreambooth_lora_sd15_advanced.py",
      "code": "def encode_prompt(text_encoders, tokenizers, prompt, text_input_ids_list=None):\n    for i, text_encoder in enumerate(text_encoders):\n        if tokenizers is not None:\n            tokenizer = tokenizers[i]\n            text_input_ids = tokenize_prompt(tokenizer, prompt)\n        else:\n            assert text_input_ids_list is not None\n            text_input_ids = text_input_ids_list[i]\n\n        prompt_embeds = text_encoder(\n            text_input_ids.to(text_encoder.device),\n            output_hidden_states=True,\n        )\n\n    return prompt_embeds[0]\n",
      "variables": [
        "text_encoders",
        "tokenizers",
        "prompt",
        "text_input_ids_list",
        "i",
        "text_encoder",
        "tokenizer",
        "text_input_ids",
        "prompt_embeds"
      ],
      "anonymized_code": "def encode_prompt(var_1, var_2, var_3, var_4=None):\n    for var_5, var_6 in enumerate(var_1):\n        if var_2 is not None:\n            var_7 = var_2[var_5]\n            var_8 = tokenize_prompt(var_7, var_3)\n        else:\n            assert var_4 is not None\n            var_8 = var_4[var_5]\n\n        var_9 = var_6(\n            var_8.to(var_6.device),\n            output_hidden_states=True,\n        )\n\n    return var_9[0]\n",
      "lines_processed": 15,
      "total_lines": 2066,
      "llm_code": "def encode_prompt(prompt, tokenizer, model, text_encodings=None):\n    for idx, token in enumerate(prompt):\n        if tokenizer is not None:\n            encoded_token = tokenizer[token]\n            tokenized_prompt = tokenize_prompt(encoded_token, model)\n        else:\n            assert text_encodings is not None\n            tokenized_prompt = text_encodings[idx]\n\n        token_output = token(\n            tokenized_prompt.to(token.device),\n            output_hidden_states=True,\n        )\n\n    return token_output[0]",
      "llm_variables": [
        "prompt",
        "tokenizer",
        "model",
        "text_encodings",
        "idx",
        "token",
        "encoded_token",
        "tokenized_prompt",
        "token_output"
      ]
    },
    {
      "file_path": "train_cogvideox_image_to_video_lora.py",
      "code": "def get_optimizer(args, params_to_optimize, use_deepspeed: bool = False):\n    # Use DeepSpeed optimizer\n    if use_deepspeed:\n        from accelerate.utils import DummyOptim\n\n        return DummyOptim(\n            params_to_optimize,\n            lr=args.learning_rate,\n            betas=(args.adam_beta1, args.adam_beta2),\n            eps=args.adam_epsilon,\n            weight_decay=args.adam_weight_decay,\n        )\n\n    # Optimizer creation\n    supported_optimizers = [\"adam\", \"adamw\", \"prodigy\"]\n    if args.optimizer not in supported_optimizers:\n        logger.warning(\n            f\"Unsupported choice of optimizer: {args.optimizer}. Supported optimizers include {supported_optimizers}. Defaulting to AdamW\"\n        )\n",
      "variables": [
        "args",
        "params_to_optimize",
        "use_deepspeed",
        "supported_optimizers"
      ],
      "anonymized_code": "def get_optimizer(var_1, var_2, var_3: bool = False):\n    # Use DeepSpeed optimizer\n    if var_3:\n        from accelerate.utils import DummyOptim\n\n        return DummyOptim(\n            var_2,\n            lr=var_1.learning_rate,\n            betas=(var_1.adam_beta1, var_1.adam_beta2),\n            eps=var_1.adam_epsilon,\n            weight_decay=var_1.adam_weight_decay,\n        )\n\n    # Optimizer creation\n    var_4 = [\"adam\", \"adamw\", \"prodigy\"]\n    if var_1.optimizer not in var_4:\n        logger.warning(\n            f\"Unsupported choice of optimizer: {var_1.optimizer}. Supported optimizers include {var_4}. Defaulting to AdamW\"\n        )\n",
      "lines_processed": 19,
      "total_lines": 1619,
      "llm_code": "def get_optimizer(optimizer, optimizer_params, use_dummy_optim: bool = False):\n    # Use DeepSpeed optimizer\n    if use_dummy_optim:\n        from accelerate.utils import DummyOptim\n\n        return DummyOptim(\n            optimizer_params,\n            lr=optimizer.learning_rate,\n            betas=(optimizer.adam_beta1, optimizer.adam_beta2),\n            eps=optimizer.adam_epsilon,\n            weight_decay=optimizer.adam_weight_decay,\n        )\n\n    # Optimizer creation\n    var_4 = [\"adam\", \"adamw\", \"prodigy\"]\n    if optimizer.optimizer not in var_4:\n        logger.warning(\n            f\"Unsupported choice of optimizer: {optimizer.optimizer}. Supported optimizers include {var_4}. Defaulting to AdamW\"\n        )\n",
      "llm_variables": [
        "optimizer",
        "optimizer_params",
        "use_dummy_optim",
        "var_4"
      ]
    },
    {
      "file_path": "train_dreambooth_lora_flux_advanced.py",
      "code": "def collate_fn(examples, with_prior_preservation=False):\n    pixel_values = [example[\"instance_images\"] for example in examples]\n    prompts = [example[\"instance_prompt\"] for example in examples]\n\n    # Concat class and instance examples for prior preservation.\n    # We do this to avoid doing two forward passes.\n    if with_prior_preservation:\n        pixel_values += [example[\"class_images\"] for example in examples]\n        prompts += [example[\"class_prompt\"] for example in examples]\n\n    pixel_values = torch.stack(pixel_values)\n    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n\n    batch = {\"pixel_values\": pixel_values, \"prompts\": prompts}\n    return batch\n",
      "variables": [
        "examples",
        "with_prior_preservation",
        "pixel_values",
        "example",
        "prompts",
        "batch"
      ],
      "anonymized_code": "def collate_fn(var_1, var_2=False):\n    var_3 = [var_4[\"instance_images\"] for var_4 in var_1]\n    var_5 = [var_4[\"instance_prompt\"] for var_4 in var_1]\n\n    # Concat class and instance var_1 for prior preservation.\n    # We do this to avoid doing two forward passes.\n    if var_2:\n        var_3 += [var_4[\"class_images\"] for var_4 in var_1]\n        var_5 += [var_4[\"class_prompt\"] for var_4 in var_1]\n\n    var_3 = torch.stack(var_3)\n    var_3 = var_3.to(memory_format=torch.contiguous_format).float()\n\n    var_6 = {\"var_3\": var_3, \"var_5\": var_5}\n    return var_6\n",
      "lines_processed": 15,
      "total_lines": 2441,
      "llm_code": "def collate_fn(images, use_class=False):\n    instance_images = [instance_prompts[\"instance_images\"] for instance_prompts in images]\n    item = [instance_prompts[\"instance_prompt\"] for instance_prompts in images]\n\n    # Concat class and instance images for prior preservation.\n    # We do this to avoid doing two forward passes.\n    if use_class:\n        instance_images += [instance_prompts[\"class_images\"] for instance_prompts in images]\n        item += [instance_prompts[\"class_prompt\"] for instance_prompts in images]\n\n    instance_images = torch.stack(instance_images)\n    instance_images = instance_images.to(memory_format=torch.contiguous_format).float()\n\n    output = {\"instance_images\": instance_images, \"item\": item}\n    return output\n",
      "llm_variables": [
        "images",
        "use_class",
        "instance_images",
        "instance_prompts",
        "item",
        "output"
      ]
    },
    {
      "file_path": "train_amused.py",
      "code": "def process_image(image, size):\n    image = exif_transpose(image)\n\n    if not image.mode == \"RGB\":\n        image = image.convert(\"RGB\")\n\n    orig_height = image.height\n    orig_width = image.width\n\n    image = transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR)(image)\n\n    c_top, c_left, _, _ = transforms.RandomCrop.get_params(image, output_size=(size, size))\n    image = transforms.functional.crop(image, c_top, c_left, size, size)\n\n    image = transforms.ToTensor()(image)\n\n    micro_conds = torch.tensor(\n        [orig_width, orig_height, c_top, c_left, 6.0],\n    )\n",
      "variables": [
        "image",
        "size",
        "orig_height",
        "orig_width",
        "c_top",
        "c_left",
        "_",
        "micro_conds"
      ],
      "anonymized_code": "def process_image(var_1, var_2):\n    var_1 = exif_transpose(var_1)\n\n    if not var_1.mode == \"RGB\":\n        var_1 = var_1.convert(\"RGB\")\n\n    var_3 = var_1.height\n    var_4 = var_1.width\n\n    var_1 = transforms.Resize(var_2, interpolation=transforms.InterpolationMode.BILINEAR)(var_1)\n\n    var_5, var_6, var_7, var_7 = transforms.RandomCrop.get_params(var_1, output_size=(var_2, var_2))\n    var_1 = transforms.functional.crop(var_1, var_5, var_6, var_2, var_2)\n\n    var_1 = transforms.ToTensor()(var_1)\n\n    var_8 = torch.tensor(\n        [var_4, var_3, var_5, var_6, 6.0],\n    )\n",
      "lines_processed": 19,
      "total_lines": 975,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "train_amused.py",
      "code": "def encode_prompt(text_encoder, input_ids):\n    outputs = text_encoder(input_ids, return_dict=True, output_hidden_states=True)\n    encoder_hidden_states = outputs.hidden_states[-2]\n    cond_embeds = outputs[0]\n    return encoder_hidden_states, cond_embeds\n",
      "variables": [
        "text_encoder",
        "input_ids",
        "outputs",
        "encoder_hidden_states",
        "cond_embeds"
      ],
      "anonymized_code": "def encode_prompt(var_1, var_2):\n    var_3 = var_1(var_2, return_dict=True, output_hidden_states=True)\n    var_4 = var_3.hidden_states[-2]\n    var_5 = var_3[0]\n    return var_4, var_5\n",
      "lines_processed": 5,
      "total_lines": 975,
      "llm_code": "def encode_prompt(prompt, model):\n    output = model(prompt, return_dict=True, output_hidden_states=True)\n    hidden_states = output.hidden_states[-2]\n    first_output = output[0]\n    return hidden_states, first_output",
      "llm_variables": [
        "prompt",
        "model",
        "output",
        "hidden_states",
        "first_output"
      ]
    },
    {
      "file_path": "train_amused.py",
      "code": "def save_checkpoint(args, accelerator, global_step):\n    output_dir = args.output_dir\n\n    # _before_ saving state, check if this save would set us over the `checkpoints_total_limit`\n    if accelerator.is_main_process and args.checkpoints_total_limit is not None:\n        checkpoints = os.listdir(output_dir)\n        checkpoints = [d for d in checkpoints if d.startswith(\"checkpoint\")]\n        checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n\n        # before we save the new checkpoint, we need to have at _most_ `checkpoints_total_limit - 1` checkpoints\n        if len(checkpoints) >= args.checkpoints_total_limit:\n            num_to_remove = len(checkpoints) - args.checkpoints_total_limit + 1\n            removing_checkpoints = checkpoints[0:num_to_remove]\n\n            logger.info(\n                f\"{len(checkpoints)} checkpoints already exist, removing {len(removing_checkpoints)} checkpoints\"\n            )\n            logger.info(f\"removing checkpoints: {', '.join(removing_checkpoints)}\")\n\n",
      "variables": [
        "args",
        "accelerator",
        "global_step",
        "output_dir",
        "checkpoints",
        "d",
        "x",
        "num_to_remove",
        "removing_checkpoints"
      ],
      "anonymized_code": "def save_checkpoint(var_1, var_2, var_3):\n    var_4 = var_1.var_4\n\n    # _before_ saving state, check if this save would set us over the `checkpoints_total_limit`\n    if var_2.is_main_process and var_1.checkpoints_total_limit is not None:\n        var_5 = os.listdir(var_4)\n        var_5 = [var_6 for var_6 in var_5 if var_6.startswith(\"checkpoint\")]\n        var_5 = sorted(var_5, key=lambda var_7: int(var_7.split(\"-\")[1]))\n\n        # before we save the new checkpoint, we need to have at _most_ `checkpoints_total_limit - 1` var_5\n        if len(var_5) >= var_1.checkpoints_total_limit:\n            var_8 = len(var_5) - var_1.checkpoints_total_limit + 1\n            var_9 = var_5[0:var_8]\n\n            logger.info(\n                f\"{len(var_5)} var_5 already exist, removing {len(var_9)} var_5\"\n            )\n            logger.info(f\"removing var_5: {', '.join(var_9)}\")\n\n",
      "lines_processed": 19,
      "total_lines": 975,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "train_control_cogview4.py",
      "code": "def log_validation(cogview4_transformer, args, accelerator, weight_dtype, step, is_final_validation=False):\n    logger.info(\"Running validation... \")\n\n    if not is_final_validation:\n        cogview4_transformer = accelerator.unwrap_model(cogview4_transformer)\n        pipeline = CogView4ControlPipeline.from_pretrained(\n            args.pretrained_model_name_or_path,\n            transformer=cogview4_transformer,\n            torch_dtype=weight_dtype,\n        )\n    else:\n        transformer = CogView4Transformer2DModel.from_pretrained(args.output_dir, torch_dtype=weight_dtype)\n        pipeline = CogView4ControlPipeline.from_pretrained(\n            args.pretrained_model_name_or_path,\n            transformer=transformer,\n            torch_dtype=weight_dtype,\n        )\n\n    pipeline.to(accelerator.device)\n",
      "variables": [
        "cogview4_transformer",
        "args",
        "accelerator",
        "weight_dtype",
        "step",
        "is_final_validation",
        "pipeline",
        "transformer"
      ],
      "anonymized_code": "def log_validation(var_1, var_2, var_3, var_4, var_5, var_6=False):\n    logger.info(\"Running validation... \")\n\n    if not var_6:\n        var_1 = var_3.unwrap_model(var_1)\n        var_7 = CogView4ControlPipeline.from_pretrained(\n            var_2.pretrained_model_name_or_path,\n            var_8=var_1,\n            torch_dtype=var_4,\n        )\n    else:\n        var_8 = CogView4Transformer2DModel.from_pretrained(var_2.output_dir, torch_dtype=var_4)\n        var_7 = CogView4ControlPipeline.from_pretrained(\n            var_2.pretrained_model_name_or_path,\n            var_8=var_8,\n            torch_dtype=var_4,\n        )\n\n    var_7.to(var_3.device)\n",
      "lines_processed": 19,
      "total_lines": 1242,
      "llm_code": "def log_validation(input_model, config, device, dtype, model_name, use_custom_model=False):\n    logger.info(\"Running validation... \")\n\n    if not use_custom_model:\n        input_model = device.unwrap_model(input_model)\n        var_7 = CogView4ControlPipeline.from_pretrained(\n            config.pretrained_model_name_or_path,\n            var_8=input_model,\n            torch_dtype=dtype,\n        )\n    else:\n        var_8 = CogView4Transformer2DModel.from_pretrained(config.output_dir, torch_dtype=dtype)\n        var_7 = CogView4ControlPipeline.from_pretrained(\n            config.pretrained_model_name_or_path,\n            var_8=var_8,\n            torch_dtype=dtype,\n        )\n\n    var_7.to(device.device)\n",
      "llm_variables": [
        "input_model",
        "config",
        "device",
        "dtype",
        "model_name",
        "use_custom_model",
        "var_7",
        "var_8"
      ]
    },
    {
      "file_path": "train_control_cogview4.py",
      "code": "def collate_fn(examples):\n    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n    conditioning_pixel_values = torch.stack([example[\"conditioning_pixel_values\"] for example in examples])\n    conditioning_pixel_values = conditioning_pixel_values.to(memory_format=torch.contiguous_format).float()\n    captions = [example[\"captions\"] for example in examples]\n    return {\"pixel_values\": pixel_values, \"conditioning_pixel_values\": conditioning_pixel_values, \"captions\": captions}\n",
      "variables": [
        "examples",
        "pixel_values",
        "example",
        "conditioning_pixel_values",
        "captions"
      ],
      "anonymized_code": "def collate_fn(var_1):\n    var_2 = torch.stack([var_3[\"var_2\"] for var_3 in var_1])\n    var_2 = var_2.to(memory_format=torch.contiguous_format).float()\n    var_4 = torch.stack([var_3[\"var_4\"] for var_3 in var_1])\n    var_4 = var_4.to(memory_format=torch.contiguous_format).float()\n    var_5 = [var_3[\"var_5\"] for var_3 in var_1]\n    return {\"var_2\": var_2, \"var_4\": var_4, \"var_5\": var_5}\n",
      "lines_processed": 7,
      "total_lines": 1242,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "train_cogvideox_lora.py",
      "code": "def get_optimizer(args, params_to_optimize, use_deepspeed: bool = False):\n    # Use DeepSpeed optimizer\n    if use_deepspeed:\n        from accelerate.utils import DummyOptim\n\n        return DummyOptim(\n            params_to_optimize,\n            lr=args.learning_rate,\n            betas=(args.adam_beta1, args.adam_beta2),\n            eps=args.adam_epsilon,\n            weight_decay=args.adam_weight_decay,\n        )\n\n    # Optimizer creation\n    supported_optimizers = [\"adam\", \"adamw\", \"prodigy\"]\n    if args.optimizer not in supported_optimizers:\n        logger.warning(\n            f\"Unsupported choice of optimizer: {args.optimizer}. Supported optimizers include {supported_optimizers}. Defaulting to AdamW\"\n        )\n",
      "variables": [
        "args",
        "params_to_optimize",
        "use_deepspeed",
        "supported_optimizers"
      ],
      "anonymized_code": "def get_optimizer(var_1, var_2, var_3: bool = False):\n    # Use DeepSpeed optimizer\n    if var_3:\n        from accelerate.utils import DummyOptim\n\n        return DummyOptim(\n            var_2,\n            lr=var_1.learning_rate,\n            betas=(var_1.adam_beta1, var_1.adam_beta2),\n            eps=var_1.adam_epsilon,\n            weight_decay=var_1.adam_weight_decay,\n        )\n\n    # Optimizer creation\n    var_4 = [\"adam\", \"adamw\", \"prodigy\"]\n    if var_1.optimizer not in var_4:\n        logger.warning(\n            f\"Unsupported choice of optimizer: {var_1.optimizer}. Supported optimizers include {var_4}. Defaulting to AdamW\"\n        )\n",
      "lines_processed": 19,
      "total_lines": 1607,
      "llm_code": "def get_optimizer(optimizer, params, use_dummy: bool = False):\n    # Use DeepSpeed optimizer\n    if use_dummy:\n        from accelerate.utils import DummyOptim\n\n        return DummyOptim(\n            params,\n            lr=optimizer.learning_rate,\n            betas=(optimizer.adam_beta1, optimizer.adam_beta2),\n            eps=optimizer.adam_epsilon,\n            weight_decay=optimizer.adam_weight_decay,\n        )\n\n    # Optimizer creation\n    var_4 = [\"adam\", \"adamw\", \"prodigy\"]\n    if optimizer.optimizer not in var_4:\n        logger.warning(\n            f\"Unsupported choice of optimizer: {optimizer.optimizer}. Supported optimizers include {var_4}. Defaulting to AdamW\"\n        )\n",
      "llm_variables": [
        "optimizer",
        "params",
        "use_dummy",
        "var_4"
      ]
    },
    {
      "file_path": "utils.py",
      "code": "def bytes_to_giga_bytes(bytes):\n    return f\"{(bytes / 1024 / 1024 / 1024):.3f}\"\n",
      "variables": [
        "bytes"
      ],
      "anonymized_code": "def bytes_to_giga_bytes(var_1):\n    return f\"{(var_1 / 1024 / 1024 / 1024):.3f}\"\n",
      "lines_processed": 2,
      "total_lines": 98,
      "llm_code": "def bytes_to_giga_bytes(bytes):\n    return f\"{(bytes / 1024 / 1024 / 1024):.3f}\"",
      "llm_variables": [
        "bytes"
      ]
    },
    {
      "file_path": "utils.py",
      "code": "def collate_csv(input_files: List[str], output_file: str):\n    \"\"\"Collates multiple identically structured CSVs into a single CSV file.\"\"\"\n    with open(output_file, mode=\"w\", newline=\"\") as outfile:\n        writer = csv.DictWriter(outfile, fieldnames=BENCHMARK_FIELDS)\n        writer.writeheader()\n\n        for file in input_files:\n            with open(file, mode=\"r\") as infile:\n                reader = csv.DictReader(infile)\n                for row in reader:\n                    writer.writerow(row)\n",
      "variables": [
        "input_files",
        "output_file",
        "outfile",
        "writer",
        "file",
        "infile",
        "reader",
        "row"
      ],
      "anonymized_code": "def collate_csv(var_1: List[str], var_2: str):\n    \"\"\"Collates multiple identically structured CSVs into a single CSV var_5.\"\"\"\n    with open(var_2, mode=\"w\", newline=\"\") as var_3:\n        var_4 = csv.DictWriter(var_3, fieldnames=BENCHMARK_FIELDS)\n        var_4.writeheader()\n\n        for var_5 in var_1:\n            with open(var_5, mode=\"r\") as var_6:\n                var_7 = csv.DictReader(var_6)\n                for var_8 in var_7:\n                    var_4.writerow(var_8)\n",
      "lines_processed": 11,
      "total_lines": 98,
      "llm_code": "def collate_csv(csv_files: List[str], output_file: str):\n    \"\"\"Collates multiple identically structured CSVs into a single CSV file.\"\"\"\n    with open(output_file, mode=\"w\", newline=\"\") as output:\n        writer = csv.DictWriter(output, fieldnames=BENCHMARK_FIELDS)\n        writer.writeheader()\n\n        for file in csv_files:\n            with open(file, mode=\"r\") as f:\n                reader = csv.DictReader(f)\n                for row in reader:\n                    writer.writerow(row)\n",
      "llm_variables": [
        "csv_files",
        "output_file",
        "output",
        "writer",
        "file",
        "f",
        "reader",
        "row"
      ]
    },
    {
      "file_path": "utils.py",
      "code": "def generate_csv_dict(\n    pipeline_cls: str, ckpt: str, args: argparse.Namespace, benchmark_info: BenchmarkInfo\n) -> Dict[str, Union[str, bool, float]]:\n    \"\"\"Packs benchmarking data into a dictionary for latter serialization.\"\"\"\n    data_dict = {\n        \"pipeline_cls\": pipeline_cls,\n        \"ckpt_id\": ckpt,\n        \"batch_size\": args.batch_size,\n        \"num_inference_steps\": args.num_inference_steps,\n        \"model_cpu_offload\": args.model_cpu_offload,\n        \"run_compile\": args.run_compile,\n        \"time (secs)\": benchmark_info.time,\n        \"memory (gbs)\": benchmark_info.memory,\n        \"actual_gpu_memory (gbs)\": f\"{(TOTAL_GPU_MEMORY):.3f}\",\n        \"github_sha\": GITHUB_SHA,\n    }\n    return data_dict\n",
      "variables": [
        "pipeline_cls",
        "ckpt",
        "args",
        "benchmark_info",
        "data_dict"
      ],
      "anonymized_code": "def generate_csv_dict(\n    var_1: str, var_2: str, var_3: argparse.Namespace, var_4: BenchmarkInfo\n) -> Dict[str, Union[str, bool, float]]:\n    \"\"\"Packs benchmarking data into a dictionary for latter serialization.\"\"\"\n    var_5 = {\n        \"var_1\": var_1,\n        \"ckpt_id\": var_2,\n        \"batch_size\": var_3.batch_size,\n        \"num_inference_steps\": var_3.num_inference_steps,\n        \"model_cpu_offload\": var_3.model_cpu_offload,\n        \"run_compile\": var_3.run_compile,\n        \"time (secs)\": var_4.time,\n        \"memory (gbs)\": var_4.memory,\n        \"actual_gpu_memory (gbs)\": f\"{(TOTAL_GPU_MEMORY):.3f}\",\n        \"github_sha\": GITHUB_SHA,\n    }\n    return var_5\n",
      "lines_processed": 17,
      "total_lines": 98,
      "llm_code": "def generate_csv_dict(\n    input_file: str, output_file: str, args: argparse.Namespace, benchmark_info: BenchmarkInfo\n) -> Dict[str, Union[str, bool, float]]:\n    \"\"\"Packs benchmarking data into a dictionary for latter serialization.\"\"\"\n    data = {\n        \"input_file\": input_file,\n        \"ckpt_id\": output_file,\n        \"batch_size\": args.batch_size,\n        \"num_inference_steps\": args.num_inference_steps,\n        \"model_cpu_offload\": args.model_cpu_offload,\n        \"run_compile\": args.run_compile,\n        \"time (secs)\": benchmark_info.time,\n        \"memory (gbs)\": benchmark_info.memory,\n        \"actual_gpu_memory (gbs)\": f\"{(TOTAL_GPU_MEMORY):.3f}\",\n        \"github_sha\": GITHUB_SHA,\n    }\n    return data\n",
      "llm_variables": [
        "input_file",
        "output_file",
        "args",
        "benchmark_info",
        "data"
      ]
    },
    {
      "file_path": "utils.py",
      "code": "def benchmark_fn(f, *args, **kwargs):\n    t0 = benchmark.Timer(\n        stmt=\"f(*args, **kwargs)\",\n        globals={\"args\": args, \"kwargs\": kwargs, \"f\": f},\n        num_threads=torch.get_num_threads(),\n    )\n    return f\"{(t0.blocked_autorange().mean):.3f}\"\n",
      "variables": [
        "f",
        "args",
        "kwargs",
        "t0"
      ],
      "anonymized_code": "def benchmark_fn(var_1, *var_2, **var_3):\n    var_4 = benchmark.Timer(\n        stmt=\"var_1(*var_2, **var_3)\",\n        globals={\"var_2\": var_2, \"var_3\": var_3, \"var_1\": var_1},\n        num_threads=torch.get_num_threads(),\n    )\n    return var_1\"{(var_4.blocked_autorange().mean):.3f}\"\n",
      "lines_processed": 7,
      "total_lines": 98,
      "llm_code": "def benchmark_fn(func, *args, **kwargs):\n    timer = benchmark.Timer(\n        stmt=\"func(*args, **kwargs)\",\n        globals={\"args\": args, \"kwargs\": kwargs, \"func\": func},\n        num_threads=torch.get_num_threads(),\n    )\n    return func\"{(timer.blocked_autorange().mean):.3f}\"\n",
      "llm_variables": [
        "func",
        "args",
        "kwargs",
        "timer"
      ]
    },
    {
      "file_path": "utils.py",
      "code": "def write_to_csv(file_name: str, data_dict: Dict[str, Union[str, bool, float]]):\n    \"\"\"Serializes a dictionary into a CSV file.\"\"\"\n    with open(file_name, mode=\"w\", newline=\"\") as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=BENCHMARK_FIELDS)\n        writer.writeheader()\n        writer.writerow(data_dict)\n",
      "variables": [
        "file_name",
        "data_dict",
        "csvfile",
        "writer"
      ],
      "anonymized_code": "def write_to_csv(var_1: str, var_2: Dict[str, Union[str, bool, float]]):\n    \"\"\"Serializes a dictionary into a CSV file.\"\"\"\n    with open(var_1, mode=\"w\", newline=\"\") as var_3:\n        var_4 = csv.DictWriter(var_3, fieldnames=BENCHMARK_FIELDS)\n        var_4.writeheader()\n        var_4.writerow(var_2)\n",
      "lines_processed": 6,
      "total_lines": 98,
      "llm_code": "def write_to_csv(filename: str, data: Dict[str, Union[str, bool, float]]):\n    \"\"\"Serializes a dictionary into a CSV file.\"\"\"\n    with open(filename, mode=\"w\", newline=\"\") as file:\n        writer = csv.DictWriter(file, fieldnames=BENCHMARK_FIELDS)\n        writer.writeheader()\n        writer.writerow(data)\n",
      "llm_variables": [
        "filename",
        "data",
        "file",
        "writer"
      ]
    },
    {
      "file_path": "run_all.py",
      "code": "def run_command(command: List[str], return_stdout=False):\n    \"\"\"\n    Runs `command` with `subprocess.check_output` and will potentially return the `stdout`. Will also properly capture\n    if an error occurred while running `command`\n    \"\"\"\n    try:\n        output = subprocess.check_output(command, stderr=subprocess.STDOUT)\n        if return_stdout:\n            if hasattr(output, \"decode\"):\n                output = output.decode(\"utf-8\")\n            return output\n    except subprocess.CalledProcessError as e:\n        raise SubprocessCallException(\n            f\"Command `{' '.join(command)}` failed with the following error:\\n\\n{e.output.decode()}\"\n        ) from e\n",
      "variables": [
        "command",
        "return_stdout",
        "output"
      ],
      "anonymized_code": "def run_command(var_1: List[str], var_2=False):\n    \"\"\"\n    Runs `var_1` with `subprocess.check_output` and will potentially return the `stdout`. Will also properly capture\n    if an error occurred while running `var_1`\n    \"\"\"\n    try:\n        var_3 = subprocess.check_output(var_1, stderr=subprocess.STDOUT)\n        if var_2:\n            if hasattr(var_3, \"decode\"):\n                var_3 = var_3.decode(\"utf-8\")\n            return var_3\n    except subprocess.CalledProcessError as e:\n        raise SubprocessCallException(\n            f\"Command `{' '.join(var_1)}` failed with the following error:\\n\\n{e.var_3.decode()}\"\n        ) from e\n",
      "lines_processed": 15,
      "total_lines": 101,
      "llm_code": "def run_command(commands: List[str], capture_output=False):\n    \"\"\"\n    Runs `commands` with `subprocess.check_output` and will potentially return the `stdout`. Will also properly capture\n    if an error occurred while running `commands`\n    \"\"\"\n    try:\n        output = subprocess.check_output(commands, stderr=subprocess.STDOUT)\n        if capture_output:\n            if hasattr(output, \"decode\"):\n                output = output.decode(\"utf-8\")\n            return output\n    except subprocess.CalledProcessError as e:\n        raise SubprocessCallException(\n            f\"Command `{' '.join(commands)}` failed with the following error:\\n\\n{e.output.decode()}\"\n        ) from e\n",
      "llm_variables": [
        "commands",
        "capture_output",
        "output"
      ]
    },
    {
      "file_path": "run_all.py",
      "code": "def main():\n    python_files = glob.glob(PATTERN)\n\n    for file in python_files:\n        print(f\"****** Running file: {file} ******\")\n\n        # Run with canonical settings.\n        if file != \"benchmark_text_to_image.py\" and file != \"benchmark_ip_adapters.py\":\n            command = f\"python {file}\"\n            run_command(command.split())\n\n            command += \" --run_compile\"\n            run_command(command.split())\n\n    # Run variants.\n    for file in python_files:\n        # See: https://github.com/pytorch/pytorch/issues/129637\n        if file == \"benchmark_ip_adapters.py\":\n            continue\n",
      "variables": [
        "python_files",
        "file",
        "command"
      ],
      "anonymized_code": "def main():\n    var_1 = glob.glob(PATTERN)\n\n    for var_2 in var_1:\n        print(f\"****** Running var_2: {var_2} ******\")\n\n        # Run with canonical settings.\n        if var_2 != \"benchmark_text_to_image.py\" and var_2 != \"benchmark_ip_adapters.py\":\n            var_3 = f\"python {var_2}\"\n            run_command(var_3.split())\n\n            var_3 += \" --run_compile\"\n            run_command(var_3.split())\n\n    # Run variants.\n    for var_2 in var_1:\n        # See: https://github.com/pytorch/pytorch/issues/129637\n        if var_2 == \"benchmark_ip_adapters.py\":\n            continue\n",
      "lines_processed": 19,
      "total_lines": 101,
      "llm_code": "def main():\n    files = glob.glob(PATTERN)\n\n    for file in files:\n        print(f\"****** Running file: {file} ******\")\n\n        # Run with canonical settings.\n        if file != \"benchmark_text_to_image.py\" and file != \"benchmark_ip_adapters.py\":\n            command = f\"python {file}\"\n            run_command(command.split())\n\n            command += \" --run_compile\"\n            run_command(command.split())\n\n    # Run variants.\n    for file in files:\n        # See: https://github.com/pytorch/pytorch/issues/129637\n        if file == \"benchmark_ip_adapters.py\":\n            continue\n",
      "llm_variables": [
        "files",
        "file",
        "command"
      ]
    },
    {
      "file_path": "push_results.py",
      "code": "def push_to_hf_dataset():\n    all_csvs = sorted(glob.glob(f\"{BASE_PATH}/*.csv\"))\n    collate_csv(all_csvs, FINAL_CSV_FILE)\n\n    # If there's an existing benchmark file, we should report the changes.\n    csv_path = has_previous_benchmark()\n    if csv_path is not None:\n        current_results = pd.read_csv(FINAL_CSV_FILE)\n        previous_results = pd.read_csv(csv_path)\n\n        numeric_columns = current_results.select_dtypes(include=[\"float64\", \"int64\"]).columns\n        numeric_columns = [\n            c for c in numeric_columns if c not in [\"batch_size\", \"num_inference_steps\", \"actual_gpu_memory (gbs)\"]\n        ]\n\n        for column in numeric_columns:\n            previous_results[column] = previous_results[column].map(lambda x: filter_float(x))\n\n            # Calculate the percentage change\n",
      "variables": [
        "all_csvs",
        "csv_path",
        "current_results",
        "previous_results",
        "numeric_columns",
        "c",
        "column",
        "x"
      ],
      "anonymized_code": "def push_to_hf_dataset():\n    var_1 = sorted(glob.glob(f\"{BASE_PATH}/*.csv\"))\n    collate_csv(var_1, FINAL_CSV_FILE)\n\n    # If there's an existing benchmark file, we should report the changes.\n    var_2 = has_previous_benchmark()\n    if var_2 is not None:\n        var_3 = pd.read_csv(FINAL_CSV_FILE)\n        var_4 = pd.read_csv(var_2)\n\n        var_5 = var_3.select_dtypes(include=[\"float64\", \"int64\"]).columns\n        var_5 = [\n            var_6 for var_6 in var_5 if var_6 not in [\"batch_size\", \"num_inference_steps\", \"actual_gpu_memory (gbs)\"]\n        ]\n\n        for var_7 in var_5:\n            var_4[var_7] = var_4[var_7].map(lambda var_8: filter_float(var_8))\n\n            # Calculate the percentage change\n",
      "lines_processed": 19,
      "total_lines": 72,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "push_results.py",
      "code": "def filter_float(value):\n    if isinstance(value, str):\n        return float(value.split()[0])\n    return value\n",
      "variables": [
        "value"
      ],
      "anonymized_code": "def filter_float(var_1):\n    if isinstance(var_1, str):\n        return float(var_1.split()[0])\n    return var_1\n",
      "lines_processed": 4,
      "total_lines": 72,
      "llm_code": "def filter_float(value):\n    if isinstance(value, str):\n        return float(value.split()[0])\n    return value",
      "llm_variables": [
        "value"
      ]
    },
    {
      "file_path": "push_results.py",
      "code": "def has_previous_benchmark() -> str:\n    csv_path = None\n    try:\n        csv_path = hf_hub_download(repo_id=REPO_ID, repo_type=\"dataset\", filename=FINAL_CSV_FILE)\n    except EntryNotFoundError:\n        csv_path = None\n    return csv_path\n",
      "variables": [
        "csv_path"
      ],
      "anonymized_code": "def has_previous_benchmark() -> str:\n    var_1 = None\n    try:\n        var_1 = hf_hub_download(repo_id=REPO_ID, repo_type=\"dataset\", filename=FINAL_CSV_FILE)\n    except EntryNotFoundError:\n        var_1 = None\n    return var_1\n",
      "lines_processed": 7,
      "total_lines": 72,
      "llm_code": "def has_previous_benchmark() -> str:\n    file_path = None\n    try:\n        file_path = hf_hub_download(repo_id=REPO_ID, repo_type=\"dataset\", filename=FINAL_CSV_FILE)\n    except EntryNotFoundError:\n        file_path = None\n    return file_path\n",
      "llm_variables": [
        "file_path"
      ]
    },
    {
      "file_path": "train_dreambooth_lora_sdxl_advanced.py",
      "code": "def import_model_class_from_model_name_or_path(\n    pretrained_model_name_or_path: str, revision: str, subfolder: str = \"text_encoder\"\n):\n    text_encoder_config = PretrainedConfig.from_pretrained(\n        pretrained_model_name_or_path, subfolder=subfolder, revision=revision\n    )\n    model_class = text_encoder_config.architectures[0]\n\n    if model_class == \"CLIPTextModel\":\n        from transformers import CLIPTextModel\n\n        return CLIPTextModel\n    elif model_class == \"CLIPTextModelWithProjection\":\n        from transformers import CLIPTextModelWithProjection\n\n        return CLIPTextModelWithProjection\n    else:\n        raise ValueError(f\"{model_class} is not supported.\")\n",
      "variables": [
        "pretrained_model_name_or_path",
        "revision",
        "subfolder",
        "text_encoder_config",
        "model_class"
      ],
      "anonymized_code": "def import_model_class_from_model_name_or_path(\n    var_1: str, var_2: str, var_3: str = \"text_encoder\"\n):\n    var_4 = PretrainedConfig.from_pretrained(\n        var_1, var_3=var_3, var_2=var_2\n    )\n    var_5 = var_4.architectures[0]\n\n    if var_5 == \"CLIPTextModel\":\n        from transformers import CLIPTextModel\n\n        return CLIPTextModel\n    elif var_5 == \"CLIPTextModelWithProjection\":\n        from transformers import CLIPTextModelWithProjection\n\n        return CLIPTextModelWithProjection\n    else:\n        raise ValueError(f\"{var_5} is not supported.\")\n",
      "lines_processed": 18,
      "total_lines": 2474,
      "llm_code": "def import_model_class_from_model_name_or_path(\n    model_name_or_path: str, config_name: str, config_file: str = \"text_encoder\"\n):\n    config = PretrainedConfig.from_pretrained(\n        model_name_or_path, config_file=config_file, config_name=config_name\n    )\n    arch = config.architectures[0]\n\n    if arch == \"CLIPTextModel\":\n        from transformers import CLIPTextModel\n\n        return CLIPTextModel\n    elif arch == \"CLIPTextModelWithProjection\":\n        from transformers import CLIPTextModelWithProjection\n\n        return CLIPTextModelWithProjection\n    else:\n        raise ValueError(f\"{arch} is not supported.\")\n",
      "llm_variables": [
        "model_name_or_path",
        "config_name",
        "config_file",
        "config",
        "arch"
      ]
    }
  ],
  "psf_black": [
    {
      "file_path": "release.py",
      "code": "def _handle_debug(debug: bool) -> None:\n    \"\"\"Turn on debugging if asked otherwise INFO default\"\"\"\n    log_level = logging.DEBUG if debug else logging.INFO\n    logging.basicConfig(\n        format=\"[%(asctime)s] %(levelname)s: %(message)s (%(filename)s:%(lineno)d)\",\n        level=log_level,\n    )\n",
      "variables": [
        "debug",
        "log_level"
      ],
      "anonymized_code": "def _handle_debug(var_1: bool) -> None:\n    \"\"\"Turn on debugging if asked otherwise INFO default\"\"\"\n    var_2 = logging.DEBUG if var_1 else logging.INFO\n    logging.basicConfig(\n        format=\"[%(asctime)s] %(levelname)s: %(message)s (%(filename)s:%(lineno)d)\",\n        level=var_2,\n    )\n",
      "lines_processed": 7,
      "total_lines": 244,
      "llm_code": "def _handle_debug(debug_enabled: bool) -> None:\n    \"\"\"Turn on debugging if asked otherwise INFO default\"\"\"\n    log_level = logging.DEBUG if debug_enabled else logging.INFO\n    logging.basicConfig(\n        format=\"[%(asctime)s] %(levelname)s: %(message)s (%(filename)s:%(lineno)d)\",\n        level=log_level,\n    )\n",
      "llm_variables": [
        "debug_enabled",
        "log_level"
      ]
    },
    {
      "file_path": "release.py",
      "code": "def tuple_calver(calver: str) -> tuple[int, ...]:  # mypy can't notice maxsplit below\n    \"\"\"Convert a calver string into a tuple of ints for sorting\"\"\"\n    try:\n        return tuple(map(int, calver.split(\".\", maxsplit=2)))\n    except ValueError:\n        return (0, 0, 0)\n",
      "variables": [
        "calver"
      ],
      "anonymized_code": "def tuple_calver(var_1: str) -> tuple[int, ...]:  # mypy can't notice maxsplit below\n    \"\"\"Convert a var_1 string into a tuple of ints for sorting\"\"\"\n    try:\n        return tuple(map(int, var_1.split(\".\", maxsplit=2)))\n    except ValueError:\n        return (0, 0, 0)\n",
      "lines_processed": 6,
      "total_lines": 244,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "release.py",
      "code": "def main() -> int:\n    args = parse_args()\n\n    # Need parent.parent cause script is in scripts/ directory\n    sf = SourceFiles(Path(__file__).parent.parent)\n\n    if args.add_changes_template:\n        return sf.add_template_to_changes()\n\n    LOG.info(f\"Current version detected to be {sf.current_version}\")\n    LOG.info(f\"Next version will be {sf.next_version}\")\n    return sf.update_repo_for_release()\n",
      "variables": [
        "args",
        "sf"
      ],
      "anonymized_code": "def main() -> int:\n    var_1 = parse_args()\n\n    # Need parent.parent cause script is in scripts/ directory\n    var_2 = SourceFiles(Path(__file__).parent.parent)\n\n    if var_1.add_changes_template:\n        return var_2.add_template_to_changes()\n\n    LOG.info(f\"Current version detected to be {var_2.current_version}\")\n    LOG.info(f\"Next version will be {var_2.next_version}\")\n    return var_2.update_repo_for_release()\n",
      "lines_processed": 12,
      "total_lines": 244,
      "llm_code": "def main() -> int:\n    args = parse_args()\n\n    # Need parent.parent cause script is in scripts/ directory\n    source_files = SourceFiles(Path(__file__).parent.parent)\n\n    if args.add_changes_template:\n        return source_files.add_template_to_changes()\n\n    LOG.info(f\"Current version detected to be {source_files.current_version}\")\n    LOG.info(f\"Next version will be {source_files.next_version}\")\n    return source_files.update_repo_for_release()",
      "llm_variables": [
        "args",
        "source_files"
      ]
    },
    {
      "file_path": "release.py",
      "code": "def get_git_tags(versions_only: bool = True) -> list[str]:\n    \"\"\"Pull out all tags or calvers only\"\"\"\n    cp = run([\"git\", \"tag\"], capture_output=True, check=True, encoding=\"utf8\")\n    if not cp.stdout:\n        LOG.error(f\"Returned no git tags stdout: {cp.stderr}\")\n        raise NoGitTagsError\n    git_tags = cp.stdout.splitlines()\n    if versions_only:\n        return [t for t in git_tags if t[0].isdigit()]\n    return git_tags\n",
      "variables": [
        "versions_only",
        "cp",
        "git_tags",
        "t"
      ],
      "anonymized_code": "def get_git_tags(var_1: bool = True) -> list[str]:\n    \"\"\"Pull out all tags or calvers only\"\"\"\n    var_2 = run([\"git\", \"tag\"], capture_output=True, check=True, encoding=\"utf8\")\n    if not var_2.stdout:\n        LOG.error(f\"Returned no git tags stdout: {var_2.stderr}\")\n        raise NoGitTagsError\n    var_3 = var_2.stdout.splitlines()\n    if var_1:\n        return [var_4 for var_4 in var_3 if var_4[0].isdigit()]\n    return var_3\n",
      "lines_processed": 10,
      "total_lines": 244,
      "llm_code": "def get_git_tags(include_calvers: bool = True) -> list[str]:\n    \"\"\"Pull out all tags or calvers only\"\"\"\n    git_tags = run([\"git\", \"tag\"], capture_output=True, check=True, encoding=\"utf8\")\n    if not git_tags.stdout:\n        LOG.error(f\"Returned no git tags stdout: {git_tags.stderr}\")\n        raise NoGitTagsError\n    tag_lines = git_tags.stdout.splitlines()\n    if include_calvers:\n        return [tag for tag in tag_lines if tag[0].isdigit()]\n    return tag_lines\n",
      "llm_variables": [
        "include_calvers",
        "git_tags",
        "tag_lines",
        "tag"
      ]
    },
    {
      "file_path": "release.py",
      "code": "def parse_args() -> argparse.Namespace:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"-a\",\n        \"--add-changes-template\",\n        action=\"store_true\",\n        help=\"Add the Unreleased template to CHANGES.md\",\n    )\n    parser.add_argument(\n        \"-d\", \"--debug\", action=\"store_true\", help=\"Verbose debug output\"\n    )\n    args = parser.parse_args()\n    _handle_debug(args.debug)\n    return args\n",
      "variables": [
        "parser",
        "args"
      ],
      "anonymized_code": "def parse_args() -> argparse.Namespace:\n    var_1 = argparse.ArgumentParser()\n    var_1.add_argument(\n        \"-a\",\n        \"--add-changes-template\",\n        action=\"store_true\",\n        help=\"Add the Unreleased template to CHANGES.md\",\n    )\n    var_1.add_argument(\n        \"-d\", \"--debug\", action=\"store_true\", help=\"Verbose debug output\"\n    )\n    var_2 = var_1.parse_args()\n    _handle_debug(var_2.debug)\n    return var_2\n",
      "lines_processed": 14,
      "total_lines": 244,
      "llm_code": "def parse_args() -> argparse.Namespace:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"-a\",\n        \"--add-changes-template\",\n        action=\"store_true\",\n        help=\"Add the Unreleased template to CHANGES.md\",\n    )\n    parser.add_argument(\n        \"-d\", \"--debug\", action=\"store_true\", help=\"Verbose debug output\"\n    )\n    args = parser.parse_args()\n    _handle_debug(args.debug)\n    return args\n",
      "llm_variables": [
        "parser",
        "args"
      ]
    },
    {
      "file_path": "conf.py",
      "code": "def replace_pr_numbers_with_links(content: str) -> str:\n    \"\"\"Replaces all PR numbers with the corresponding GitHub link.\"\"\"\n    return re.sub(r\"#(\\d+)\", r\"[#\\1](https://github.com/psf/black/pull/\\1)\", content)\n",
      "variables": [
        "content"
      ],
      "anonymized_code": "def replace_pr_numbers_with_links(var_1: str) -> str:\n    \"\"\"Replaces all PR numbers with the corresponding GitHub link.\"\"\"\n    return re.sub(r\"#(\\d+)\", r\"[#\\1](https://github.com/psf/black/pull/\\1)\", var_1)\n",
      "lines_processed": 3,
      "total_lines": 241,
      "llm_code": "def replace_pr_numbers_with_links(text: str) -> str:\n    \"\"\"Replaces all PR numbers with the corresponding GitHub link.\"\"\"\n    return re.sub(r\"#(\\d+)\", r\"[#$1](https://github.com/psf/black/pull/$1)\", text)",
      "llm_variables": [
        "text"
      ]
    },
    {
      "file_path": "conf.py",
      "code": "def setup(app: Sphinx) -> None:\n    \"\"\"Sets up a minimal sphinx extension.\"\"\"\n    app.connect(\"include-read\", handle_include_read)\n",
      "variables": [
        "app"
      ],
      "anonymized_code": "def setup(var_1: Sphinx) -> None:\n    \"\"\"Sets up a minimal sphinx extension.\"\"\"\n    var_1.connect(\"include-read\", handle_include_read)\n",
      "lines_processed": 3,
      "total_lines": 241,
      "llm_code": "def setup(builder: Sphinx) -> None:\n    \"\"\"Sets up a minimal sphinx extension.\"\"\"\n    builder.connect(\"include-read\", handle_include_read)\n",
      "llm_variables": [
        "builder"
      ]
    },
    {
      "file_path": "conf.py",
      "code": "def handle_include_read(\n    app: Sphinx,\n    relative_path: Path,\n    parent_docname: str,\n    content: list[str],\n) -> None:\n    \"\"\"Handler for the include-read sphinx event.\"\"\"\n    if parent_docname == \"change_log\":\n        content[0] = replace_pr_numbers_with_links(content[0])\n",
      "variables": [
        "app",
        "relative_path",
        "parent_docname",
        "content"
      ],
      "anonymized_code": "def handle_include_read(\n    var_1: Sphinx,\n    var_2: Path,\n    var_3: str,\n    var_4: list[str],\n) -> None:\n    \"\"\"Handler for the include-read sphinx event.\"\"\"\n    if var_3 == \"change_log\":\n        var_4[0] = replace_pr_numbers_with_links(var_4[0])\n",
      "lines_processed": 9,
      "total_lines": 241,
      "llm_code": "def handle_include_read(\n    doc: Sphinx,\n    path: Path,\n    target: str,\n    content: list[str],\n) -> None:\n    \"\"\"Handler for the include-read sphinx event.\"\"\"\n    if target == \"change_log\":\n        content[0] = replace_pr_numbers_with_links(content[0])\n",
      "llm_variables": [
        "doc",
        "path",
        "target",
        "content"
      ]
    },
    {
      "file_path": "conf.py",
      "code": "def make_pypi_svg(version: str) -> None:\n    template: Path = CURRENT_DIR / \"_static\" / \"pypi_template.svg\"\n    target: Path = CURRENT_DIR / \"_static\" / \"pypi.svg\"\n    with open(str(template), encoding=\"utf8\") as f:\n        svg: str = string.Template(f.read()).substitute(version=version)\n    with open(str(target), \"w\", encoding=\"utf8\") as f:\n        f.write(svg)\n",
      "variables": [
        "version",
        "template",
        "target",
        "f",
        "svg"
      ],
      "anonymized_code": "def make_pypi_svg(var_1: str) -> None:\n    var_2: Path = CURRENT_DIR / \"_static\" / \"pypi_template.var_5\"\n    var_3: Path = CURRENT_DIR / \"_static\" / \"pypi.var_5\"\n    with open(str(var_2), encoding=\"utf8\") as var_4:\n        var_5: str = string.Template(var_4.read()).substitute(var_1=var_1)\n    with open(str(var_3), \"w\", encoding=\"utf8\") as var_4:\n        var_4.write(var_5)\n",
      "lines_processed": 7,
      "total_lines": 241,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "migrate-black.py",
      "code": "def git(*args: str) -> str:\n    return check_output([\"git\", *args]).decode(\"utf8\").strip()\n",
      "variables": [
        "args"
      ],
      "anonymized_code": "def git(*var_1: str) -> str:\n    return check_output([\"git\", *var_1]).decode(\"utf8\").strip()\n",
      "lines_processed": 2,
      "total_lines": 96,
      "llm_code": "def git(*args: str) -> str:\n    return check_output([\"git\", *args]).decode(\"utf8\").strip()\n",
      "llm_variables": [
        "args"
      ]
    },
    {
      "file_path": "gallery.py",
      "code": "def get_package_source(package: str, version: Optional[str]) -> str:\n    if package == \"cpython\":\n        if version is None:\n            version = \"main\"\n        return f\"https://github.com/python/cpython/archive/{version}.zip\"\n    elif package == \"pypy\":\n        if version is None:\n            version = \"branch/default\"\n        return (\n            f\"https://foss.heptapod.net/pypy/pypy/repository/{version}/archive.tar.bz2\"\n        )\n    else:\n        return get_pypi_download_url(package, version)\n",
      "variables": [
        "package",
        "version"
      ],
      "anonymized_code": "def get_package_source(var_1: str, var_2: Optional[str]) -> str:\n    if var_1 == \"cpython\":\n        if var_2 is None:\n            var_2 = \"main\"\n        return f\"https://github.com/python/cpython/archive/{var_2}.zip\"\n    elif var_1 == \"pypy\":\n        if var_2 is None:\n            var_2 = \"branch/default\"\n        return (\n            f\"https://foss.heptapod.net/pypy/pypy/repository/{var_2}/archive.tar.bz2\"\n        )\n    else:\n        return get_pypi_download_url(var_1, var_2)\n",
      "lines_processed": 13,
      "total_lines": 295,
      "llm_code": "def get_package_source(package: str, branch: Optional[str]) -> str:\n    if package == \"cpython\":\n        if branch is None:\n            branch = \"main\"\n        return f\"https://github.com/python/cpython/archive/{branch}.zip\"\n    elif package == \"pypy\":\n        if branch is None:\n            branch = \"branch/default\"\n        return (\n            f\"https://foss.heptapod.net/pypy/pypy/repository/{branch}/archive.tar.bz2\"\n        )\n    else:\n        return get_pypi_download_url(package, branch)\n",
      "llm_variables": [
        "package",
        "branch"
      ]
    },
    {
      "file_path": "gallery.py",
      "code": "def download_and_extract_top_packages(\n    directory: Path,\n    workers: int = 8,\n    limit: slice = DEFAULT_SLICE,\n) -> Generator[Path, None, None]:\n    with ThreadPoolExecutor(max_workers=workers) as executor:\n        bound_downloader = partial(get_package, version=None, directory=directory)\n        for package in executor.map(bound_downloader, get_top_packages()[limit]):\n            if package is not None:\n                yield package\n",
      "variables": [
        "directory",
        "workers",
        "limit",
        "executor",
        "bound_downloader",
        "package"
      ],
      "anonymized_code": "def download_and_extract_top_packages(\n    var_1: Path,\n    var_2: int = 8,\n    var_3: slice = DEFAULT_SLICE,\n) -> Generator[Path, None, None]:\n    with ThreadPoolExecutor(max_workers=var_2) as var_4:\n        var_5 = partial(get_package, version=None, var_1=var_1)\n        for var_6 in var_4.map(var_5, get_top_packages()[var_3]):\n            if var_6 is not None:\n                yield var_6\n",
      "lines_processed": 10,
      "total_lines": 295,
      "llm_code": "def download_and_extract_top_packages(\n    path: Path,\n    max_workers: int = 8,\n    slice: slice = DEFAULT_SLICE,\n) -> Generator[Path, None, None]:\n    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n        func = partial(get_package, version=None, path=path)\n        for result in executor.map(func, get_top_packages()[slice]):\n            if result is not None:\n                yield result",
      "llm_variables": [
        "path",
        "max_workers",
        "slice",
        "executor",
        "func",
        "result"
      ]
    },
    {
      "file_path": "gallery.py",
      "code": "def format_repo_with_version(\n    repo: Path,\n    from_branch: Optional[str],\n    black_repo: Path,\n    black_version: BlackVersion,\n    input_directory: Path,\n) -> str:\n    current_branch = f\"black-{black_version.version}\"\n    git_switch_branch(black_version.version, repo=black_repo)\n    git_switch_branch(current_branch, repo=repo, new=True, from_branch=from_branch)\n\n    format_cmd: list[Union[Path, str]] = [\n        black_runner(black_version.version, black_repo),\n        (black_repo / \"black.py\").resolve(),\n        \".\",\n    ]\n    if black_version.config:\n        format_cmd.extend([\"--config\", input_directory / black_version.config])\n\n",
      "variables": [
        "repo",
        "from_branch",
        "black_repo",
        "black_version",
        "input_directory",
        "current_branch",
        "format_cmd"
      ],
      "anonymized_code": "def format_repo_with_version(\n    var_1: Path,\n    var_2: Optional[str],\n    var_3: Path,\n    var_4: BlackVersion,\n    var_5: Path,\n) -> str:\n    var_6 = f\"black-{var_4.version}\"\n    git_switch_branch(var_4.version, var_1=var_3)\n    git_switch_branch(var_6, var_1=var_1, new=True, var_2=var_2)\n\n    var_7: list[Union[Path, str]] = [\n        black_runner(var_4.version, var_3),\n        (var_3 / \"black.py\").resolve(),\n        \".\",\n    ]\n    if var_4.config:\n        var_7.extend([\"--config\", var_5 / var_4.config])\n\n",
      "lines_processed": 19,
      "total_lines": 295,
      "llm_code": "def format_repo_with_version(\n    repo: Path,\n    version: Optional[str],\n    black_dir: Path,\n    black_version: BlackVersion,\n    config_path: Path,\n) -> str:\n    black_version_str = f\"black-{black_version.version}\"\n    git_switch_branch(black_version.version, repo=black_dir)\n    git_switch_branch(black_version_str, repo=repo, new=True, version=version)\n\n    var_7: list[Union[Path, str]] = [\n        black_runner(black_version.version, black_dir),\n        (black_dir / \"black.py\").resolve(),\n        \".\",\n    ]\n    if black_version.config:\n        var_7.extend([\"--config\", config_path / black_version.config])\n\n",
      "llm_variables": [
        "repo",
        "version",
        "black_dir",
        "black_version",
        "config_path",
        "black_version_str",
        "var_7"
      ]
    },
    {
      "file_path": "gallery.py",
      "code": "def main() -> None:\n    parser = ArgumentParser(description=\"\"\"Black Gallery is a script that\n    automates the process of applying different Black versions to a selected\n    PyPI package and seeing the results between versions.\"\"\")\n\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"-p\", \"--pypi-package\", help=\"PyPI package to download.\")\n    group.add_argument(\n        \"-t\", \"--top-packages\", help=\"Top n PyPI packages to download.\", type=int\n    )\n\n    parser.add_argument(\"-b\", \"--black-repo\", help=\"Black's Git repository.\", type=Path)\n    parser.add_argument(\n        \"-v\",\n        \"--version\",\n        help=(\n            \"Version for given PyPI package. Will be discarded if used with -t option.\"\n        ),\n    )\n",
      "variables": [
        "parser",
        "group"
      ],
      "anonymized_code": "def main() -> None:\n    var_1 = ArgumentParser(description=\"\"\"Black Gallery is a script that\n    automates the process of applying different Black versions to a selected\n    PyPI package and seeing the results between versions.\"\"\")\n\n    var_2 = var_1.add_mutually_exclusive_group(required=True)\n    var_2.add_argument(\"-p\", \"--pypi-package\", help=\"PyPI package to download.\")\n    var_2.add_argument(\n        \"-t\", \"--top-packages\", help=\"Top n PyPI packages to download.\", type=int\n    )\n\n    var_1.add_argument(\"-b\", \"--black-repo\", help=\"Black's Git repository.\", type=Path)\n    var_1.add_argument(\n        \"-v\",\n        \"--version\",\n        help=(\n            \"Version for given PyPI package. Will be discarded if used with -t option.\"\n        ),\n    )\n",
      "lines_processed": 19,
      "total_lines": 295,
      "llm_code": "def main() -> None:\n    parser = ArgumentParser(description=\"\"\"Black Gallery is a script that\n    automates the process of applying different Black versions to a selected\n    PyPI package and seeing the results between versions.\"\"\")\n\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"-p\", \"--pypi-package\", help=\"PyPI package to download.\")\n    group.add_argument(\n        \"-t\", \"--top-packages\", help=\"Top n PyPI packages to download.\", type=int\n    )\n\n    parser.add_argument(\"-b\", \"--black-repo\", help=\"Black's Git repository.\", type=Path)\n    parser.add_argument(\n        \"-v\",\n        \"--version\",\n        help=(\n            \"Version for given PyPI package. Will be discarded if used with -t option.\"\n        ),\n    )\n",
      "llm_variables": [
        "parser",
        "group"
      ]
    },
    {
      "file_path": "gallery.py",
      "code": "def get_first_archive_member(archive: ArchiveKind) -> str:\n    if isinstance(archive, tarfile.TarFile):\n        return archive.getnames()[0]\n    elif isinstance(archive, zipfile.ZipFile):\n        return archive.namelist()[0]\n",
      "variables": [
        "archive"
      ],
      "anonymized_code": "def get_first_archive_member(var_1: ArchiveKind) -> str:\n    if isinstance(var_1, tarfile.TarFile):\n        return var_1.getnames()[0]\n    elif isinstance(var_1, zipfile.ZipFile):\n        return var_1.namelist()[0]\n",
      "lines_processed": 5,
      "total_lines": 295,
      "llm_code": "def get_first_archive_member(kind: ArchiveKind) -> str:\n    if isinstance(kind, tarfile.TarFile):\n        return kind.getnames()[0]\n    elif isinstance(kind, zipfile.ZipFile):\n        return kind.namelist()[0]",
      "llm_variables": [
        "kind"
      ]
    },
    {
      "file_path": "gallery.py",
      "code": "def init_repos(options: Namespace) -> tuple[Path, ...]:\n    options.output.mkdir(exist_ok=True)\n\n    if options.top_packages:\n        source_directories = tuple(\n            download_and_extract_top_packages(\n                directory=options.output,\n                workers=options.workers,\n                limit=slice(None, options.top_packages),\n            )\n        )\n    else:\n        source_directories = (\n            download_and_extract(\n                package=options.pypi_package,\n                version=options.version,\n                directory=options.output,\n            ),\n        )\n",
      "variables": [
        "options",
        "source_directories"
      ],
      "anonymized_code": "def init_repos(var_1: Namespace) -> tuple[Path, ...]:\n    var_1.output.mkdir(exist_ok=True)\n\n    if var_1.top_packages:\n        var_2 = tuple(\n            download_and_extract_top_packages(\n                directory=var_1.output,\n                workers=var_1.workers,\n                limit=slice(None, var_1.top_packages),\n            )\n        )\n    else:\n        var_2 = (\n            download_and_extract(\n                package=var_1.pypi_package,\n                version=var_1.version,\n                directory=var_1.output,\n            ),\n        )\n",
      "lines_processed": 19,
      "total_lines": 295,
      "llm_code": "def init_repos(args: Namespace) -> tuple[Path,...]:\n    args.output.mkdir(exist_ok=True)\n\n    if args.top_packages:\n        var_2 = tuple(\n            download_and_extract_top_packages(\n                directory=args.output,\n                workers=args.workers,\n                limit=slice(None, args.top_packages),\n            )\n        )\n    else:\n        var_2 = (\n            download_and_extract(\n                package=args.pypi_package,\n                version=args.version,\n                directory=args.output,\n            ),\n        )",
      "llm_variables": [
        "args",
        "var_2"
      ]
    },
    {
      "file_path": "gallery.py",
      "code": "def download_and_extract(package: str, version: Optional[str], directory: Path) -> Path:\n    source = get_package_source(package, version)\n\n    local_file, _ = urlretrieve(source, directory / f\"{package}-src\")\n    with get_archive_manager(local_file) as archive:\n        archive.extractall(path=directory)\n        result_dir = get_first_archive_member(archive)\n    return directory / result_dir\n",
      "variables": [
        "package",
        "version",
        "directory",
        "source",
        "local_file",
        "_",
        "archive",
        "result_dir"
      ],
      "anonymized_code": "def download_and_extract(var_1: str, var_2: Optional[str], var_3: Path) -> Path:\n    var_4 = get_package_source(var_1, var_2)\n\n    var_5, var_6 = urlretrieve(var_4, var_3 / f\"{var_1}-src\")\n    with get_archive_manager(var_5) as var_7:\n        var_7.extractall(path=var_3)\n        var_8 = get_first_archive_member(var_7)\n    return var_3 / var_8\n",
      "lines_processed": 8,
      "total_lines": 295,
      "llm_code": "def download_and_extract(package_name: str, version: Optional[str], target_dir: Path) -> Path:\n    source_url = get_package_source(package_name, version)\n\n    downloaded_file, temp_path = urlretrieve(source_url, target_dir / f\"{package_name}-src\")\n    with get_archive_manager(downloaded_file) as archive:\n        archive.extractall(path=target_dir)\n        first_member = get_first_archive_member(archive)\n    return target_dir / first_member\n",
      "llm_variables": [
        "package_name",
        "version",
        "target_dir",
        "source_url",
        "downloaded_file",
        "temp_path",
        "archive",
        "first_member"
      ]
    },
    {
      "file_path": "gallery.py",
      "code": "def get_archive_manager(local_file: str) -> ArchiveKind:\n    if tarfile.is_tarfile(local_file):\n        return tarfile.open(local_file)\n    elif zipfile.is_zipfile(local_file):\n        return zipfile.ZipFile(local_file)\n    else:\n        raise ValueError(\"Unknown archive kind.\")\n",
      "variables": [
        "local_file"
      ],
      "anonymized_code": "def get_archive_manager(var_1: str) -> ArchiveKind:\n    if tarfile.is_tarfile(var_1):\n        return tarfile.open(var_1)\n    elif zipfile.is_zipfile(var_1):\n        return zipfile.ZipFile(var_1)\n    else:\n        raise ValueError(\"Unknown archive kind.\")\n",
      "lines_processed": 7,
      "total_lines": 295,
      "llm_code": "def get_archive_manager(archive_path: str) -> ArchiveKind:\n    if tarfile.is_tarfile(archive_path):\n        return tarfile.open(archive_path)\n    elif zipfile.is_zipfile(archive_path):\n        return zipfile.ZipFile(archive_path)\n    else:\n        raise ValueError(\"Unknown archive kind.\")\n",
      "llm_variables": [
        "archive_path"
      ]
    },
    {
      "file_path": "gallery.py",
      "code": "def get_top_packages() -> list[str]:\n    with urlopen(PYPI_TOP_PACKAGES) as page:\n        result = json.load(page)\n\n    return [package[\"project\"] for package in result[\"rows\"]]\n",
      "variables": [
        "page",
        "result",
        "package"
      ],
      "anonymized_code": "def get_top_packages() -> list[str]:\n    with urlopen(PYPI_TOP_PACKAGES) as var_1:\n        var_2 = json.load(var_1)\n\n    return [var_3[\"project\"] for var_3 in var_2[\"rows\"]]\n",
      "lines_processed": 5,
      "total_lines": 295,
      "llm_code": "def get_top_packages() -> list[str]:\n    with urlopen(PYPI_TOP_PACKAGES) as response:\n        data = json.load(response)\n\n    return [item[\"project\"] for item in data[\"rows\"]]\n",
      "llm_variables": [
        "response",
        "data",
        "item"
      ]
    },
    {
      "file_path": "fuzz.py",
      "code": "def test_idempotent_any_syntatically_valid_python(\n    src_contents: str, mode: black.FileMode\n) -> None:\n    # Before starting, let's confirm that the input string is valid Python:\n    compile(src_contents, \"<string>\", \"exec\")  # else the bug is in hypothesmith\n\n    # Then format the code...\n    dst_contents = black.format_str(src_contents, mode=mode)\n\n    # And check that we got equivalent and stable output.\n    black.assert_equivalent(src_contents, dst_contents)\n    black.assert_stable(src_contents, dst_contents, mode=mode)\n",
      "variables": [
        "src_contents",
        "mode",
        "dst_contents"
      ],
      "anonymized_code": "def test_idempotent_any_syntatically_valid_python(\n    var_1: str, var_2: black.FileMode\n) -> None:\n    # Before starting, let's confirm that the input string is valid Python:\n    compile(var_1, \"<string>\", \"exec\")  # else the bug is in hypothesmith\n\n    # Then format the code...\n    var_3 = black.format_str(var_1, var_2=var_2)\n\n    # And check that we got equivalent and stable output.\n    black.assert_equivalent(var_1, var_3)\n    black.assert_stable(var_1, var_3, var_2=var_2)\n",
      "lines_processed": 12,
      "total_lines": 73,
      "llm_code": "def test_idempotent_any_syntatically_valid_python(\n    input_code: str, file_mode: black.FileMode\n) -> None:\n    # Before starting, let's confirm that the input string is valid Python:\n    compile(input_code, \"<string>\", \"exec\")  # else the bug is in hypothesmith\n\n    # Then format the code...\n    formatted_code = black.format_str(input_code, file_mode=file_mode)\n\n    # And check that we got equivalent and stable output.\n    black.assert_equivalent(input_code, formatted_code)\n    black.assert_stable(input_code, formatted_code, file_mode=file_mode)\n",
      "llm_variables": [
        "input_code",
        "file_mode",
        "formatted_code"
      ]
    },
    {
      "file_path": "make_width_table.py",
      "code": "def make_width_table() -> Iterable[tuple[int, int, int]]:\n    start_codepoint = -1\n    end_codepoint = -1\n    range_width = -2\n    for codepoint in range(0, sys.maxunicode + 1):\n        width = wcwidth.wcwidth(chr(codepoint))\n        if width <= 1:\n            # Ignore narrow characters along with zero-width characters so that\n            # they are treated as single-width.  Note that treating zero-width\n            # characters as single-width is consistent with the heuristics built\n            # on top of str.isascii() in the str_width() function in strings.py.\n            continue\n        if start_codepoint < 0:\n            start_codepoint = codepoint\n            range_width = width\n        elif width != range_width or codepoint != end_codepoint + 1:\n            yield (start_codepoint, end_codepoint, range_width)\n            start_codepoint = codepoint\n            range_width = width\n",
      "variables": [
        "start_codepoint",
        "end_codepoint",
        "range_width",
        "codepoint",
        "width"
      ],
      "anonymized_code": "def make_width_table() -> Iterable[tuple[int, int, int]]:\n    var_1 = -1\n    var_2 = -1\n    var_3 = -2\n    for var_4 in range(0, sys.maxunicode + 1):\n        var_5 = wcwidth.wcwidth(chr(var_4))\n        if var_5 <= 1:\n            # Ignore narrow characters along with zero-var_5 characters so that\n            # they are treated as single-var_5.  Note that treating zero-var_5\n            # characters as single-var_5 is consistent with the heuristics built\n            # on top of str.isascii() in the str_width() function in strings.py.\n            continue\n        if var_1 < 0:\n            var_1 = var_4\n            var_3 = var_5\n        elif var_5 != var_3 or var_4 != var_2 + 1:\n            yield (var_1, var_2, var_3)\n            var_1 = var_4\n            var_3 = var_5\n",
      "lines_processed": 19,
      "total_lines": 66,
      "llm_code": "def make_width_table() -> Iterable[tuple[int, int, int]]:\n    start = -1\n    previous = -1\n    width = -2\n    for index in range(0, sys.maxunicode + 1):\n        current_width = wcwidth.wcwidth(chr(index))\n        if current_width <= 1:\n            # Ignore narrow characters along with zero-var_5 characters so that\n            # they are treated as single-var_5.  Note that treating zero-var_5\n            # characters as single-var_5 is consistent with the heuristics built\n            # on top of str.isascii() in the str_width() function in strings.py.\n            continue\n        if start < 0:\n            start = index\n            width = current_width\n        elif current_width!= width or index!= previous + 1:\n            yield (start, previous, width)\n            start = index\n            width = current_width",
      "llm_variables": [
        "start",
        "previous",
        "width",
        "index",
        "current_width"
      ]
    },
    {
      "file_path": "make_width_table.py",
      "code": "def main() -> None:\n    table_path = join(dirname(__file__), \"..\", \"src\", \"black\", \"_width_table.py\")\n    with open(table_path, \"w\") as f:\n        f.write(f\"\"\"# Generated by {basename(__file__)}\n# wcwidth {wcwidth.__version__}\n# Unicode {wcwidth.list_versions()[-1]}\nfrom typing import Final\n\nWIDTH_TABLE: Final[list[tuple[int, int, int]]] = [\n\"\"\")\n        for triple in make_width_table():\n            f.write(f\"    {triple!r},\\n\")\n        f.write(\"]\\n\")\n",
      "variables": [
        "table_path",
        "f",
        "triple"
      ],
      "anonymized_code": "def main() -> None:\n    var_1 = join(dirname(__file__), \"..\", \"src\", \"black\", \"_width_table.py\")\n    with open(var_1, \"w\") as var_2:\n        var_2.write(var_2\"\"\"# Generated by {basename(__file__)}\n# wcwidth {wcwidth.__version__}\n# Unicode {wcwidth.list_versions()[-1]}\nfrom typing import Final\n\nWIDTH_TABLE: Final[list[tuple[int, int, int]]] = [\n\"\"\")\n        for var_3 in make_width_table():\n            var_2.write(var_2\"    {var_3!r},\\n\")\n        var_2.write(\"]\\n\")\n",
      "lines_processed": 13,
      "total_lines": 66,
      "llm_code": "def main() -> None:\n    file_path = join(dirname(__file__), \"..\", \"src\", \"black\", \"_width_table.py\")\n    with open(file_path, \"w\") as file:\n        file.write(file\"\"\"# Generated by {basename(__file__)}\n# wcwidth {wcwidth.__version__}\n# Unicode {wcwidth.list_versions()[-1]}\nfrom typing import Final\n\nWIDTH_TABLE: Final[list[tuple[int, int, int]]] = [\n\"\"\")\n        for row in make_width_table():\n            file.write(file\"    {row!r},\\n\")\n        file.write(\"]\\n\")\n",
      "llm_variables": [
        "file_path",
        "file",
        "row"
      ]
    },
    {
      "file_path": "diff_shades_gha_helper.py",
      "code": "def config(event: Literal[\"push\", \"pull_request\"]) -> None:\n    import diff_shades  # type: ignore[import-not-found]\n\n    if event == \"push\":\n        jobs = [{\"mode\": \"preview-changes\", \"force-flag\": \"--force-preview-style\"}]\n        # Push on main, let's use PyPI Black as the baseline.\n        baseline_name = str(get_pypi_version())\n        baseline_cmd = f\"git checkout {baseline_name}\"\n        target_rev = os.getenv(\"GITHUB_SHA\")\n        assert target_rev is not None\n        target_name = \"main-\" + target_rev[:SHA_LENGTH]\n        target_cmd = f\"git checkout {target_rev}\"\n\n    elif event == \"pull_request\":\n        jobs = [\n            {\"mode\": \"preview-changes\", \"force-flag\": \"--force-preview-style\"},\n            {\"mode\": \"assert-no-changes\", \"force-flag\": \"--force-stable-style\"},\n        ]\n        # PR, let's use main as the baseline.\n",
      "variables": [
        "event",
        "jobs",
        "baseline_name",
        "baseline_cmd",
        "target_rev",
        "target_name",
        "target_cmd"
      ],
      "anonymized_code": "def config(var_1: Literal[\"push\", \"pull_request\"]) -> None:\n    import diff_shades  # type: ignore[import-not-found]\n\n    if var_1 == \"push\":\n        var_2 = [{\"mode\": \"preview-changes\", \"force-flag\": \"--force-preview-style\"}]\n        # Push on main, let's use PyPI Black as the baseline.\n        var_3 = str(get_pypi_version())\n        var_4 = f\"git checkout {var_3}\"\n        var_5 = os.getenv(\"GITHUB_SHA\")\n        assert var_5 is not None\n        var_6 = \"main-\" + var_5[:SHA_LENGTH]\n        var_7 = f\"git checkout {var_5}\"\n\n    elif var_1 == \"pull_request\":\n        var_2 = [\n            {\"mode\": \"preview-changes\", \"force-flag\": \"--force-preview-style\"},\n            {\"mode\": \"assert-no-changes\", \"force-flag\": \"--force-stable-style\"},\n        ]\n        # PR, let's use main as the baseline.\n",
      "lines_processed": 19,
      "total_lines": 231,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "diff_shades_gha_helper.py",
      "code": "def get_pr_revision(pr: int) -> str:\n    data = http_get(f\"https://api.github.com/repos/{REPO}/pulls/{pr}\")\n    assert isinstance(data[\"head\"][\"sha\"], str)\n    return data[\"head\"][\"sha\"]\n",
      "variables": [
        "pr",
        "data"
      ],
      "anonymized_code": "def get_pr_revision(var_1: int) -> str:\n    var_2 = http_get(f\"https://api.github.com/repos/{REPO}/pulls/{var_1}\")\n    assert isinstance(var_2[\"head\"][\"sha\"], str)\n    return var_2[\"head\"][\"sha\"]\n",
      "lines_processed": 4,
      "total_lines": 231,
      "llm_code": "def get_pr_revision(pr_number: int) -> str:\n    response = http_get(f\"https://api.github.com/repos/{REPO}/pulls/{pr_number}\")\n    assert isinstance(response[\"head\"][\"sha\"], str)\n    return response[\"head\"][\"sha\"]\n",
      "llm_variables": [
        "pr_number",
        "response"
      ]
    },
    {
      "file_path": "diff_shades_gha_helper.py",
      "code": "def http_get(url: str, *, is_json: bool = True, **kwargs: Any) -> Any:\n    headers = kwargs.get(\"headers\") or {}\n    headers[\"User-Agent\"] = USER_AGENT\n    if \"github\" in url:\n        if GH_API_TOKEN:\n            headers[\"Authorization\"] = f\"token {GH_API_TOKEN}\"\n        headers[\"Accept\"] = \"application/vnd.github.v3+json\"\n    kwargs[\"headers\"] = headers\n\n    r = http.request(\"GET\", url, **kwargs)\n    if is_json:\n        data = json.loads(r.data.decode(\"utf-8\"))\n    else:\n        data = r.data\n    print(f\"[INFO]: issued GET request for {r.geturl()}\")\n    if not (200 <= r.status < 300):\n        pprint.pprint(dict(r.info()))\n        pprint.pprint(data)\n        raise RuntimeError(f\"unexpected status code: {r.status}\")\n",
      "variables": [
        "url",
        "is_json",
        "kwargs",
        "headers",
        "r",
        "data"
      ],
      "anonymized_code": "def http_get(var_1: str, *, var_2: bool = True, **var_3: Any) -> Any:\n    var_4 = var_3.get(\"var_4\") or {}\n    var_4[\"User-Agent\"] = USER_AGENT\n    if \"github\" in var_1:\n        if GH_API_TOKEN:\n            var_4[\"Authorization\"] = f\"token {GH_API_TOKEN}\"\n        var_4[\"Accept\"] = \"application/vnd.github.v3+json\"\n    var_3[\"var_4\"] = var_4\n\n    var_5 = http.request(\"GET\", var_1, **var_3)\n    if var_2:\n        var_6 = json.loads(var_5.var_6.decode(\"utf-8\"))\n    else:\n        var_6 = var_5.var_6\n    print(f\"[INFO]: issued GET request for {var_5.geturl()}\")\n    if not (200 <= var_5.status < 300):\n        pprint.pprint(dict(var_5.info()))\n        pprint.pprint(var_6)\n        raise RuntimeError(f\"unexpected status code: {var_5.status}\")\n",
      "lines_processed": 19,
      "total_lines": 231,
      "llm_code": "def http_get(url: str, *, use_auth: bool = True, **headers: Any) -> Any:\n    params = headers.get(\"params\") or {}\n    params[\"User-Agent\"] = USER_AGENT\n    if \"github\" in url:\n        if GH_API_TOKEN:\n            params[\"Authorization\"] = f\"token {GH_API_TOKEN}\"\n        params[\"Accept\"] = \"application/vnd.github.v3+json\"\n    headers[\"params\"] = params\n\n    response = http.request(\"GET\", url, **headers)\n    if use_auth:\n        content = json.loads(response.content.decode(\"utf-8\"))\n    else:\n        content = response.content\n    print(f\"[INFO]: issued GET request for {response.geturl()}\")\n    if not (200 <= response.status < 300):\n        pprint.pprint(dict(response.info()))\n        pprint.pprint(content)\n        raise RuntimeError(f\"unexpected status code: {response.status}\")\n",
      "llm_variables": [
        "url",
        "use_auth",
        "headers",
        "params",
        "response",
        "content"
      ]
    },
    {
      "file_path": "diff_shades_gha_helper.py",
      "code": "def get_pypi_version() -> Version:\n    data = http_get(\"https://pypi.org/pypi/black/json\")\n    versions = [Version(v) for v in data[\"releases\"]]\n    sorted_versions = sorted(versions, reverse=True)\n    return sorted_versions[0]\n",
      "variables": [
        "data",
        "versions",
        "v",
        "sorted_versions"
      ],
      "anonymized_code": "def get_pypi_version() -> Version:\n    var_1 = http_get(\"https://pypi.org/pypi/black/json\")\n    var_2 = [Version(var_3) for var_3 in var_1[\"releases\"]]\n    var_4 = sorted(var_2, reverse=True)\n    return var_4[0]\n",
      "lines_processed": 5,
      "total_lines": 231,
      "llm_code": null,
      "llm_variables": null
    },
    {
      "file_path": "diff_shades_gha_helper.py",
      "code": "def get_main_revision() -> str:\n    data = http_get(\n        f\"https://api.github.com/repos/{REPO}/commits\",\n        fields={\"per_page\": \"1\", \"sha\": \"main\"},\n    )\n    assert isinstance(data[0][\"sha\"], str)\n    return data[0][\"sha\"]\n",
      "variables": [
        "data"
      ],
      "anonymized_code": "def get_main_revision() -> str:\n    var_1 = http_get(\n        f\"https://api.github.com/repos/{REPO}/commits\",\n        fields={\"per_page\": \"1\", \"sha\": \"main\"},\n    )\n    assert isinstance(var_1[0][\"sha\"], str)\n    return var_1[0][\"sha\"]\n",
      "lines_processed": 7,
      "total_lines": 231,
      "llm_code": "def get_main_revision() -> str:\n    response = http_get(\n        f\"https://api.github.com/repos/{REPO}/commits\",\n        fields={\"per_page\": \"1\", \"sha\": \"main\"},\n    )\n    assert isinstance(response[0][\"sha\"], str)\n    return response[0][\"sha\"]\n",
      "llm_variables": [
        "response"
      ]
    },
    {
      "file_path": "diff_shades_gha_helper.py",
      "code": "def set_output(name: str, value: str) -> None:\n    if len(value) < 200:\n        print(f\"[INFO]: setting '{name}' to '{value}'\")\n    else:\n        print(f\"[INFO]: setting '{name}' to [{len(value)} chars]\")\n\n    if \"GITHUB_OUTPUT\" in os.environ:\n        if \"\\n\" in value:\n            # https://docs.github.com/en/actions/using-workflows/workflow-commands-for-github-actions#multiline-strings\n            delimiter = b64encode(os.urandom(16)).decode()\n            value = f\"{delimiter}\\n{value}\\n{delimiter}\"\n            command = f\"{name}<<{value}\"\n        else:\n            command = f\"{name}={value}\"\n        with open(os.environ[\"GITHUB_OUTPUT\"], \"a\") as f:\n            print(command, file=f)\n",
      "variables": [
        "name",
        "value",
        "delimiter",
        "command",
        "f"
      ],
      "anonymized_code": "def set_output(var_1: str, var_2: str) -> None:\n    if len(var_2) < 200:\n        print(var_5\"[INFO]: setting '{var_1}' to '{var_2}'\")\n    else:\n        print(var_5\"[INFO]: setting '{var_1}' to [{len(var_2)} chars]\")\n\n    if \"GITHUB_OUTPUT\" in os.environ:\n        if \"\\n\" in var_2:\n            # https://docs.github.com/en/actions/using-workflows/workflow-commands-for-github-actions#multiline-strings\n            var_3 = b64encode(os.urandom(16)).decode()\n            var_2 = var_5\"{var_3}\\n{var_2}\\n{var_3}\"\n            var_4 = var_5\"{var_1}<<{var_2}\"\n        else:\n            var_4 = var_5\"{var_1}={var_2}\"\n        with open(os.environ[\"GITHUB_OUTPUT\"], \"a\") as var_5:\n            print(var_4, file=var_5)\n",
      "lines_processed": 16,
      "total_lines": 231,
      "llm_code": "def set_output(name: str, value: str) -> None:\n    if len(value) < 200:\n        print(output_file\"[INFO]: setting '{name}' to '{value}'\")\n    else:\n        print(output_file\"[INFO]: setting '{name}' to [{len(value)} chars]\")\n\n    if \"GITHUB_OUTPUT\" in os.environ:\n        if \"\\n\" in value:\n            # https://docs.github.com/en/actions/using-workflows/workflow-commands-for-github-actions#multiline-strings\n            key = b64encode(os.urandom(16)).decode()\n            value = output_file\"{key}\\n{value}\\n{key}\"\n            output_line = output_file\"{name}<<{value}\"\n        else:\n            output_line = output_file\"{name}={value}\"\n        with open(os.environ[\"GITHUB_OUTPUT\"], \"a\") as output_file:\n            print(output_line, file=output_file)\n",
      "llm_variables": [
        "name",
        "value",
        "key",
        "output_line",
        "output_file"
      ]
    },
    {
      "file_path": "diff_shades_gha_helper.py",
      "code": "def comment_details(run_id: str) -> None:\n    data = http_get(f\"https://api.github.com/repos/{REPO}/actions/runs/{run_id}\")\n    if data[\"event\"] != \"pull_request\" or data[\"conclusion\"] == \"cancelled\":\n        set_output(\"needs-comment\", \"false\")\n        return\n\n    set_output(\"needs-comment\", \"true\")\n    jobs = http_get(data[\"jobs_url\"])[\"jobs\"]\n    job = next(j for j in jobs if j[\"name\"] == \"analysis / preview-changes\")\n    diff_step = next(s for s in job[\"steps\"] if s[\"name\"] == DIFF_STEP_NAME)\n    diff_url = job[\"html_url\"] + f\"#step:{diff_step['number']}:1\"\n\n    artifacts = http_get(data[\"artifacts_url\"])[\"artifacts\"]\n    comment_artifact = next(a for a in artifacts if a[\"name\"] == COMMENT_FILE)\n    comment_url = comment_artifact[\"archive_download_url\"]\n    comment_zip = BytesIO(http_get(comment_url, is_json=False))\n    with zipfile.ZipFile(comment_zip) as zfile:\n        with zfile.open(COMMENT_FILE) as rf:\n            comment_data = json.loads(rf.read().decode(\"utf-8\"))\n",
      "variables": [
        "run_id",
        "data",
        "jobs",
        "job",
        "j",
        "diff_step",
        "s",
        "diff_url",
        "artifacts",
        "comment_artifact",
        "a",
        "comment_url",
        "comment_zip",
        "zfile",
        "rf",
        "comment_data"
      ],
      "anonymized_code": "def comment_details(var_1: str) -> None:\n    var_2 = http_get(f\"https://api.github.com/repos/{REPO}/actions/runs/{var_1}\")\n    if var_2[\"event\"] != \"pull_request\" or var_2[\"conclusion\"] == \"cancelled\":\n        set_output(\"needs-comment\", \"false\")\n        return\n\n    set_output(\"needs-comment\", \"true\")\n    var_3 = http_get(var_2[\"jobs_url\"])[\"var_3\"]\n    var_4 = next(var_5 for var_5 in var_3 if var_5[\"name\"] == \"analysis / preview-changes\")\n    var_6 = next(var_7 for var_7 in var_4[\"steps\"] if var_7[\"name\"] == DIFF_STEP_NAME)\n    var_8 = var_4[\"html_url\"] + f\"#step:{var_6['number']}:1\"\n\n    var_9 = http_get(var_2[\"artifacts_url\"])[\"var_9\"]\n    var_10 = next(var_11 for var_11 in var_9 if var_11[\"name\"] == COMMENT_FILE)\n    var_12 = var_10[\"archive_download_url\"]\n    var_13 = BytesIO(http_get(var_12, is_json=False))\n    with zipfile.ZipFile(var_13) as var_14:\n        with var_14.open(COMMENT_FILE) as var_15:\n            var_16 = json.loads(var_15.read().decode(\"utf-8\"))\n",
      "lines_processed": 19,
      "total_lines": 231,
      "llm_code": null,
      "llm_variables": null
    }
  ],
  "django_django": [
    {
      "file_path": "global_settings.py",
      "code": "def gettext_noop(s):\n    return s\n",
      "variables": [
        "s"
      ],
      "anonymized_code": "def gettext_noop(var_1):\n    return var_1\n",
      "lines_processed": 2,
      "total_lines": 669,
      "llm_code": "def gettext_noop(message):\n    return message",
      "llm_variables": [
        "message"
      ]
    }
  ]
}